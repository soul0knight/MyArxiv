<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-27T00:00:00Z">2024-03-27</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RoboDuet: A Framework Affording Mobile-Manipulation and Cross-Embodiment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17367v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17367v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoping Pan, Qingwei Ben, Zhecheng Yuan, Guangqi Jiang, Yandong Ji, Jiangmiao Pang, Houde Liu, Huazhe Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combining the mobility of legged robots with the manipulation skills of arms
has the potential to significantly expand the operational range and enhance the
capabilities of robotic systems in performing various mobile manipulation
tasks. Existing approaches are confined to imprecise six degrees of freedom
(DoF) manipulation and possess a limited arm workspace. In this paper, we
propose a novel framework, RoboDuet, which employs two collaborative policies
to realize locomotion and manipulation simultaneously, achieving whole-body
control through interactions between each other. Surprisingly, going beyond the
large-range pose tracking, we find that the two-policy framework may enable
cross-embodiment deployment such as using different quadrupedal robots or other
arms. Our experiments demonstrate that the policies trained through RoboDuet
can accomplish stable gaits, agile 6D end-effector pose tracking, and zero-shot
exchange of legged robots, and can be deployed in the real world to perform
various mobile manipulation tasks. Our project page with demo videos is at
https://locomanip-duet.github.io .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Symmetry in RL-based Legged Locomotion Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17320v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17320v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi Su, Xiaoyu Huang, Daniel Ordoñez-Apraez, Yunfei Li, Zhongyu Li, Qiayuan Liao, Giulio Turrisi, Massimiliano Pontil, Claudio Semini, Yi Wu, Koushil Sreenath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model-free reinforcement learning is a promising approach for autonomously
solving challenging robotics control problems, but faces exploration difficulty
without information of the robot's kinematics and dynamics morphology. The
under-exploration of multiple modalities with symmetric states leads to
behaviors that are often unnatural and sub-optimal. This issue becomes
particularly pronounced in the context of robotic systems with morphological
symmetries, such as legged robots for which the resulting asymmetric and
aperiodic behaviors compromise performance, robustness, and transferability to
real hardware. To mitigate this challenge, we can leverage symmetry to guide
and improve the exploration in policy learning via equivariance/invariance
constraints. In this paper, we investigate the efficacy of two approaches to
incorporate symmetry: modifying the network architectures to be strictly
equivariant/invariant, and leveraging data augmentation to approximate
equivariant/invariant actor-critics. We implement the methods on challenging
loco-manipulation and bipedal locomotion tasks and compare with an
unconstrained baseline. We find that the strictly equivariant policy
consistently outperforms other methods in sample efficiency and task
performance in simulation. In addition, symmetry-incorporated approaches
exhibit better gait quality, higher robustness and can be deployed zero-shot in
real-world experiments.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-26T00:00:00Z">2024-03-26</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">44</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SLEDGE: Synthesizing Simulation Environments for Driving Agents with
  Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kashyap Chitta, Daniel Dauner, Andreas Geiger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SLEDGE is the first generative simulator for vehicle motion planning trained
on real-world driving logs. Its core component is a learned model that is able
to generate agent bounding boxes and lane graphs. The model's outputs serve as
an initial state for traffic simulation. The unique properties of the entities
to be generated for SLEDGE, such as their connectivity and variable count per
scene, render the naive application of most modern generative models to this
task non-trivial. Therefore, together with a systematic study of existing lane
graph representations, we introduce a novel raster-to-vector autoencoder
(RVAE). It encodes agents and the lane graph into distinct channels in a
rasterized latent map. This facilitates both lane-conditioned agent generation
and combined generation of lanes and agents with a Diffusion Transformer. Using
generated entities in SLEDGE enables greater control over the simulation, e.g.
upsampling turns or increasing traffic density. Further, SLEDGE can support
500m long routes, a capability not found in existing data-driven simulators
like nuPlan. It presents new challenges for planning algorithms, evidenced by
failure rates of over 40% for PDM, the winner of the 2023 nuPlan challenge,
when tested on hard routes and dense traffic generated by our model. Compared
to nuPlan, SLEDGE requires 500$\times$ less storage to set up (<4GB), making it
a more accessible option and helping with democratizing future research in this
field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Agent Clarity-Aware Dynamic Coverage with Gaussian Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17917v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17917v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Devansh R. Agrawal, Dimitra Panagou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents two algorithms for multi-agent dynamic coverage in
spatiotemporal environments, where the coverage algorithms are informed by the
method of data assimilation. In particular, we show that by considering the
information assimilation algorithm, here a Numerical Gaussian Process Kalman
Filter, the influence of measurements taken at one position on the uncertainty
of the estimate at another location can be computed. We use this relationship
to propose new coverage algorithms. Furthermore, we show that the controllers
naturally extend to the multi-agent context, allowing for a distributed-control
central-information paradigm for multi-agent coverage. Finally, we demonstrate
the algorithms through a realistic simulation of a team of UAVs collecting wind
data over a region in Austria.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures, submitted to CDC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CMP: Cooperative Motion Prediction with Multi-Agent Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoyuan Wu, Yuping Wang, Hengbo Ma, Zhaowei Li, Hang Qiu, Jiachen Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The confluence of the advancement of Autonomous Vehicles (AVs) and the
maturity of Vehicle-to-Everything (V2X) communication has enabled the
capability of cooperative connected and automated vehicles (CAVs). Building on
top of cooperative perception, this paper explores the feasibility and
effectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR
signals as input to enhance tracking and prediction capabilities. Unlike
previous work that focuses separately on either cooperative perception or
motion prediction, our framework, to the best of our knowledge, is the first to
address the unified problem where CAVs share information in both perception and
prediction modules. Incorporated into our design is the unique capability to
tolerate realistic V2X bandwidth limitations and transmission delays, while
dealing with bulky perception representations. We also propose a prediction
aggregation module, which unifies the predictions obtained by different CAVs
and generates the final prediction. Through extensive experiments and ablation
studies, we demonstrate the effectiveness of our method in cooperative
perception, tracking, and motion prediction tasks. In particular, CMP reduces
the average prediction error by 17.2\% with fewer missing detections compared
with the no cooperation setting. Our work marks a significant step forward in
the cooperative capabilities of CAVs, showcasing enhanced performance in
complex scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi Agent Pathfinding for Noise Restricted Hybrid Fuel Unmanned Aerial
  Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17849v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17849v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Drew Scott, Satyanarayana G. Manyam, David W. Casbeer, Manish Kumar, Isaac E. Weintraub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi Agent Path Finding (MAPF) seeks the optimal set of paths for multiple
agents from respective start to goal locations such that no paths conflict. We
address the MAPF problem for a fleet of hybrid-fuel unmanned aerial vehicles
which are subject to location-dependent noise restrictions. We solve this
problem by searching a constraint tree for which the subproblem at each node is
a set of shortest path problems subject to the noise and fuel constraints and
conflict zone avoidance. A labeling algorithm is presented to solve this
subproblem, including the conflict zones which are treated as dynamic
obstacles. We present the experimental results of the algorithms for various
graph sizes and number of agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrhman Werby, Chenguang Huang, Martin Büchner, Abhinav Valada, Wolfram Burgard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent open-vocabulary robot mapping methods enrich dense geometric maps with
pre-trained visual-language features. While these maps allow for the prediction
of point-wise saliency maps when queried for a certain language concept,
large-scale environments and abstract queries beyond the object level still
pose a considerable hurdle, ultimately limiting language-grounded robotic
navigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D
scene graph mapping approach for language-grounded robot navigation. Leveraging
open-vocabulary vision foundation models, we first obtain state-of-the-art
open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene
graph hierarchy consisting of floor, room, and object concepts, each enriched
with open-vocabulary features. Our approach is able to represent multi-story
buildings and allows robotic traversal of those using a cross-floor Voronoi
graph. HOV-SG is evaluated on three distinct datasets and surpasses previous
baselines in open-vocabulary semantic accuracy on the object, room, and floor
level while producing a 75% reduction in representation size compared to dense
open-vocabulary maps. In order to prove the efficacy and generalization
capabilities of HOV-SG, we showcase successful long-horizon
language-conditioned robot navigation within real-world multi-storage
environments. We provide code and trial video data at http://hovsg.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and video are available at http://hovsg.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scenario-Based Curriculum Generation for Multi-Agent Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Axel Brunnbauer, Luigi Berducci, Peter Priller, Dejan Nickovic, Radu Grosu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The automated generation of diverse and complex training scenarios has been
an important ingredient in many complex learning tasks. Especially in
real-world application domains, such as autonomous driving, auto-curriculum
generation is considered vital for obtaining robust and general policies.
However, crafting traffic scenarios with multiple, heterogeneous agents is
typically considered as a tedious and time-consuming task, especially in more
complex simulation environments. In our work, we introduce MATS-Gym, a
Multi-Agent Traffic Scenario framework to train agents in CARLA, a
high-fidelity driving simulator. MATS-Gym is a multi-agent training framework
for autonomous driving that uses partial scenario specifications to generate
traffic scenarios with variable numbers of agents. This paper unifies various
existing approaches to traffic scenario description into a single training
framework and demonstrates how it can be integrated with techniques from
unsupervised environment design to automate the generation of adaptive
auto-curricula. The code is available at
https://github.com/AutonomousDrivingExaminer/mats-gym.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 Pages, Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ System Calibration of a Field Phenotyping Robot with Multiple
  High-Precision Profile Laser Scanners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Esser, Gereon Tombrink, Andre Cornelißen, Lasse Klingbeil, Heiner Kuhlmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The creation of precise and high-resolution crop point clouds in agricultural
fields has become a key challenge for high-throughput phenotyping applications.
This work implements a novel calibration method to calibrate the laser scanning
system of an agricultural field robot consisting of two industrial-grade laser
scanners used for high-precise 3D crop point cloud creation. The calibration
method optimizes the transformation between the scanner origins and the robot
pose by minimizing 3D point omnivariances within the point cloud. Moreover, we
present a novel factor graph-based pose estimation method that fuses total
station prism measurements with IMU and GNSS heading information for
high-precise pose determination during calibration. The root-mean-square error
of the distances to a georeferenced ground truth point cloud results in 0.8 cm
after parameter optimization. Furthermore, our results show the importance of a
reference point cloud in the calibration method needed to estimate the vertical
translation of the calibration. Challenges arise due to non-static parameters
while the robot moves, indicated by systematic deviations to a ground truth
terrestrial laser scan.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optical Flow Based Detection and Tracking of Moving Objects for
  Autonomous Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        MReza Alipour Sormoli, Mehrdad Dianati, Sajjad Mozaffari, Roger woodman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate velocity estimation of surrounding moving objects and their
trajectories are critical elements of perception systems in
Automated/Autonomous Vehicles (AVs) with a direct impact on their safety. These
are non-trivial problems due to the diverse types and sizes of such objects and
their dynamic and random behaviour. Recent point cloud based solutions often
use Iterative Closest Point (ICP) techniques, which are known to have certain
limitations. For example, their computational costs are high due to their
iterative nature, and their estimation error often deteriorates as the relative
velocities of the target objects increase (>2 m/sec). Motivated by such
shortcomings, this paper first proposes a novel Detection and Tracking of
Moving Objects (DATMO) for AVs based on an optical flow technique, which is
proven to be computationally efficient and highly accurate for such problems.
\textcolor{black}{This is achieved by representing the driving scenario as a
vector field and applying vector calculus theories to ensure spatiotemporal
continuity.} We also report the results of a comprehensive performance
evaluation of the proposed DATMO technique, carried out in this study using
synthetic and real-world data. The results of this study demonstrate the
superiority of the proposed technique, compared to the DATMO techniques in the
literature, in terms of estimation accuracy and processing time in a wide range
of relative velocities of moving objects. Finally, we evaluate and discuss the
sensitivity of the estimation error of the proposed DATMO technique to various
system and environmental parameters, as well as the relative velocities of the
moving objects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript has been accepted as a regular paper in Transactions
  on Intelligent Transportation Systems (DOI: 10.1109/TITS.2024.3382495)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LiDAR-Based Crop Row Detection Algorithm for Over-Canopy Autonomous
  Navigation in Agriculture Fields <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17774v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17774v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiji Liu, Francisco Yandun, George Kantor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous navigation is crucial for various robotics applications in
agriculture. However, many existing methods depend on RTK-GPS systems, which
are expensive and susceptible to poor signal coverage. This paper introduces a
state-of-the-art LiDAR-based navigation system that can achieve over-canopy
autonomous navigation in row-crop fields, even when the canopy fully blocks the
interrow spacing. Our crop row detection algorithm can detect crop rows across
diverse scenarios, encompassing various crop types, growth stages, weed
presence, and discontinuities within the crop rows. Without utilizing the
global localization of the robot, our navigation system can perform autonomous
navigation in these challenging scenarios, detect the end of the crop rows, and
navigate to the next crop row autonomously, providing a crop-agnostic approach
to navigate the whole row-crop field. This navigation system has undergone
tests in various simulated agricultural fields, achieving an average of
$2.98cm$ autonomous driving accuracy without human intervention on the custom
Amiga robot. In addition, the qualitative results of our crop row detection
algorithm from the actual soybean fields validate our LiDAR-based crop row
detection algorithm's potential for practical agricultural applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 9 figures, submitted to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Goal-Directed Object Pushing in Cluttered Scenes with
  Location-Based Attention <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17667v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17667v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nils Dengler, Juan Del Aguila Ferrandis, João Moura, Sethu Vijayakumar, Maren Bennewitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-prehensile planar pushing is a challenging task due to its underactuated
nature with hybrid-dynamics, where a robot needs to reason about an object's
long-term behaviour and contact-switching, while being robust to contact
uncertainty. The presence of clutter in the environment further complicates
this task, introducing the need to include more sophisticated spatial analysis
to avoid collisions. Building upon prior work on reinforcement learning (RL)
with multimodal categorical exploration for planar pushing, in this paper we
incorporate location-based attention to enable robust navigation through
clutter. Unlike previous RL literature addressing this obstacle avoidance
pushing task, our framework requires no predefined global paths and considers
the target orientation of the manipulated object. Our results demonstrate that
the learned policies successfully navigate through a wide range of complex
obstacle configurations, including dynamic obstacles, with smooth motions,
achieving the desired target object pose. We also validate the transferability
of the learned policies to robotic hardware using the KUKA iiwa robot arm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object
  Detection with Sparse LiDAR and Large Domain Gaps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maciej K Wozniak, Mattias Hansson, Marko Thiel, Patric Jensfelt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we address a gap in existing unsupervised domain adaptation
approaches on LiDAR-based 3D object detection, which have predominantly
concentrated on adapting between established, high-density autonomous driving
datasets. We focus on sparser point clouds, capturing scenarios from different
perspectives: not just from vehicles on the road but also from mobile robots on
sidewalks, which encounter significantly different environmental conditions and
sensor configurations. We introduce Unsupervised Adversarial Domain Adaptation
for 3D Object Detection (UADA3D). UADA3D does not depend on pre-trained source
models or teacher-student architectures. Instead, it uses an adversarial
approach to directly learn domain-invariant features. We demonstrate its
efficacy in various adaptation scenarios, showing significant improvements in
both self-driving car and mobile robot domains. Our code is open-source and
will be available soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online Tree Reconstruction and Forest Inventory on a Mobile Robotic
  System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17622v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17622v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonard Freißmuth, Matias Mattamala, Nived Chebrolu, Simon Schaefer, Stefan Leutenegger, Maurice Fallon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Terrestrial laser scanning (TLS) is the standard technique used to create
accurate point clouds for digital forest inventories. However, the measurement
process is demanding, requiring up to two days per hectare for data collection,
significant data storage, as well as resource-heavy post-processing of 3D data.
In this work, we present a real-time mapping and analysis system that enables
online generation of forest inventories using mobile laser scanners that can be
mounted e.g. on mobile robots. Given incrementally created and locally accurate
submaps-data payloads-our approach extracts tree candidates using a custom,
Voronoi-inspired clustering algorithm. Tree candidates are reconstructed using
an adapted Hough algorithm, which enables robust modeling of the tree stem.
Further, we explicitly incorporate the incremental nature of the data
collection by consistently updating the database using a pose graph LiDAR SLAM
system. This enables us to refine our estimates of the tree traits if an area
is revisited later during a mission. We demonstrate competitive accuracy to TLS
or manual measurements using laser scanners that we mounted on backpacks or
mobile robots operating in conifer, broad-leaf and mixed forests. Our results
achieve RMSE of 1.93 cm, a bias of 0.65 cm and a standard deviation of 1.81 cm
(averaged across these sequences)-with no post-processing required after the
mission is complete.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interactive Identification of Granular Materials using Force
  Measurements <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuli Hynninen, Tran Nguyen Le, Ville Kyrki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to identify granular materials facilitates the emergence of
various new applications in robotics, ranging from cooking at home to truck
loading at mining sites. However, granular material identification remains a
challenging and underexplored area. In this work, we present a novel
interactive material identification framework that enables robots to identify a
wide range of granular materials using only a force-torque sensor for
perception. Our framework, comprising interactive exploration, feature
extraction, and classification stages, prioritizes simplicity and transparency
for seamless integration into various manipulation pipelines. We evaluate the
proposed approach through extensive experiments with a real-world dataset
comprising 11 granular materials, which we also make publicly available.
Additionally, we conducted a comprehensive qualitative analysis of the dataset
to offer deeper insights into its nature, aiding future development. Our
results show that the proposed method is capable of accurately identifying a
wide range of granular materials solely relying on force measurements obtained
from direct interaction with the materials. Code and dataset are available at:
https://irobotics.aalto.fi/indentify_granular/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to 2024 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aerial Robots Carrying Flexible Cables: Dynamic Shape Optimal Control
  via Spectral Method Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17565v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17565v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaolei Shen, Chiara Gabellieri, Antonio Franchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present a model-based optimal boundary control design for an
aerial robotic system composed of a quadrotor carrying a flexible cable. The
whole system is modeled by partial differential equations (PDEs) combined with
boundary conditions described by ordinary differential equations (ODEs). The
proper orthogonal decomposition (POD) method is adopted to project the original
infinite-dimensional system on a subspace spanned by orthogonal basis
functions. Based on the reduced order model, nonlinear model predictive control
(NMPC) is implemented online to realize shape trajectory tracking of the
flexible cable in an optimal predictive fashion. The proposed reduced modeling
and optimal control paradigms are numerically verified against an accurate
high-dimensional FDM-based model in different scenarios and the controller's
superior performance is shown compared to an optimally tuned PID controller.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Time-Optimal Flight with Safety Constraints and Data-driven Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Krinner, Angel Romero, Leonard Bauersfeld, Melanie Zeilinger, Andrea Carron, Davide Scaramuzza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time-optimal quadrotor flight is an extremely challenging problem due to the
limited control authority encountered at the limit of handling. Model
Predictive Contouring Control (MPCC) has emerged as a leading model-based
approach for time optimization problems such as drone racing. However, the
standard MPCC formulation used in quadrotor racing introduces the notion of the
gates directly in the cost function, creating a multi-objective optimization
that continuously trades off between maximizing progress and tracking the path
accurately. This paper introduces three key components that enhance the MPCC
approach for drone racing. First and foremost, we provide safety guarantees in
the form of a constraint and terminal set. The safety set is designed as a
spatial constraint which prevents gate collisions while allowing for
time-optimization only in the cost function. Second, we augment the existing
first principles dynamics with a residual term that captures complex
aerodynamic effects and thrust forces learned directly from real world data.
Third, we use Trust Region Bayesian Optimization (TuRBO), a state of the art
global Bayesian Optimization algorithm, to tune the hyperparameters of the MPC
controller given a sparse reward based on lap time minimization. The proposed
approach achieves similar lap times to the best state-of-the-art RL and
outperforms the best time-optimal controller while satisfying constraints. In
both simulation and real-world, our approach consistently prevents gate crashes
with 100\% success rate, while pushing the quadrotor to its physical limit
reaching speeds of more than 80km/h.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepMIF: Deep Monotonic Implicit Fields for Large-Scale LiDAR 3D Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kutay Yılmaz, Matthias Nießner, Anastasiia Kornilova, Alexey Artemov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, significant progress has been achieved in sensing real large-scale
outdoor 3D environments, particularly by using modern acquisition equipment
such as LiDAR sensors. Unfortunately, they are fundamentally limited in their
ability to produce dense, complete 3D scenes. To address this issue, recent
learning-based methods integrate neural implicit representations and
optimizable feature grids to approximate surfaces of 3D scenes. However,
naively fitting samples along raw LiDAR rays leads to noisy 3D mapping results
due to the nature of sparse, conflicting LiDAR measurements. Instead, in this
work we depart from fitting LiDAR data exactly, instead letting the network
optimize a non-metric monotonic implicit field defined in 3D space. To fit our
field, we design a learning system integrating a monotonicity loss that enables
optimizing neural monotonic fields and leverages recent progress in large-scale
3D mapping. Our algorithm achieves high-quality dense 3D mapping performance as
captured by multiple quantitative and perceptual measures and visual results
obtained for Mai City, Newer College, and KITTI benchmarks. The code of our
approach will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Design and Preliminary Evaluation of a Torso Stabiliser for Individuals
  with Spinal Cord Injury 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17531v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17531v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rejin John Varghese, Man-Yan Tong, Isabella Szczech, Peter Bryan, Dario Farina, Etienne Burdet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spinal cord injuries (SCIs) generally result in sensory and mobility
impairments, with torso instability being particularly debilitating. Existing
torso stabilisers are often rigid and restrictive. This paper presents an early
investigation into a non-restrictive 1 degree-of-freedom (DoF) mechanical torso
stabiliser inspired by devices such as centrifugal clutches and seat-belt
mechanisms. Firstly, the paper presents a motion-capture (MoCap) and
OpenSim-based kinematic analysis of the cable-based system to understand
requisite device characteristics. The simulated evaluation resulted in the
cable-based device to require 55-60cm of unrestricted travel, and to lock at a
threshold cable velocity of 80-100cm/sec. Next, the developed 1-DoF device is
introduced. The proposed mechanical device is transparent during activities of
daily living, and transitions to compliant blocking when incipient fall is
detected. Prototype behaviour was then validated using a MoCap-based kinematic
analysis to verify non-restrictive movement, reliable transition to blocking,
and compliance of the blocking.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 4 figures, 10 references. Submitted to IEEE EMBC 2024
  conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High-Power, Flexible, Robust Hand: Development of Musculoskeletal Hand
  Using Machined Springs and Realization of Self-Weight Supporting Motion with
  Humanoid <span class="chip">IROS2017</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17459v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17459v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shogo Makino, Kento Kawaharazuka, Masaya Kawamura, Yuki Asano, Kei Okada, Masayuki Inaba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human can not only support their body during standing or walking, but also
support them by hand, so that they can dangle a bar and others. But most
humanoid robots support their body only in the foot and they use their hand
just to manipulate objects because their hands are too weak to support their
body. Strong hands are supposed to enable humanoid robots to act in much
broader scene. Therefore, we developed new life-size five-fingered hand that
can support the body of life-size humanoid robot. It is tendon-driven and
underactuated hand and actuators in forearms produce large gripping force. This
hand has flexible joints using machined springs, which can be designed
integrally with the attachment. Thus, it has both structural strength and
impact resistance in spite of small size. As other characteristics, this hand
has force sensors to measure external force and the fingers can be flexed along
objects though the number of actuators to flex fingers is less than that of
fingers. We installed the developed hand on musculoskeletal humanoid "Kengoro"
and achieved two self-weight supporting motions: push-up motion and dangling
motion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at IROS2017</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Five-fingered Hand with Wide Range of Thumb Using Combination of
  Machined Springs and Variable Stiffness Joints <span class="chip">IROS2018</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17452v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17452v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shogo Makino, Kento Kawaharazuka, Ayaka Fujii, Masaya Kawamura, Tasuku Makabe, Moritaka Onitsuka, Yuki Asano, Kei Okada, Koji Kawasaki, Masayuki Inaba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human hands can not only grasp objects of various shape and size and
manipulate them in hands but also exert such a large gripping force that they
can support the body in the situations such as dangling a bar and climbing a
ladder. On the other hand, it is difficult for most robot hands to manage both.
Therefore in this paper we developed the hand which can grasp various objects
and exert large gripping force. To develop such hand, we focused on the thumb
CM joint with wide range of motion and the MP joints of four fingers with the
DOF of abduction and adduction. Based on the hand with large gripping force and
flexibility using machined spring, we applied above mentioned joint mechanism
to the hand. The thumb CM joint has wide range of motion because of the
combination of three machined springs and MP joints of four fingers have
variable rigidity mechanism instead of driving each joint independently in
order to move joint in limited space and by limited actuators. Using the
developed hand, we achieved the grasping of various objects, supporting a large
load and several motions with an arm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at IROS2018</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Line-Of-Sight guidance law based on vector fields path
  following for underactuated unmanned surface vehicle 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Qi, Ronghua Wang, Nailong Wu, Yuxin Fan, Jigang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The focus of this paper is to develop a methodology that enables an unmanned
surface vehicle (USV) to efficiently track a planned path. The introduction of
a vector field-based adaptive line-of-sight guidance law (VFALOS) for accurate
trajectory tracking and minimizing the overshoot response time during USV
tracking of curved paths improves the overall line-of-sight (LOS) guidance
method. These improvements contribute to faster convergence to the desired
path, reduce oscillations, and can mitigate the effects of persistent external
disturbances. It is shown that the proposed guidance law exhibits k-exponential
stability when converging to the desired path consisting of straight and curved
lines. The results in the paper show that the proposed method effectively
improves the accuracy of the USV tracking the desired path while ensuring the
safety of the USV work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive LiDAR-Radar Fusion for Outdoor Odometry Across Dense Smoke
  Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17441v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17441v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chiyun Noh, Ayoung Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust odometry estimation in perceptually degraded environments represents a
key challenge in the field of robotics. In this paper, we propose a LiDAR-radar
fusion method for robust odometry for adverse environment with LiDAR
degeneracy. By comparing the LiDAR point cloud with the radar static point
cloud obtained through preprocessing module, it is possible to identify
instances of LiDAR degeneracy to overcome perceptual limits. We demonstrate the
effectiveness of our method in challenging conditions such as dense smoke,
showcasing its ability to reliably estimate odometry and identify/remove
dynamic points prone to LiDAR degeneracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cyclic pursuit formation control for arbitrary desired shapes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Fujioka, Masaki Ogura, Naoki Wakamiya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A multi-agent system comprises numerous agents that autonomously make
decisions to collectively accomplish tasks, drawing significant attention for
their wide-ranging applications. Within this context, formation control emerges
as a prominent task, wherein agents collaboratively shape and maneuver while
preserving formation integrity. Our focus centers on cyclic pursuit, a method
facilitating the formation of circles, ellipses, and figure-eights under the
assumption that agents can only perceive the relative positions of those
preceding them. However, this method's scope has been restricted to these
specific shapes, leaving the feasibility of forming other shapes uncertain. In
response, our study proposes a novel method based on cyclic pursuit capable of
forming a broader array of shapes, enabling agents to individually shape while
pursuing preceding agents, thereby extending the repertoire of achievable
formations. We present two scenarios concerning the information available to
agents and devise formation control methods tailored to each scenario. Through
extensive simulations, we demonstrate the efficacy of our proposed method in
forming multiple shapes, including those represented as Fourier series, thereby
underscoring the versatility and effectiveness of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Natural-artificial hybrid swarm: Cyborg-insect group navigation in
  unknown obstructed soft terrain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Bai, Phuoc Thanh Tran Ngoc, Huu Duoc Nguyen, Duc Long Le, Quang Huy Ha, Kazuki Kai, Yu Xiang See To, Yaosheng Deng, Jie Song, Naoki Wakamiya, Hirotaka Sato, Masaki Ogura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Navigating multi-robot systems in complex terrains has always been a
challenging task. This is due to the inherent limitations of traditional robots
in collision avoidance, adaptation to unknown environments, and sustained
energy efficiency. In order to overcome these limitations, this research
proposes a solution by integrating living insects with miniature electronic
controllers to enable robotic-like programmable control, and proposing a novel
control algorithm for swarming. Although these creatures, called cyborg
insects, have the ability to instinctively avoid collisions with neighbors and
obstacles while adapting to complex terrains, there is a lack of literature on
the control of multi-cyborg systems. This research gap is due to the difficulty
in coordinating the movements of a cyborg system under the presence of insects'
inherent individual variability in their reactions to control input. In
response to this issue, we propose a novel swarm navigation algorithm
addressing these challenges. The effectiveness of the algorithm is demonstrated
through an experimental validation in which a cyborg swarm was successfully
navigated through an unknown sandy field with obstacles and hills. This
research contributes to the domain of swarm robotics and showcases the
potential of integrating biological organisms with robotics and control theory
to create more intelligent autonomous systems with real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Objective Trajectory Planning with Dual-Encoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17353v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17353v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beibei Zhang, Tian Xiang, Chentao Mao, Yuhua Zheng, Shuai Li, Haoyi Niu, Xiangming Xi, Wenyuan Bai, Feng Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time-jerk optimal trajectory planning is crucial in advancing robotic arms'
performance in dynamic tasks. Traditional methods rely on solving complex
nonlinear programming problems, bringing significant delays in generating
optimized trajectories. In this paper, we propose a two-stage approach to
accelerate time-jerk optimal trajectory planning. Firstly, we introduce a
dual-encoder based transformer model to establish a good preliminary
trajectory. This trajectory is subsequently refined through sequential
quadratic programming to improve its optimality and robustness. Our approach
outperforms the state-of-the-art by up to 79.72\% in reducing trajectory
planning time. Compared with existing methods, our method shrinks the
optimality gap with the objective function value decreasing by up to 29.9\%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 7 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unified Path and Gait Planning for Safe Bipedal Robot Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyang Peng, Victor Paredes, Ayonga Hereid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safe path and gait planning are essential for bipedal robots to navigate
complex real-world environments. The prevailing approaches often plan the path
and gait separately in a hierarchical fashion, potentially resulting in unsafe
movements due to neglecting the physical constraints of walking robots. A
safety-critical path must not only avoid obstacles but also ensure that the
robot's gaits are subject to its dynamic and kinematic constraints. This work
presents a novel approach that unifies path planning and gait planning via a
Model Predictive Control (MPC) using the Linear Inverted Pendulum (LIP) model
representing bipedal locomotion. This approach considers environmental
constraints, such as obstacles, and the robot's kinematics and dynamics
constraints. By using discrete-time Control Barrier Functions for obstacle
avoidance, our approach generates the next foot landing position, ensuring
robust walking gaits and a safe navigation path within clustered environments.
We validated our proposed approach in simulation using a Digit robot in 20
randomly created environments. The results demonstrate improved performance in
terms of safety and robustness when compared to hierarchical path and gait
planning frameworks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse-Graph-Enabled Formation Planning for Large-Scale Aerial Swarms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17288v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17288v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Zhou, Lun Quan, Chao Xu, Guangtong Xu, Fei Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The formation trajectory planning using complete graphs to model
collaborative constraints becomes computationally intractable as the number of
drones increases due to the curse of dimensionality. To tackle this issue, this
paper presents a sparse graph construction method for formation planning to
realize better efficiency-performance trade-off. Firstly, a sparsification
mechanism for complete graphs is designed to ensure the global rigidity of
sparsified graphs, which is a necessary condition for uniquely corresponding to
a geometric shape. Secondly, a good sparse graph is constructed to preserve the
main structural feature of complete graphs sufficiently. Since the graph-based
formation constraint is described by Laplacian matrix, the sparse graph
construction problem is equivalent to submatrix selection, which has
combinatorial time complexity and needs a scoring metric. Via comparative
simulations, the Max-Trace matrix-revealing metric shows the promising
performance. The sparse graph is integrated into the formation planning.
Simulation results with 72 drones in complex environments demonstrate that when
preserving 30\% connection edges, our method has comparative formation error
and recovery performance w.r.t. complete graphs. Meanwhile, the planning
efficiency is improved by approximate an order of magnitude. Benchmark
comparisons and ablation studies are conducted to fully validate the merits of
our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Massive Interaction with Generalist Robotics: A Systematic
  <span class="highlight-title">Review</span> of XR-enabled Remote Human-Robot Interaction Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11384v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11384v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xian Wang, Luyao Shen, Lik-Hang Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rising interest of generalist robots seek to create robots with
versatility to handle multiple tasks in a variety of environments, and human
will interact with such robots through immersive interfaces. In the context of
human-robot interaction (HRI), this survey provides an exhaustive review of the
applications of extended reality (XR) technologies in the field of remote HRI.
We developed a systematic search strategy based on the PRISMA methodology. From
the initial 2,561 articles selected, 100 research papers that met our inclusion
criteria were included. We categorized and summarized the domain in detail,
delving into XR technologies, including augmented reality (AR), virtual reality
(VR), and mixed reality (MR), and their applications in facilitating intuitive
and effective remote control and interaction with robotic systems. The survey
highlights existing articles on the application of XR technologies, user
experience enhancement, and various interaction designs for XR in remote HRI,
providing insights into current trends and future directions. We also
identified potential gaps and opportunities for future research to improve
remote HRI systems through XR technology to guide and inform future XR and
robotics research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guessing human intentions to avoid dangerous situations in caregiving
  robots <span class="chip">IROS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16291v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16291v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noé Zapata, Gerardo Pérez, Lucas Bonilla, Pedro Núñez, Pilar Bachiller, Pablo Bustos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For robots to interact socially, they must interpret human intentions and
anticipate their potential outcomes accurately. This is particularly important
for social robots designed for human care, which may face potentially dangerous
situations for people, such as unseen obstacles in their way, that should be
avoided. This paper explores the Artificial Theory of Mind (ATM) approach to
inferring and interpreting human intentions. We propose an algorithm that
detects risky situations for humans, selecting a robot action that removes the
danger in real time. We use the simulation-based approach to ATM and adopt the
'like-me' policy to assign intentions and actions to people. Using this
strategy, the robot can detect and act with a high rate of success under
time-constrained situations. The algorithm has been implemented as part of an
existing robotics cognitive architecture and tested in simulation scenarios.
Three experiments have been conducted to test the implementation's robustness,
precision and real-time response, including a simulated scenario, a
human-in-the-loop hybrid configuration and a real-world scenario.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures. Submitted to IROS2024. For associated mpeg file
  see https://youtu.be/87UEB8P97KY</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safe Explicable Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.03773v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.03773v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akkamahadevi Hanni, Andrew Boateng, Yu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human expectations arise from their understanding of others and the world. In
the context of human-AI interaction, this understanding may not align with
reality, leading to the AI agent failing to meet expectations and compromising
team performance. Explicable planning, introduced as a method to bridge this
gap, aims to reconcile human expectations with the agent's optimal behavior,
facilitating interpretable decision-making. However, an unresolved critical
issue is ensuring safety in explicable planning, as it could result in
explicable behaviors that are unsafe. To address this, we propose Safe
Explicable Planning (SEP), which extends the prior work to support the
specification of a safety bound. The goal of SEP is to find behaviors that
align with human expectations while adhering to the specified safety criterion.
Our approach generalizes the consideration of multiple objectives stemming from
multiple models rather than a single model, yielding a Pareto set of safe
explicable policies. We present both an exact method, guaranteeing finding the
Pareto set, and a more efficient greedy method that finds one of the policies
in the Pareto set. Additionally, we offer approximate solutions based on state
aggregation to improve scalability. We provide formal proofs that validate the
desired theoretical properties of these methods. Evaluation through simulations
and physical robot experiments confirms the effectiveness of our approach for
safe explicable planning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Resilient source seeking with robot swarms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.02937v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.02937v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Acuaviva, Jesus Bautista, Weijia Yao, Juan Jimenez, Hector Garcia de Marina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a solution for locating the source, or maximum, of an unknown
scalar field using a swarm of mobile robots. Unlike relying on the traditional
gradient information, the swarm determines an ascending direction to approach
the source with arbitrary precision. The ascending direction is calculated from
measurements of the field strength at the robot locations and their relative
positions concerning the centroid. Rather than focusing on individual robots,
we focus the analysis on the density of robots per unit area to guarantee a
more resilient swarm, i.e., the functionality remains even if individuals go
missing or are misplaced during the mission. We reinforce the robustness of the
algorithm by providing sufficient conditions for the swarm shape so that the
ascending direction is almost parallel to the gradient. The swarm can respond
to an unexpected environment by morphing its shape and exploiting the existence
of multiple ascending directions. Finally, we validate our approach numerically
with hundreds of robots. The fact that a large number of robots always
calculate an ascending direction compensates for the loss of individuals and
mitigates issues arising from the actuator and sensor noises.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, submitted to CDC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tuning-free Quasi-stiffness Control Framework of a Powered Transfemoral
  Prosthesis for Task-adaptive Walking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15030v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15030v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teng Ma, Shucong Yin, Zhimin Hou, Binxin Huang, Haoyong Yu, Chenglong Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Impedance-based control represents a prevalent strategy in the development of
powered transfemoral prostheses. However, creating a task-adaptive, tuning-free
controller that effectively generalizes across diverse locomotion modes and
terrain conditions continues to be a significant challenge. This letter
proposes a tuning-free and task-adaptive quasi-stiffness control framework for
powered prostheses that generalizes across various walking tasks, including the
torque-angle relationship reconstruction part and the quasi-stiffness
controller design part. A Gaussian Process Regression (GPR) model is introduced
to predict the target features of the human joint angle and torque in a new
task. Subsequently, a Kernelized Movement Primitives (KMP) is employed to
reconstruct the torque-angle relationship of the new task from multiple human
reference trajectories and estimated target features. Based on the torque-angle
relationship of the new task, a quasi-stiffness control approach is designed
for a powered prosthesis. Finally, the proposed framework is validated through
practical examples, including varying speeds and inclines walking tasks.
Notably, the proposed framework not only aligns with but frequently surpasses
the performance of a benchmark finite state machine impedance controller
(FSMIC) without necessitating manual impedance tuning and has the potential to
expand to variable walking tasks in daily life for the transfemoral amputees.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 10 figures. This work has been submitted to the IEEE-RAL for
  possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain Randomization via Entropy Maximization <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01885v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01885v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriele Tiboni, Pascal Klink, Jan Peters, Tatiana Tommasi, Carlo D'Eramo, Georgia Chalvatzaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Varying dynamics parameters in simulation is a popular Domain Randomization
(DR) approach for overcoming the reality gap in Reinforcement Learning (RL).
Nevertheless, DR heavily hinges on the choice of the sampling distribution of
the dynamics parameters, since high variability is crucial to regularize the
agent's behavior but notoriously leads to overly conservative policies when
randomizing excessively. In this paper, we propose a novel approach to address
sim-to-real transfer, which automatically shapes dynamics distributions during
training in simulation without requiring real-world data. We introduce DOmain
RAndomization via Entropy MaximizatiON (DORAEMON), a constrained optimization
problem that directly maximizes the entropy of the training distribution while
retaining generalization capabilities. In achieving this, DORAEMON gradually
increases the diversity of sampled dynamics parameters as long as the
probability of success of the current policy is sufficiently high. We
empirically validate the consistent benefits of DORAEMON in obtaining highly
adaptive and generalizable policies, i.e. solving the task at hand across the
widest range of dynamics parameters, as opposed to representative baselines
from the DR literature. Notably, we also demonstrate the Sim2Real applicability
of DORAEMON through its successful zero-shot transfer in a robotic manipulation
setup under unknown real-world parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2024. Project website at
  https://gabrieletiboni.github.io/doraemon/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03246v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03246v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingrui Li, Shuhong Liu, Heng Zhou, Guohao Zhu, Na Cheng, Tianchen Deng, Hongyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present SGS-SLAM, the first semantic visual SLAM system based on Gaussian
Splatting. It incorporates appearance, geometry, and semantic features through
multi-channel optimization, addressing the oversmoothing limitations of neural
implicit SLAM systems in high-quality rendering, scene understanding, and
object-level geometry. We introduce a unique semantic feature loss that
effectively compensates for the shortcomings of traditional depth and color
losses in object optimization. Through a semantic-guided keyframe selection
strategy, we prevent erroneous reconstructions caused by cumulative errors.
Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-art
performance in camera pose estimation, map reconstruction, precise semantic
segmentation, and object-level geometric accuracy, while ensuring real-time
rendering capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Source-free Domain Adaptive Semantic Segmentation via
  Importance-aware and Prototype-contrast Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01598v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01598v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Cao, Hui Zhang, Xiao Lu, Zheng Xiao, Kailun Yang, Yaonan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain adaptive semantic segmentation enables robust pixel-wise understanding
in real-world driving scenes. Source-free domain adaptation, as a more
practical technique, addresses the concerns of data privacy and storage
limitations in typical unsupervised domain adaptation methods, making it
especially relevant in the context of intelligent vehicles. It utilizes a
well-trained source model and unlabeled target data to achieve adaptation in
the target domain. However, in the absence of source data and target labels,
current solutions cannot sufficiently reduce the impact of domain shift and
fully leverage the information from the target data. In this paper, we propose
an end-to-end source-free domain adaptation semantic segmentation method via
Importance-Aware and Prototype-Contrast (IAPC) learning. The proposed IAPC
framework effectively extracts domain-invariant knowledge from the well-trained
source model and learns domain-specific knowledge from the unlabeled target
domain. Specifically, considering the problem of domain shift in the prediction
of the target domain by the source model, we put forward an importance-aware
mechanism for the biased target prediction probability distribution to extract
domain-invariant knowledge from the source model. We further introduce a
prototype-contrast strategy, which includes a prototype-symmetric cross-entropy
loss and a prototype-enhanced cross-entropy loss, to learn target intra-domain
knowledge without relying on labels. A comprehensive variety of experiments on
two domain adaptive semantic segmentation benchmarks demonstrates that the
proposed end-to-end IAPC solution outperforms existing state-of-the-art
methods. The source code is publicly available at
https://github.com/yihong-97/Source-free-IAPC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions on Intelligent Vehicles (T-IV). The
  source code is publicly available at
  https://github.com/yihong-97/Source-free-IAPC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Robotics Meets Wireless Communications: An Introductory Tutorial 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.02021v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.02021v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Bonilla Licea, Mounir Ghogho, Martin Saska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The importance of ground Mobile Robots (MRs) and Unmanned Aerial Vehicles
(UAVs) within the research community, industry, and society is growing fast.
Many of these agents are nowadays equipped with communication systems that are,
in some cases, essential to successfully achieve certain tasks. In this
context, we have begun to witness the development of a new interdisciplinary
research field at the intersection of robotics and communications. This
research field has been boosted by the intention of integrating UAVs within the
5G and 6G communication networks. This research will undoubtedly lead to many
important applications in the near future. Nevertheless, one of the main
obstacles to the development of this research area is that most researchers
address these problems by oversimplifying either the robotics or the
communications aspect. This impedes the ability of reaching the full potential
of this new interdisciplinary research area. In this tutorial, we present some
of the modelling tools necessary to address problems involving both robotics
and communication from an interdisciplinary perspective. As an illustrative
example of such problems, we focus in this tutorial on the issue of
communication-aware trajectory planning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 192 references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Motion Generation from Fine-grained Textual Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13518v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13518v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunhang Li, Yansong Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of text2motion is to generate human motion sequences from given
textual descriptions, where the model explores diverse mappings from natural
language instructions to human body movements. While most existing works are
confined to coarse-grained motion descriptions, e.g., "A man squats.",
fine-grained descriptions specifying movements of relevant body parts are
barely explored. Models trained with coarse-grained texts may not be able to
learn mappings from fine-grained motion-related words to motion primitives,
resulting in the failure to generate motions from unseen descriptions. In this
paper, we build a large-scale language-motion dataset specializing in
fine-grained textual descriptions, FineHumanML3D, by feeding GPT-3.5-turbo with
step-by-step instructions with pseudo-code compulsory checks. Accordingly, we
design a new text2motion model, FineMotionDiffuse, making full use of
fine-grained textual information. Our quantitative evaluation shows that
FineMotionDiffuse trained on FineHumanML3D improves FID by a large margin of
0.38, compared with competitive baselines. According to the qualitative
evaluation and case study, our model outperforms MotionDiffuse in generating
spatially or chronologically composite motions, by learning the implicit
mappings from fine-grained descriptions to the corresponding basic motions. We
release our data at https://github.com/KunhangL/finemotiondiffuse.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attention-based Estimation and Prediction of Human Intent to augment
  Haptic Glove aided Control of Robotic Hand 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.07953v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.07953v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muneeb Ahmed, Rajesh Kumar, Qaim Abbas, Brejesh Lall, Arzad A. Kherani, Sudipto Mukherjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The letter focuses on Haptic Glove (HG) based control of a Robotic Hand (RH)
executing in-hand manipulation of certain objects of interest. The high
dimensional motion signals in HG and RH possess intrinsic variability of
kinematics resulting in difficulty to establish a direct mapping of the motion
signals from HG onto the RH. An estimation mechanism is proposed to quantify
the motion signal acquired from the human controller in relation to the
intended goal pose of the object being held by the robotic hand. A control
algorithm is presented to transform the synthesized intent at the RH and allow
relocation of the object to the expected goal pose. The lag in synthesis of the
intent in the presence of communication delay leads to a requirement of
predicting the estimated intent. We leverage an attention-based convolutional
neural network encoder to predict the trajectory of intent for a certain
lookahead to compensate for the delays. The proposed methodology is evaluated
across objects of different shapes, mass, and materials. We present a
comparative performance of the estimation and prediction mechanisms on
5G-driven real-world robotic setup against benchmark methodologies. The
test-MSE in prediction of human intent is reported to yield ~ 97.3 -98.7%
improvement of accuracy in comparison to LSTM-based benchmark
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Full Attitude Intelligent Controller Design of a Heliquad under Complete
  Failure of an Actuator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2011.07529v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2011.07529v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eeshan Kulkarni, Suresh Sundaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we design a reliable Heliquad and develop an intelligent
controller to handle one actuators complete failure. Heliquad is a multi-copter
similar to Quadcopter, with four actuators diagonally symmetric from the
center. Each actuator has two control inputs; the first input changes the
propeller blades collective pitch (also called variable pitch), and the other
input changes the rotation speed. For reliable operation and high torque
characteristic requirement for yaw control, a cambered airfoil is used to
design propeller blades. A neural network-based control allocation is designed
to provide complete control authority even under a complete loss of one
actuator. Nonlinear quaternion based outer loop position control, with
proportional-derivative inner loop for attitude control and neural
network-based control allocation is used in controller design. The proposed
controller and Heliquad designs performance is evaluated using a
software-in-loop simulation to track the position reference command under
failure. The results clearly indicate that the Heliquad with an intelligent
controller provides necessary tracking performance even under a complete loss
of one actuator.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, For video go to
  https://indianinstituteofscience-my.sharepoint.com/:v:/g/personal/eeshank_iisc_ac_in/EcMg2uTtE91AsHDejNkb6YMBNckaXGjeh_YMzDV6sAHZAQ?e=DrRqmN</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Autonomous Hook-Based Grasping and Transportation with Quadcopters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.02444v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.02444v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Péter Antal, Tamás Péni, Roland Tóth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Payload grasping and transportation with quadcopters is an active research
area that has rapidly developed over the last decade. To grasp a payload
without human interaction, most state-of-the-art approaches apply robotic arms
that are attached to the quadcopter body. However, due to the large weight and
power consumption of these aerial manipulators, their agility and flight time
are limited. This paper proposes a motion control and planning method for
transportation with a lightweight, passive manipulator structure that consists
of a hook attached to a quadrotor using a 1 DoF revolute joint. To perform
payload grasping, transportation, and release, first, time-optimal reference
trajectories are designed through specific waypoints to ensure the fast and
reliable execution of the tasks. Then, a two-stage motion control approach is
developed based on a robust geometric controller for precise and reliable
reference tracking and a linear--quadratic payload regulator for rapid setpoint
stabilization of the payload swing. Furthermore, stability of the closed-loop
system is mathematically proven to give safety guarantee for its operation. The
proposed control architecture and design are evaluated in a high-fidelity
physical simulator, and also in real flight experiments, using a custom-made
quadrotor--hook manipulator platform.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robustness Evaluation of Localization Techniques for Autonomous Racing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07658v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07658v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tian Yi Lim, Edoardo Ghignone, Nicolas Baumann, Michele Magno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces SynPF, an MCL-based algorithm tailored for high-speed
racing environments. Benchmarked against Cartographer, a state-of-the-art
pose-graph SLAM algorithm, SynPF leverages synergies from previous
particle-filtering methods and synthesizes them for the high-performance racing
domain. Our extensive in-field evaluations reveal that while Cartographer
excels under nominal conditions, it struggles when subjected to wheel-slip, a
common phenomenon in a racing scenario due to varying grip levels and
aggressive driving behaviour. Conversely, SynPF demonstrates robustness in
these challenging conditions and a low-latency computation time of 1.25 ms on
on-board computers without a GPU. Using the F1TENTH platform, a 1:10 scaled
autonomous racing vehicle, this work not only highlights the vulnerabilities of
existing algorithms in high-speed scenarios, tested up until 7.6 m/s, but also
emphasizes the potential of SynPF as a viable alternative, especially in
deteriorating odometry conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the Design, Automation and Test in Europe Conference 2024
  as an extended abstract</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OCC-VO: Dense Mapping via 3D Occupancy-Based Visual Odometry for
  Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11011v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11011v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heng Li, Yifan Duan, Xinran Zhang, Haiyi Liu, Jianmin Ji, Yanyong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Odometry (VO) plays a pivotal role in autonomous systems, with a
principal challenge being the lack of depth information in camera images. This
paper introduces OCC-VO, a novel framework that capitalizes on recent advances
in deep learning to transform 2D camera images into 3D semantic occupancy,
thereby circumventing the traditional need for concurrent estimation of ego
poses and landmark locations. Within this framework, we utilize the TPV-Former
to convert surround view cameras' images into 3D semantic occupancy. Addressing
the challenges presented by this transformation, we have specifically tailored
a pose estimation and mapping algorithm that incorporates Semantic Label
Filter, Dynamic Object Filter, and finally, utilizes Voxel PFilter for
maintaining a consistent global semantic map. Evaluations on the Occ3D-nuScenes
not only showcase a 20.6% improvement in Success Ratio and a 29.6% enhancement
in trajectory accuracy against ORB-SLAM3, but also emphasize our ability to
construct a comprehensive map. Our implementation is open-sourced and available
at: https://github.com/USTCLH/OCC-VO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Motion Planning Diffusion: Learning and Planning of Robot Motions with
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.01557v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.01557v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joao Carvalho, An T. Le, Mark Baierl, Dorothea Koert, Jan Peters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning priors on trajectory distributions can help accelerate robot motion
planning optimization. Given previously successful plans, learning trajectory
generative models as priors for a new planning problem is highly desirable.
Prior works propose several ways on utilizing this prior to bootstrapping the
motion planning problem. Either sampling the prior for initializations or using
the prior distribution in a maximum-a-posterior formulation for trajectory
optimization. In this work, we propose learning diffusion models as priors. We
then can sample directly from the posterior trajectory distribution conditioned
on task goals, by leveraging the inverse denoising process of diffusion models.
Furthermore, diffusion has been recently shown to effectively encode data
multimodality in high-dimensional settings, which is particularly well-suited
for large trajectory dataset. To demonstrate our method efficacy, we compare
our proposed method - Motion Planning Diffusion - against several baselines in
simulated planar robot and 7-dof robot arm manipulator environments. To assess
the generalization capabilities of our method, we test it in environments with
previously unseen obstacles. Our experiments show that diffusion models are
strong priors to encode high-dimensional trajectory distributions of robot
motions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feeling Optimistic? Ambiguity Attitudes for Online Decision Making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04225v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04225v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jared J. Beard, R. Michael Butts, Yu Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the complexity of many decision making problems, tree search
algorithms often have inadequate information to produce accurate transition
models. Robust methods, designed to make safe decisions when faced with these
uncertainties, often overlook the impact expressions of uncertainty have on how
the decision is made. This work introduces the Ambiguity Attitude Graph Search
(AAGS), advocating for more precise representation of ambiguities (uncertainty
from a set of plausible models) in decision making. Additionally, AAGS allows
users to adjust their ambiguity attitude (or preference), promoting exploration
and improving users' ability to control how an agent should respond when faced
with a set of valid alternatives. Simulation in a dynamic sailing environment
shows how highly stochastic environments can lead robust methods to fail.
Results further demonstrate how adjusting ambiguity attitudes better fulfills
objectives while mitigating this failure mode of robust approaches. Because
this approach is a generalization of the robust framework, these results
further demonstrate how algorithms focused on ambiguity have applicability
beyond safety-critical systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures, 2 algorithms. Submitted to the 2024 IEEE/RSJ
  International Conference on Intelligent Robots and Systems in Abu Dhabi, UAE
  (Oct 14-18, 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual Whole-Body Control for Legged Loco-Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16967v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16967v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghuan Liu, Zixuan Chen, Xuxin Cheng, Yandong Ji, Ruihan Yang, Xiaolong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of mobile manipulation using legged robots equipped with
an arm, namely legged loco-manipulation. The robot legs, while usually utilized
for mobility, offer an opportunity to amplify the manipulation capabilities by
conducting whole-body control. That is, the robot can control the legs and the
arm at the same time to extend its workspace. We propose a framework that can
conduct the whole-body control autonomously with visual observations. Our
approach, namely Visual Whole-Body Control(VBC), is composed of a low-level
policy using all degrees of freedom to track the end-effector manipulator
position and a high-level policy proposing the end-effector position based on
visual inputs. We train both levels of policies in simulation and perform
Sim2Real transfer for real robot deployment. We perform extensive experiments
and show significant improvements over baselines in picking up diverse objects
in different configurations (heights, locations, orientations) and
environments. Project page: https://wholebody-b1.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contribute equally. Project page:
  https://wholebody-b1.github.io</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-25T00:00:00Z">2024-03-25</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">83</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploiting Priors from 3D Diffusion Models for RGB-Based One-Shot View
  Planning <span class="chip">IROS
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sicong Pan, Liren Jin, Xuying Huang, Cyrill Stachniss, Marija Popović, Maren Bennewitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object reconstruction is relevant for many autonomous robotic tasks that
require interaction with the environment. A key challenge in such scenarios is
planning view configurations to collect informative measurements for
reconstructing an initially unknown object. One-shot view planning enables
efficient data collection by predicting view configurations and planning the
globally shortest path connecting all views at once. However, geometric priors
about the object are required to conduct one-shot view planning. In this work,
we propose a novel one-shot view planning approach that utilizes the powerful
3D generation capabilities of diffusion models as priors. By incorporating such
geometric priors into our pipeline, we achieve effective one-shot view planning
starting with only a single RGB image of the object to be reconstructed. Our
planning experiments in simulation and real-world setups indicate that our
approach balances well between object reconstruction quality and movement cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Sicong Pan and Liren Jin have equal contribution. Submitted to IROS
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CurbNet: Curb Detection Framework Based on LiDAR Point Cloud
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoyang Zhao, Fulong Ma, Yuxuan Liu, Weiqing Qi, Ming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Curb detection is an important function in intelligent driving and can be
used to determine drivable areas of the road. However, curbs are difficult to
detect due to the complex road environment. This paper introduces CurbNet, a
novel framework for curb detection, leveraging point cloud segmentation.
Addressing the dearth of comprehensive curb datasets and the absence of 3D
annotations, we have developed the 3D-Curb dataset, encompassing 7,100 frames,
which represents the largest and most categorically diverse collection of curb
point clouds currently available. Recognizing that curbs are primarily
characterized by height variations, our approach harnesses spatially-rich 3D
point clouds for training. To tackle the challenges presented by the uneven
distribution of curb features on the xy-plane and their reliance on z-axis
high-frequency features, we introduce the multi-scale and channel attention
(MSCA) module, a bespoke solution designed to optimize detection performance.
Moreover, we propose an adaptive weighted loss function group, specifically
formulated to counteract the imbalance in the distribution of curb point clouds
relative to other categories. Our extensive experimentation on 2 major datasets
has yielded results that surpass existing benchmarks set by leading curb
detection and point cloud segmentation models. By integrating multi-clustering
and curve fitting techniques in our post-processing stage, we have
substantially reduced noise in curb detection, thereby enhancing precision to
0.8744. Notably, CurbNet has achieved an exceptional average metrics of over
0.95 at a tolerance of just 0.15m, thereby establishing a new benchmark.
Furthermore, corroborative real-world experiments and dataset analyzes mutually
validate each other, solidifying CurbNet's superior detection proficiency and
its robust generalizability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DBPF: A Framework for Efficient and Robust Dynamic Bin-Picking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichuan Li, Junkai Zhao, Yixiao Li, Zheng Wu, Rui Cao, Masayoshi Tomizuka, Yunhui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficiency and reliability are critical in robotic bin-picking as they
directly impact the productivity of automated industrial processes. However,
traditional approaches, demanding static objects and fixed collisions, lead to
deployment limitations, operational inefficiencies, and process unreliability.
This paper introduces a Dynamic Bin-Picking Framework (DBPF) that challenges
traditional static assumptions. The DBPF endows the robot with the reactivity
to pick multiple moving arbitrary objects while avoiding dynamic obstacles,
such as the moving bin. Combined with scene-level pose generation, the proposed
pose selection metric leverages the Tendency-Aware Manipulability Network
optimizing suction pose determination. Heuristic task-specific designs like
velocity-matching, dynamic obstacle avoidance, and the resight policy, enhance
the picking success rate and reliability. Empirical experiments demonstrate the
importance of these components. Our method achieves an average 84% success
rate, surpassing the 60% of the most comparable baseline, crucially, with zero
collisions. Further evaluations under diverse dynamic scenarios showcase DBPF's
robust performance in dynamic bin-picking. Results suggest that our framework
offers a promising solution for efficient and reliable robotic bin-picking
under dynamics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures. This paper has been accepted by IEEE RA-L on
  2024-03-24. See the supplementary video at youtube:
  https://youtu.be/n5af2VsKhkg</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Action Planning with Multiple Heterogeneous Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16781v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16781v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martina Lippi, Michael C. Welle, Marco Moletta, Alessandro Marino, Andrea Gasparri, Danica Kragic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual planning methods are promising to handle complex settings where
extracting the system state is challenging. However, none of the existing works
tackles the case of multiple heterogeneous agents which are characterized by
different capabilities and/or embodiment. In this work, we propose a method to
realize visual action planning in multi-agent settings by exploiting a roadmap
built in a low-dimensional structured latent space and used for planning. To
enable multi-agent settings, we infer possible parallel actions from a dataset
composed of tuples associated with individual actions. Next, we evaluate
feasibility and cost of them based on the capabilities of the multi-agent
system and endow the roadmap with this information, building a capability
latent space roadmap (C-LSR). Additionally, a capability suggestion strategy is
designed to inform the human operator about possible missing capabilities when
no paths are found. The approach is validated in a simulated burger cooking
task and a real-world box packing task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-Cost Teleoperation with Haptic Feedback through Vision-based Tactile
  Sensors for Rigid and Soft Object Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16764v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16764v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martina Lippi, Michael C. Welle, Maciej K. Wozniak, Andrea Gasparri, Danica Kragic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Haptic feedback is essential for humans to successfully perform complex and
delicate manipulation tasks. A recent rise in tactile sensors has enabled
robots to leverage the sense of touch and expand their capability drastically.
However, many tasks still need human intervention/guidance. For this reason, we
present a teleoperation framework designed to provide haptic feedback to human
operators based on the data from camera-based tactile sensors mounted on the
robot gripper. Partial autonomy is introduced to prevent slippage of grasped
objects during task execution. Notably, we rely exclusively on low-cost
off-the-shelf hardware to realize an affordable solution. We demonstrate the
versatility of the framework on nine different objects ranging from rigid to
soft and fragile ones, using three different operators on real hardware.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://vision-tactile-manip.github.io/teleop/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Robotic Skill Learning System Built Upon Diffusion Policies and
  Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nils Ingelhag, Jesper Munkeby, Jonne van Haastregt, Anastasia Varava, Michael C. Welle, Danica Kragic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we build upon two major recent developments in the field,
Diffusion Policies for visuomotor manipulation and large pre-trained multimodal
foundational models to obtain a robotic skill learning system. The system can
obtain new skills via the behavioral cloning approach of visuomotor diffusion
policies given teleoperated demonstrations. Foundational models are being used
to perform skill selection given the user's prompt in natural language. Before
executing a skill the foundational model performs a precondition check given an
observation of the workspace. We compare the performance of different
foundational models to this end as well as give a detailed experimental
evaluation of the skills taught by the user in simulation and the real world.
Finally, we showcase the combined system on a challenging food serving scenario
in the real world. Videos of all experimental executions, as well as the
process of teaching new skills in simulation and the real world, are available
on the project's website.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://roboskillframework.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BatDeck: Advancing Nano-drone Navigation with Low-power Ultrasound-based
  Obstacle Avoidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanna Müller, Victor Kartsch, Michele Magno, Luca Benini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nano-drones, distinguished by their agility, minimal weight, and
cost-effectiveness, are particularly well-suited for exploration in confined,
cluttered and narrow spaces. Recognizing transparent, highly reflective or
absorbing materials, such as glass and metallic surfaces is challenging, as
classical sensors, such as cameras or laser rangers, often do not detect them.
Inspired by bats, which can fly at high speeds in complete darkness with the
help of ultrasound, this paper introduces \textit{BatDeck}, a pioneering
sensor-deck employing a lightweight and low-power ultrasonic sensor for
nano-drone autonomous navigation. This paper first provides insights about
sensor characteristics, highlighting the influence of motor noise on the
ultrasound readings, then it introduces the results of extensive experimental
tests for obstacle avoidance (OA) in a diverse environment. Results show that
\textit{BatDeck} allows exploration for a flight time of 8 minutes while
covering 136m on average before crash in a challenging environment with
transparent and reflective obstacles, proving the effectiveness of ultrasonic
sensors for OA on nano-drones.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synapse: Learning Preferential Concepts from Visual Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sadanand Modak, Noah Patton, Isil Dillig, Joydeep Biswas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of preference learning, which aims to learn
user-specific preferences (e.g., "good parking spot", "convenient drop-off
location") from visual input. Despite its similarity to learning factual
concepts (e.g., "red cube"), preference learning is a fundamentally harder
problem due to its subjective nature and the paucity of person-specific
training data. We address this problem using a new framework called Synapse,
which is a neuro-symbolic approach designed to efficiently learn preferential
concepts from limited demonstrations. Synapse represents preferences as
neuro-symbolic programs in a domain-specific language (DSL) that operates over
images, and leverages a novel combination of visual parsing, large language
models, and program synthesis to learn programs representing individual
preferences. We evaluate Synapse through extensive experimentation including a
user case study focusing on mobility-related concepts in mobile robotics and
autonomous driving. Our evaluation demonstrates that Synapse significantly
outperforms existing baselines as well as its own ablations. The code and other
details can be found on the project website https://amrl.cs.utexas.edu/synapse .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 7 figures; Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain Adaptive Detection of MAVs: A Benchmark and Noise Suppression
  Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16669v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16669v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yin Zhang, Jinhong Deng, Peidong Liu, Wen Li, Shiyu Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual detection of Micro Air Vehicles (MAVs) has attracted increasing
attention in recent years due to its important application in various tasks.
The existing methods for MAV detection assume that the training set and testing
set have the same distribution. As a result, when deployed in new domains, the
detectors would have a significant performance degradation due to domain
discrepancy. In this paper, we study the problem of cross-domain MAV detection.
The contributions of this paper are threefold. 1) We propose a
Multi-MAV-Multi-Domain (M3D) dataset consisting of both simulation and
realistic images. Compared to other existing datasets, the proposed one is more
comprehensive in the sense that it covers rich scenes, diverse MAV types, and
various viewing angles. A new benchmark for cross-domain MAV detection is
proposed based on the proposed dataset. 2) We propose a Noise Suppression
Network (NSN) based on the framework of pseudo-labeling and a large-to-small
training procedure. To reduce the challenging pseudo-label noises, two novel
modules are designed in this network. The first is a prior-based curriculum
learning module for allocating adaptive thresholds for pseudo labels with
different difficulties. The second is a masked copy-paste augmentation module
for pasting truly-labeled MAVs on unlabeled target images and thus decreasing
pseudo-label noises. 3) Extensive experimental results verify the superior
performance of the proposed method compared to the state-of-the-art ones. In
particular, it achieves mAP of 46.9%(+5.8%), 50.5%(+3.7%), and 61.5%(+11.3%) on
the tasks of simulation-to-real adaptation, cross-scene adaptation, and
cross-camera adaptation, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 11 figures. Accepted by IEEE Transactions on Automation
  Science and Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Skill Q-Network: Learning Adaptive Skill Ensemble for Mapless Navigation
  in Unknown Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunki Seong, David Hyunchul Shim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on the acquisition of mapless navigation skills within
unknown environments. We introduce the Skill Q-Network (SQN), a novel
reinforcement learning method featuring an adaptive skill ensemble mechanism.
Unlike existing methods, our model concurrently learns a high-level skill
decision process alongside multiple low-level navigation skills, all without
the need for prior knowledge. Leveraging a tailored reward function for mapless
navigation, the SQN is capable of learning adaptive maneuvers that incorporate
both exploration and goal-directed skills, enabling effective navigation in new
environments. Our experiments demonstrate that our SQN can effectively navigate
complex environments, exhibiting a 40% higher performance compared to baseline
models. Without explicit guidance, SQN discovers how to combine low-level skill
policies, showcasing both goal-directed navigations to reach destinations and
exploration maneuvers to escape from local minimum regions in challenging
scenarios. Remarkably, our adaptive skill ensemble method enables zero-shot
transfer to out-of-distribution domains, characterized by unseen observations
from non-convex obstacles or uneven, subterranean-like environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trajectory Planning of Robotic Manipulator in Dynamic Environment
  Exploiting DRL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Osama Ahmad, Zawar Hussain, Hammad Naeem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study is about the implementation of a reinforcement learning algorithm
in the trajectory planning of manipulators. We have a 7-DOF robotic arm to pick
and place the randomly placed block at a random target point in an unknown
environment. The obstacle is randomly moving which creates a hurdle in picking
the object. The objective of the robot is to avoid the obstacle and pick the
block with constraints to a fixed timestamp. In this literature, we have
applied a deep deterministic policy gradient (DDPG) algorithm and compared the
model's efficiency with dense and sparse rewards.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICIESTR-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging the Sim-to-Real Gap with Bayesian Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16644v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16644v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Rothfuss, Bhavya Sukhija, Lenart Treven, Florian Dörfler, Stelian Coros, Andreas Krause
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present SIM-FSVGD for learning robot dynamics from data. As opposed to
traditional methods, SIM-FSVGD leverages low-fidelity physical priors, e.g., in
the form of simulators, to regularize the training of neural network models.
While learning accurate dynamics already in the low data regime, SIM-FSVGD
scales and excels also when more data is available. We empirically show that
learning with implicit physical priors results in accurate mean model
estimation as well as precise uncertainty quantification. We demonstrate the
effectiveness of SIM-FSVGD in bridging the sim-to-real gap on a
high-performance RC racecar system. Using model-based RL, we demonstrate a
highly dynamic parking maneuver with drifting, using less than half the data
compared to the state of the art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Symbolic and User-friendly Geometric Algebra Routines (SUGAR) for
  Computations in Matlab 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manel Velasco, Isiah Zaplana, Arnau Dória-Cerezo, Pau Martí
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geometric algebra (GA) is a mathematical tool for geometric computing,
providing a framework that allows a unified and compact approach to geometric
relations which in other mathematical systems are typically described using
different more complicated elements. This fact has led to an increasing
adoption of GA in applied mathematics and engineering problems. However, the
scarcity of symbolic implementations of GA and its inherent complexity,
requiring a specific mathematical background, make it challenging and less
intuitive for engineers to work with. This prevents wider adoption among more
applied professionals. To address this challenge, this paper introduces SUGAR
(Symbolic and User-friendly Geometric Algebra Routines), an open-source toolbox
designed for Matlab and licensed under the MIT License. SUGAR facilitates the
translation of GA concepts into Matlab and provides a collection of
user-friendly functions tailored for GA computations, including support for
symbolic operations. It supports both numeric and symbolic computations in
high-dimensional GAs. Specifically tailored for applied mathematics and
engineering applications, SUGAR has been meticulously engineered to represent
geometric elements and transformations within two and three-dimensional
projective and conformal geometric algebras, aligning with established
computational methodologies in the literature. Furthermore, SUGAR efficiently
handles functions of multivectors, such as exponential, logarithmic,
sinusoidal, and cosine functions, enhancing its applicability across various
engineering domains, including robotics, control systems, and power
electronics. Finally, this work includes four distinct validation examples,
demonstrating SUGAR's capabilities across the above-mentioned fields and its
practical utility in addressing real-world applied mathematics and engineering
problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 6 figures, journal paper submitted to ACM TOMS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Technical Development of a Semi-Autonomous Robotic Partition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binh Vinh Duc Nguyen, Andrew Vande Moere
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This technical description details the design and engineering process of a
semi-autonomous robotic partition. This robotic partition prototype was
subsequently employed in a longer-term evaluation in-the-wild study conducted
by the authors in a real-world office setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ROXIE: Defining a Robotic eXplanation and Interpretability Engine <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francisco J. Rodríguez-Lera, Miguel A. González-Santamarta, Alejandro González-Cantón, Laura Fernández-Becerra, David Sobrín-Hidalgo, Angel Manuel Guerrero-Higueras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In an era where autonomous robots increasingly inhabit public spaces, the
imperative for transparency and interpretability in their decision-making
processes becomes paramount. This paper presents the overview of a Robotic
eXplanation and Interpretability Engine (ROXIE), which addresses this critical
need, aiming to demystify the opaque nature of complex robotic behaviors. This
paper elucidates the key features and requirements needed for providing
information and explanations about robot decision-making processes. It also
overviews the suite of software components and libraries available for
deployment with ROS 2, empowering users to provide comprehensive explanations
and interpretations of robot processes and behaviors, thereby fostering trust
and collaboration in human-robot interactions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures, 1 tables, Submitted to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Research Challenges for Adaptive Architecture: Empowering Occupants of
  Multi-Occupancy Buildings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binh Vinh Duc Nguyen, Andrew Vande Moere
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This positional paper outlines our vision of 'adaptive architecture', which
involves the integration of robotic technology to physically change an
architectural space in supporting the changing needs of its occupants, in
response to the CHI'24 workshop "HabiTech - Inhabiting Buildings, Data &
Technology" call on "How do new technologies enable and empower the inhabitants
of multi-occupancy buildings?". Specifically, while adaptive architecture holds
promise for enhancing occupant satisfaction, comfort, and overall health and
well-being, there remains a range of research challenges of (1) how it can
effectively support individual occupants, while (2) mediating the conflicting
needs of collocated others, and (3) integrating meaningfully into the
sociocultural characteristics of their building community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Adaptive Workplace: Orchestrating Architectural Services around the
  Wellbeing of Individual Occupants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Vande Moere, Sara Arko, Alena Safrova Drasilova, Tomáš Ondráček, Ilaria Pigliautile, Benedetta Pioppi, Anna Laura Pisello, Jakub Prochazka, Paula Acuna Roncancio, Davide Schaumann, Marcel Schweiker, Binh Vinh Duc Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the academic consortia members of the EU Horizon project SONATA
("Situation-aware OrchestratioN of AdapTive Architecture"), we respond to the
workshop call for "Office Wellbeing by Design: Don't Stand for Anything Less"
by proposing the "Adaptive Workplace" concept. In essence, our vision aims to
adapt a workplace to the ever-changing needs of individual occupants, instead
of that occupants are expected to adapt to their workplace.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Counter-example guided Imitation Learning of Feedback Controllers from
  Temporal Logic Specifications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thao Dang, Alexandre Donzé, Inzemamul Haque, Nikolaos Kekatos, Indranil Saha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel method for imitation learning for control requirements
expressed using Signal Temporal Logic (STL). More concretely we focus on the
problem of training a neural network to imitate a complex controller. The
learning process is guided by efficient data aggregation based on
counter-examples and a coverage measure. Moreover, we introduce a method to
evaluate the performance of the learned controller via parameterization and
parameter estimation of the STL requirements. We demonstrate our approach with
a flying robot case study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Active Admittance Control with Iterative Learning for General-Purpose
  Contact-Rich Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Zhou, Yuyao Sun, Wenbo Liu, Ruixuan Jiao, Fang Fang, Shihua Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Force interaction is inevitable when robots face multiple operation
scenarios. How to make the robot competent in force control for generalized
operations such as multi-tasks still remains a challenging problem. Aiming at
the reproducibility of interaction tasks and the lack of a generalized force
control framework for multi-task scenarios, this paper proposes a novel hybrid
control framework based on active admittance control with iterative learning
parameters-tunning mechanism. The method adopts admittance control as the
underlying algorithm to ensure flexibility, and iterative learning as the
high-level algorithm to regulate the parameters of the admittance model. The
whole algorithm has flexibility and learning ability, which is capable of
achieving the goal of excellent versatility. Four representative interactive
robot manipulation tasks are chosen to investigate the consistency and
generalisability of the proposed method. Experiments are designed to verify the
effectiveness of the whole framework, and an average of 98.21% and 91.52%
improvement of RMSE is obtained relative to the traditional admittance control
as well as the model-free adaptive control, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Arm-Constrained Curriculum Learning for Loco-Manipulation of the
  Wheel-Legged Robot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zifan Wang, Yufei Jia, Lu Shi, Haoyu Wang, Haizhou Zhao, Xueyang Li, Jinni Zhou, Jun Ma, Guyue Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incorporating a robotic manipulator into a wheel-legged robot enhances its
agility and expands its potential for practical applications. However, the
presence of potential instability and uncertainties presents additional
challenges for control objectives. In this paper, we introduce an
arm-constrained curriculum learning architecture to tackle the issues
introduced by adding the manipulator. Firstly, we develop an arm-constrained
reinforcement learning algorithm to ensure safety and stability in control
performance. Additionally, to address discrepancies in reward settings between
the arm and the base, we propose a reward-aware curriculum learning method. The
policy is first trained in Isaac gym and transferred to the physical robot to
do dynamic grasping tasks, including the door-opening task, fan-twitching task
and the relay-baton-picking and following task. The results demonstrate that
our proposed approach effectively controls the arm-equipped wheel-legged robot
to master dynamic grasping skills, allowing it to chase and catch a moving
object while in motion. The code can be found at
https://github.com/aCodeDog/legged-robots-manipulation. To view the
supplemental video, please visit https://youtu.be/sNXT-rwPNMM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hallucination Detection in Foundation Models for Decision-Making: A
  Flexible Definition and <span class="highlight-title">Review</span> of the State of the Art 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neeloy Chakraborty, Melkior Ornik, Katherine Driggs-Campbell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous systems are soon to be ubiquitous, from manufacturing autonomy to
agricultural field robots, and from health care assistants to the entertainment
industry. The majority of these systems are developed with modular
sub-components for decision-making, planning, and control that may be
hand-engineered or learning-based. While these existing approaches have been
shown to perform well under the situations they were specifically designed for,
they can perform especially poorly in rare, out-of-distribution scenarios that
will undoubtedly arise at test-time. The rise of foundation models trained on
multiple tasks with impressively large datasets from a variety of fields has
led researchers to believe that these models may provide common sense reasoning
that existing planners are missing. Researchers posit that this common sense
reasoning will bridge the gap between algorithm development and deployment to
out-of-distribution tasks, like how humans adapt to unexpected scenarios. Large
language models have already penetrated the robotics and autonomous systems
domains as researchers are scrambling to showcase their potential use cases in
deployment. While this application direction is very promising empirically,
foundation models are known to hallucinate and generate decisions that may
sound reasonable, but are in fact poor. We argue there is a need to step back
and simultaneously design systems that can quantify the certainty of a model's
decision, and detect when it may be hallucinating. In this work, we discuss the
current use cases of foundation models for decision-making tasks, provide a
general definition for hallucinations with examples, discuss existing
approaches to hallucination detection and mitigation with a focus on decision
problems, and explore areas for further research in this exciting field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spatially temporally distributed informative path planning for
  multi-robot systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binh Nguyen, Linh Nguyen, Truong X. Nghiem, Hung La, Jose Baca, Pablo Rangel, Miguel Cid Montoya, Thang Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the problem of informative path planning for a mobile
robotic sensor network in spatially temporally distributed mapping. The robots
are able to gather noisy measurements from an area of interest during their
movements to build a Gaussian Process (GP) model of a spatio-temporal field.
The model is then utilized to predict the spatio-temporal phenomenon at
different points of interest. To spatially and temporally navigate the group of
robots so that they can optimally acquire maximal information gains while their
connectivity is preserved, we propose a novel multistep prediction informative
path planning optimization strategy employing our newly defined local cost
functions. By using the dual decomposition method, it is feasible and practical
to effectively solve the optimization problem in a distributed manner. The
proposed method was validated through synthetic experiments utilizing
real-world data sets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time Model Predictive Control with Zonotope-Based Neural Networks
  for Bipedal Social Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16485v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16485v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdulaziz Shamsah, Krishanu Agarwal, Shreyas Kousik, Ye Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study addresses the challenge of bipedal navigation in a dynamic
human-crowded environment, a research area that remains largely underexplored
in the field of legged navigation. We propose two cascaded zonotope-based
neural networks: a Pedestrian Prediction Network (PPN) for pedestrians' future
trajectory prediction and an Ego-agent Social Network (ESN) for ego-agent
social path planning. Representing future paths as zonotopes allows for
efficient reachability-based planning and collision checking. The ESN is then
integrated with a Model Predictive Controller (ESN-MPC) for footstep planning
for our bipedal robot Digit designed by Agility Robotics. ESN-MPC solves for a
collision-free optimal trajectory by optimizing through the gradients of ESN.
ESN-MPC optimal trajectory is sent to the low-level controller for full-order
simulation of Digit. The overall proposed framework is validated with extensive
simulations on randomly generated initial settings with varying human crowd
densities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Cooperative Maneuver Planning in Mixed Traffic at Urban
  Intersections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16478v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16478v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marvin Klimke, Max Bastian Mertens, Benjamin Völz, Michael Buchholz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Connected automated driving promises a significant improvement of traffic
efficiency and safety on highways and in urban areas. Apart from sharing of
awareness and perception information over wireless communication links,
cooperative maneuver planning may facilitate active guidance of connected
automated vehicles at urban intersections. Research in automatic intersection
management put forth a large body of works that mostly employ rule-based or
optimization-based approaches primarily in fully automated simulated
environments. In this work, we present two cooperative planning approaches that
are capable of handling mixed traffic, i.e., the road being shared by automated
vehicles and regular vehicles driven by humans. Firstly, we propose an
optimization-based planner trained on real driving data that cyclically selects
the most efficient out of multiple predicted coordinated maneuvers.
Additionally, we present a cooperative planning approach based on graph-based
reinforcement learning, which conquers the lack of ground truth data for
cooperative maneuvers. We present evaluation results of both cooperative
planners in high-fidelity simulation and real-world traffic. Simulative
experiments in fully automated traffic and mixed traffic show that cooperative
maneuver planning leads to less delay due to interaction and a reduced number
of stops. In real-world experiments with three prototype connected automated
vehicles in public traffic, both planners demonstrate their ability to perform
efficient cooperative maneuvers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>M. Klimke and M. Mertens are both first authors with equal
  contribution. 11 pages, 10 figures, 2 tables, submitted to IEEE Transactions
  on Intelligent Vehicles</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Producing and Leveraging Online Map Uncertainty in Trajectory Prediction <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xunjiang Gu, Guanyu Song, Igor Gilitschenski, Marco Pavone, Boris Ivanovic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-definition (HD) maps have played an integral role in the development of
modern autonomous vehicle (AV) stacks, albeit with high associated labeling and
maintenance costs. As a result, many recent works have proposed methods for
estimating HD maps online from sensor data, enabling AVs to operate outside of
previously-mapped regions. However, current online map estimation approaches
are developed in isolation of their downstream tasks, complicating their
integration in AV stacks. In particular, they do not produce uncertainty or
confidence estimates. In this work, we extend multiple state-of-the-art online
map estimation methods to additionally estimate uncertainty and show how this
enables more tightly integrating online mapping with trajectory forecasting. In
doing so, we find that incorporating uncertainty yields up to 50% faster
training convergence and up to 15% better prediction performance on the
real-world nuScenes driving dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 14 figures, 6 tables. CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AeroBridge: Autonomous Drone Handoff System for Emergency Battery
  Service 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16430v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16430v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avishkar Seth, Alice James, Endrowednes Kuantama, Richard Han, Subhas Mukhopadhyay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes an Emergency Battery Service (EBS) for drones in which an
EBS drone flies to a drone in the field with a depleted battery and transfers a
fresh battery to the exhausted drone. The authors present a unique battery
transfer mechanism and drone localization that uses the Cross Marker Position
(CMP) method. The main challenges include a stable and balanced transfer that
precisely localizes the receiver drone. The proposed EBS drone mitigates the
effects of downwash due to the vertical proximity between the drones by
implementing diagonal alignment with the receiver, reducing the distance to 0.5
m between the two drones. CFD analysis shows that diagonal instead of
perpendicular alignment minimizes turbulence, and the authors verify the actual
system for change in output airflow and thrust measurements. The CMP
marker-based localization method enables position lock for the EBS drone with
up to 0.9 cm accuracy. The performance of the transfer mechanism is validated
experimentally by successful mid-air transfer in 5 seconds, where the EBS drone
is within 0.5 m vertical distance from the receiver drone, wherein 4m/s
turbulence does not affect the transfer process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Visual Place Recognition via Fast and Slow Adaptive Biasing in
  Event Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gokul B. Nair, Michael Milford, Tobias Fischer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras are increasingly popular in robotics due to their beneficial
features, such as low latency, energy efficiency, and high dynamic range.
Nevertheless, their downstream task performance is greatly influenced by the
optimization of bias parameters. These parameters, for instance, regulate the
necessary change in light intensity to trigger an event, which in turn depends
on factors such as the environment lighting and camera motion. This paper
introduces feedback control algorithms that automatically tune the bias
parameters through two interacting methods: 1) An immediate, on-the-fly fast
adaptation of the refractory period, which sets the minimum interval between
consecutive events, and 2) if the event rate exceeds the specified bounds even
after changing the refractory period repeatedly, the controller adapts the
pixel bandwidth and event thresholds, which stabilizes after a short period of
noise events across all pixels (slow adaptation). Our evaluation focuses on the
visual place recognition task, where incoming query images are compared to a
given reference database. We conducted comprehensive evaluations of our
algorithms' adaptive feedback control in real-time. To do so, we collected the
QCR-Fast-and-Slow dataset that contains DAVIS346 event camera streams from 366
repeated traversals of a Scout Mini robot navigating through a 100 meter long
indoor lab setting (totaling over 35km distance traveled) in varying brightness
conditions with ground truth location information. Our proposed feedback
controllers result in superior performance when compared to the standard bias
settings and prior feedback control methods. Our findings also detail the
impact of bias adjustments on task performance and feature ablation studies on
the fast and slow adaptation mechanisms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures, paper under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Terrain-Attentive Learning for Efficient 6-DoF Kinodynamic Modeling on
  Vertically Challenging Terrain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16419v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16419v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aniket Datar, Chenhui Pan, Mohammad Nazeri, Anuj Pokhrel, Xuesu Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wheeled robots have recently demonstrated superior mechanical capability to
traverse vertically challenging terrain (e.g., extremely rugged boulders
comparable in size to the vehicles themselves). Negotiating such terrain
introduces significant variations of vehicle pose in all six Degrees-of-Freedom
(DoFs), leading to imbalanced contact forces, varying momentum, and chassis
deformation due to non-rigid tires and suspensions. To autonomously navigate on
vertically challenging terrain, all these factors need to be efficiently
reasoned within limited onboard computation and strict real-time constraints.
In this paper, we propose a 6-DoF kinodynamics learning approach that is
attentive only to the specific underlying terrain critical to the current
vehicle-terrain interaction, so that it can be efficiently queried in real-time
motion planners onboard small robots. Physical experiment results show our
Terrain-Attentive Learning demonstrates on average 51.1% reduction in model
prediction error among all 6 DoFs compared to a state-of-the-art model for
vertically challenging terrain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ASDF: Assembly State Detection Utilizing Late Fusion by Integrating 6D
  Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16400v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16400v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannah Schieber, Shiyu Li, Niklas Corell, Philipp Beckerle, Julian Kreimeier, Daniel Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In medical and industrial domains, providing guidance for assembly processes
is critical to ensure efficiency and safety. Errors in assembly can lead to
significant consequences such as extended surgery times, and prolonged
manufacturing or maintenance times in industry. Assembly scenarios can benefit
from in-situ AR visualization to provide guidance, reduce assembly times and
minimize errors. To enable in-situ visualization 6D pose estimation can be
leveraged. Existing 6D pose estimation techniques primarily focus on individual
objects and static captures. However, assembly scenarios have various dynamics
including occlusion during assembly and dynamics in the assembly objects
appearance. Existing work, combining object detection/6D pose estimation and
assembly state detection focuses either on pure deep learning-based approaches,
or limit the assembly state detection to building blocks. To address the
challenges of 6D pose estimation in combination with assembly state detection,
our approach ASDF builds upon the strengths of YOLOv8, a real-time capable
object detection framework. We extend this framework, refine the object pose
and fuse pose knowledge with network-detected pose information. Utilizing our
late fusion in our Pose2State module results in refined 6D pose estimation and
assembly state detection. By combining both pose and state information, our
Pose2State module predicts the final assembly state with precision. Our
evaluation on our ASDF dataset shows that our Pose2State module leads to an
improved assembly state detection and that the improvement of the assembly
state further leads to a more robust 6D pose estimation. Moreover, on the GBOT
dataset, we outperform the pure deep learning-based network, and even
outperform the hybrid and pure tracking-based approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SE(3) Linear Parameter Varying Dynamical Systems for Globally
  Asymptotically Stable End-Effector Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunan Sun, Nadia Figueroa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Linear Parameter Varying Dynamical Systems (LPV-DS) encode trajectories into
an autonomous first-order DS that enables reactive responses to perturbations,
while ensuring globally asymptotic stability at the target. However, the
current LPV-DS framework is established on Euclidean data only and has not been
applicable to broader robotic applications requiring pose control. In this
paper we present an extension to the current LPV-DS framework, named
Quaternion-DS, which efficiently learns a DS-based motion policy for
orientation. Leveraging techniques from differential geometry and Riemannian
statistics, our approach properly handles the non-Euclidean orientation data in
quaternion space, enabling the integration with positional control, namely
SE(3) LPV-DS, so that the synergistic behaviour within the full SE(3) pose is
preserved. Through simulation and real robot experiments, we validate our
method, demonstrating its ability to efficiently and accurately reproduce the
original SE(3) trajectory while exhibiting strong robustness to perturbations
in task space.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bipedal Safe Navigation over Uncertain Rough Terrain: Unifying Terrain
  Mapping and Locomotion Stability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16356v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16356v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kasidit Muenprasitivej, Jesse Jiang, Abdulaziz Shamsah, Samuel Coogan, Ye Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of bipedal robot navigation in complex environments with
uncertain and rough terrain. In particular, we consider a scenario in which the
robot is expected to reach a desired goal location by traversing an environment
with uncertain terrain elevation. Such terrain uncertainties induce not only
untraversable regions but also robot motion perturbations. Thus, the problems
of terrain mapping and locomotion stability are intertwined. We evaluate three
different kernels for Gaussian process (GP) regression to learn the terrain
elevation. We also learn the motion deviation resulting from both the terrain
as well as the discrepancy between the reduced-order Prismatic Inverted
Pendulum Model used for planning and the full-order locomotion dynamics. We
propose a hierarchical locomotion-dynamics-aware sampling-based navigation
planner. The global navigation planner plans a series of local waypoints to
reach the desired goal locations while respecting locomotion stability
constraints. Then, a local navigation planner is used to generate a sequence of
dynamically feasible footsteps to reach local waypoints. We develop a novel
trajectory evaluation metric to minimize motion deviation and maximize
information gain of the terrain elevation map. We evaluate the efficacy of our
planning framework on Digit bipedal robot simulation in MuJoCo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human Stress Response and Perceived Safety during Encounters with
  Quadruped Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Gupta, Hyonyoung Shin, Emily Norman, Keri K. Stephens, Nanshu Lu, Luis Sentis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the rise of mobile robot deployments in home and work settings,
perceived safety of users and bystanders is understudied in the human-robot
interaction (HRI) literature. To address this, we present a study designed to
identify elements of a human-robot encounter that correlate with observed
stress response. Stress is a key component of perceived safety and is strongly
associated with human physiological response. In this study a Boston Dynamics
Spot and a Unitree Go1 navigate autonomously through a shared environment
occupied by human participants wearing multimodal physiological sensors to
track their electrocardiography (ECG) and electrodermal activity (EDA). The
encounters are varied through several trials and participants self-rate their
stress levels after each encounter. The study resulted in a multidimensional
dataset archiving various objective and subjective aspects of a human-robot
encounter, containing insights for understanding perceived safety in such
encounters. To this end, acute stress responses were decoded from the human
participants' ECG and EDA and compared across different human-robot encounter
conditions. Statistical analysis of data indicate that on average (1)
participants feel more stress during encounters compared to baselines, (2)
participants feel more stress encountering multiple robots compared to a single
robot and (3) participants stress increases during navigation behavior compared
with search behavior.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figs, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring CausalWorld: Enhancing robotic manipulation via knowledge
  transfer and curriculum learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinrui Wang, Yan Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores a learning-based tri-finger robotic arm manipulating
task, which requires complex movements and coordination among the fingers. By
employing reinforcement learning, we train an agent to acquire the necessary
skills for proficient manipulation. To enhance the efficiency and effectiveness
of the learning process, two knowledge transfer strategies, fine-tuning and
curriculum learning, were utilized within the soft actor-critic architecture.
Fine-tuning allows the agent to leverage pre-trained knowledge and adapt it to
new tasks. Several variations like model transfer, policy transfer, and
across-task transfer were implemented and evaluated. To eliminate the need for
pretraining, curriculum learning decomposes the advanced task into simpler,
progressive stages, mirroring how humans learn. The number of learning stages,
the context of the sub-tasks, and the transition timing were found to be the
critical design parameters. The key factors of two learning strategies and
corresponding effects were explored in context-aware and context-unaware
scenarios, enabling us to identify the scenarios where the methods demonstrate
optimal performance, derive conclusive insights, and contribute to a broader
range of learning-based engineering applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Impact-Aware Bimanual Catching of Large-Momentum Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Yan, Theodoros Stouraitis, João Moura, Wenfu Xu, Michael Gienger, Sethu Vijayakumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates one of the most challenging tasks in dynamic
manipulation -- catching large-momentum moving objects. Beyond the realm of
quasi-static manipulation, dealing with highly dynamic objects can
significantly improve the robot's capability of interacting with its
surrounding environment. Yet, the inevitable motion mismatch between the fast
moving object and the approaching robot will result in large impulsive forces,
which lead to the unstable contacts and irreversible damage to both the object
and the robot. To address the above problems, we propose an online optimization
framework to: 1) estimate and predict the linear and angular motion of the
object; 2) search and select the optimal contact locations across every surface
of the object to mitigate impact through sequential quadratic programming
(SQP); 3) simultaneously optimize the end-effector motion, stiffness, and
contact force for both robots using multi-mode trajectory optimization (MMTO);
and 4) realise the impact-aware catching motion on the compliant robotic system
based on indirect force controller. We validate the impulse distribution,
contact selection, and impact-aware MMTO algorithms in simulation and
demonstrate the benefits of the proposed framework in real-world experiments
including catching large-momentum moving objects with well-defined motion,
constrained motion and free-flying motion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DASA: Delay-Adaptive Multi-Agent Stochastic Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolo Dal Fabbro, Arman Adibi, H. Vincent Poor, Sanjeev R. Kulkarni, Aritra Mitra, George J. Pappas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a setting in which $N$ agents aim to speedup a common Stochastic
Approximation (SA) problem by acting in parallel and communicating with a
central server. We assume that the up-link transmissions to the server are
subject to asynchronous and potentially unbounded time-varying delays. To
mitigate the effect of delays and stragglers while reaping the benefits of
distributed computation, we propose \texttt{DASA}, a Delay-Adaptive algorithm
for multi-agent Stochastic Approximation. We provide a finite-time analysis of
\texttt{DASA} assuming that the agents' stochastic observation processes are
independent Markov chains. Significantly advancing existing results,
\texttt{DASA} is the first algorithm whose convergence rate depends only on the
mixing time $\tmix$ and on the average delay $\tau_{avg}$ while jointly
achieving an $N$-fold convergence speedup under Markovian sampling. Our work is
relevant for various SA applications, including multi-agent and distributed
temporal difference (TD) learning, Q-learning and stochastic optimization with
correlated data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TwoStep: Multi-agent Task Planning using Classical Planners and Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ishika Singh, David Traum, Jesse Thomason
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classical planning formulations like the Planning Domain Definition Language
(PDDL) admit action sequences guaranteed to achieve a goal state given an
initial state if any are possible. However, reasoning problems defined in PDDL
do not capture temporal aspects of action taking, for example that two agents
in the domain can execute an action simultaneously if postconditions of each do
not interfere with preconditions of the other. A human expert can decompose a
goal into largely independent constituent parts and assign each agent to one of
these subgoals to take advantage of simultaneous actions for faster execution
of plan steps, each using only single agent planning. By contrast, large
language models (LLMs) used for directly inferring plan steps do not guarantee
execution success, but do leverage commonsense reasoning to assemble action
sequences. We combine the strengths of classical planning and LLMs by
approximating human intuitions for two-agent planning goal decomposition. We
demonstrate that LLM-based goal decomposition leads to faster planning times
than solving multi-agent PDDL problems directly while simultaneously achieving
fewer plan execution steps than a single agent plan alone and preserving
execution success. Additionally, we find that LLM-based approximations of
subgoals can achieve similar multi-agent execution steps than those specified
by human experts. Website and resources at https://glamor-usc.github.io/twostep
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal and Semantic Evaluation Metrics for Foundation Models in
  Post-Hoc Analysis of Robotic Sub-tasks <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Salfity, Selma Wanna, Minkyu Choi, Mitch Pryor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works in Task and Motion Planning (TAMP) show that training control
policies on language-supervised robot trajectories with quality labeled data
markedly improves agent task success rates. However, the scarcity of such data
presents a significant hurdle to extending these methods to general use cases.
To address this concern, we present an automated framework to decompose
trajectory data into temporally bounded and natural language-based descriptive
sub-tasks by leveraging recent prompting strategies for Foundation Models (FMs)
including both Large Language Models (LLMs) and Vision Language Models (VLMs).
Our framework provides both time-based and language-based descriptions for
lower-level sub-tasks that comprise full trajectories. To rigorously evaluate
the quality of our automatic labeling framework, we contribute an algorithm
SIMILARITY to produce two novel metrics, temporal similarity and semantic
similarity. The metrics measure the temporal alignment and semantic fidelity of
language descriptions between two sub-task decompositions, namely an FM
sub-task decomposition prediction and a ground-truth sub-task decomposition. We
present scores for temporal similarity and semantic similarity above 90%,
compared to 30% of a randomized baseline, for multiple robotic environments,
demonstrating the effectiveness of our proposed framework. Our results enable
building diverse, large-scale, language-supervised datasets for improved
robotic TAMP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures. IROS 2024 Submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Speeding Up Path Planning via Reinforcement Learning in MCTS for
  Automated Parking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17234v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17234v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinlong Zheng, Xiaozhou Zhang, Donghao Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address a method that integrates reinforcement learning
into the Monte Carlo tree search to boost online path planning under fully
observable environments for automated parking tasks. Sampling-based planning
methods under high-dimensional space can be computationally expensive and
time-consuming. State evaluation methods are useful by leveraging the prior
knowledge into the search steps, making the process faster in a real-time
system. Given the fact that automated parking tasks are often executed under
complex environments, a solid but lightweight heuristic guidance is challenging
to compose in a traditional analytical way. To overcome this limitation, we
propose a reinforcement learning pipeline with a Monte Carlo tree search under
the path planning framework. By iteratively learning the value of a state and
the best action among samples from its previous cycle's outcomes, we are able
to model a value estimator and a policy generator for given states. By doing
that, we build up a balancing mechanism between exploration and exploitation,
speeding up the path planning process while maintaining its quality without
using human expert driver data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PROSPECT: Precision Robot Spectroscopy Exploration and Characterization
  Tool 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17232v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17232v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathaniel Hanson, Gary Lvov, Vedant Rautela, Samuel Hibbard, Ethan Holand, Charles DiMarzio, Taşkın Padır
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Near Infrared (NIR) spectroscopy is widely used in industrial quality control
and automation to test the purity and material quality of items. In this
research, we propose a novel sensorized end effector and acquisition strategy
to capture spectral signatures from objects and register them with a 3D point
cloud. Our methodology first takes a 3D scan of an object generated by a
time-of-flight depth camera and decomposes the object into a series of planned
viewpoints covering the surface. We generate motion plans for a robot
manipulator and end-effector to visit these viewpoints while maintaining a
fixed distance and surface normal to ensure maximal spectral signal quality
enabled by the spherical motion of the end-effector. By continuously acquiring
surface reflectance values as the end-effector scans the target object, the
autonomous system develops a four-dimensional model of the target object:
position in an R^3 coordinate frame, and a wavelength vector denoting the
associated spectral signature. We demonstrate this system in building
spectral-spatial object profiles of increasingly complex geometries. As a point
of comparison, we show our proposed system and spectral acquisition planning
yields more consistent signal signals than naive point scanning strategies for
capturing spectral information over complex surface geometries. Our work
represents a significant step towards high-resolution spectral-spatial sensor
fusion for automated quality assessment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dyna-LfLH: Learning Agile Navigation in Dynamic Environments from
  Learned Hallucination <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17231v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17231v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saad Abdul Ghani, Zizhao Wang, Peter Stone, Xuesu Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a self-supervised learning method to safely learn a
motion planner for ground robots to navigate environments with dense and
dynamic obstacles. When facing highly-cluttered, fast-moving, hard-to-predict
obstacles, classical motion planners may not be able to keep up with limited
onboard computation. For learning-based planners, high-quality demonstrations
are difficult to acquire for imitation learning while reinforcement learning
becomes inefficient due to the high probability of collision during
exploration. To safely and efficiently provide training data, the Learning from
Hallucination (LfH) approaches synthesize difficult navigation environments
based on past successful navigation experiences in relatively easy or
completely open ones, but unfortunately cannot address dynamic obstacles. In
our new Dynamic Learning from Learned Hallucination (Dyna-LfLH), we design and
learn a novel latent distribution and sample dynamic obstacles from it, so the
generated training data can be used to learn a motion planner to navigate in
dynamic environments. Dyna-LfLH is evaluated on a ground robot in both
simulated and physical environments and achieves up to 25% better success rate
compared to baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to International Conference on Intelligent Robots and
  Systems (IROS) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Contact Inertial Estimation and Localization in Legged Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17161v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17161v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sergi Martinez, Robert Griffin, Carlos Mastalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimal estimation is a promising tool for multi-contact inertial estimation
and localization. To harness its advantages in robotics, it is crucial to solve
these large and challenging optimization problems efficiently. To tackle this,
we (i) develop a multiple-shooting solver that exploits both temporal and
parametric structures through a parametrized Riccati recursion. Additionally,
we (ii) propose an inertial local manifold that ensures its full physical
consistency. It also enhances convergence compared to the singularity-free
log-Cholesky approach. To handle its singularities, we (iii) introduce a
nullspace approach in our optimal estimation solver. We (iv) finally develop
the analytical derivatives of contact dynamics for both inertial
parametrizations. Our framework can successfully solve estimation problems for
complex maneuvers such as brachiation in humanoids. We demonstrate its
numerical capabilities across various robotics tasks and its benefits in
experimental trials with the Go1 robot.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hearing the shape of an arena with spectral swarm robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17147v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17147v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leo Cazenille, Nicolas Lobato-Dauzier, Alessia Loi, Mika Ito, Olivier Marchal, Nathanael Aubert-Kato, Nicolas Bredeche, Anthony J. Genot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Swarm robotics promises adaptability to unknown situations and robustness
against failures. However, it still struggles with global tasks that require
understanding the broader context in which the robots operate, such as
identifying the shape of the arena in which the robots are embedded. Biological
swarms, such as shoals of fish, flocks of birds, and colonies of insects,
routinely solve global geometrical problems through the diffusion of local
cues. This paradigm can be explicitly described by mathematical models that
could be directly computed and exploited by a robotic swarm. Diffusion over a
domain is mathematically encapsulated by the Laplacian, a linear operator that
measures the local curvature of a function. Crucially the geometry of a domain
can generally be reconstructed from the eigenspectrum of its Laplacian. Here we
introduce spectral swarm robotics where robots diffuse information to their
neighbors to emulate the Laplacian operator - enabling them to "hear" the
spectrum of their arena. We reveal a universal scaling that links the optimal
number of robots (a global parameter) with their optimal radius of interaction
(a local parameter). We validate experimentally spectral swarm robotics under
challenging conditions with the one-shot classification of arena shapes using a
sparse swarm of Kilobots. Spectral methods can assist with challenging tasks
where robots need to build an emergent consensus on their environment, such as
adaptation to unknown terrains, division of labor, or quorum sensing. Spectral
methods may extend beyond robotics to analyze and coordinate swarms of agents
of various natures, such as traffic or crowds, and to better understand the
long-range dynamics of natural systems emerging from short-range interactions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Step Duration for Precise Foot Placement: Achieving Robust
  Bipedal Locomotion on Terrains with Restricted Footholds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17136v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17136v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyang Xiang, Victor Paredes, Ayonga Hereid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel multi-step preview foot placement planning
algorithm designed to enhance the robustness of bipedal robotic walking across
challenging terrains with restricted footholds. Traditional one-step preview
planning struggles to maintain stability when stepping areas are severely
limited, such as with random stepping stones. In this work, we developed a
discrete-time Model Predictive Control (MPC) based on the step-to-step discrete
evolution of the Divergent Component of Motion (DCM) of bipedal locomotion.
This approach adaptively changes the step duration for optimal foot placement
under constraints, thereby ensuring the robot's operational viability over
multiple future steps and significantly improving its ability to navigate
through environments with tight constraints on possible footholds. The
effectiveness of this planning algorithm is demonstrated through simulations
that include a variety of complex stepping-stone configurations and external
perturbations. These tests underscore the algorithm's improved performance for
navigating foothold-restricted environments, even with the presence of external
disturbances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures, submitted to CDC 2024, for associated simulation
  video, see https://youtu.be/2jhikPlZmbE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grounding Language Plans in Demonstrations Through Counterfactual
  Perturbations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17124v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17124v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanwei Wang, Tsun-Hsuan Wang, Jiayuan Mao, Michael Hagenow, Julie Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grounding the common-sense reasoning of Large Language Models in physical
domains remains a pivotal yet unsolved problem for embodied AI. Whereas prior
works have focused on leveraging LLMs directly for planning in symbolic spaces,
this work uses LLMs to guide the search of task structures and constraints
implicit in multi-step demonstrations. Specifically, we borrow from
manipulation planning literature the concept of mode families, which group
robot configurations by specific motion constraints, to serve as an abstraction
layer between the high-level language representations of an LLM and the
low-level physical trajectories of a robot. By replaying a few human
demonstrations with synthetic perturbations, we generate coverage over the
demonstrations' state space with additional successful executions as well as
counterfactuals that fail the task. Our explanation-based learning framework
trains an end-to-end differentiable neural network to predict successful
trajectories from failures and as a by-product learns classifiers that ground
low-level states and images in mode families without dense labeling. The
learned grounding classifiers can further be used to translate language plans
into reactive policies in the physical domain in an interpretable manner. We
show our approach improves the interpretability and reactivity of imitation
learning through 2D navigation and simulated and real robot manipulation tasks.
Website: https://sites.google.com/view/grounding-plans
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision-Based Dexterous Motion Planning by Dynamic Movement Primitives
  with Human Hand Demonstration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17111v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17111v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nuo Chen, Ya-Jun Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a vision-based framework for a 7-degree-of-freedom
robotic manipulator, with the primary objective of facilitating its capacity to
acquire information from human hand demonstrations for the execution of
dexterous pick-and-place tasks. Most existing works only focus on the position
demonstration without considering the orientations. In this paper, by employing
a single depth camera, MediaPipe is applied to generate the three-dimensional
coordinates of a human hand, thereby comprehensively recording the hand's
motion, encompassing the trajectory of the wrist, orientation of the hand, and
the grasp motion. A mean filter is applied during data pre-processing to smooth
the raw data. The demonstration is designed to pick up an object at a specific
angle, navigate around obstacles in its path and subsequently, deposit it
within a sloped container. The robotic system demonstrates its learning
capabilities, facilitated by the implementation of Dynamic Movement Primitives,
enabling the assimilation of user actions into its trajectories with different
start and end poi
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Berry Twist: a Twisting-Tube Soft Robotic Gripper for Blackberry
  Harvesting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes F. Elfferich, Ebrahim Shahabi, Cosimo Della Santina, Dimitra Dodou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As global demand for fruits and vegetables continues to rise, the
agricultural industry faces challenges in securing adequate labor. Robotic
harvesting devices offer a promising solution to solve this issue. However,
harvesting delicate fruits, notably blackberries, poses unique challenges due
to their fragility. This study introduces and evaluates a prototype robotic
gripper specifically designed for blackberry harvesting. The gripper features
an innovative fabric tube mechanism employing motorized twisting action to
gently envelop the fruit, ensuring uniform pressure application and minimizing
damage. Three types of tubes were developed, varying in elasticity and
compressibility using foam padding, spandex, and food-safe cotton cheesecloth.
Performance testing focused on assessing each gripper's ability to detach and
release blackberries, with emphasis on quantifying damage rates. Results
indicate the proposed gripper achieved an 82% success rate in detaching
blackberries and a 95% success rate in releasing them, showcasing the promised
potential for robotic harvesting applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comparative Analysis of Visual Odometry in Virtual and Real-World
  Railways Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17084v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17084v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gianluca D'Amico, Mauro Marinoni, Giorgio Buttazzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perception tasks play a crucial role in the development of automated
operations and systems across multiple application fields. In the railway
transportation domain, these tasks can improve the safety, reliability, and
efficiency of various perations, including train localization, signal
recognition, and track discrimination. However, collecting considerable and
precisely labeled datasets for testing such novel algorithms poses extreme
challenges in the railway environment due to the severe restrictions in
accessing the infrastructures and the practical difficulties associated with
properly equipping trains with the required sensors, such as cameras and
LiDARs. The remarkable innovations of graphic engine tools offer new solutions
to craft realistic synthetic datasets. To illustrate the advantages of
employing graphic simulation for early-stage testing of perception tasks in the
railway domain, this paper presents a comparative analysis of the performance
of a SLAM algorithm applied both in a virtual synthetic environment and a
real-world scenario. The analysis leverages virtual railway environments
created with the latest version of Unreal Engine, facilitating data collection
and allowing the examination of challenging scenarios, including
low-visibility, dangerous operational modes, and complex environments. The
results highlight the feasibility and potentiality of graphic simulation to
advance perception tasks in the railway domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trajectory Optimization with Global Yaw Parameterization for
  Field-of-View Constrained Autonomous Flight 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuwei Wu, Yuezhan Tao, Igor Spasojevic, Vijay Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory generation for quadrotors with limited field-of-view sensors has
numerous applications such as aerial exploration, coverage, inspection,
videography, and target tracking. Most previous works simplify the task of
optimizing yaw trajectories by either aligning the heading of the robot with
its velocity, or potentially restricting the feasible space of candidate
trajectories by using a limited yaw domain to circumvent angular singularities.
In this paper, we propose a novel \textit{global} yaw parameterization method
for trajectory optimization that allows a 360-degree yaw variation as demanded
by the underlying algorithm. This approach effectively bypasses inherent
singularities by including supplementary quadratic constraints and transforming
the final decision variables into the desired state representation. This method
significantly reduces the needed control effort, and improves optimization
feasibility. Furthermore, we apply the method to several examples of different
applications that require jointly optimizing over both the yaw and position
trajectories. Ultimately, we present a comprehensive numerical analysis and
evaluation of our proposed method in both simulation and real-world
experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Calib3D: Calibrating Model Preferences for Reliable 3D Scene
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingdong Kong, Xiang Xu, Jun Cen, Wenwei Zhang, Liang Pan, Kai Chen, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safety-critical 3D scene understanding tasks necessitate not only accurate
but also confident predictions from 3D perception models. This study introduces
Calib3D, a pioneering effort to benchmark and scrutinize the reliability of 3D
scene understanding models from an uncertainty estimation viewpoint. We
comprehensively evaluate 28 state-of-the-art models across 10 diverse 3D
datasets, uncovering insightful phenomena that cope with both the aleatoric and
epistemic uncertainties in 3D scene understanding. We discover that despite
achieving impressive levels of accuracy, existing models frequently fail to
provide reliable uncertainty estimates -- a pitfall that critically undermines
their applicability in safety-sensitive contexts. Through extensive analysis of
key factors such as network capacity, LiDAR representations, rasterization
resolutions, and 3D data augmentation techniques, we correlate these aspects
directly with the model calibration efficacy. Furthermore, we introduce DeptS,
a novel depth-aware scaling approach aimed at enhancing 3D model calibration.
Extensive experiments across a wide range of configurations validate the
superiority of our method. We hope this work could serve as a cornerstone for
fostering reliable 3D scene understanding. Code and benchmark toolkits are
publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint; 37 pages, 8 figures, 11 tables; Code at
  https://github.com/ldkong1205/Calib3D</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing LiDAR Placements for Robust Driving Perception in Adverse
  Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Li, Lingdong Kong, Hanjiang Hu, Xiaohao Xu, Xiaonan Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The robustness of driving perception systems under unprecedented conditions
is crucial for safety-critical usages. Latest advancements have prompted
increasing interests towards multi-LiDAR perception. However, prevailing
driving datasets predominantly utilize single-LiDAR systems and collect data
devoid of adverse conditions, failing to capture the complexities of real-world
environments accurately. Addressing these gaps, we proposed Place3D, a
full-cycle pipeline that encompasses LiDAR placement optimization, data
generation, and downstream evaluations. Our framework makes three appealing
contributions. 1) To identify the most effective configurations for multi-LiDAR
systems, we introduce a Surrogate Metric of the Semantic Occupancy Grids
(M-SOG) to evaluate LiDAR placement quality. 2) Leveraging the M-SOG metric, we
propose a novel optimization strategy to refine multi-LiDAR placements. 3)
Centered around the theme of multi-condition multi-LiDAR perception, we collect
a 364,000-frame dataset from both clean and adverse conditions. Extensive
experiments demonstrate that LiDAR placements optimized using our approach
outperform various baselines. We showcase exceptional robustness in both 3D
object detection and LiDAR semantic segmentation tasks, under diverse adverse
weather and sensor failure conditions. Code and benchmark toolkit are publicly
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint; 40 pages, 11 figures, 15 tables; Code at
  https://github.com/ywyeli/Place3D</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DriveCoT: Integrating Chain-of-Thought Reasoning with End-to-End Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16996v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16996v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianqi Wang, Enze Xie, Ruihang Chu, Zhenguo Li, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end driving has made significant progress in recent years,
demonstrating benefits such as system simplicity and competitive driving
performance under both open-loop and closed-loop settings. Nevertheless, the
lack of interpretability and controllability in its driving decisions hinders
real-world deployment for end-to-end driving systems. In this paper, we collect
a comprehensive end-to-end driving dataset named DriveCoT, leveraging the CARLA
simulator. It contains sensor data, control decisions, and chain-of-thought
labels to indicate the reasoning process. We utilize the challenging driving
scenarios from the CARLA leaderboard 2.0, which involve high-speed driving and
lane-changing, and propose a rule-based expert policy to control the vehicle
and generate ground truth labels for its reasoning process across different
driving aspects and the final decisions. This dataset can serve as an open-loop
end-to-end driving benchmark, enabling the evaluation of accuracy in various
chain-of-thought aspects and the final decision. In addition, we propose a
baseline model called DriveCoT-Agent, trained on our dataset, to generate
chain-of-thought predictions and final decisions. The trained model exhibits
strong performance in both open-loop and closed-loop evaluations, demonstrating
the effectiveness of our proposed dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian Methods for Trust in Collaborative Multi-Agent Autonomy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        R. Spencer Hallyburton, Miroslav Pajic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent, collaborative sensor fusion is a vital component of a
multi-national intelligence toolkit. In safety-critical and/or contested
environments, adversaries may infiltrate and compromise a number of agents. We
analyze state of the art multi-target tracking algorithms under this
compromised agent threat model. We prove that the track existence probability
test ("track score") is significantly vulnerable to even small numbers of
adversaries. To add security awareness, we design a trust estimation framework
using hierarchical Bayesian updating. Our framework builds beliefs of trust on
tracks and agents by mapping sensor measurements to trust pseudomeasurements
(PSMs) and incorporating prior trust beliefs in a Bayesian context. In case
studies, our trust estimation algorithm accurately estimates the
trustworthiness of tracks/agents, subject to observability limitations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Symbolic and Subsymbolic Temporal Task Constraints from
  Bimanual Human Demonstrations <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16953v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16953v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Dreher, Tamim Asfour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning task models of bimanual manipulation from human demonstration and
their execution on a robot should take temporal constraints between actions
into account. This includes constraints on (i) the symbolic level such as
precedence relations or temporal overlap in the execution, and (ii) the
subsymbolic level such as the duration of different actions, or their starting
and end points in time. Such temporal constraints are crucial for temporal
planning, reasoning, and the exact timing for the execution of bimanual actions
on a bimanual robot. In our previous work, we addressed the learning of
temporal task constraints on the symbolic level and demonstrated how a robot
can leverage this knowledge to respond to failures during execution. In this
work, we propose a novel model-driven approach for the combined learning of
symbolic and subsymbolic temporal task constraints from multiple bimanual human
demonstrations. Our main contributions are a subsymbolic foundation of a
temporal task model that describes temporal nexuses of actions in the task
based on distributions of temporal differences between semantic action
keypoints, as well as a method based on fuzzy logic to derive symbolic temporal
task constraints from this representation. This complements our previous work
on learning comprehensive temporal task models by integrating symbolic and
subsymbolic information based on a subsymbolic foundation, while still
maintaining the symbolic expressiveness of our previous approach. We compare
our proposed approach with our previous pure-symbolic approach and show that we
can reproduce and even outperform it. Additionally, we show how the subsymbolic
temporal task constraints can synchronize otherwise unimanual movement
primitives for bimanual behavior on a humanoid robot.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, submitted to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DHP-Mapping: A Dense Panoptic Mapping System with Hierarchical World
  Representation and Label Optimization Techniques <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16880v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16880v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianshuai Hu, Jianhao Jiao, Yucheng Xu, Hongji Liu, Sheng Wang, Ming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Maps provide robots with crucial environmental knowledge, thereby enabling
them to perform interactive tasks effectively. Easily accessing accurate
abstract-to-detailed geometric and semantic concepts from maps is crucial for
robots to make informed and efficient decisions. To comprehensively model the
environment and effectively manage the map data structure, we propose
DHP-Mapping, a dense mapping system that utilizes multiple Truncated Signed
Distance Field (TSDF) submaps and panoptic labels to hierarchically model the
environment. The output map is able to maintain both voxel- and submap-level
metric and semantic information. Two modules are presented to enhance the
mapping efficiency and label consistency: (1) an inter-submaps label fusion
strategy to eliminate duplicate points across submaps and (2) a conditional
random field (CRF) based approach to enhance panoptic labels through object
label comprehension and contextual information. We conducted experiments with
two public datasets including indoor and outdoor scenarios. Our system performs
comparably to state-of-the-art (SOTA) methods across geometry and label
accuracy evaluation metrics. The experiment results highlight the effectiveness
and scalability of our system, as it is capable of constructing precise
geometry and maintaining consistent panoptic labels. Our code is publicly
available at https://github.com/hutslib/DHP-Mapping.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submit to IROS 2024. Project website
  https://github.com/hutslib/DHP-Mapping</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Proprioception Is All You Need: Terrain Classification for Boreal
  Forests <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16877v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16877v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Damien LaRocque, William Guimont-Martin, David-Alexandre Duclos, Philippe Giguère, François Pomerleau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works in field robotics highlighted the importance of resiliency
against different types of terrains. Boreal forests, in particular, are home to
many mobility-impeding terrains that should be considered for off-road
autonomous navigation. Also, being one of the largest land biomes on Earth,
boreal forests are an area where autonomous vehicles are expected to become
increasingly common. In this paper, we address this issue by introducing
BorealTC, a publicly available dataset for proprioceptive-based terrain
classification (TC). Recorded with a Husky A200, our dataset contains 116 min
of Inertial Measurement Unit (IMU), motor current, and wheel odometry data,
focusing on typical boreal forest terrains, notably snow, ice, and silty loam.
Combining our dataset with another dataset from the state-of-the-art, we
evaluate both a Convolutional Neural Network (CNN) and the novel state space
model (SSM)-based Mamba architecture on a TC task. Interestingly, we show that
while CNN outperforms Mamba on each separate dataset, Mamba achieves greater
accuracy when trained on a combination of both. In addition, we demonstrate
that Mamba's learning capacity is greater than a CNN for increasing amounts of
data. We show that the combination of two TC datasets yields a latent space
that can be interpreted with the properties of the terrains. We also discuss
the implications of merging datasets on classification. Our source code and
dataset are publicly available online:
https://github.com/norlab-ulaval/BorealTC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the 2024 IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TAIL: A Terrain-Aware Multi-Modal SLAM <span class="highlight-title">Dataset</span> for Robot Locomotion in
  Deformable Granular Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16875v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16875v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Yao, Yangtao Ge, Guowei Shi, Zirui Wang, Ningbo Yang, Zheng Zhu, Hexiang Wei, Yuntian Zhao, Jing Wu, Zhenzhong Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Terrain-aware perception holds the potential to improve the robustness and
accuracy of autonomous robot navigation in the wilds, thereby facilitating
effective off-road traversals. However, the lack of multi-modal perception
across various motion patterns hinders the solutions of Simultaneous
Localization And Mapping (SLAM), especially when confronting non-geometric
hazards in demanding landscapes. In this paper, we first propose a
Terrain-Aware multI-modaL (TAIL) dataset tailored to deformable and sandy
terrains. It incorporates various types of robotic proprioception and distinct
ground interactions for the unique challenges and benchmark of multi-sensor
fusion SLAM. The versatile sensor suite comprises stereo frame cameras,
multiple ground-pointing RGB-D cameras, a rotating 3D LiDAR, an IMU, and an RTK
device. This ensemble is hardware-synchronized, well-calibrated, and
self-contained. Utilizing both wheeled and quadrupedal locomotion, we
efficiently collect comprehensive sequences to capture rich unstructured
scenarios. It spans the spectrum of scope, terrain interactions, scene changes,
ground-level properties, and dynamic robot characteristics. We benchmark
several state-of-the-art SLAM methods against ground truth and provide
performance validations. Corresponding challenges and limitations are also
reported. All associated resources are accessible upon request at
\url{https://tailrobot.github.io/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Robotics and Automation Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Semi-Lagrangian Approach for Time and Energy Path Planning
  Optimization in Static Flow Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Víctor C. da S. Campos, Armando A. Neto, Douglas G. Macharet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient path planning for autonomous mobile robots is a critical problem
across numerous domains, where optimizing both time and energy consumption is
paramount. This paper introduces a novel methodology that considers the dynamic
influence of an environmental flow field and considers geometric constraints,
including obstacles and forbidden zones, enriching the complexity of the
planning problem. We formulate it as a multi-objective optimal control problem,
propose a novel transformation called Harmonic Transformation, and apply a
semi-Lagrangian scheme to solve it. The set of Pareto efficient solutions is
obtained considering two distinct approaches: a deterministic method and an
evolutionary-based one, both of which are designed to make use of the proposed
Harmonic Transformation. Through an extensive analysis of these approaches, we
demonstrate their efficacy in finding optimized paths.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, initial paper submission; Preprint submitted to the IEEE
  Transactions on Intelligent Transportation Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProIn: Learning to Predict Trajectory Based on Progressive Interactions
  for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinke Dong, Haifeng Yuan, Hongkun Liu, Wei Jing, Fangzhen Li, Hongmin Liu, Bin Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate motion prediction of pedestrians, cyclists, and other surrounding
vehicles (all called agents) is very important for autonomous driving. Most
existing works capture map information through an one-stage interaction with
map by vector-based attention, to provide map constraints for social
interaction and multi-modal differentiation. However, these methods have to
encode all required map rules into the focal agent's feature, so as to retain
all possible intentions' paths while at the meantime to adapt to potential
social interaction. In this work, a progressive interaction network is proposed
to enable the agent's feature to progressively focus on relevant maps, in order
to better learn agents' feature representation capturing the relevant map
constraints. The network progressively encode the complex influence of map
constraints into the agent's feature through graph convolutions at the
following three stages: after historical trajectory encoder, after social
interaction, and after multi-modal differentiation. In addition, a weight
allocation mechanism is proposed for multi-modal training, so that each mode
can obtain learning opportunities from a single-mode ground truth. Experiments
have validated the superiority of progressive interactions to the existing
one-stage interaction, and demonstrate the effectiveness of each component.
Encouraging results were obtained in the challenging benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Model Predictive Control for Legged Robots through
  Distributed Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11742v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11742v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Amatucci, Giulio Turrisi, Angelo Bratta, Victor Barasuol, Claudio Semini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach to enhance Model Predictive Control
(MPC) for legged robots through Distributed Optimization. Our method focuses on
decomposing the robot dynamics into smaller, parallelizable subsystems, and
utilizing the Alternating Direction Method of Multipliers (ADMM) to ensure
consensus among them. Each subsystem is managed by its own Optimal Control
Problem, with ADMM facilitating consistency between their optimizations. This
approach not only decreases the computational time but also allows for
effective scaling with more complex robot configurations, facilitating the
integration of additional subsystems such as articulated arms on a quadruped
robot. We demonstrate, through numerical evaluations, the convergence of our
approach on two systems with increasing complexity. In addition, we showcase
that our approach converges towards the same solution when compared to a
state-of-the-art centralized whole-body MPC implementation. Moreover, we
quantitatively compare the computational efficiency of our method to the
centralized approach, revealing up to a 75\% reduction in computational time.
Overall, our approach offers a promising avenue for accelerating MPC solutions
for legged robots, paving the way for more effective utilization of the
computational performance of modern hardware.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Precise 3D Human Pose Estimation with Multi-Perspective
  Spatial-Temporal Relational <span class="highlight-title">Transformer</span>s <span class="chip">IJCNN 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.16700v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.16700v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianbin Jiao, Xina Cheng, Weijie Chen, Xiaoting Yin, Hao Shi, Kailun Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D human pose estimation captures the human joint points in three-dimensional
space while keeping the depth information and physical structure. That is
essential for applications that require precise pose information, such as
human-computer interaction, scene understanding, and rehabilitation training.
Due to the challenges in data collection, mainstream datasets of 3D human pose
estimation are primarily composed of multi-view video data collected in
laboratory environments, which contains rich spatial-temporal correlation
information besides the image frame content. Given the remarkable
self-attention mechanism of transformers, capable of capturing the
spatial-temporal correlation from multi-view video datasets, we propose a
multi-stage framework for 3D sequence-to-sequence (seq2seq) human pose
detection. Firstly, the spatial module represents the human pose feature by
intra-image content, while the frame-image relation module extracts temporal
relationships and 3D spatial positional relationship features between the
multi-perspective images. Secondly, the self-attention mechanism is adopted to
eliminate the interference from non-human body parts and reduce computing
resources. Our method is evaluated on Human3.6M, a popular 3D human pose
detection dataset. Experimental results demonstrate that our approach achieves
state-of-the-art performance on this dataset. The source code will be available
at https://github.com/WUJINHUAN/3D-human-pose.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IJCNN 2024. The source code will be available at
  https://github.com/WUJINHUAN/3D-human-pose</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement
  Learning with Diverse Human Feedback <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02423v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02423v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifu Yuan, Jianye Hao, Yi Ma, Zibin Dong, Hebin Liang, Jinyi Liu, Zhixin Feng, Kai Zhao, Yan Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning with Human Feedback (RLHF) has received significant
attention for performing tasks without the need for costly manual reward design
by aligning human preferences. It is crucial to consider diverse human feedback
types and various learning methods in different environments. However,
quantifying progress in RLHF with diverse feedback is challenging due to the
lack of standardized annotation platforms and widely used unified benchmarks.
To bridge this gap, we introduce Uni-RLHF, a comprehensive system
implementation tailored for RLHF. It aims to provide a complete workflow from
real human feedback, fostering progress in the development of practical
problems. Uni-RLHF contains three packages: 1) a universal multi-feedback
annotation platform, 2) large-scale crowdsourced feedback datasets, and 3)
modular offline RLHF baseline implementations. Uni-RLHF develops a
user-friendly annotation interface tailored to various feedback types,
compatible with a wide range of mainstream RL environments. We then establish a
systematic pipeline of crowdsourced annotations, resulting in large-scale
annotated datasets comprising more than 15 million steps across 30+ popular
tasks. Through extensive experiments, the results in the collected datasets
demonstrate competitive performance compared to those from well-designed manual
rewards. We evaluate various design choices and offer insights into their
strengths and potential areas of improvement. We wish to build valuable
open-source platforms, datasets, and baselines to facilitate the development of
more robust and reliable RLHF solutions based on realistic human feedback. The
website is available at https://uni-rlhf.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2024. The website is
  available at https://uni-rlhf.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Modular Pneumatic Soft Gripper Design for Aerial Grasping and Landing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.00390v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.00390v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hiu Ching Cheung, Ching-Wei Chang, Bailun Jiang, Chih-Yung Wen, Henry K. Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aerial robots have garnered significant attention due to their potential
applications in various industries, such as inspection, search and rescue, and
drone delivery. Successful missions often depend on the ability of these robots
to grasp and land effectively. This paper presents a novel modular soft gripper
design tailored explicitly for aerial grasping and landing operations. The
proposed modular pneumatic soft gripper incorporates a feed-forward
proportional controller to regulate pressure, enabling compliant gripping
capabilities. The modular connectors of the soft fingers offer two
configurations for the 4-tip soft gripper, H-base (cylindrical) and X-base
(spherical), allowing adaptability to different target objects. Additionally,
the gripper can serve as a soft landing gear when deflated, eliminating the
need for an extra landing gear. This design reduces weight, simplifies aerial
manipulation control, and enhances flight efficiency. We demonstrate the
efficacy of indoor aerial grasping and achieve a maximum payload of 217 g using
the proposed soft aerial vehicle and its H-base pneumatic soft gripper (808 g).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 13 figures, accepted by IEEE RoboSoft 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Integral Consensus Control of Multi-Agent Networks Perturbed by
  Matched and Unmatched Disturbances: The Case of Directed Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jose Guadalupe Romero, David Navarro-Alarcon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents a new method to design consensus controllers for perturbed
double integrator systems whose interconnection is described by a directed
graph containing a rooted spanning tree. We propose new robust controllers to
solve the consensus and synchronization problems when the systems are under the
effects of matched and unmatched disturbances. In both problems, we present
simple continuous controllers, whose integral actions allow us to handle the
disturbances. A rigorous stability analysis based on Lyapunov's direct method
for unperturbed networked systems is presented. To assess the performance of
our result, a representative simulation study is presented.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Group Theoretic Metric for Robot State Estimation Leveraging Chebyshev
  Interpolation <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.17463v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.17463v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Varun Agrawal, Frank Dellaert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new metric for robot state estimation based on the recently
introduced $\text{SE}_2(3)$ Lie group definition. Our metric is related to
prior metrics for SLAM but explicitly takes into account the linear velocity of
the state estimate, improving over current pose-based trajectory analysis. This
has the benefit of providing a single, quantitative metric to evaluate state
estimation algorithms against, while being compatible with existing tools and
libraries. Since ground truth data generally consists of pose data from motion
capture systems, we also propose an approach to compute the ground truth linear
velocity based on polynomial interpolation. Using Chebyshev interpolation and a
pseudospectral parameterization, we can accurately estimate the ground truth
linear velocity of the trajectory in an optimal fashion with best approximation
error. We demonstrate how this approach performs on multiple robotic platforms
where accurate state estimation is vital, and compare it to alternative
approaches such as finite differences. The pseudospectral parameterization also
provides a means of trajectory data compression as an additional benefit.
Experimental results show our method provides a valid and accurate means of
comparing state estimation systems, which is also easy to interpret and report.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ I-PHYRE: Interactive Physical Reasoning <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03009v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03009v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiqian Li, Kewen Wu, Chi Zhang, Yixin Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current evaluation protocols predominantly assess physical reasoning in
stationary scenes, creating a gap in evaluating agents' abilities to interact
with dynamic events. While contemporary methods allow agents to modify initial
scene configurations and observe consequences, they lack the capability to
interact with events in real time. To address this, we introduce I-PHYRE, a
framework that challenges agents to simultaneously exhibit intuitive physical
reasoning, multi-step planning, and in-situ intervention. Here, intuitive
physical reasoning refers to a quick, approximate understanding of physics to
address complex problems; multi-step denotes the need for extensive sequence
planning in I-PHYRE, considering each intervention can significantly alter
subsequent choices; and in-situ implies the necessity for timely object
manipulation within a scene, where minor timing deviations can result in task
failure. We formulate four game splits to scrutinize agents' learning and
generalization of essential principles of interactive physical reasoning,
fostering learning through interaction with representative scenarios. Our
exploration involves three planning strategies and examines several supervised
and reinforcement agents' zero-shot generalization proficiency on I-PHYRE. The
outcomes highlight a notable gap between existing learning algorithms and human
performance, emphasizing the imperative for more research in enhancing agents
with interactive physical reasoning capabilities. The environment and baselines
will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via
  Vision-Language Foundation Models <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10670v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10670v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Kuang, Hai Lin, Meng Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object navigation (ObjectNav) requires an agent to navigate through unseen
environments to find queried objects. Many previous methods attempted to solve
this task by relying on supervised or reinforcement learning, where they are
trained on limited household datasets with close-set objects. However, two key
challenges are unsolved: understanding free-form natural language instructions
that demand open-set objects, and generalizing to new environments in a
zero-shot manner. Aiming to solve the two challenges, in this paper, we propose
OpenFMNav, an Open-set Foundation Model based framework for zero-shot object
Navigation. We first unleash the reasoning abilities of large language models
(LLMs) to extract proposed objects from natural language instructions that meet
the user's demand. We then leverage the generalizability of large vision
language models (VLMs) to actively discover and detect candidate objects from
the scene, building a Versatile Semantic Score Map (VSSM). Then, by conducting
common sense reasoning on VSSM, our method can perform effective
language-guided exploration and exploitation of the scene and finally reach the
goal. By leveraging the reasoning and generalizing abilities of foundation
models, our method can understand free-form human instructions and perform
effective open-set zero-shot navigation in diverse environments. Extensive
experiments on the HM3D ObjectNav benchmark show that our method surpasses all
the strong baselines on all metrics, proving our method's effectiveness.
Furthermore, we perform real robot demonstrations to validate our method's
open-set-ness and generalizability to real-world environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Directionality-Aware Mixture Model Parallel Sampling for Efficient
  Linear Parameter Varying Dynamical System Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.02609v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.02609v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunan Sun, Haihui Gao, Tianyu Li, Nadia Figueroa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Linear Parameter Varying Dynamical System (LPV-DS) is an effective
approach that learns stable, time-invariant motion policies using statistical
modeling and semi-definite optimization to encode complex motions for reactive
robot control. Despite its strengths, the LPV-DS learning approach faces
challenges in achieving a high model accuracy without compromising the
computational efficiency. To address this, we introduce the
Directionality-Aware Mixture Model (DAMM), a novel statistical model that
applies the Riemannian metric on the n-sphere $\mathbb{S}^n$ to efficiently
blend non-Euclidean directional data with $\mathbb{R}^m$ Euclidean states.
Additionally, we develop a hybrid Markov chain Monte Carlo technique that
combines Gibbs Sampling with Split/Merge Proposal, allowing for parallel
computation to drastically speed up inference. Our extensive empirical tests
demonstrate that LPV-DS integrated with DAMM achieves higher reproduction
accuracy, better model efficiency, and near real-time/online learning compared
to standard estimation methods on various datasets. Lastly, we demonstrate its
suitability for incrementally learning multi-behavior policies in real-world
robot experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DTG : Diffusion-based Trajectory Generation for Mapless Global
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09900v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09900v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Liang, Amirreza Payandeh, Daeun Song, Xuesu Xiao, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel end-to-end diffusion-based trajectory generation method,
DTG, for mapless global navigation in challenging outdoor scenarios with
occlusions and unstructured off-road features like grass, buildings, bushes,
etc. Given a distant goal, our approach computes a trajectory that satisfies
the following goals: (1) minimize the travel distance to the goal; (2) maximize
the traversability by choosing paths that do not lie in undesirable areas.
Specifically, we present a novel Conditional RNN(CRNN) for diffusion models to
efficiently generate trajectories. Furthermore, we propose an adaptive training
method that ensures that the diffusion model generates more traversable
trajectories. We evaluate our methods in various outdoor scenes and compare the
performance with other global navigation algorithms on a Husky robot. In
practice, we observe at least a 15% improvement in traveling distance and
around a 7% improvement in traversability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GelLink: A Compact Multi-phalanx Finger with Vision-based Tactile
  Sensing and Proprioception <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14887v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14887v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Ma, Jialiang Zhao, Edward Adelson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compared to fully-actuated robotic end-effectors, underactuated ones are
generally more adaptive, robust, and cost-effective. However, state estimation
for underactuated hands is usually more challenging. Vision-based tactile
sensors, like Gelsight, can mitigate this issue by providing high-resolution
tactile sensing and accurate proprioceptive sensing. As such, we present
GelLink, a compact, underactuated, linkage-driven robotic finger with low-cost,
high-resolution vision-based tactile sensing and proprioceptive sensing
capabilities. In order to reduce the amount of embedded hardware, i.e. the
cameras and motors, we optimize the linkage transmission with a planar linkage
mechanism simulator and develop a planar reflection simulator to simplify the
tactile sensing hardware. As a result, GelLink only requires one motor to
actuate the three phalanges, and one camera to capture tactile signals along
the entire finger. Overall, GelLink is a compact robotic finger that shows
adaptability and robustness when performing grasping tasks. The integration of
vision-based tactile sensors can significantly enhance the capabilities of
underactuated fingers and potentially broaden their future usage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Supplement video: https://www.youtube.com/watch?v=hZwUpAig5C0 . 7
  pages, 9 figures. ICRA 2024 (IEEE International Conference on Robotics and
  Automation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast LiDAR Informed Visual Search in Unseen Indoor Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14150v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14150v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Gupta, Kyle Morgenstein, Steven Ortega, Luis Sentis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the problem of planning for visual search without prior
map information. We leverage the pixel-wise environment perception problem
where one is given wide Field of View 2D scan data and must perform LiDAR
segmentation to contextually label points in the surroundings. These pixel
classifications provide an informed prior on which to plan next best viewpoints
during visual search tasks. We present LIVES: LiDAR Informed Visual Search, a
method aimed at finding objects of interest in unknown indoor environments. A
robust map-free classifier is trained from expert data collected using a simple
cart platform equipped with a map-based classifier. An autonomous exploration
planner takes the contextual data from scans and uses that prior to plan
viewpoints more likely to yield detection of the search target. We propose a
utility function that accounts for traditional metrics like information gain
and path cost and for the contextual information. LIVES is baselined against
several existing exploration methods in simulation to verify its performance.
It is validated in real-world experiments with single and multiple search
objects with a Spot robot in two unseen environments. Videos of experiments,
implementation details and open source code can be found at
https://sites.google.com/view/lives-2024/home.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages + references. 6 figures. 1 algorithm. 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diagrammatic Instructions to Specify Spatial Objectives and Constraints
  with Applications to Mobile Base Placement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12465v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12465v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qilin Sun, Weiming Zhi, Tianyi Zhang, Matthew Johnson-Roberson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Spatial Diagrammatic Instructions (SDIs), an approach
for human operators to specify objectives and constraints that are related to
spatial regions in the working environment. Human operators are enabled to
sketch out regions directly on camera images that correspond to the objectives
and constraints. These sketches are projected to 3D spatial coordinates, and
continuous Spatial Instruction Maps (SIMs) are learned upon them. These maps
can then be integrated into optimization problems for tasks of robots. In
particular, we demonstrate how Spatial Diagrammatic Instructions can be applied
to solve the Base Placement Problem of mobile manipulators, which concerns the
best place to put the manipulator to facilitate a certain task. Human operators
can specify, via sketch, spatial regions of interest for a manipulation task
and permissible regions for the mobile manipulator to be at. Then, an
optimization problem that maximizes the manipulator's reachability, or
coverage, over the designated regions of interest while remaining in the
permissible regions is solved. We provide extensive empirical evaluations, and
show that our formulation of Spatial Instruction Maps provides accurate
representations of user-specified diagrammatic instructions. Furthermore, we
demonstrate that our diagrammatic approach to the Mobile Base Placement Problem
enables higher quality solutions and faster run-time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Greedy Perspectives: Multi-Drone View Planning for Collaborative
  Perception in Cluttered Environments <span class="chip">IROS'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10863v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10863v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krishna Suresh, Aditya Rauniyar, Micah Corah, Sebastian Scherer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deployment of teams of aerial robots could enable large-scale filming of
dynamic groups of people (actors) in complex environments for applications in
areas such as team sports and cinematography. Toward this end, methods for
submodular maximization via sequential greedy planning can be used for scalable
optimization of camera views across teams of robots but face challenges with
efficient coordination in cluttered environments. Obstacles can produce
occlusions and increase chances of inter-robot collision which can violate
requirements for near-optimality guarantees. To coordinate teams of aerial
robots in filming groups of people in dense environments, a more general
view-planning approach is required. We explore how collision and occlusion
impact performance in filming applications through the development of a
multi-robot multi-actor view planner with an occlusion-aware objective for
filming groups of people and compare with a formation planner and a greedy
planner that ignores inter-robot collisions. We evaluate our approach based on
five test environments and complex multi-actor behaviors. Compared with a
formation planner, our sequential planner generates 14% greater view reward
over the actors for three scenarios and comparable performance to formation
planning on two others. We also observe near identical view rewards for
sequential planning both with and without inter-robot collision constraints
which indicates that robots are able to avoid collisions without impairing
performance in the perception task. Overall, we demonstrate effective
coordination of teams of aerial robots for filming groups that may split,
merge, or spread apart and in environments cluttered with obstacles that may
cause collisions or occlusions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS'24; 8 pages, 8 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing Crowd-Aware Multi-Agent Path Finding through Local
  Broadcasting with Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10275v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10275v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phu Pham, Aniket Bera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Agent Path Finding (MAPF) in crowded environments presents a
challenging problem in motion planning, aiming to find collision-free paths for
all agents in the system. MAPF finds a wide range of applications in various
domains, including aerial swarms, autonomous warehouse robotics, and
self-driving vehicles. Current approaches to MAPF generally fall into two main
categories: centralized and decentralized planning. Centralized planning
suffers from the curse of dimensionality when the number of agents or states
increases and thus does not scale well in large and complex environments. On
the other hand, decentralized planning enables agents to engage in real-time
path planning within a partially observable environment, demonstrating implicit
coordination. However, they suffer from slow convergence and performance
degradation in dense environments. In this paper, we introduce CRAMP, a novel
crowd-aware decentralized reinforcement learning approach to address this
problem by enabling efficient local communication among agents via Graph Neural
Networks (GNNs), facilitating situational awareness and decision-making
capabilities in congested environments. We test CRAMP on simulated environments
and demonstrate that our method outperforms the state-of-the-art decentralized
methods for MAPF on various metrics. CRAMP improves the solution quality up to
59% measured in makespan and collision count, and up to 35% improvement in
success rate in comparison to previous methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SO(2)-Equivariant Downwash Models for Close Proximity Flight 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18983v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18983v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        H. Smith, A. Shankar, J. Gielis, J. Blumenkamp, A. Prorok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multirotors flying in close proximity induce aerodynamic wake effects on each
other through propeller downwash. Conventional methods have fallen short of
providing adequate 3D force-based models that can be incorporated into robust
control paradigms for deploying dense formations. Thus, learning a model for
these downwash patterns presents an attractive solution. In this paper, we
present a novel learning-based approach for modelling the downwash forces that
exploits the latent geometries (i.e. symmetries) present in the problem. We
demonstrate that when trained with only 5 minutes of real-world flight data,
our geometry-aware model outperforms state-of-the-art baseline models trained
with more than 15 minutes of data. In dense real-world flights with two
vehicles, deploying our model online improves 3D trajectory tracking by nearly
36% on average (and vertical tracking by 56%).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decision-Oriented Learning Using Differentiable Submodular Maximization
  for Multi-Robot Coordination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01519v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01519v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyao Shi, Chak Lam Shek, Nare Karapetyan, Pratap Tokekar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a differentiable, decision-oriented learning framework for cost
prediction in a class of multi-robot decision-making problems, in which the
robots need to trade off the task performance with the costs of taking actions
when they select actions to take. Specifically, we consider the cases where the
task performance is measured by a known monotone submodular function (e.g.,
coverage, mutual information), and the cost of actions depends on the context
(e.g., wind and terrain conditions). We need to learn a function that maps the
context to the costs. Classically, we treat such a learning problem and the
downstream decision-making problem as two decoupled problems, i.e., we first
learn to predict the cost function without considering the downstream
decision-making problem, and then use the learned function for predicting the
cost and using it in the decision-making problem. However, the loss function
used in learning a prediction function may not be aligned with the downstream
decision-making. We propose a decision-oriented learning framework that
incorporates the downstream task performance in the prediction phase via a
differentiable optimization layer. The main computational challenge in such a
framework is to make the combinatorial optimization, i.e., non-monotone
submodular maximization, differentiable. This function is not naturally
differentiable. We propose the Differentiable Cost Scaled Greedy algorithm
(D-CSG), which is a continuous and differentiable relaxation of CSG. We
demonstrate the efficacy of the proposed framework through numerical
simulations. The results show that the proposed framework can result in better
performance than the traditional two-stage approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2303.01543</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Pre-Train</span>ed Masked Image Model for Mobile Robot Navigation <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07021v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07021v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishnu Dutt Sharma, Anukriti Singh, Pratap Tokekar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  2D top-down maps are commonly used for the navigation and exploration of
mobile robots through unknown areas. Typically, the robot builds the navigation
maps incrementally from local observations using onboard sensors. Recent works
have shown that predicting the structural patterns in the environment through
learning-based approaches can greatly enhance task efficiency. While many such
works build task-specific networks using limited datasets, we show that the
existing foundational vision networks can accomplish the same without any
fine-tuning. Specifically, we use Masked Autoencoders, pre-trained on street
images, to present novel applications for field-of-view expansion, single-agent
topological exploration, and multi-agent exploration for indoor mapping, across
different input modalities. Our work motivates the use of foundational vision
models for generalized structure prediction-driven applications, especially in
the dearth of training data. For more qualitative results see
https://raaslab.org/projects/MIM4Robots.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ C3D: Cascade Control with Change Point Detection and Deep Koopman
  Learning for Autonomous Surface Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05972v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05972v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianwen Li, Hyunsang Park, Wenjian Hao, Lei Xin, Jalil Chavez-Galaviz, Ajinkya Chaudhary, Meredith Bloss, Kyle Pattison, Christopher Vo, Devesh Upadhyay, Shreyas Sundaram, Shaoshuai Mou, Nina Mahmoudian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we discuss the development and deployment of a robust
autonomous system capable of performing various tasks in the maritime domain
under unknown dynamic conditions. We investigate a data-driven approach based
on modular design for ease of transfer of autonomy across different maritime
surface vessel platforms. The data-driven approach alleviates issues related to
a priori identification of system models that may become deficient under
evolving system behaviors or shifting, unanticipated, environmental influences.
Our proposed learning-based platform comprises a deep Koopman system model and
a change point detector that provides guidance on domain shifts prompting
relearning under severe exogenous and endogenous perturbations. Motion control
of the autonomous system is achieved via an optimal controller design. The
Koopman linearized model naturally lends itself to a linear-quadratic regulator
(LQR) control design. We propose the C3D control architecture Cascade Control
with Change Point Detection and Deep Koopman Learning. The framework is
verified in station keeping task on an ASV in both simulation and real
experiments. The approach achieved at least 13.9 percent improvement in mean
distance error in all test cases compared to the methods that do not consider
system changes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Feedback Law in Stochastic Optimal Nonlinear Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2004.01041v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2004.01041v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Naveed Gul Mohamed, Suman Chakravorty, Raman Goyal, Ran Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of nonlinear stochastic optimal control. This problem
is thought to be fundamentally intractable owing to Bellman's ``curse of
dimensionality". We present a result that shows that repeatedly solving an
open-loop deterministic problem from the current state with progressively
shorter horizons, similar to Model Predictive Control (MPC), results in a
feedback policy that is $O(\epsilon^4)$ near to the true global stochastic
optimal policy, \nxx{where $\epsilon$ is a perturbation parameter modulating
the noise.} We show that the optimal deterministic feedback problem has a
perturbation structure in that higher-order terms of the feedback law do not
affect lower-order terms, and that this structure is lost in the optimal
stochastic feedback problem. Consequently, solving the Stochastic Dynamic
Programming problem is highly susceptible to noise, even when tractable, and in
practice, the MPC-type feedback law offers superior performance even for
stochastic systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2002.10505,
  arXiv:2002.09478</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Body-mounted MR-conditional Robot for Minimally Invasive Liver
  Intervention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07822v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07822v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhefeng Huang, Anthony L. Gunderman, Samuel E. Wilcox, Saikat Sengupta, Jay Shah, Aiming Lu, David Woodrum, Yue Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  MR-guided microwave ablation (MWA) has proven effective in treating
hepatocellular carcinoma (HCC) with small-sized tumors, but the
state-of-the-art technique suffers from sub-optimal workflow due to speed and
accuracy of needle placement. This paper presents a compact body-mounted
MR-conditional robot that can operate in closed-bore MR scanners for accurate
needle guidance. The robotic platform consists of two stacked Cartesian XY
stages, each with two degrees of freedom, that facilitate needle guidance. The
robot is actuated using 3D-printed pneumatic turbines with MR-conditional bevel
gear transmission systems. Pneumatic valves and control mechatronics are
located inside the MRI control room and are connected to the robot with
pneumatic transmission lines and optical fibers. Free space experiments
indicated robot-assisted needle insertion error of 2.6$\pm$1.3 mm at an
insertion depth of 80 mm. The MR-guided phantom studies were conducted to
verify the MR-conditionality and targeting performance of the robot. Future
work will focus on the system optimization and validations in animal trials.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mind Meets Robots: A <span class="highlight-title">Review</span> of EEG-Based Brain-Robot Interaction Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06186v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06186v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchong Zhang, Nona Rajabi, Farzaneh Taleb, Andrii Matviienko, Yong Ma, Mårten Björkman, Danica Kragic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain-robot interaction (BRI) empowers individuals to control
(semi-)automated machines through their brain activity, either passively or
actively. In the past decade, BRI systems have achieved remarkable success,
predominantly harnessing electroencephalogram (EEG) signals as the central
component. This paper offers an up-to-date and exhaustive examination of 87
curated studies published during the last five years (2018-2023), focusing on
identifying the research landscape of EEG-based BRI systems. This review aims
to consolidate and underscore methodologies, interaction modes, application
contexts, system evaluation, existing challenges, and potential avenues for
future investigations in this domain. Based on our analysis, we present a BRI
system model with three entities: Brain, Robot, and Interaction, depicting the
internal relationships of a BRI system. We especially investigate the essence
and principles on interaction modes between human brains and robots, a domain
that has not yet been identified anywhere. We then discuss these entities with
different dimensions encompassed. Within this model, we scrutinize and classify
current research, reveal insights, specify challenges, and provide
recommendations for future research trajectories in this field. Meanwhile, we
envision our findings offer a design space for future human-robot interaction
(HRI) research, informing the creation of efficient BRI frameworks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reinforcement Learning with Options and State Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10855v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10855v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayoub Ghriss, Masashi Sugiyama, Alessandro Lazaric
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current thesis aims to explore the reinforcement learning field and build
on existing methods to produce improved ones to tackle the problem of learning
in high-dimensional and complex environments. It addresses such goals by
decomposing learning tasks in a hierarchical fashion known as Hierarchical
Reinforcement Learning.
  We start in the first chapter by getting familiar with the Markov Decision
Process framework and presenting some of its recent techniques that the
following chapters use. We then proceed to build our Hierarchical Policy
learning as an answer to the limitations of a single primitive policy. The
hierarchy is composed of a manager agent at the top and employee agents at the
lower level.
  In the last chapter, which is the core of this thesis, we attempt to learn
lower-level elements of the hierarchy independently of the manager level in
what is known as the "Eigenoption". Based on the graph structure of the
environment, Eigenoptions allow us to build agents that are aware of the
geometric and dynamic properties of the environment. Their decision-making has
a special property: it is invariant to symmetric transformations of the
environment, allowing as a consequence to greatly reduce the complexity of the
learning task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Master Thesis 2018, MVA ENS Paris-Saclay, Tokyo RIKEN AIP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-BEV: Zero-shot Projection of Any First-Person Modality to BEV Maps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13848v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13848v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gianluca Monaci, Leonid Antsfeld, Boris Chidlovskii, Christian Wolf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bird's-eye view (BEV) maps are an important geometrically structured
representation widely used in robotics, in particular self-driving vehicles and
terrestrial robots. Existing algorithms either require depth information for
the geometric projection, which is not always reliably available, or are
trained end-to-end in a fully supervised way to map visual first-person
observations to BEV representation, and are therefore restricted to the output
modality they have been trained for. In contrast, we propose a new model
capable of performing zero-shot projections of any modality available in a
first person view to the corresponding BEV map. This is achieved by
disentangling the geometric inverse perspective projection from the modality
transformation, eg. RGB to occupancy. The method is general and we showcase
experiments projecting to BEV three different modalities: semantic
segmentation, motion vectors and object bounding boxes detected in first
person. We experimentally show that the model outperforms competing methods, in
particular the widely used baseline resorting to monocular depth estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vehicle Trajectory Tracking Through Magnetic Sensors: A Case Study of
  Two-lane Road 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.09020v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.09020v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaojiang Ren, Yan Wang, Yingfan Geng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intelligent Transportation Systems (ITS) have a pressing need for efficient
and reliable traffic surveillance solutions. This paper for the first time
proposes a surveillance system that utilizes low-cost magnetic sensors for
detecting and tracking vehicles continuously along the road. The system uses
multiple sensors mounted along the roadside and lane boundaries to capture the
movement of vehicles. Real-time measurement data is collected by base stations
and processed to produce vehicle trajectories that include position, timestamp,
and speed. To address the challenge of tracking vehicles continuously on a road
network using a large amount of unlabeled magnetic sensor measurements, we
first define a vehicle trajectory tracking problem. We then propose a
graph-based data association algorithm to track each detected vehicle, and
design a related online algorithm framework respectively. We finally validate
the performance via both experimental simulation and real-world road
deployment. The experimental results demonstrate that the proposed solution
provides a cost-effective solution to capture the driving status of vehicles
and on that basis form various traffic safety and efficiency applications.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-24T00:00:00Z">2024-03-24</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">24</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single-Motor Robotic Gripper with Multi-Surface Fingers for Variable
  Grasping Configurations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toshihiro Nishimura, Yosuke Suzuki, Tokuo Tsuj, Tetsuyou Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study proposes a novel robotic gripper with variable grasping
configurations for grasping various objects. The fingers of the developed
gripper incorporate multiple different surfaces. The gripper possesses the
function of altering the finger surfaces facing a target object by rotating the
fingers in its longitudinal direction. In the proposed design equipped with two
fingers, the two fingers incorporate three and four surfaces, respectively,
resulting in the nine available grasping configurations by the combination of
these finger surfaces. The developed gripper is equipped with the functions of
opening/closing its fingers for grasping and rotating its fingers to alter the
grasping configuration -all achieved with a single motor. To enable the two
motions using a single motor, this study introduces a self-motion switching
mechanism utilizing magnets. This mechanism automatically transitions between
gripper motions based on the direction of the motor rotation when the gripper
is fully opened. In this state, rotating the motor towards closing initiates
the finger closing action, while further opening the fingers from the fully
opened state activates the finger rotation. This letter presents the gripper
design, the mechanics of the self-motion switching mechanism, the control
method, and the grasping configuration selection strategy. The performance of
the gripper is experimentally demonstrated.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combined Task and Motion Planning Via Sketch Decompositions (Extended
  Version with Supplementary Material) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16277v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16277v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Magí Dalmau-Moreno, Néstor García, Vicenç Gómez, Héctor Geffner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The challenge in combined task and motion planning (TAMP) is the effective
integration of a search over a combinatorial space, usually carried out by a
task planner, and a search over a continuous configuration space, carried out
by a motion planner. Using motion planners for testing the feasibility of task
plans and filling out the details is not effective because it makes the
geometrical constraints play a passive role. This work introduces a new
interleaved approach for integrating the two dimensions of TAMP that makes use
of sketches, a recent simple but powerful language for expressing the
decomposition of problems into subproblems. A sketch has width 1 if it
decomposes the problem into subproblems that can be solved greedily in linear
time. In the paper, a general sketch is introduced for several classes of TAMP
problems which has width 1 under suitable assumptions. While sketch
decompositions have been developed for classical planning, they offer two
important benefits in the context of TAMP. First, when a task plan is found to
be unfeasible due to the geometric constraints, the combinatorial search
resumes in a specific sub-problem. Second, the sampling of object
configurations is not done once, globally, at the start of the search, but
locally, at the start of each subproblem. Optimizations of this basic setting
are also considered and experimental results over existing and new
pick-and-place benchmarks are reported.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M^3RS: Multi-robot, Multi-objective, and Multi-mode Routing and
  Scheduling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ishaan Mehta, Junseo Kim, Sharareh Taghipour, Sajad Saeedi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a novel problem coined multi-robot,
multi-objective, and multi-mode routing and scheduling (M^3RS). The formulation
for M^3RS is introduced for time-bound multi-robot, multi-objective routing and
scheduling missions where each task has multiple execution modes. Different
execution modes have distinct resource consumption, associated execution time,
and quality. M^3RS assigns the optimal sequence of tasks and the execution
modes to each agent. The routes and associated modes depend on user preferences
for different objective criteria. The need for M^3RS comes from multi-robot
applications in which a trade-off between multiple criteria arises from
different task execution modes. We use M^3RS for the application of multi-robot
disinfection in public locations. The objectives considered for disinfection
application are disinfection quality and number of tasks completed. A
mixed-integer linear programming model is proposed for M^3RS. Then, a
time-efficient column generation scheme is presented to tackle the issue of
computation times for larger problem instances. The advantage of using multiple
modes over fixed execution mode is demonstrated using experiments on synthetic
data. The results suggest that M^3RS provides flexibility to the user in terms
of available solutions and performs well in joint performance metrics. The
application of the proposed problem is shown for a team of disinfection
robots.} The videos for the experiments are available on the project website:
https://sites.google.com/view/g-robot/m3rs/ .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HT-LIP Model based Robust Control of Quadrupedal Robot Locomotion under
  Unknown Vertical Ground Motion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Iqbal, Sushant Veer, Christopher Niezrecki, Yan Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a hierarchical control framework that enables robust
quadrupedal locomotion on a dynamic rigid surface (DRS) with general and
unknown vertical motions. The key novelty of the framework lies in its higher
layer, which is a discrete-time, provably stabilizing footstep controller. The
basis of the footstep controller is a new hybrid, time-varying, linear inverted
pendulum (HT-LIP) model that is low-dimensional and accurately captures the
essential robot dynamics during DRS locomotion. A new set of sufficient
stability conditions are then derived to directly guide the controller design
for ensuring the asymptotic stability of the HT-LIP model under general,
unknown, vertical DRS motions. Further, the footstep controller is cast as a
computationally efficient quadratic program that incorporates the proposed
HT-LIP model and stability conditions. The middle layer takes the desired
footstep locations generated by the higher layer as input to produce
kinematically feasible full-body reference trajectories, which are then
accurately tracked by a lower-layer torque controller. Hardware experiments on
a Unitree Go1 quadrupedal robot confirm the robustness of the proposed
framework under various unknown, aperiodic, vertical DRS motions and
uncertainties (e.g., slippery and uneven surfaces, solid and liquid loads, and
sudden pushes).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Legged Robot State Estimation within Non-inertial Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian He, Sangli Teng, Tzu-Yuan Lin, Maani Ghaffari, Yan Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the robot state estimation problem within a
non-inertial environment. The proposed state estimation approach relaxes the
common assumption of static ground in the system modeling. The process and
measurement models explicitly treat the movement of the non-inertial
environments without requiring knowledge of its motion in the inertial frame or
relying on GPS or sensing environmental landmarks. Further, the proposed state
estimator is formulated as an invariant extended Kalman filter (InEKF) with the
deterministic part of its process model obeying the group-affine property,
leading to log-linear error dynamics. The observability analysis of the filter
confirms that the robot's pose (i.e., position and orientation) and velocity
relative to the non-inertial environment are observable. Hardware experiments
on a humanoid robot moving on a rotating and translating treadmill demonstrate
the high convergence rate and accuracy of the proposed InEKF even under
significant treadmill pitch sway, as well as large estimation errors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KITchen: A Real-World Benchmark and <span class="highlight-title">Dataset</span> for 6D Object Pose
  Estimation in Kitchen Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrahman Younes, Tamim Asfour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the recent progress on 6D object pose estimation methods for robotic
grasping, a substantial performance gap persists between the capabilities of
these methods on existing datasets and their efficacy in real-world mobile
manipulation tasks, particularly when robots rely solely on their monocular
egocentric field of view (FOV). Existing real-world datasets primarily focus on
table-top grasping scenarios, where a robotic arm is placed in a fixed position
and the objects are centralized within the FOV of fixed external camera(s).
Assessing performance on such datasets may not accurately reflect the
challenges encountered in everyday mobile manipulation tasks within kitchen
environments such as retrieving objects from higher shelves, sinks,
dishwashers, ovens, refrigerators, or microwaves. To address this gap, we
present Kitchen, a novel benchmark designed specifically for estimating the 6D
poses of objects located in diverse positions within kitchen settings. For this
purpose, we recorded a comprehensive dataset comprising around 205k real-world
RGBD images for 111 kitchen objects captured in two distinct kitchens,
utilizing one humanoid robot with its egocentric perspectives. Subsequently, we
developed a semi-automated annotation pipeline, to streamline the labeling
process of such datasets, resulting in the generation of 2D object labels, 2D
object segmentation masks, and 6D object poses with minimized human effort. The
benchmark, the dataset, and the annotation pipeline are available at
https://kitchen-dataset.github.io/KITchen.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mixed-Initiative Human-Robot Teaming under Suboptimality with Online
  Bayesian Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manisha Natarajan, Chunyue Xue, Sanne van Waveren, Karen Feigh, Matthew Gombolay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For effective human-agent teaming, robots and other artificial intelligence
(AI) agents must infer their human partner's abilities and behavioral response
patterns and adapt accordingly. Most prior works make the unrealistic
assumption that one or more teammates can act near-optimally. In real-world
collaboration, humans and autonomous agents can be suboptimal, especially when
each only has partial domain knowledge. In this work, we develop computational
modeling and optimization techniques for enhancing the performance of
suboptimal human-agent teams, where the human and the agent have asymmetric
capabilities and act suboptimally due to incomplete environmental knowledge. We
adopt an online Bayesian approach that enables a robot to infer people's
willingness to comply with its assistance in a sequential decision-making game.
Our user studies show that user preferences and team performance indeed vary
with robot intervention styles, and our approach for mixed-initiative
collaborations enhances objective team performance ($p<.001$) and subjective
measures, such as user's trust ($p<.001$) and perceived likeability of the
robot ($p<.001$).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 pages for supplementary</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Realtime Robust Shape Estimation of Deformable Linear Object <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaming Zhang, Zhaomeng Zhang, Yihao Liu, Yaqian Chen, Amir Kheradmand, Mehran Armand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Realtime shape estimation of continuum objects and manipulators is essential
for developing accurate planning and control paradigms. The existing methods
that create dense point clouds from camera images, and/or use distinguishable
markers on a deformable body have limitations in realtime tracking of large
continuum objects/manipulators. The physical occlusion of markers can often
compromise accurate shape estimation. We propose a robust method to estimate
the shape of linear deformable objects in realtime using scattered and
unordered key points. By utilizing a robust probability-based labeling
algorithm, our approach identifies the true order of the detected key points
and then reconstructs the shape using piecewise spline interpolation. The
approach only relies on knowing the number of the key points and the interval
between two neighboring points. We demonstrate the robustness of the method
when key points are partially occluded. The proposed method is also integrated
into a simulation in Unity for tracking the shape of a cable with a length of
1m and a radius of 5mm. The simulation results show that our proposed approach
achieves an average length error of 1.07% over the continuum's centerline and
an average cross-section error of 2.11mm. The real-world experiments of
tracking and estimating a heavy-load cable prove that the proposed approach is
robust under occlusion and complex entanglement scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted to IEEE ICRA 2024 as a contributed paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D
  Gaussian Field 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarui Hu, Xianhao Chen, Boyin Feng, Guanglin Li, Liangjing Yang, Hujun Bao, Guofeng Zhang, Zhaopeng Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently neural radiance fields (NeRF) have been widely exploited as 3D
representations for dense simultaneous localization and mapping (SLAM). Despite
their notable successes in surface modeling and novel view synthesis, existing
NeRF-based methods are hindered by their computationally intensive and
time-consuming volume rendering pipeline. This paper presents an efficient
dense RGB-D SLAM system, i.e., CG-SLAM, based on a novel uncertainty-aware 3D
Gaussian field with high consistency and geometric stability. Through an
in-depth analysis of Gaussian Splatting, we propose several techniques to
construct a consistent and stable 3D Gaussian field suitable for tracking and
mapping. Additionally, a novel depth uncertainty model is proposed to ensure
the selection of valuable Gaussian primitives during optimization, thereby
improving tracking efficiency and accuracy. Experiments on various datasets
demonstrate that CG-SLAM achieves superior tracking and mapping performance
with a notable tracking speed of up to 15 Hz. We will make our source code
publicly available. Project page: https://zju3dv.github.io/cg-slam.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://zju3dv.github.io/cg-slam</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are NeRFs ready for autonomous driving? Towards closing the
  real-to-simulation gap 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16092v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16092v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carl Lindström, Georg Hess, Adam Lilja, Maryam Fatemi, Lars Hammarstrand, Christoffer Petersson, Lennart Svensson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (NeRFs) have emerged as promising tools for advancing
autonomous driving (AD) research, offering scalable closed-loop simulation and
data augmentation capabilities. However, to trust the results achieved in
simulation, one needs to ensure that AD systems perceive real and rendered data
in the same way. Although the performance of rendering methods is increasing,
many scenarios will remain inherently challenging to reconstruct faithfully. To
this end, we propose a novel perspective for addressing the real-to-simulated
data gap. Rather than solely focusing on improving rendering fidelity, we
explore simple yet effective methods to enhance perception model robustness to
NeRF artifacts without compromising performance on real data. Moreover, we
conduct the first large-scale investigation into the real-to-simulated data gap
in an AD setting using a state-of-the-art neural rendering technique.
Specifically, we evaluate object detectors and an online mapping model on real
and simulated data, and study the effects of different pre-training strategies.
Our results show notable improvements in model robustness to simulated data,
even improving real-world performance in some cases. Last, we delve into the
correlation between the real-to-simulated gap and image reconstruction metrics,
identifying FID and LPIPS as strong indicators.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RPMArt: Towards Robust Perception and Manipulation for Articulated
  Objects <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16023v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16023v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junbo Wang, Wenhai Liu, Qiaojun Yu, Yang You, Liu Liu, Weiming Wang, Cewu Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Articulated objects are commonly found in daily life. It is essential that
robots can exhibit robust perception and manipulation skills for articulated
objects in real-world robotic applications. However, existing methods for
articulated objects insufficiently address noise in point clouds and struggle
to bridge the gap between simulation and reality, thus limiting the practical
deployment in real-world scenarios. To tackle these challenges, we propose a
framework towards Robust Perception and Manipulation for Articulated Objects
(RPMArt), which learns to estimate the articulation parameters and manipulate
the articulation part from the noisy point cloud. Our primary contribution is a
Robust Articulation Network (RoArtNet) that is able to predict both joint
parameters and affordable points robustly by local feature learning and point
tuple voting. Moreover, we introduce an articulation-aware classification
scheme to enhance its ability for sim-to-real transfer. Finally, with the
estimated affordable point and articulation joint constraint, the robot can
generate robust actions to manipulate articulated objects. After learning only
from synthetic data, RPMArt is able to transfer zero-shot to real-world
articulated objects. Experimental results confirm our approach's effectiveness,
with our framework achieving state-of-the-art performance in both noise-added
simulation and real-world environments. The code and data will be open-sourced
for reproduction. More results are published on the project website at
https://r-pmart.github.io .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, submitted to 2024 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS 2024), project website at
  https://r-pmart.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MQE: Unleashing the Power of Interaction with Multi-agent Quadruped
  Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16015v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16015v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyan Xiong, Bo Chen, Shiyu Huang, Wei-Wei Tu, Zhaofeng He, Yang Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of deep reinforcement learning (DRL) has significantly advanced
the field of robotics, particularly in the control and coordination of
quadruped robots. However, the complexity of real-world tasks often
necessitates the deployment of multi-robot systems capable of sophisticated
interaction and collaboration. To address this need, we introduce the
Multi-agent Quadruped Environment (MQE), a novel platform designed to
facilitate the development and evaluation of multi-agent reinforcement learning
(MARL) algorithms in realistic and dynamic scenarios. MQE emphasizes complex
interactions between robots and objects, hierarchical policy structures, and
challenging evaluation scenarios that reflect real-world applications. We
present a series of collaborative and competitive tasks within MQE, ranging
from simple coordination to complex adversarial interactions, and benchmark
state-of-the-art MARL algorithms. Our findings indicate that hierarchical
reinforcement learning can simplify task learning, but also highlight the need
for advanced algorithms capable of handling the intricate dynamics of
multi-agent interactions. MQE serves as a stepping stone towards bridging the
gap between simulation and practical deployment, offering a rich environment
for future research in multi-agent systems and robot learning. For open-sourced
code and more details of MQE, please refer to
https://ziyanx02.github.io/multiagent-quadruped-environment/ .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Open-source code is available at
  https://github.com/ziyanx02/multiagent-quadruped-environment</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust-Locomotion-by-Logic: Perturbation-Resilient Bipedal Locomotion
  via Signal Temporal Logic Guided Model Predictive Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15993v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15993v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyuan Gu, Yuntian Zhao, Yipu Chen, Rongming Guo, Jennifer K. Leestma, Gregory S. Sawicki, Ye Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces a robust planning framework that utilizes a model
predictive control (MPC) approach, enhanced by incorporating signal temporal
logic (STL) specifications. This marks the first-ever study to apply STL-guided
trajectory optimization for bipedal locomotion, specifically designed to handle
both translational and orientational perturbations. Existing recovery
strategies often struggle with reasoning complex task logic and evaluating
locomotion robustness systematically, making them susceptible to failures
caused by inappropriate recovery strategies or lack of robustness. To address
these issues, we design an analytical robustness metric for bipedal locomotion
and quantify this metric using STL specifications, which guide the generation
of recovery trajectories to achieve maximum locomotion robustness. To enable
safe and computational-efficient crossed-leg maneuver, we design data-driven
self-leg-collision constraints that are $1000$ times faster than the
traditional inverse-kinematics-based approach. Our framework outperforms a
state-of-the-art locomotion controller, a standard MPC without STL, and a
linear-temporal-logic-based planner in a high-fidelity dynamic simulation,
especially in scenarios involving crossed-leg maneuvers. Additionally, the
Cassie bipedal robot achieves robust performance under horizontal and
orientational perturbations such as those observed in ship motions. These
environments are validated in simulations and deployed on hardware.
Furthermore, our proposed method demonstrates versatility on stepping stones
and terrain-agnostic features on inclined terrains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Make a Donut: Hierarchical EMD-Space Planning for Zero-Shot Deformable
  Manipulation with Tools 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02787v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02787v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang You, Bokui Shen, Congyue Deng, Haoran Geng, Songlin Wei, He Wang, Leonidas Guibas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deformable object manipulation stands as one of the most captivating yet
formidable challenges in robotics. While previous techniques have predominantly
relied on learning latent dynamics through demonstrations, typically
represented as either particles or images, there exists a pertinent limitation:
acquiring suitable demonstrations, especially for long-horizon tasks, can be
elusive. Moreover, basing learning entirely on demonstrations can hamper the
model's ability to generalize beyond the demonstrated tasks. In this work, we
introduce a demonstration-free hierarchical planning approach capable of
tackling intricate long-horizon tasks without necessitating any training. We
employ large language models (LLMs) to articulate a high-level, stage-by-stage
plan corresponding to a specified task. For every individual stage, the LLM
provides both the tool's name and the Python code to craft intermediate subgoal
point clouds. With the tool and subgoal for a particular stage at our disposal,
we present a granular closed-loop model predictive control strategy. This
leverages Differentiable Physics with Point-to-Point correspondence
(DiffPhysics-P2P) loss in the earth mover distance (EMD) space, applied
iteratively. Experimental findings affirm that our technique surpasses multiple
benchmarks in dough manipulation, spanning both short and long horizons.
Remarkably, our model demonstrates robust generalization capabilities to novel
and previously unencountered complex tasks without any preliminary
demonstrations. We further substantiate our approach with experimental trials
on real-world robotic platforms. Our project page:
https://qq456cvb.github.io/projects/donut.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Development and Evaluation of a Learning-based Model for Real-time
  Haptic Texture Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.13332v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.13332v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Negin Heravi, Heather Culbertson, Allison M. Okamura, Jeannette Bohg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current Virtual Reality (VR) environments lack the rich haptic signals that
humans experience during real-life interactions, such as the sensation of
texture during lateral movement on a surface. Adding realistic haptic textures
to VR environments requires a model that generalizes to variations of a user's
interaction and to the wide variety of existing textures in the world. Current
methodologies for haptic texture rendering exist, but they usually develop one
model per texture, resulting in low scalability. We present a deep
learning-based action-conditional model for haptic texture rendering and
evaluate its perceptual performance in rendering realistic texture vibrations
through a multi part human user study. This model is unified over all materials
and uses data from a vision-based tactile sensor (GelSight) to render the
appropriate surface conditioned on the user's action in real time. For
rendering texture, we use a high-bandwidth vibrotactile transducer attached to
a 3D Systems Touch device. The result of our user study shows that our
learning-based method creates high-frequency texture renderings with comparable
or better quality than state-of-the-art methods without the need for learning a
separate model per texture. Furthermore, we show that the method is capable of
rendering previously unseen textures using a single GelSight image of their
surface.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in IEEE Transactions on Haptics 2024. 12
  pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Graphical Inverse Kinematics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.08812v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.08812v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oliver Limoyo, Filip Marić, Matthew Giamou, Petra Alexson, Ivan Petrović, Jonathan Kelly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quickly and reliably finding accurate inverse kinematics (IK) solutions
remains a challenging problem for many robot manipulators. Existing numerical
solvers are broadly applicable but typically only produce a single solution and
rely on local search techniques to minimize nonconvex objective functions. More
recent learning-based approaches that approximate the entire feasible set of
solutions have shown promise as a means to generate multiple fast and accurate
IK results in parallel. However, existing learning-based techniques have a
significant drawback: each robot of interest requires a specialized model that
must be trained from scratch. To address this key shortcoming, we propose a
novel distance-geometric robot representation coupled with a graph structure
that allows us to leverage the sample efficiency of Euclidean equivariant
functions and the generalizability of graph neural networks (GNNs). Our
approach is generative graphical inverse kinematics (GGIK), the first learned
IK solver able to accurately and efficiently produce a large number of diverse
solutions in parallel while also displaying the ability to generalize -- a
single learned model can be used to produce IK solutions for a variety of
different robots. When compared to several other learned IK methods, GGIK
provides more accurate solutions with the same amount of data. GGIK can
generalize reasonably well to robot manipulators unseen during training.
Additionally, GGIK can learn a constrained distribution that encodes joint
limits and scales efficiently to larger robots and a high number of sampled
solutions. Finally, GGIK can be used to complement local IK solvers by
providing reliable initializations for a local optimization process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Transactions on Robotics, June 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SG-Bot: Object Rearrangement via Coarse-to-Fine Robotic Imagination on
  Scene Graphs <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.12188v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.12188v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyao Zhai, Xiaoni Cai, Dianye Huang, Yan Di, Fabian Manhardt, Federico Tombari, Nassir Navab, Benjamin Busam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object rearrangement is pivotal in robotic-environment interactions,
representing a significant capability in embodied AI. In this paper, we present
SG-Bot, a novel rearrangement framework that utilizes a coarse-to-fine scheme
with a scene graph as the scene representation. Unlike previous methods that
rely on either known goal priors or zero-shot large models, SG-Bot exemplifies
lightweight, real-time, and user-controllable characteristics, seamlessly
blending the consideration of commonsense knowledge with automatic generation
capabilities. SG-Bot employs a three-fold procedure--observation, imagination,
and execution--to adeptly address the task. Initially, objects are discerned
and extracted from a cluttered scene during the observation. These objects are
first coarsely organized and depicted within a scene graph, guided by either
commonsense or user-defined criteria. Then, this scene graph subsequently
informs a generative model, which forms a fine-grained goal scene considering
the shape information from the initial scene and object semantics. Finally, for
execution, the initial and envisioned goal scenes are matched to formulate
robotic action policies. Experimental results demonstrate that SG-Bot
outperforms competitors by a large margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2024 accepted. Project website:
  https://sites.google.com/view/sg-bot</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EnduRL: Enhancing Safety, Stability, and Efficiency of Mixed Traffic
  Under Real-World Perturbations Via Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12261v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12261v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bibek Poudel, Weizi Li, Kevin Heaslip
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-driven vehicles (HVs) amplify naturally occurring perturbations in
traffic, leading to congestion--a major contributor to increased fuel
consumption, higher collision risks, and reduced road capacity utilization.
While previous research demonstrates that Robot Vehicles (RVs) can be leveraged
to mitigate these issues, most such studies rely on simulations with simplistic
models of human car-following behaviors. In this work, we analyze real-world
driving trajectories and extract a wide range of acceleration profiles. We then
incorporates these profiles into simulations for training RVs to mitigate
congestion. We evaluate the safety, efficiency, and stability of mixed traffic
via comprehensive experiments conducted in two mixed traffic environments (Ring
and Bottleneck) at various traffic densities, configurations, and RV
penetration rates. The results show that under real-world perturbations, prior
RV controllers experience performance degradation on all three objectives
(sometimes even lower than 100% HVs). To address this, we introduce a
reinforcement learning based RV that employs a congestion stage classifier to
optimize the safety, efficiency, and stability of mixed traffic. Our RVs
demonstrate significant improvements: safety by up to 66%, efficiency by up to
54%, and stability by up to 97%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Soft finger rotational stability for precision grasps <span class="chip">IROS24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04846v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04846v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hun Jang, Valentyn Petrichenko, Joonbum Bae, Kevin Haninger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Soft robotic fingers can safely grasp fragile or variable form objects, but
their force capacity is limited, especially with less contact area: precision
grasps and when objects are smaller or not spherical. Current research is
improving force capacity through mechanical design by increasing contact area
or stiffness, typically without models which explain soft finger force
limitations. To address this, this paper considers two types of soft grip
failure, slip and dynamic rotational stability. For slip, the validity of a
Coulomb model investigated, identifying the effect of contact area, pressure,
and relative pose. For rotational stability, bulk linear stiffness of the
fingers is used to develop conditions for dynamic stability and identify when
rotation leads to slip. Together, these models suggest contact area improves
force capacity by increasing transverse stiffness and normal force. The models
are validated on pneumatic fingers, both custom PneuNets-based and commercially
available. The models are used to find grip parameters which increase force
capacity without failure.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted IROS24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Combining Sampling- and Gradient-based Planning for Contact-rich
  Manipulation <span class="chip">ICRA24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04822v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04822v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filippo Rozzi, Loris Roveda, Kevin Haninger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Planning over discontinuous dynamics is needed for robotics tasks like
contact-rich manipulation, which presents challenges in the numerical stability
and speed of planning methods when either neural network or analytical models
are used. On the one hand, sampling-based planners require higher sample
complexity in high-dimensional problems and cannot describe safety constraints
such as force limits. On the other hand, gradient-based solvers can suffer from
local optima and convergence issues when the Hessian is poorly conditioned. We
propose a planning method with both sampling- and gradient-based elements,
using the Cross-entropy Method to initialize a gradient-based solver, providing
better search over local minima and the ability to handle explicit constraints.
We show the approach allows smooth, stable contact-rich planning for an
impedance-controlled robot making contact with a stiff environment,
benchmarking against gradient-only MPC and CEM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted ICRA24. Video available at https://youtu.be/COqR90392Kw
  Code available at https://gitlab.cc-asp.fraunhofer.de/hanikevi/contact_mpc</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SwarmPRM: Probabilistic Roadmap Motion Planning for Large-Scale Swarm
  Robotic Systems <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16699v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16699v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunze Hu, Xuru Yang, Kangjie Zhou, Qinghang Liu, Kang Ding, Han Gao, Pingping Zhu, Chang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale swarm robotic systems consisting of numerous cooperative agents
show considerable promise for performing autonomous tasks across various
sectors. Nonetheless, traditional motion planning approaches often face a
trade-off between scalability and solution quality due to the exponential
growth of the joint state space of robots. In response, this work proposes
SwarmPRM, a hierarchical, scalable, computationally efficient, and risk-aware
sampling-based motion planning approach for large-scale swarm robots. SwarmPRM
utilizes a Gaussian Mixture Model (GMM) to represent the swarm's macroscopic
state and constructs a Probabilistic Roadmap in Gaussian space, referred to as
the Gaussian roadmap, to generate a transport trajectory of GMM. This
trajectory is then followed by each robot at the microscopic stage. To enhance
trajectory safety, SwarmPRM incorporates the conditional value-at-risk (CVaR)
in the collision checking process to impart the property of risk awareness to
the constructed Gaussian roadmap. SwarmPRM then crafts a linear programming
formulation to compute the optimal GMM transport trajectory within this
roadmap. Extensive simulations demonstrate that SwarmPRM outperforms
state-of-the-art methods in computational efficiency, scalability, and
trajectory quality while offering the capability to adjust the risk tolerance
of generated trajectories.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DRL-Based Trajectory Tracking for Motion-Related Modules in Autonomous
  Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15991v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15991v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinda Xu, Lidong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving systems are always built on motion-related modules such as
the planner and the controller. An accurate and robust trajectory tracking
method is indispensable for these motion-related modules as a primitive
routine. Current methods often make strong assumptions about the model such as
the context and the dynamics, which are not robust enough to deal with the
changing scenarios in a real-world system. In this paper, we propose a Deep
Reinforcement Learning (DRL)-based trajectory tracking method for the
motion-related modules in autonomous driving systems. The representation
learning ability of DL and the exploration nature of RL bring strong robustness
and improve accuracy. Meanwhile, it enhances versatility by running the
trajectory tracking in a model-free and data-driven manner. Through extensive
experiments, we demonstrate both the efficiency and effectiveness of our method
compared to current methods. Code and documentation are released to facilitate
both further research and industrial deployment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report. Code:
  https://github.com/MARMOTatZJU/drl-based-trajectory-tracking Documentation:
  https://drl-based-trajectory-tracking.readthedocs.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Number Sense as an Emergent Property of the Manipulating Brain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.04132v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.04132v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neehar Kondapaneni, Pietro Perona
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to understand and manipulate numbers and quantities emerges
during childhood, but the mechanism through which humans acquire and develop
this ability is still poorly understood. We explore this question through a
model, assuming that the learner is able to pick up and place small objects
from, and to, locations of its choosing, and will spontaneously engage in such
undirected manipulation. We further assume that the learner's visual system
will monitor the changing arrangements of objects in the scene and will learn
to predict the effects of each action by comparing perception with a
supervisory signal from the motor system. We model perception using standard
deep networks for feature extraction and classification, and gradient descent
learning. Our main finding is that, from learning the task of action
prediction, an unexpected image representation emerges exhibiting regularities
that foreshadow the perception and representation of numbers and quantity.
These include distinct categories for zero and the first few natural numbers, a
strict ordering of the numbers, and a one-dimensional signal that correlates
with numerical quantity. As a result, our model acquires the ability to
estimate numerosity, i.e. the number of objects in the scene, as well as
subitization, i.e. the ability to recognize at a glance the exact number of
objects in small scenes. Remarkably, subitization and numerosity estimation
extrapolate to scenes containing many objects, far beyond the three objects
used during training. We conclude that important aspects of a facility with
numbers and quantities may be learned with supervision from a simple
pre-training task. Our observations suggest that cross-modal learning is a
powerful learning mechanism that may be harnessed in artificial intelligence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 5 figures, 15 supplemental figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Effective Integration of Weighted Cost-to-go and Conflict Heuristic
  within Suboptimal CBS <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.11624v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.11624v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rishi Veerapaneni, Tushar Kusnur, Maxim Likhachev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conflict-Based Search (CBS) is a popular multi-agent path finding (MAPF)
solver that employs a low-level single agent planner and a high-level
constraint tree to resolve conflicts. The vast majority of modern MAPF solvers
focus on improving CBS by reducing the size of this tree through various
strategies with few methods modifying the low level planner. Typically low
level planners in existing CBS methods use an unweighted cost-to-go heuristic,
with suboptimal CBS methods also using a conflict heuristic to help the high
level search. In this paper, we show that, contrary to prevailing CBS beliefs,
a weighted cost-to-go heuristic can be used effectively alongside the conflict
heuristic in two possible variants. In particular, one of these variants can
obtain large speedups, 2-100x, across several scenarios and suboptimal CBS
methods. Importantly, we discover that performance is related not to the
weighted cost-to-go heuristic but rather to the relative conflict heuristic
weight's ability to effectively balance low-level and high-level work.
Additionally, to the best of our knowledge, we show the first theoretical
relation of prioritized planning and bounded suboptimal CBS and demonstrate
that our methods are their natural generalization. Update March 2024: We found
that the relative speedup decreases to around 1.2-10x depending on how the
conflict heuristic is computed (see appendix for more details).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-23T00:00:00Z">2024-03-23</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">26</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Risk-Calibrated Human-Robot Interaction via Set-Valued Intent Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Justin Lidard, Hang Pham, Ariel Bachman, Bryan Boateng, Anirudha Majumdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tasks where robots must cooperate with humans, such as navigating around a
cluttered home or sorting everyday items, are challenging because they exhibit
a wide range of valid actions that lead to similar outcomes. Moreover,
zero-shot cooperation between human-robot partners is an especially challenging
problem because it requires the robot to infer and adapt on the fly to a latent
human intent, which could vary significantly from human to human. Recently,
deep learned motion prediction models have shown promising results in
predicting human intent but are prone to being confidently incorrect. In this
work, we present Risk-Calibrated Interactive Planning (RCIP), which is a
framework for measuring and calibrating risk associated with uncertain action
selection in human-robot cooperation, with the fundamental idea that the robot
should ask for human clarification when the risk associated with the
uncertainty in the human's intent cannot be controlled. RCIP builds on the
theory of set-valued risk calibration to provide a finite-sample statistical
guarantee on the cumulative loss incurred by the robot while minimizing the
cost of human clarification in complex multi-step settings. Our main insight is
to frame the risk control problem as a sequence-level multi-hypothesis testing
problem, allowing efficient calibration using a low-dimensional parameter that
controls a pre-trained risk-aware policy. Experiments across a variety of
simulated and real-world environments demonstrate RCIP's ability to predict and
adapt to a diverse set of dynamic human intents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website with additional information, videos, and code:
  https://risk-calibrated-planning.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explore until Confident: Efficient Exploration for Embodied Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15941v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15941v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Allen Z. Ren, Jaden Clark, Anushri Dixit, Masha Itkina, Anirudha Majumdar, Dorsa Sadigh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of Embodied Question Answering (EQA), which refers to
settings where an embodied agent such as a robot needs to actively explore an
environment to gather information until it is confident about the answer to a
question. In this work, we leverage the strong semantic reasoning capabilities
of large vision-language models (VLMs) to efficiently explore and answer such
questions. However, there are two main challenges when using VLMs in EQA: they
do not have an internal memory for mapping the scene to be able to plan how to
explore over time, and their confidence can be miscalibrated and can cause the
robot to prematurely stop exploration or over-explore. We propose a method that
first builds a semantic map of the scene based on depth information and via
visual prompting of a VLM - leveraging its vast knowledge of relevant regions
of the scene for exploration. Next, we use conformal prediction to calibrate
the VLM's question answering confidence, allowing the robot to know when to
stop exploration - leading to a more calibrated and efficient exploration
strategy. To test our framework in simulation, we also contribute a new EQA
dataset with diverse, realistic human-robot scenarios and scenes built upon the
Habitat-Matterport 3D Research Dataset (HM3D). Both simulated and real robot
experiments show our proposed approach improves the performance and efficiency
over baselines that do no leverage VLM for exploration or do not calibrate its
confidence. Webpage with experiment videos and code:
https://explore-eqa.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ iA$^*$: Imperative Learning-based A$^*$ Search for Pathfinding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyu Chen, Fan Yang, Chen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pathfinding problem, which aims to identify a collision-free path between
two points, is crucial for many applications, such as robot navigation and
autonomous driving. Classic methods, such as A$^*$ search, perform well on
small-scale maps but face difficulties scaling up. Conversely, data-driven
approaches can improve pathfinding efficiency but require extensive data
labeling and lack theoretical guarantees, making it challenging for practical
applications. To combine the strengths of the two methods, we utilize the
imperative learning (IL) strategy and propose a novel self-supervised
pathfinding framework, termed imperative learning-based A$^*$ (iA$^*$).
Specifically, iA$^*$ is a bilevel optimization process where the lower-level
optimization is dedicated to finding the optimal path by a differentiable A$^*$
search module, and the upper-level optimization narrows down the search space
to improve efficiency via setting suitable initial values from a data-driven
model. Besides, the model within the upper-level optimization is a fully
convolutional network, trained by the calculated loss in the lower-level
optimization. Thus, the framework avoids extensive data labeling and can be
applied in diverse environments. Our comprehensive experiments demonstrate that
iA$^*$ surpasses both classical and data-driven methods in pathfinding
efficiency and shows superior robustness among different tasks, validated with
public datasets and simulation environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated System-level Testing of Unmanned Aerial Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15857v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15857v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hassan Sartaj, Asmar Muqeet, Muhammad Zohaib Iqbal, Muhammad Uzair Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unmanned aerial systems (UAS) rely on various avionics systems that are
safety-critical and mission-critical. A major requirement of international
safety standards is to perform rigorous system-level testing of avionics
software systems. The current industrial practice is to manually create test
scenarios, manually/automatically execute these scenarios using simulators, and
manually evaluate outcomes. The test scenarios typically consist of setting
certain flight or environment conditions and testing the system under test in
these settings. The state-of-the-art approaches for this purpose also require
manual test scenario development and evaluation. In this paper, we propose a
novel approach to automate the system-level testing of the UAS. The proposed
approach (AITester) utilizes model-based testing and artificial intelligence
(AI) techniques to automatically generate, execute, and evaluate various test
scenarios. The test scenarios are generated on the fly, i.e., during test
execution based on the environmental context at runtime. The approach is
supported by a toolset. We empirically evaluate the proposed approach on two
core components of UAS, an autopilot system of an unmanned aerial vehicle (UAV)
and cockpit display systems (CDS) of the ground control station (GCS). The
results show that the AITester effectively generates test scenarios causing
deviations from the expected behavior of the UAV autopilot and reveals
potential flaws in the GCS-CDS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ARO: Large Language Model Supervised Robotics Text2Skill Autonomous
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwen Chen, Yuyao Ye, Ziyi Chen, Chuheng Zhang, Marcelo H. Ang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotics learning highly relies on human expertise and efforts, such as
demonstrations, design of reward functions in reinforcement learning,
performance evaluation using human feedback, etc. However, reliance on human
assistance can lead to expensive learning costs and make skill learning
difficult to scale. In this work, we introduce the Large Language Model
Supervised Robotics Text2Skill Autonomous Learning (ARO) framework, which aims
to replace human participation in the robot skill learning process with
large-scale language models that incorporate reward function design and
performance evaluation. We provide evidence that our approach enables fully
autonomous robot skill learning, capable of completing partial tasks without
human intervention. Furthermore, we also analyze the limitations of this
approach in task understanding and optimization stability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Learning based Policy Optimization for Temporal Tasks via
  Dropout 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15826v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15826v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Navid Hashemi, Bardh Hoxha, Danil Prokhorov, Georgios Fainekos, Jyotirmoy Deshmukh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a model-based approach for training feedback
controllers for an autonomous agent operating in a highly nonlinear
environment. We desire the trained policy to ensure that the agent satisfies
specific task objectives, expressed in discrete-time Signal Temporal Logic
(DT-STL). One advantage for reformulation of a task via formal frameworks, like
DT-STL, is that it permits quantitative satisfaction semantics. In other words,
given a trajectory and a DT-STL formula, we can compute the robustness, which
can be interpreted as an approximate signed distance between the trajectory and
the set of trajectories satisfying the formula. We utilize feedback
controllers, and we assume a feed forward neural network for learning these
feedback controllers. We show how this learning problem is similar to training
recurrent neural networks (RNNs), where the number of recurrent units is
proportional to the temporal horizon of the agent's task objectives. This poses
a challenge: RNNs are susceptible to vanishing and exploding gradients, and
na\"{i}ve gradient descent-based strategies to solve long-horizon task
objectives thus suffer from the same problems. To tackle this challenge, we
introduce a novel gradient approximation algorithm based on the idea of dropout
or gradient sampling. We show that, the existing smooth semantics for
robustness are inefficient regarding gradient computation when the
specification becomes complex. To address this challenge, we propose a new
smooth semantics for DT-STL that under-approximates the robustness value and
scales well for backpropagation over a complex specification. We show that our
control synthesis methodology, can be quite helpful for stochastic gradient
descent to converge with less numerical issues, enabling scalable
backpropagation over long time horizons and trajectories over high dimensional
state spaces.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Early Social Maneuvers for Enhanced Social Navigation <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15813v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15813v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yigit Yıldırim, Mehmet Suzer, Emre Ugur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Socially compliant navigation is an integral part of safety features in
Human-Robot Interaction. Traditional approaches to mobile navigation prioritize
physical aspects, such as efficiency, but social behaviors gain traction as
robots appear more in daily life. Recent techniques to improve the social
compliance of navigation often rely on predefined features or reward functions,
introducing assumptions about social human behavior. To address this
limitation, we propose a novel Learning from Demonstration (LfD) framework for
social navigation that exclusively utilizes raw sensory data. Additionally, the
proposed system contains mechanisms to consider the future paths of the
surrounding pedestrians, acknowledging the temporal aspect of the problem. The
final product is expected to reduce the anxiety of people sharing their
environment with a mobile robot, helping them trust that the robot is aware of
their presence and will not harm them. As the framework is currently being
developed, we outline its components, present experimental results, and discuss
future work towards realizing this framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the workshop of Robot Trust for Symbiotic Societies
  (RTSS) at ICRA 2024 on March 23, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Impact of Evolutionary Computation on Robotic Design: A Case Study
  with an Underactuated Hand Exoskeleton <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15812v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15812v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baris Akbas, Huseyin Taner Yuksel, Aleyna Soylemez, Mazhar Eid Zyada, Mine Sarac, Fabio Stroppa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic exoskeletons can enhance human strength and aid people with physical
disabilities. However, designing them to ensure safety and optimal performance
presents significant challenges. Developing exoskeletons should incorporate
specific optimization algorithms to find the best design. This study
investigates the potential of Evolutionary Computation (EC) methods in robotic
design optimization, with an underactuated hand exoskeleton (U-HEx) used as a
case study. We propose improving the performance and usability of the U-HEx
design, which was initially optimized using a naive brute-force approach, by
integrating EC techniques such as Genetic Algorithm and Big Bang-Big Crunch
Algorithm. Comparative analysis revealed that EC methods consistently yield
more precise and optimal solutions than brute force in a significantly shorter
time. This allowed us to improve the optimization by increasing the number of
variables in the design, which was impossible with naive methods. The results
show significant improvements in terms of the torque magnitude the device
transfers to the user, enhancing its efficiency. These findings underline the
importance of performing proper optimization while designing exoskeletons, as
well as providing a significant improvement to this specific robotic design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages (+ref), 4 figures, IEEE International Conference on Robotics
  and Automation (ICRA) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AirCrab: A Hybrid Aerial-Ground Manipulator with An Active Wheel 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muqing Cao, Jiayan Zhao, Xinhang Xu, Lihua Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by the behavior of birds, we present AirCrab, a hybrid aerial ground
manipulator (HAGM) with a single active wheel and a 3-degree of freedom (3-DoF)
manipulator. AirCrab leverages a single point of contact with the ground to
reduce position drift and improve manipulation accuracy. The single active
wheel enables locomotion on narrow surfaces without adding significant weight
to the robot. To realize accurate attitude maintenance using propellers on the
ground, we design a control allocation method for AirCrab that prioritizes
attitude control and dynamically adjusts the thrust input to reduce energy
consumption. Experiments verify the effectiveness of the proposed control
method and the gain in manipulation accuracy with ground contact. A series of
operations to complete the letters 'NTU' demonstrates the capability of the
robot to perform challenging hybrid aerial-ground manipulation missions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vid2Real HRI: Align video-based HRI study designs with real-world
  settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15798v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15798v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elliott Hauser, Yao-Cheng Chan, Sadanand Modak, Joydeep Biswas, Justin Hart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  HRI research using autonomous robots in real-world settings can produce
results with the highest ecological validity of any study modality, but many
difficulties limit such studies' feasibility and effectiveness. We propose
Vid2Real HRI, a research framework to maximize real-world insights offered by
video-based studies. The Vid2Real HRI framework was used to design an online
study using first-person videos of robots as real-world encounter surrogates.
The online study ($n = 385$) distinguished the within-subjects effects of four
robot behavioral conditions on perceived social intelligence and human
willingness to help the robot enter an exterior door. A real-world,
between-subjects replication ($n = 26$) using two conditions confirmed the
validity of the online study's findings and the sufficiency of the participant
recruitment target ($22$) based on a power analysis of online study results.
The Vid2Real HRI framework offers HRI researchers a principled way to take
advantage of the efficiency of video-based study modalities while generating
directly transferable knowledge of real-world HRI. Code and data from the study
are provided at https://vid2real.github.io/vid2realHRI
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DriveEnv-NeRF: Exploration of A NeRF-Based Autonomous Driving
  Environment for Real-World Performance Validation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mu-Yi Shen, Chia-Chi Hsu, Hao-Yu Hou, Yu-Chen Huang, Wei-Fang Sun, Chia-Che Chang, Yu-Lun Liu, Chun-Yi Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we introduce the DriveEnv-NeRF framework, which leverages
Neural Radiance Fields (NeRF) to enable the validation and faithful forecasting
of the efficacy of autonomous driving agents in a targeted real-world scene.
Standard simulator-based rendering often fails to accurately reflect real-world
performance due to the sim-to-real gap, which represents the disparity between
virtual simulations and real-world conditions. To mitigate this gap, we propose
a workflow for building a high-fidelity simulation environment of the targeted
real-world scene using NeRF. This approach is capable of rendering realistic
images from novel viewpoints and constructing 3D meshes for emulating
collisions. The validation of these capabilities through the comparison of
success rates in both simulated and real environments demonstrates the benefits
of using DriveEnv-NeRF as a real-world performance indicator. Furthermore, the
DriveEnv-NeRF framework can serve as a training environment for autonomous
driving agents under various lighting conditions. This approach enhances the
robustness of the agents and reduces performance degradation when deployed to
the target real scene, compared to agents fully trained using the standard
simulator rendering pipeline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/muyishen2040/DriveEnvNeRF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RicMonk: A Three-Link Brachiation Robot with Passive Grippers for
  Energy-Efficient Brachiation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shourie S. Grama, Mahdi Javadi, Shivesh Kumar, Hossein Zamani Boroujeni, Frank Kirchner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the design, analysis, and performance evaluation of
RicMonk, a novel three-link brachiation robot equipped with passive hook-shaped
grippers. Brachiation, an agile and energy-efficient mode of locomotion
observed in primates, has inspired the development of RicMonk to explore
versatile locomotion and maneuvers on ladder-like structures. The robot's
anatomical resemblance to gibbons and the integration of a tail mechanism for
energy injection contribute to its unique capabilities. The paper discusses the
use of the Direct Collocation methodology for optimizing trajectories for the
robot's dynamic behaviors and stabilization of these trajectories using a
Time-varying Linear Quadratic Regulator. With RicMonk we demonstrate
bidirectional brachiation, and provide comparative analysis with its
predecessor, AcroMonk - a two-link brachiation robot, to demonstrate that the
presence of a passive tail helps improve energy efficiency. The system design,
controllers, and software implementation are publicly available on GitHub and
the video demonstration of the experiments can be viewed YouTube.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Open sourced system design, controllers, software implementation can
  be found at https://github.com/dfki-ric-underactuated-lab/ricmonk and a video
  demonstrating the experiments performed with RicMonk can be found at
  https://www.youtube.com/watch?v=hOuDQI7CD8w</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed Robust Learning based Formation Control of Mobile Robots
  based on Bioinspired Neural Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15716v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15716v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Xu, Tao Yan, Simon X. Yang, S. Andrew Gadsden, Mohammad Biglarbegian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenges of distributed formation control in
multiple mobile robots, introducing a novel approach that enhances real-world
practicability. We first introduce a distributed estimator using a variable
structure and cascaded design technique, eliminating the need for derivative
information to improve the real time performance. Then, a kinematic tracking
control method is developed utilizing a bioinspired neural dynamic-based
approach aimed at providing smooth control inputs and effectively resolving the
speed jump issue. Furthermore, to address the challenges for robots operating
with completely unknown dynamics and disturbances, a learning-based robust
dynamic controller is developed. This controller provides real time parameter
estimates while maintaining its robustness against disturbances. The overall
stability of the proposed method is proved with rigorous mathematical analysis.
At last, multiple comprehensive simulation studies have shown the advantages
and effectiveness of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by IEEE Transactions on Intelligent Vehicles</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PNAS-MOT: Multi-Modal Object Tracking with Pareto Neural Architecture
  Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15712v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15712v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chensheng Peng, Zhaoyu Zeng, Jinling Gao, Jundong Zhou, Masayoshi Tomizuka, Xinbing Wang, Chenghu Zhou, Nanyang Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple object tracking is a critical task in autonomous driving. Existing
works primarily focus on the heuristic design of neural networks to obtain high
accuracy. As tracking accuracy improves, however, neural networks become
increasingly complex, posing challenges for their practical application in real
driving scenarios due to the high level of latency. In this paper, we explore
the use of the neural architecture search (NAS) methods to search for efficient
architectures for tracking, aiming for low real-time latency while maintaining
relatively high accuracy. Another challenge for object tracking is the
unreliability of a single sensor, therefore, we propose a multi-modal framework
to improve the robustness. Experiments demonstrate that our algorithm can run
on edge devices within lower latency constraints, thus greatly reducing the
computational requirements for multi-modal object tracking while keeping lower
latency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Robotics and Automation Letters 2024. Code is available at
  https://github.com/PholyPeng/PNAS-MOT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-Driven Predictive Control for Robust Exoskeleton Locomotion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15658v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15658v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kejun Li, Jeeseop Kim, Xiaobin Xiong, Kaveh Akbari Hamed, Yisong Yue, Aaron D. Ames
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exoskeleton locomotion must be robust while being adaptive to different users
with and without payloads. To address these challenges, this work introduces a
data-driven predictive control (DDPC) framework to synthesize walking gaits for
lower-body exoskeletons, employing Hankel matrices and a state transition
matrix for its data-driven model. The proposed approach leverages DDPC through
a multi-layer architecture. At the top layer, DDPC serves as a planner
employing Hankel matrices and a state transition matrix to generate a
data-driven model that can learn and adapt to varying users and payloads. At
the lower layer, our method incorporates inverse kinematics and passivity-based
control to map the planned trajectory from DDPC into the full-order states of
the lower-body exoskeleton. We validate the effectiveness of this approach
through numerical simulations and hardware experiments conducted on the
Atalante lower-body exoskeleton with different payloads. Moreover, we conducted
a comparative analysis against the model predictive control (MPC) framework
based on the reduced-order linear inverted pendulum (LIP) model. Through this
comparison, the paper demonstrates that DDPC enables robust bipedal walking at
various velocities while accounting for model uncertainties and unknown
perturbations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring 3D Human Pose Estimation and Forecasting from the Robot's
  Perspective: The HARPER <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14447v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14447v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Avogaro, Andrea Toaiari, Federico Cunico, Xiangmin Xu, Haralambos Dafas, Alessandro Vinciarelli, Emma Li, Marco Cristani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce HARPER, a novel dataset for 3D body pose estimation and forecast
in dyadic interactions between users and Spot, the quadruped robot manufactured
by Boston Dynamics. The key-novelty is the focus on the robot's perspective,
i.e., on the data captured by the robot's sensors. These make 3D body pose
analysis challenging because being close to the ground captures humans only
partially. The scenario underlying HARPER includes 15 actions, of which 10
involve physical contact between the robot and users. The Corpus contains not
only the recordings of the built-in stereo cameras of Spot, but also those of a
6-camera OptiTrack system (all recordings are synchronized). This leads to
ground-truth skeletal representations with a precision lower than a millimeter.
In addition, the Corpus includes reproducible benchmarks on 3D Human Pose
Estimation, Human Pose Forecasting, and Collision Prediction, all based on
publicly available baseline approaches. This enables future HARPER users to
rigorously compare their results with those we provide in this work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LONER: LiDAR Only Neural Representations for Real-Time SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.04937v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.04937v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seth Isaacson, Pou-Chun Kung, Mani Ramanagopal, Ram Vasudevan, Katherine A. Skinner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes LONER, the first real-time LiDAR SLAM algorithm that uses
a neural implicit scene representation. Existing implicit mapping methods for
LiDAR show promising results in large-scale reconstruction, but either require
groundtruth poses or run slower than real-time. In contrast, LONER uses LiDAR
data to train an MLP to estimate a dense map in real-time, while simultaneously
estimating the trajectory of the sensor. To achieve real-time performance, this
paper proposes a novel information-theoretic loss function that accounts for
the fact that different regions of the map may be learned to varying degrees
throughout online training. The proposed method is evaluated qualitatively and
quantitatively on two open-source datasets. This evaluation illustrates that
the proposed loss function converges faster and leads to more accurate geometry
reconstruction than other loss functions used in depth-supervised neural
implicit frameworks. Finally, this paper shows that LONER estimates
trajectories competitively with state-of-the-art LiDAR SLAM methods, while also
producing dense maps competitive with existing real-time implicit mapping
methods that use groundtruth poses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First two authors equally contributed. Webpage:
  https://umautobots.github.io/loner</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ARTEMIS: AI-driven Robotic Triage Labeling and Emergency Medical
  Information System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08865v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08865v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Revanth Krishna Senthilkumaran, Mridu Prashanth, Hrishikesh Viswanath, Sathvika Kotha, Kshitij Tiwari, Aniket Bera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mass casualty incidents (MCIs) pose a significant challenge to emergency
medical services by overwhelming available resources and personnel. Effective
victim assessment is the key to minimizing casualties during such a crisis. We
introduce ARTEMIS, an AI-driven Robotic Triage Labeling and Emergency Medical
Information System, to aid first responders in MCI events. It leverages speech
processing, natural language processing, and deep learning to help with acuity
classification. This is deployed on a quadruped that performs victim
localization and preliminary injury severity assessment. First responders
access victim information through a Graphical User Interface that is updated in
real-time. To validate our proposed algorithmic triage protocol, we used the
Unitree Go1 quadruped. The robot identifies humans, interacts with them, gets
vitals and information, and assigns an acuity label. Simulations of an MCI in
software and a controlled environment outdoors were conducted. The system
achieved a triage-level classification precision of over 74% on average and 99%
for the most critical victims, i.e. level 1 acuity, outperforming
state-of-the-art deep learning-based triage labeling systems. In this paper, we
showcase the potential of human-robot interaction in assisting medical
personnel in MCI events.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NaVid: Video-based VLM Plans the Next Step for Vision-and-Language
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15852v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15852v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiazhao Zhang, Kunyu Wang, Rongtao Xu, Gengze Zhou, Yicong Hong, Xiaomeng Fang, Qi Wu, Zhizheng Zhang, He Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-and-Language Navigation (VLN) stands as a key research problem of
Embodied AI, aiming at enabling agents to navigate in unseen environments
following linguistic instructions. In this field, generalization is a
long-standing challenge, either to out-of-distribution scenes or from Sim to
Real. In this paper, we propose NaVid, a video-based large vision language
model (VLM), to mitigate such a generalization gap. NaVid makes the first
endeavour to showcase the capability of VLMs to achieve state-of-the-art level
navigation performance without any maps, odometer and depth inputs. Following
human instruction, NaVid only requires an on-the-fly video stream from a
monocular RGB camera equipped on the robot to output the next-step action. Our
formulation mimics how humans navigate and naturally gets rid of the problems
introduced by odometer noises, and the Sim2Real gaps from map or depth inputs.
Moreover, our video-based approach can effectively encode the historical
observations of robots as spatio-temporal contexts for decision-making and
instruction following. We train NaVid with 550k navigation samples collected
from VLN-CE trajectories, including action-planning and instruction-reasoning
samples, along with 665k large-scale web data. Extensive experiments show that
NaVid achieves SOTA performance in simulation environments and the real world,
demonstrating superior cross-dataset and Sim2Real transfer. We thus believe our
proposed VLM approach plans the next step for not only the navigation agents
but also this research field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tactile Estimation of Extrinsic Contact Patch for Stable Placement <span class="chip">ICRA2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14552v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14552v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kei Ota, Devesh K. Jha, Krishna Murthy Jatavallabhula, Asako Kanezaki, Joshua B. Tenenbaum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise perception of contact interactions is essential for fine-grained
manipulation skills for robots. In this paper, we present the design of
feedback skills for robots that must learn to stack complex-shaped objects on
top of each other (see Fig.1). To design such a system, a robot should be able
to reason about the stability of placement from very gentle contact
interactions. Our results demonstrate that it is possible to infer the
stability of object placement based on tactile readings during contact
formation between the object and its environment. In particular, we estimate
the contact patch between a grasped object and its environment using force and
tactile observations to estimate the stability of the object during a contact
formation. The contact patch could be used to estimate the stability of the
object upon release of the grasp. The proposed method is demonstrated in
various pairs of objects that are used in a very popular board game.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICRA2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Integration of Large Language Models within Cognitive Architectures for
  Autonomous Robots <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14945v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14945v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miguel Á. González-Santamarta, Francisco J. Rodríguez-Lera, Ángel Manuel Guerrero-Higueras, Vicente Matellán-Olivera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Symbolic reasoning systems have been used in cognitive architectures to
provide inference and planning capabilities. However, defining domains and
problems has proven difficult and prone to errors. Moreover, Large Language
Models (LLMs) have emerged as tools to process natural language for different
tasks. In this paper, we propose the use of LLMs to tackle these problems. This
way, this paper proposes the integration of LLMs in the ROS 2-integrated
cognitive architecture MERLIN2 for autonomous robots. Specifically, we present
the design, development and deployment of how to leverage the reasoning
capabilities of LLMs inside the deliberative processes of MERLIN2. As a result,
the deliberative system is updated from a PDDL-based planner system to a
natural language planning system. This proposal is evaluated quantitatively and
qualitatively, measuring the impact of incorporating the LLMs in the cognitive
architecture. Results show that a classical approach achieves better
performance but the proposed solution provides an enhanced interaction through
natural language.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, 2 tables, Submitted to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Space Filling Curves for Coverage Path Planning with Online Obstacle
  Avoidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.01426v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.01426v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashay Wakode, Arpita Sinha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper presents a strategy for robotic exploration problem using
Space-Filling curves (SFC). The strategy plans a path that avoids unknown
obstacles while ensuring complete coverage of the free space in region of
interest. The region of interest is first tessellated, and the tiles/cells are
connected using a SFC pattern. A robot follows the SFC to explore the entire
area. However, obstacles can block the systematic movement of the robot. We
overcome this problem by determining an alternate path online that avoids the
blocked cells while ensuring all the accessible cells are visited at least
once. The proposed strategy chooses next waypoint based on the graph
connectivity of the cells and the obstacle encountered so far. It is online,
exhaustive and works in situations demanding non-uniform coverage. The
completeness of the strategy is proved and its desirable properties are
discussed with examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fully Spiking Neural Network for Legged Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05022v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05022v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyang Jiang, Qiang Zhang, Jingkai Sun, Jiahang Cao, Jingtong Ma, Renjing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, legged robots based on deep reinforcement learning have made
remarkable progress. Quadruped robots have demonstrated the ability to complete
challenging tasks in complex environments and have been deployed in real-world
scenarios to assist humans. Simultaneously, bipedal and humanoid robots have
achieved breakthroughs in various demanding tasks. Current reinforcement
learning methods can utilize diverse robot bodies and historical information to
perform actions. However, prior research has not emphasized the speed and
energy consumption of network inference, as well as the biological significance
of the neural networks themselves. Most of the networks employed are
traditional artificial neural networks that utilize multilayer perceptrons
(MLP). In this paper, we successfully apply a novel Spiking Neural Network
(SNN) to process legged robots, achieving outstanding results across a range of
simulated terrains. SNN holds a natural advantage over traditional neural
networks in terms of inference speed and energy consumption, and their
pulse-form processing of body perception signals offers improved biological
interpretability. Applying more biomimetic neural networks to legged robots can
further reduce the heat dissipation and structural burden caused by the high
power consumption of neural networks. To the best of our knowledge, this is the
first work to implement SNN in legged robots.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language
  Models <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10062v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10062v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shyam Sundar Kannan, Vishnunandan L. N. Venkatesh, Byung-Cheol Min
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce SMART-LLM, an innovative framework designed for
embodied multi-robot task planning. SMART-LLM: Smart Multi-Agent Robot Task
Planning using Large Language Models (LLMs), harnesses the power of LLMs to
convert high-level task instructions provided as input into a multi-robot task
plan. It accomplishes this by executing a series of stages, including task
decomposition, coalition formation, and task allocation, all guided by
programmatic LLM prompts within the few-shot prompting paradigm. We create a
benchmark dataset designed for validating the multi-robot task planning
problem, encompassing four distinct categories of high-level instructions that
vary in task complexity. Our evaluation experiments span both simulation and
real-world scenarios, demonstrating that the proposed model can achieve
promising results for generating multi-robot task plans. The experimental
videos, code, and datasets from the work can be found at
https://sites.google.com/view/smart-llm/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SurgicalPart-SAM: Part-to-Whole Collaborative <span class="highlight-title">Prompt</span>ing for Surgical
  Instrument Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14481v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14481v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxi Yue, Jing Zhang, Kun Hu, Qiuxia Wu, Zongyuan Ge, Yong Xia, Jiebo Luo, Zhiyong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Segment Anything Model (SAM) exhibits promise in generic object
segmentation and offers potential for various applications. Existing methods
have applied SAM to surgical instrument segmentation (SIS) by tuning SAM-based
frameworks with surgical data. However, they fall short in two crucial aspects:
(1) Straightforward model tuning with instrument masks treats each instrument
as a single entity, neglecting their complex structures and fine-grained
details; and (2) Instrument category-based prompts are not flexible and
informative enough to describe instrument structures. To address these
problems, in this paper, we investigate text promptable SIS and propose
SurgicalPart-SAM (SP-SAM), a novel SAM efficient-tuning approach that
explicitly integrates instrument structure knowledge with SAM's generic
knowledge, guided by expert knowledge on instrument part compositions.
Specifically, we achieve this by proposing (1) Collaborative Prompts that
describe instrument structures via collaborating category-level and part-level
texts; (2) Cross-Modal Prompt Encoder that encodes text prompts jointly with
visual embeddings into discriminative part-level representations; and (3)
Part-to-Whole Adaptive Fusion and Hierarchical Decoding that adaptively fuse
the part-level representations into a whole for accurate instrument
segmentation in surgical scenarios. Built upon them, SP-SAM acquires a better
capability to comprehend surgical instruments in terms of both overall
structure and part-level details. Extensive experiments on both the EndoVis2018
and EndoVis2017 datasets demonstrate SP-SAM's state-of-the-art performance with
minimal tunable parameters. The code will be available at
https://github.com/wenxi-yue/SurgicalPart-SAM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report. The source code will be released at
  https://github.com/wenxi-yue/SurgicalPart-SAM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bird's Eye View Based <span class="highlight-title">Pretrain</span>ed World model for Visual Navigation <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18847v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18847v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kiran Lekkala, Chen Liu, Laurent Itti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sim2Real transfer has gained popularity because it helps transfer from
inexpensive simulators to real world. This paper presents a novel system that
fuses components in a traditional World Model into a robust system, trained
entirely within a simulator, that Zero-Shot transfers to the real world. To
facilitate transfer, we use an intermediary representation that is based on
\textit{Bird's Eye View (BEV)} images. Thus, our robot learns to navigate in a
simulator by first learning to translate from complex \textit{First-Person View
(FPV)} based RGB images to BEV representations, then learning to navigate using
those representations. Later, when tested in the real world, the robot uses the
perception model that translates FPV-based RGB images to embeddings that were
learned by the FPV to BEV translator and that can be used by the downstream
policy. The incorporation of state-checking modules using \textit{Anchor
images} and Mixture Density LSTM not only interpolates uncertain and missing
observations but also enhances the robustness of the model in the real-world.
We trained the model using data from a Differential drive robot in the CARLA
simulator. Our methodology's effectiveness is shown through the deployment of
trained models onto a real-world Differential drive robot. Lastly we release a
comprehensive codebase, dataset and models for training and deployment
(\url{https://sites.google.com/view/value-explicit-pretraining}).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review at the IROS 2024; Accepted at NeurIPS 2023, Robot
  Learning Workshop</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-22T00:00:00Z">2024-03-22</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">47</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Augmented Reality based Simulated Data (ARSim) with multi-view
  consistency for AV perception networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aqeel Anwar, Tae Eun Choe, Zian Wang, Sanja Fidler, Minwoo Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting a diverse range of objects under various driving scenarios is
essential for the effectiveness of autonomous driving systems. However, the
real-world data collected often lacks the necessary diversity presenting a
long-tail distribution. Although synthetic data has been utilized to overcome
this issue by generating virtual scenes, it faces hurdles such as a significant
domain gap and the substantial efforts required from 3D artists to create
realistic environments. To overcome these challenges, we present ARSim, a fully
automated, comprehensive, modular framework designed to enhance real multi-view
image data with 3D synthetic objects of interest. The proposed method
integrates domain adaptation and randomization strategies to address covariate
shift between real and simulated data by inferring essential domain attributes
from real data and employing simulation-based randomization for other
attributes. We construct a simplified virtual scene using real data and
strategically place 3D synthetic assets within it. Illumination is achieved by
estimating light distribution from multiple images capturing the surroundings
of the vehicle. Camera parameters from real data are employed to render
synthetic assets in each frame. The resulting augmented multi-view consistent
dataset is used to train a multi-camera perception network for autonomous
vehicles. Experimental results on various AV perception tasks demonstrate the
superior performance of networks trained on the augmented dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 15 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OceanPlan: Hierarchical Planning and Replanning for Natural Language AUV
  Piloting in Large-scale Unexplored Ocean Environments <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15369v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15369v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruochu Yang, Fumin Zhang, Mengxue Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop a hierarchical LLM-task-motion planning and replanning framework
to efficiently ground an abstracted human command into tangible Autonomous
Underwater Vehicle (AUV) control through enhanced representations of the world.
We also incorporate a holistic replanner to provide real-world feedback with
all planners for robust AUV operation. While there has been extensive research
in bridging the gap between LLMs and robotic missions, they are unable to
guarantee success of AUV applications in the vast and unknown ocean
environment. To tackle specific challenges in marine robotics, we design a
hierarchical planner to compose executable motion plans, which achieves
planning efficiency and solution quality by decomposing long-horizon missions
into sub-tasks. At the same time, real-time data stream is obtained by a
replanner to address environmental uncertainties during plan execution.
Experiments validate that our proposed framework delivers successful AUV
performance of long-duration missions through natural language piloting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safe and Stable Teleoperation of Quadrotor UAVs under Haptic Shared
  Autonomy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dawei Zhang, Roberto Tron
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel approach that aims to address both safety and stability of
a haptic teleoperation system within a framework of Haptic Shared Autonomy
(HSA). We use Control Barrier Functions (CBFs) to generate the control input
that follows the user's input as closely as possible while guaranteeing safety.
In the context of stability of the human-in-the-loop system, we limit the force
feedback perceived by the user via a small $L_2$-gain, which is achieved by
limiting the control and the force feedback via a differential constraint.
Specifically, with the property of HSA, we propose two pathways to design the
control and the force feedback: Sequential Control Force (SCF) and Joint
Control Force (JCF). Both designs can achieve safety and stability but with
different responses to the user's commands. We conducted experimental
simulations to evaluate and investigate the properties of the designed methods.
We also tested the proposed method on a physical quadrotor UAV and a haptic
interface.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gesture-Controlled Aerial Robot Formation for Human-Swarm Interaction in
  Safety Monitoring Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15333v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15333v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vít Krátký, Giuseppe Silano, Matouš Vrba, Christos Papaioannidis, Ioannis Mademlis, Robert Pěnička, Ioannis Pitas, Martin Saska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a formation control approach for contactless
gesture-based Human-Swarm Interaction (HSI) between a team of multi-rotor
Unmanned Aerial Vehicles (UAVs) and a human worker. The approach is intended
for monitoring the safety of human workers, especially those working at
heights. In the proposed dynamic formation scheme, one UAV acts as the leader
of the formation and is equipped with sensors for human worker detection and
gesture recognition. The follower UAVs maintain a predetermined formation
relative to the worker's position, thereby providing additional perspectives of
the monitored scene. Hand gestures allow the human worker to specify movements
and action commands for the UAV team and initiate other mission-related
commands without the need for an additional communication channel or specific
markers. Together with a novel unified human detection and tracking algorithm,
human pose estimation approach and gesture detection pipeline, the proposed
approach forms a first instance of an HSI system incorporating all these
modules onboard real-world UAVs. Simulations and field experiments with three
UAVs and a human worker in a mock-up scenario showcase the effectiveness and
responsiveness of the proposed approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Introduction to Human-Robot Interaction: A Multi-Perspective
  Introductory Course 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15323v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15323v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Williams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper I describe the design of an introductory course in Human-Robot
Interaction. This project-driven course is designed to introduce undergraduate
and graduate engineering students, especially those enrolled in Computer
Science, Mechanical Engineering, and Robotics degree programs, to key theories
and methods used in the field of Human-Robot Interaction that they would
otherwise be unlikely to see in those degree programs. To achieve this aim, the
course takes students all the way from stakeholder analysis to empirical
evaluation, covering and integrating key Qualitative, Design, Computational,
and Quantitative methods along the way. I detail the goals, audience, and
format of the course, and provide a detailed walkthrough of the course
syllabus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the Designing an Intro to HRI Course Workshop at HRI
  2024 (arXiv:2403.05588)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HortiBot: An Adaptive Multi-Arm System for Robotic Horticulture of Sweet
  Peppers <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15306v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15306v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Lenz, Rohit Menon, Michael Schreiber, Melvin Paul Jacob, Sven Behnke, Maren Bennewitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Horticultural tasks such as pruning and selective harvesting are labor
intensive and horticultural staff are hard to find. Automating these tasks is
challenging due to the semi-structured greenhouse workspaces, changing
environmental conditions such as lighting, dense plant growth with many
occlusions, and the need for gentle manipulation of non-rigid plant organs. In
this work, we present the three-armed system HortiBot, with two arms for
manipulation and a third arm as an articulated head for active perception using
stereo cameras. Its perception system detects not only peppers, but also
peduncles and stems in real time, and performs online data association to build
a world model of pepper plants. Collision-aware online trajectory generation
allows all three arms to safely track their respective targets for observation,
grasping, and cutting. We integrated perception and manipulation to perform
selective harvesting of peppers and evaluated the system in lab experiments.
Using active perception coupled with end-effector force torque sensing for
compliant manipulation, HortiBot achieves high success rates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to International Conference on Intelligent Robots and
  Systems (IROS) 2024. C. Lenz and R. Menon contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guided Decoding for Robot Motion Generation and Adaption 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nutan Chen, Elie Aljalbout, Botond Cseke, Patrick van der Smagt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address motion generation for high-DoF robot arms in complex settings with
obstacles, via points, etc. A significant advancement in this domain is
achieved by integrating Learning from Demonstration (LfD) into the motion
generation process. This integration facilitates rapid adaptation to new tasks
and optimizes the utilization of accumulated expertise by allowing robots to
learn and generalize from demonstrated trajectories.
  We train a transformer architecture on a large dataset of simulated
trajectories. This architecture, based on a conditional variational autoencoder
transformer, learns essential motion generation skills and adapts these to meet
auxiliary tasks and constraints. Our auto-regressive approach enables real-time
integration of feedback from the physical system, enhancing the adaptability
and efficiency of motion generation. We show that our model can generate motion
from initial and target points, but also that it can adapt trajectories in
navigating complex tasks, including obstacle avoidance, via points, and meeting
velocity and acceleration constraints, across platforms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TriHelper: Zero-Shot Object Navigation with Dynamic Assistance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingfeng Zhang, Qiang Zhang, Hao Wang, Erjia Xiao, Zixuan Jiang, Honglei Chen, Renjing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Navigating toward specific objects in unknown environments without additional
training, known as Zero-Shot object navigation, poses a significant challenge
in the field of robotics, which demands high levels of auxiliary information
and strategic planning. Traditional works have focused on holistic solutions,
overlooking the specific challenges agents encounter during navigation such as
collision, low exploration efficiency, and misidentification of targets. To
address these challenges, our work proposes TriHelper, a novel framework
designed to assist agents dynamically through three primary navigation
challenges: collision, exploration, and detection. Specifically, our framework
consists of three innovative components: (i) Collision Helper, (ii) Exploration
Helper, and (iii) Detection Helper. These components work collaboratively to
solve these challenges throughout the navigation process. Experiments on the
Habitat-Matterport 3D (HM3D) and Gibson datasets demonstrate that TriHelper
significantly outperforms all existing baseline methods in Zero-Shot object
navigation, showcasing superior success rates and exploration efficiency. Our
ablation studies further underscore the effectiveness of each helper in
addressing their respective challenges, notably enhancing the agent's
navigation capabilities. By proposing TriHelper, we offer a fresh perspective
on advancing the object navigation task, paving the way for future research in
the domain of Embodied AI and visual-based navigation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DITTO: Demonstration Imitation by Trajectory Transformation <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nick Heppert, Max Argus, Tim Welschehold, Thomas Brox, Abhinav Valada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Teaching robots new skills quickly and conveniently is crucial for the
broader adoption of robotic systems. In this work, we address the problem of
one-shot imitation from a single human demonstration, given by an RGB-D video
recording through a two-stage process. In the first stage which is offline, we
extract the trajectory of the demonstration. This entails segmenting
manipulated objects and determining their relative motion in relation to
secondary objects such as containers. Subsequently, in the live online
trajectory generation stage, we first \mbox{re-detect} all objects, then we
warp the demonstration trajectory to the current scene, and finally, we trace
the trajectory with the robot. To complete these steps, our method makes
leverages several ancillary models, including those for segmentation, relative
object pose estimation, and grasp prediction. We systematically evaluate
different combinations of correspondence and re-detection methods to validate
our design decision across a diverse range of tasks. Specifically, we collect
demonstrations of ten different tasks including pick-and-place tasks as well as
articulated object manipulation. Finally, we perform extensive evaluations on a
real robot system to demonstrate the effectiveness and utility of our approach
in real-world scenarios. We make the code publicly available at
http://ditto.cs.uni-freiburg.de.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, 3 tables, submitted to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CRPlace: Camera-Radar Fusion with BEV Representation for Place
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaowei Fu, Yifan Duan, Yao Li, Chengzhen Meng, Yingjie Wang, Jianmin Ji, Yanyong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of complementary characteristics from camera and radar data
has emerged as an effective approach in 3D object detection. However, such
fusion-based methods remain unexplored for place recognition, an equally
important task for autonomous systems. Given that place recognition relies on
the similarity between a query scene and the corresponding candidate scene, the
stationary background of a scene is expected to play a crucial role in the
task. As such, current well-designed camera-radar fusion methods for 3D object
detection can hardly take effect in place recognition because they mainly focus
on dynamic foreground objects. In this paper, a background-attentive
camera-radar fusion-based method, named CRPlace, is proposed to generate
background-attentive global descriptors from multi-view images and radar point
clouds for accurate place recognition. To extract stationary background
features effectively, we design an adaptive module that generates the
background-attentive mask by utilizing the camera BEV feature and radar dynamic
points. With the guidance of a background mask, we devise a bidirectional
cross-attention-based spatial fusion strategy to facilitate comprehensive
spatial interaction between the background information of the camera BEV
feature and the radar BEV feature. As the first camera-radar fusion-based place
recognition network, CRPlace has been evaluated thoroughly on the nuScenes
dataset. The results show that our algorithm outperforms a variety of baseline
methods across a comprehensive set of metrics (recall@1 reaches 91.2%).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AV-Occupant Perceived Risk Model for Cut-In Scenarios with Empirical
  Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15171v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15171v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarah Barendswaard, Tong Duy Son
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in autonomous vehicle (AV) technologies necessitate precise
estimation of perceived risk to enhance user comfort, acceptance and trust.
This paper introduces a novel AV-Occupant Risk (AVOR) model designed for
perceived risk estimation during AV cut-in scenarios. An empirical study is
conducted with 18 participants with realistic cut-in scenarios. Two factors
were investigated: scenario risk and scene population. 76% of subjective risk
responses indicate an increase in perceived risk at cut-in initiation. The
existing perceived risk model did not capture this critical phenomenon. Our
AVOR model demonstrated a significant improvement in estimating perceived risk
during the early stages of cut-ins, especially for the high-risk scenario,
enhancing modelling accuracy by up to 54%. The concept of the AVOR model can
quantify perceived risk in other diverse driving contexts characterized by
dynamic uncertainties, enhancing the reliability and human-centred focus of AV
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Infrastructure-Assisted Collaborative Perception in Automated Valet
  Parking: A Safety Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukuan Jia, Jiawen Zhang, Shimeng Lu, Baokang Fan, Ruiqing Mao, Sheng Zhou, Zhisheng Niu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Environmental perception in Automated Valet Parking (AVP) has been a
challenging task due to severe occlusions in parking garages. Although
Collaborative Perception (CP) can be applied to broaden the field of view of
connected vehicles, the limited bandwidth of vehicular communications restricts
its application. In this work, we propose a BEV feature-based CP network
architecture for infrastructure-assisted AVP systems. The model takes the
roadside camera and LiDAR as optional inputs and adaptively fuses them with
onboard sensors in a unified BEV representation. Autoencoder and downsampling
are applied for channel-wise and spatial-wise dimension reduction, while
sparsification and quantization further compress the feature map with little
loss in data precision. Combining these techniques, the size of a BEV feature
map is effectively compressed to fit in the feasible data rate of the NR-V2X
network. With the synthetic AVP dataset, we observe that CP can effectively
increase perception performance, especially for pedestrians. Moreover, the
advantage of infrastructure-assisted CP is demonstrated in two typical
safety-critical scenarios in the AVP setting, increasing the maximum safe
cruising speed by up to 3m/s in both scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures, 4 tables, accepted by IEEE VTC2024-Spring</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RHINO-VR Experience: Teaching Mobile Robotics Concepts in an Interactive
  Museum Exhibit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik Schlachhoff, Nils Dengler, Leif Van Holland, Patrick Stotko, Jorge de Heuvel, Reinhard Klein, Maren Bennewitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In 1997, the very first tour guide robot RHINO was deployed in a museum in
Germany. With the ability to navigate autonomously through the environment, the
robot gave tours to over 2,000 visitors. Today, RHINO itself has become an
exhibit and is no longer operational. In this paper, we present RHINO-VR, an
interactive museum exhibit using virtual reality (VR) that allows museum
visitors to experience the historical robot RHINO in operation in a virtual
museum. RHINO-VR, unlike static exhibits, enables users to familiarize
themselves with basic mobile robotics concepts without the fear of damaging the
exhibit. In the virtual environment, the user is able to interact with RHINO in
VR by pointing to a location to which the robot should navigate and observing
the corresponding actions of the robot. To include other visitors who cannot
use the VR, we provide an external observation view to make RHINO visible to
them. We evaluated our system by measuring the frame rate of the VR simulation,
comparing the generated virtual 3D models with the originals, and conducting a
user study. The user-study showed that RHINO-VR improved the visitors'
understanding of the robot's functionality and that they would recommend
experiencing the VR exhibit to others.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE International Symposium on Robot and Human
  Interactive Communication (RO-MAN)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ALPINE: a climbing robot for operations in mountain environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15142v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15142v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michele Focchi, Andrea Del Prete, Daniele Fontanelli, Marco Frego, Angelika Peer, Luigi Palopoli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mountain slopes are perfect examples of harsh environments in which humans
are required to perform difficult and dangerous operations such as removing
unstable boulders, dangerous vegetation or deploying safety nets. A good
replacement for human intervention can be offered by climbing robots. The
different solutions existing in the literature are not up to the task for the
difficulty of the requirements (navigation, heavy payloads, flexibility in the
execution of the tasks). In this paper, we propose a robotic platform that can
fill this gap. Our solution is based on a robot that hangs on ropes, and uses a
retractable leg to jump away from the mountain walls. Our package of mechanical
solutions, along with the algorithms developed for motion planning and control,
delivers swift navigation on irregular and steep slopes, the possibility to
overcome or travel around significant natural barriers, and the ability to
carry heavy payloads and execute complex tasks. In the paper, we give a full
account of our main design and algorithmic choices and show the feasibility of
the solution through a large number of physically simulated scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collision Avoidance Safety Filter for an Autonomous E-Scooter using
  Ultrasonic Sensors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robin Strässer, Marc Seidel, Felix Brändle, David Meister, Raffaele Soloperto, David Hambach Ferrer, Frank Allgöwer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a collision avoidance safety filter for autonomous
electric scooters to enable safe operation of such vehicles in pedestrian
areas. In particular, we employ multiple low-cost ultrasonic sensors to detect
a wide range of possible obstacles in front of the e-scooter. Based on possibly
faulty distance measurements, we design a filter to mitigate measurement noise
and missing values as well as a gain-scheduled controller to limit the velocity
commanded to the e-scooter when required due to imminent collisions. The
proposed controller structure is able to prevent collisions with unknown
obstacles by deploying a reduced safe velocity ensuring a sufficiently large
safety distance. The collision avoidance approach is designed such that it may
be easily deployed in similar applications of general micromobility vehicles.
The effectiveness of our proposed safety filter is demonstrated in real-world
experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Set-membership target search and tracking within an unknown cluttered
  area using cooperating UAVs equipped with vision systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15113v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15113v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxime Zagar, Luc Meyer, Michel Kieffer, Hélène Piet-Lahanier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of target search and tracking using a fleet
of cooperating UAVs evolving in some unknown region of interest containing an a
priori unknown number of moving ground targets. Each drone is equipped with an
embedded Computer Vision System (CVS), providing an image with labeled pixels
and a depth map of the observed part of its environment. Moreover, a box
containing the corresponding pixels in the image frame is available when a UAV
identifies a target. Hypotheses regarding information provided by the pixel
classification, depth map construction, and target identification algorithms
are proposed to allow its exploitation by set-membership approaches. A
set-membership target location estimator is developed using the information
provided by the CVS. Each UAV evaluates sets guaranteed to contain the location
of the identified targets and a set possibly containing the locations of
targets still to be identified. Then, each UAV uses these sets to search and
track targets cooperatively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PseudoTouch: Efficiently Imaging the Surface Feel of Objects for Robotic
  Manipulation <span class="chip">IROS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15107v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15107v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrian Röfer, Nick Heppert, Abdallah Ayman, Eugenio Chisari, Abhinav Valada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans seemingly incorporate potential touch signals in their perception. Our
goal is to equip robots with a similar capability, which we term \ourmodel.
\ourmodel aims to predict the expected touch signal based on a visual patch
representing the touched area. We frame this problem as the task of learning a
low-dimensional visual-tactile embedding, wherein we encode a depth patch from
which we decode the tactile signal. To accomplish this task, we employ ReSkin,
an inexpensive and replaceable magnetic-based tactile sensor. Using ReSkin, we
collect and train PseudoTouch on a dataset comprising aligned tactile and
visual data pairs obtained through random touching of eight basic geometric
shapes. We demonstrate the efficacy of PseudoTouch through its application to
two downstream tasks: object recognition and grasp stability prediction. In the
object recognition task, we evaluate the learned embedding's performance on a
set of five basic geometric shapes and five household objects. Using
PseudoTouch, we achieve an object recognition accuracy 84% after just ten
touches, surpassing a proprioception baseline. For the grasp stability task, we
use ACRONYM labels to train and evaluate a grasp success predictor using
PseudoTouch's predictions derived from virtual depth information. Our approach
yields an impressive 32% absolute improvement in accuracy compared to the
baseline relying on partial point cloud data. We make the data, code, and
trained models publicly available at http://pseudotouch.cs.uni-freiburg.de.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, 2 tables, submitted to IROS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning from Visual Demonstrations through Differentiable Nonlinear MPC
  for Personalized Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15102v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15102v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Flavia Sofia Acerbo, Jan Swevers, Tinne Tuytelaars, Tong Duy Son
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-like autonomous driving controllers have the potential to enhance
passenger perception of autonomous vehicles. This paper proposes DriViDOC: a
model for Driving from Vision through Differentiable Optimal Control, and its
application to learn personalized autonomous driving controllers from human
demonstrations. DriViDOC combines the automatic inference of relevant features
from camera frames with the properties of nonlinear model predictive control
(NMPC), such as constraint satisfaction. Our approach leverages the
differentiability of parametric NMPC, allowing for end-to-end learning of the
driving model from images to control. The model is trained on an offline
dataset comprising various driving styles collected on a motion-base driving
simulator. During online testing, the model demonstrates successful imitation
of different driving styles, and the interpreted NMPC parameters provide
insights into the achievement of specific driving behaviors. Our experimental
results show that DriViDOC outperforms other methods involving NMPC and neural
networks, exhibiting an average improvement of 20% in imitation scores.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible. Accompanying video available at:
  https://youtu.be/WxWPuAtJ08E</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Subequivariant Reinforcement Learning Framework for Coordinated Motion
  Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15100v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15100v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Wang, Xiaoyu Tan, Xihe Qiu, Chao Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective coordination is crucial for motion control with reinforcement
learning, especially as the complexity of agents and their motions increases.
However, many existing methods struggle to account for the intricate
dependencies between joints. We introduce CoordiGraph, a novel architecture
that leverages subequivariant principles from physics to enhance coordination
of motion control with reinforcement learning. This method embeds the
principles of equivariance as inherent patterns in the learning process under
gravity influence, which aids in modeling the nuanced relationships between
joints vital for motion control. Through extensive experimentation with
sophisticated agents in diverse environments, we highlight the merits of our
approach. Compared to current leading methods, CoordiGraph notably enhances
generalization and sample efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures, 2024 IEEE International Conference on Robotics
  and Automation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Feature Selection for Inverse Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15079v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15079v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daulet Baimukashev, Gokhan Alcan, Ville Kyrki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inverse reinforcement learning (IRL) is an imitation learning approach to
learning reward functions from expert demonstrations. Its use avoids the
difficult and tedious procedure of manual reward specification while retaining
the generalization power of reinforcement learning. In IRL, the reward is
usually represented as a linear combination of features. In continuous state
spaces, the state variables alone are not sufficiently rich to be used as
features, but which features are good is not known in general. To address this
issue, we propose a method that employs polynomial basis functions to form a
candidate set of features, which are shown to allow the matching of statistical
moments of state distributions. Feature selection is then performed for the
candidates by leveraging the correlation between trajectory probabilities and
feature expectations. We demonstrate the approach's effectiveness by recovering
reward functions that capture expert policies across non-linear control tasks
of increasing complexity. Code, data, and videos are available at
https://sites.google.com/view/feature4irl.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Twin Delayed Deep Deterministic Policy Gradient Algorithm for
  Autonomous Ground Vehicle Navigation via Digital Twin Perception Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kabirat Olayemi, Mien Van, Sean McLoone, Yuzhu Sun, Jack Close, Nguyen Minh Nhat, Stephen McIlvanna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous ground vehicle (UGV) navigation has the potential to revolutionize
the transportation system by increasing accessibility to disabled people,
ensure safety and convenience of use. However, UGV requires extensive and
efficient testing and evaluation to ensure its acceptance for public use. This
testing are mostly done in a simulator which result to sim2real transfer gap.
In this paper, we propose a digital twin perception awareness approach for the
control of robot navigation without prior creation of the virtual environment
(VT) environment state. To achieve this, we develop a twin delayed deep
deterministic policy gradient (TD3) algorithm that ensures collision avoidance
and goal-based path planning. We demonstrate the performance of our approach on
different environment dynamics. We show that our approach is capable of
efficiently avoiding collision with obstacles and navigating to its desired
destination, while at the same time safely avoids obstacles using the
information received from the LIDAR sensor mounted on the robot. Our approach
bridges the gap between sim-to-real transfer and contributes to the adoption of
UGVs in real world. We validate our approach in simulation and a real-world
application in an office space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking 6-Dof Grasp Detection: A Flexible Framework for High-Quality
  Grasping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15054v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15054v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Tang, Siang Chen, Pengwei Xie, Dingchang Hu, Wenming Yang, Guijin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic grasping is a primitive skill for complex tasks and is fundamental to
intelligence. For general 6-Dof grasping, most previous methods directly
extract scene-level semantic or geometric information, while few of them
consider the suitability for various downstream applications, such as
target-oriented grasping. Addressing this issue, we rethink 6-Dof grasp
detection from a grasp-centric view and propose a versatile grasp framework
capable of handling both scene-level and target-oriented grasping. Our
framework, FlexLoG, is composed of a Flexible Guidance Module and a Local Grasp
Model. Specifically, the Flexible Guidance Module is compatible with both
global (e.g., grasp heatmap) and local (e.g., visual grounding) guidance,
enabling the generation of high-quality grasps across various tasks. The Local
Grasp Model focuses on object-agnostic regional points and predicts grasps
locally and intently. Experiment results reveal that our framework achieves
over 18% and 23% improvement on unseen splits of the GraspNet-1Billion Dataset.
Furthermore, real-world robotic tests in three distinct settings yield a 95%
success rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Linear Quadratic Guidance Law for Joint Motion Planning of a
  Pursuer-Turret Assembly 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhargav Jha, Shaunak Bopardikar, Alexander Von Moll, David Casbeer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents joint motion planning of a vehicle with an attached
rotating turret. The turret has a limited range as well as the field of view.
The objective is capture a maneuvering target such that at the terminal time it
is withing the field-of-view and range limits. Catering to it, we present a
minimum effort guidance law that commensurate for the turn rate abilities of
the vehicle and the turret. The guidance law is obtained using linearization
about the collision triangle and admits an analytical solution. Simulation
results are presented to exemplify the cooperation between the turret and the
vehicle.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boundary-Aware Value Function Generation for Safe Stochastic Motion
  Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhong Xu, Kai Yin, Jason M. Gregory, Kris Hauser, Lantao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Navigation safety is critical for many autonomous systems such as
self-driving vehicles in an urban environment. It requires an explicit
consideration of boundary constraints that describe the borders of any
infeasible, non-navigable, or unsafe regions. We propose a principled
boundary-aware safe stochastic planning framework with promising results. Our
method generates a value function that can strictly distinguish the state
values between free (safe) and non-navigable (boundary) spaces in the
continuous state, naturally leading to a safe boundary-aware policy. At the
core of our solution lies a seamless integration of finite elements and
kernel-based functions, where the finite elements allow us to characterize
safety-critical states' borders accurately, and the kernel-based function
speeds up computation for the non-safety-critical states. The proposed method
was evaluated through extensive simulations and demonstrated safe navigation
behaviors in mobile navigation tasks. Additionally, we demonstrate that our
approach can maneuver safely and efficiently in cluttered real-world
environments using a ground vehicle with strong external disturbances, such as
navigating on a slippery floor and against external human intervention.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by International Journal of Robotics Research</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SRLM: Human-in-Loop Interactive Social Robot Navigation with Large
  Language Model and Deep Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weizheng Wang, Le Mao, Ruiqi Wang, Byung-Cheol Min
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An interactive social robotic assistant must provide services in complex and
crowded spaces while adapting its behavior based on real-time human language
commands or feedback. In this paper, we propose a novel hybrid approach called
Social Robot Planner (SRLM), which integrates Large Language Models (LLM) and
Deep Reinforcement Learning (DRL) to navigate through human-filled public
spaces and provide multiple social services. SRLM infers global planning from
human-in-loop commands in real-time, and encodes social information into a
LLM-based large navigation model (LNM) for low-level motion execution.
Moreover, a DRL-based planner is designed to maintain benchmarking performance,
which is blended with LNM by a large feedback model (LFM) to address the
instability of current text and LLM-driven LNM. Finally, SRLM demonstrates
outstanding performance in extensive experiments. More details about this work
are available at: https://sites.google.com/view/navi-srlm
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoNVOI: Context-aware Navigation using Vision Language Models in Outdoor
  and Indoor Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adarsh Jagan Sathyamoorthy, Kasun Weerakoon, Mohamed Elnoor, Anuj Zore, Brian Ichter, Fei Xia, Jie Tan, Wenhao Yu, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present ConVOI, a novel method for autonomous robot navigation in
real-world indoor and outdoor environments using Vision Language Models (VLMs).
We employ VLMs in two ways: first, we leverage their zero-shot image
classification capability to identify the context or scenario (e.g., indoor
corridor, outdoor terrain, crosswalk, etc) of the robot's surroundings, and
formulate context-based navigation behaviors as simple text prompts (e.g.
``stay on the pavement"). Second, we utilize their state-of-the-art semantic
understanding and logical reasoning capabilities to compute a suitable
trajectory given the identified context. To this end, we propose a novel
multi-modal visual marking approach to annotate the obstacle-free regions in
the RGB image used as input to the VLM with numbers, by correlating it with a
local occupancy map of the environment. The marked numbers ground image
locations in the real-world, direct the VLM's attention solely to navigable
locations, and elucidate the spatial relationships between them and terrains
depicted in the image to the VLM. Next, we query the VLM to select numbers on
the marked image that satisfy the context-based behavior text prompt, and
construct a reference path using the selected numbers. Finally, we propose a
method to extrapolate the reference trajectory when the robot's environmental
context has not changed to prevent unnecessary VLM queries. We use the
reference trajectory to guide a motion planner, and demonstrate that it leads
to human-like behaviors (e.g. not cutting through a group of people, using
crosswalks, etc.) in various real-world indoor and outdoor scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Global Games with Negative Feedback for Autonomous Colony Maintenance
  using Robot Teams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Logan E. Beaver
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article we address the colony maintenance problem, where a team of
robots are tasked with continuously maintaining the energy supply of an
autonomous colony. We model this as a global game, where robots measure the
energy level of a central nest to determine whether or not to forage for energy
sources. We design a mechanism that avoids the trivial equilibrium where all
robots always forage. Furthermore, we demonstrate that when the game is played
iteratively a negative feedback term stabilizes the number of foraging robots
at a non-trivial Nash equilibrium. We compare our approach qualitatively to
existing global games, where a positive positive feedback term admits
threshold-based decision making, and encourages many robots to forage
simultaneously. We discuss how positive feedback can lead to a cascading
failure in the presence of a human who recruits robots for external tasks, and
we demonstrate the performance of our approach in simulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Autonomous Driving With Perception Uncertainties: Deep-Ensemble Based
  Adaptive Cruise Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Li, H. Eric Tseng, Anouck Girard, Ilya Kolmanovsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving depends on perception systems to understand the
environment and to inform downstream decision-making. While advanced perception
systems utilizing black-box Deep Neural Networks (DNNs) demonstrate human-like
comprehension, their unpredictable behavior and lack of interpretability may
hinder their deployment in safety critical scenarios. In this paper, we develop
an Ensemble of DNN regressors (Deep Ensemble) that generates predictions with
quantification of prediction uncertainties. In the scenario of Adaptive Cruise
Control (ACC), we employ the Deep Ensemble to estimate distance headway to the
lead vehicle from RGB images and enable the downstream controller to account
for the estimation uncertainty. We develop an adaptive cruise controller that
utilizes Stochastic Model Predictive Control (MPC) with chance constraints to
provide a probabilistic safety guarantee. We evaluate our ACC algorithm using a
high-fidelity traffic simulator and a real-world traffic dataset and
demonstrate the ability of the proposed approach to effect speed tracking and
car following while maintaining a safe distance headway. The
out-of-distribution scenarios are also examined.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Music to Dance as Language Translation using Sequence Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        André Correia, Luís A. Alexandre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthesising appropriate choreographies from music remains an open problem.
We introduce MDLT, a novel approach that frames the choreography generation
problem as a translation task. Our method leverages an existing data set to
learn to translate sequences of audio into corresponding dance poses. We
present two variants of MDLT: one utilising the Transformer architecture and
the other employing the Mamba architecture. We train our method on AIST++ and
PhantomDance data sets to teach a robotic arm to dance, but our method can be
applied to a full humanoid robot. Evaluation metrics, including Average Joint
Error and Frechet Inception Distance, consistently demonstrate that, when given
a piece of music, MDLT excels at producing realistic and high-quality
choreography. The code can be found at github.com/meowatthemoon/MDLT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Hierarchical Control For Multi-Agent Capacity-Constrained
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14545v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14545v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charlott Vallon, Alessandro Pinto, Bartolomeo Stellato, Francesco Borrelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel data-driven hierarchical control scheme for
managing a fleet of nonlinear, capacity-constrained autonomous agents in an
iterative environment. We propose a control framework consisting of a
high-level dynamic task assignment and routing layer and low-level motion
planning and tracking layer. Each layer of the control hierarchy uses a
data-driven Model Predictive Control (MPC) policy, maintaining bounded
computational complexity at each calculation of a new task assignment or
actuation input. We utilize collected data to iteratively refine estimates of
agent capacity usage, and update MPC policy parameters accordingly. Our
approach leverages tools from iterative learning control to integrate learning
at both levels of the hierarchy, and coordinates learning between levels in
order to maintain closed-loop feasibility and performance improvement of the
connected architecture.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instance-aware Exploration-Verification-Exploitation for Instance
  ImageGoal Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17587v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17587v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohan Lei, Min Wang, Wengang Zhou, Li Li, Houqiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a new embodied vision task, Instance ImageGoal Navigation (IIN) aims to
navigate to a specified object depicted by a goal image in an unexplored
environment.
  The main challenge of this task lies in identifying the target object from
different viewpoints while rejecting similar distractors.
  Existing ImageGoal Navigation methods usually adopt the simple
Exploration-Exploitation framework and ignore the identification of specific
instance during navigation.
  In this work, we propose to imitate the human behaviour of ``getting closer
to confirm" when distinguishing objects from a distance.
  Specifically, we design a new modular navigation framework named
Instance-aware Exploration-Verification-Exploitation (IEVE) for instance-level
image goal navigation.
  Our method allows for active switching among the exploration, verification,
and exploitation actions, thereby facilitating the agent in making reasonable
decisions under different situations.
  On the challenging HabitatMatterport 3D semantic (HM3D-SEM) dataset, our
method surpasses previous state-of-the-art work, with a classical segmentation
model (0.684 vs. 0.561 success) or a robust model (0.702 vs. 0.561 success)
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gaussian-SLAM: Photo-realistic Dense SLAM with Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10070v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10070v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vladimir Yugay, Yue Li, Theo Gevers, Martin R. Oswald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a dense simultaneous localization and mapping (SLAM) method that
uses 3D Gaussians as a scene representation. Our approach enables
interactive-time reconstruction and photo-realistic rendering from real-world
single-camera RGBD videos. To this end, we propose a novel effective strategy
for seeding new Gaussians for newly explored areas and their effective online
optimization that is independent of the scene size and thus scalable to larger
scenes. This is achieved by organizing the scene into sub-maps which are
independently optimized and do not need to be kept in memory. We further
accomplish frame-to-model camera tracking by minimizing photometric and
geometric losses between the input and rendered frames. The Gaussian
representation allows for high-quality photo-realistic real-time rendering of
real-world scenes. Evaluation on synthetic and real-world datasets demonstrates
competitive or superior performance in mapping, tracking, and rendering
compared to existing neural dense SLAM methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Convex Formulation of Frictional Contact for the Material Point Method
  and Rigid Bodies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13783v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13783v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeshun Zong, Chenfanfu Jiang, Xuchen Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a novel convex formulation that seamlessly
integrates the Material Point Method (MPM) with articulated rigid body dynamics
in frictional contact scenarios. We extend the linear corotational hyperelastic
model into the realm of elastoplasticity and include an efficient return
mapping algorithm. This approach is particularly effective for MPM simulations
involving significant deformation and topology changes, while preserving the
convexity of the optimization problem. Our method ensures global convergence,
enabling the use of large simulation time steps without compromising
robustness. We have validated our approach through rigorous testing and
performance evaluations, highlighting its superior capabilities in managing
complex simulations relevant to robotics. Compared to previous MPM based
robotic simulators, our method significantly improves the stability of contact
resolution -- a critical factor in robot manipulation tasks. We make our method
available in the open-source robotics toolkit, Drake.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The supplemental video is available at https://youtu.be/5jrQtF5D0DA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning High-level Semantic-Relational Concepts for SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00401v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00401v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jose Andres Millan-Romera, Hriday Bavle, Muhammad Shaheer, Martin R. Oswald, Holger Voos, Jose Luis Sanchez-Lopez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works on SLAM extend their pose graphs with higher-level semantic
concepts like Rooms exploiting relationships between them, to provide, not only
a richer representation of the situation/environment but also to improve the
accuracy of its estimation. Concretely, our previous work, Situational Graphs
(S-Graphs+), a pioneer in jointly leveraging semantic relationships in the
factor optimization process, relies on semantic entities such as Planes and
Rooms, whose relationship is mathematically defined. Nevertheless, there is no
unique approach to finding all the hidden patterns in lower-level factor-graphs
that correspond to high-level concepts of different natures. It is currently
tackled with ad-hoc algorithms, which limits its graph expressiveness.
  To overcome this limitation, in this work, we propose an algorithm based on
Graph Neural Networks for learning high-level semantic-relational concepts that
can be inferred from the low-level factor graph. Given a set of mapped Planes
our algorithm is capable of inferring Room entities relating to the Planes.
Additionally, to demonstrate the versatility of our method, our algorithm can
infer an additional semantic-relational concept, i.e. Wall, and its
relationship with its Planes. We validate our method in both simulated and real
datasets demonstrating improved performance over two baseline approaches.
Furthermore, we integrate our method into the S-Graphs+ algorithm providing
improved pose and map accuracy compared to the baseline while further enhancing
the scene representation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LaMI: Large Language Models for Multi-Modal Human-Robot Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15174v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15174v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Wang, Stephan Hasler, Daniel Tanneberg, Felix Ocker, Frank Joublin, Antonello Ceravola, Joerg Deigmoeller, Michael Gienger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an innovative large language model (LLM)-based robotic
system for enhancing multi-modal human-robot interaction (HRI). Traditional HRI
systems relied on complex designs for intent estimation, reasoning, and
behavior generation, which were resource-intensive. In contrast, our system
empowers researchers and practitioners to regulate robot behavior through three
key aspects: providing high-level linguistic guidance, creating "atomic
actions" and expressions the robot can use, and offering a set of examples.
Implemented on a physical robot, it demonstrates proficiency in adapting to
multi-modal inputs and determining the appropriate manner of action to assist
humans with its arms, following researchers' defined guidelines.
Simultaneously, it coordinates the robot's lid, neck, and ear movements with
speech output to produce dynamic, multi-modal expressions. This showcases the
system's potential to revolutionize HRI by shifting from conventional, manual
state-and-flow design methods to an intuitive, guidance-based, and
example-driven approach. Supplementary material can be found at
https://hri-eu.github.io/Lami/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bi-KVIL: Keypoints-based Visual Imitation Learning of Bimanual
  Manipulation Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03270v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03270v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianfeng Gao, Xiaoshu Jin, Franziska Krebs, Noémie Jaquier, Tamim Asfour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual imitation learning has achieved impressive progress in learning
unimanual manipulation tasks from a small set of visual observations, thanks to
the latest advances in computer vision. However, learning bimanual coordination
strategies and complex object relations from bimanual visual demonstrations, as
well as generalizing them to categorical objects in novel cluttered scenes
remain unsolved challenges. In this paper, we extend our previous work on
keypoints-based visual imitation learning (\mbox{K-VIL})~\cite{gao_kvil_2023}
to bimanual manipulation tasks. The proposed Bi-KVIL jointly extracts so-called
\emph{Hybrid Master-Slave Relationships} (HMSR) among objects and hands,
bimanual coordination strategies, and sub-symbolic task representations. Our
bimanual task representation is object-centric, embodiment-independent, and
viewpoint-invariant, thus generalizing well to categorical objects in novel
scenes. We evaluate our approach in various real-world applications, showcasing
its ability to learn fine-grained bimanual manipulation tasks from a small
number of human demonstration videos. Videos and source code are available at
https://sites.google.com/view/bi-kvil.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Event-based Simultaneous Localization and Mapping: A Comprehensive
  <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.09793v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.09793v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunping Huang, Sen Zhang, Jing Zhang, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent decades, visual simultaneous localization and mapping (vSLAM) has
gained significant interest in both academia and industry. It estimates camera
motion and reconstructs the environment concurrently using visual sensors on a
moving robot. However, conventional cameras are limited by hardware, including
motion blur and low dynamic range, which can negatively impact performance in
challenging scenarios like high-speed motion and high dynamic range
illumination. Recent studies have demonstrated that event cameras, a new type
of bio-inspired visual sensor, offer advantages such as high temporal
resolution, dynamic range, low power consumption, and low latency. This paper
presents a timely and comprehensive review of event-based vSLAM algorithms that
exploit the benefits of asynchronous and irregular event streams for
localization and mapping tasks. The review covers the working principle of
event cameras and various event representations for preprocessing event data.
It also categorizes event-based vSLAM methods into four main categories:
feature-based, direct, motion-compensation, and deep learning methods, with
detailed discussions and practical guidance for each approach. Furthermore, the
paper evaluates the state-of-the-art methods on various benchmarks,
highlighting current challenges and future opportunities in this emerging
research area. A public repository will be maintained to keep track of the
rapid developments in this field at
{\url{https://github.com/kun150kun/ESLAM-survey}}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Direct Data-Driven Control for Probabilistic Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.16973v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.16973v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander von Rohr, Dmitrii Likhachev, Sebastian Trimpe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a data-driven control method for systems with aleatoric
uncertainty, for example, robot fleets with variations between agents. Our
method leverages shared trajectory data to increase the robustness of the
designed controller and thus facilitate transfer to new variations without the
need for prior parameter and uncertainty estimations. In contrast to existing
work on experience transfer for performance, our approach focuses on robustness
and uses data collected from multiple realizations to guarantee generalization
to unseen ones. Our method is based on scenario optimization combined with
recent formulations for direct data-driven control. We derive lower bounds on
the amount of data required to achieve quadratic stability for probabilistic
systems with aleatoric uncertainty and demonstrate the benefits of our
data-driven method through a numerical example. We find that the learned
controllers generalize well to high variations in the dynamics even when based
on only a few short open-loop trajectories. Robust experience transfer enables
the design of safe and robust controllers that work out of the box without any
additional learning during deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Wind-Aware Path Planning Method for UAV-Asisted Bridge Inspection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.10519v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.10519v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Xu, Hua Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In response to the gap in considering wind conditions in the bridge
inspection using unmanned aerial vehicle (UAV) , this paper proposes a path
planning method for UAVs that takes into account the influence of wind, based
on the simulated annealing algorithm. The algorithm considers the wind factors,
including the influence of different wind speeds and directions at the same
time on the path planning of the UAV. Firstly, An environment model is
constructed specifically for UAV bridge inspection, taking into account the
various objective functions and constraint conditions of UAVs. A more
sophisticated and precise mathematical model is then developed based on this
environmental model to enable efficient and effective UAV path planning.
Secondly, the bridge separation planning model is applied in a novel way, and a
series of parameters are simulated, including the adjustment of the initial
temperature value. The experimental results demonstrate that, compared with
traditional local search algorithms, the proposed method achieves a cost
reduction of 30.05\% and significantly improves effectiveness. Compared to path
planning methods that do not consider wind factors, the proposed approach
yields more realistic and practical results for UAV applications, as
demonstrated by its improved effectiveness in simulations. These findings
highlight the value of our method in facilitating more accurate and efficient
UAV path planning in wind-prone environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>After carefully analysis, there is a bit design flaws in Algorithm 1.
  The experimental work of the paper is not comprehensive,which lacks an
  evaluation of the algorithm's running time</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Global LiDAR Localization: Challenges, Advances and Open
  Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.07433v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.07433v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huan Yin, Xuecheng Xu, Sha Lu, Xieyuanli Chen, Rong Xiong, Shaojie Shen, Cyrill Stachniss, Yue Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge about the own pose is key for all mobile robot applications. Thus
pose estimation is part of the core functionalities of mobile robots. Over the
last two decades, LiDAR scanners have become the standard sensor for robot
localization and mapping. This article aims to provide an overview of recent
progress and advancements in LiDAR-based global localization. We begin by
formulating the problem and exploring the application scope. We then present a
review of the methodology, including recent advancements in several topics,
such as maps, descriptor extraction, and cross-robot localization. The contents
of the article are organized under three themes. The first theme concerns the
combination of global place retrieval and local pose estimation. The second
theme is upgrading single-shot measurements to sequential ones for sequential
global localization. Finally, the third theme focuses on extending single-robot
global localization to cross-robot localization in multi-robot systems. We
conclude the survey with a discussion of open challenges and promising
directions in global LiDAR localization. To our best knowledge, this is the
first comprehensive survey on global LiDAR localization for mobile robots.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Publishe on International Journal of Computer Vision (IJCV)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kinematic Modularity of Elementary Dynamic Actions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15271v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15271v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moses C. Nah, Johannes Lachner, Federico Tessari, Neville Hogan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, a kinematically modular approach to robot control is
presented. The method involves structures called Elementary Dynamic Actions and
a network model combining these elements. With this control framework, a rich
repertoire of movements can be generated by combination of basic modules. The
problems of solving inverse kinematics, managing kinematic singularity and
kinematic redundancy are avoided. The modular approach is robust against
contact and physical interaction, which makes it particularly effective for
contact-rich manipulation. Each kinematic module can be learned by Imitation
Learning, thereby resulting in a modular learning strategy for robot control.
The theoretical foundations and their real robot implementation are presented.
Using a KUKA LBR iiwa14 robot, three tasks were considered: (1) generating a
sequence of discrete movements, (2) generating a combination of discrete and
rhythmic movements, and (3) a drawing and erasing task. The results obtained
indicate that this modular approach has the potential to simplify the
generation of a diverse range of robot actions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ iSLAM: Imperative SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.07894v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.07894v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taimeng Fu, Shaoshu Su, Yiren Lu, Chen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneous Localization and Mapping (SLAM) stands as one of the critical
challenges in robot navigation. A SLAM system often consists of a front-end
component for motion estimation and a back-end system for eliminating
estimation drifts. Recent advancements suggest that data-driven methods are
highly effective for front-end tasks, while geometry-based methods continue to
be essential in the back-end processes. However, such a decoupled paradigm
between the data-driven front-end and geometry-based back-end can lead to
sub-optimal performance, consequently reducing the system's capabilities and
generalization potential. To solve this problem, we proposed a novel
self-supervised imperative learning framework, named imperative SLAM (iSLAM),
which fosters reciprocal correction between the front-end and back-end, thus
enhancing performance without necessitating any external supervision.
Specifically, we formulate the SLAM problem as a bilevel optimization so that
the front-end and back-end are bidirectionally connected. As a result, the
front-end model can learn global geometric knowledge obtained through pose
graph optimization by back-propagating the residuals from the back-end
component. We showcase the effectiveness of this new framework through an
application of stereo-inertial SLAM. The experiments show that the iSLAM
training strategy achieves an accuracy improvement of 22% on average over a
baseline model. To the best of our knowledge, iSLAM is the first SLAM system
showing that the front-end and back-end components can mutually correct each
other in a self-supervised manner.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been accepted by IEEE Robotics and Automation Letters
  (RA-L)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoTAMP: Autoregressive Task and Motion Planning with LLMs as
  Translators and Checkers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06531v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06531v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongchao Chen, Jacob Arkin, Charles Dawson, Yang Zhang, Nicholas Roy, Chuchu Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For effective human-robot interaction, robots need to understand, plan, and
execute complex, long-horizon tasks described by natural language. Recent
advances in large language models (LLMs) have shown promise for translating
natural language into robot action sequences for complex tasks. However,
existing approaches either translate the natural language directly into robot
trajectories or factor the inference process by decomposing language into task
sub-goals and relying on a motion planner to execute each sub-goal. When
complex environmental and temporal constraints are involved, inference over
planning tasks must be performed jointly with motion plans using traditional
task-and-motion planning (TAMP) algorithms, making factorization into subgoals
untenable. Rather than using LLMs to directly plan task sub-goals, we instead
perform few-shot translation from natural language task descriptions to an
intermediate task representation that can then be consumed by a TAMP algorithm
to jointly solve the task and motion plan. To improve translation, we
automatically detect and correct both syntactic and semantic errors via
autoregressive re-prompting, resulting in significant improvements in task
completion. We show that our approach outperforms several methods using LLMs as
planners in complex task domains. See our project website
https://yongchao98.github.io/MIT-REALM-AutoTAMP/ for prompts, videos, and code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Multi-Robot Collaboration with Large Language Models:
  Centralized or Decentralized Systems? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15943v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15943v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongchao Chen, Jacob Arkin, Yang Zhang, Nicholas Roy, Chuchu Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A flurry of recent work has demonstrated that pre-trained large language
models (LLMs) can be effective task planners for a variety of single-robot
tasks. The planning performance of LLMs is significantly improved via prompting
techniques, such as in-context learning or re-prompting with state feedback,
placing new importance on the token budget for the context window. An
under-explored but natural next direction is to investigate LLMs as multi-robot
task planners. However, long-horizon, heterogeneous multi-robot planning
introduces new challenges of coordination while also pushing up against the
limits of context window length. It is therefore critical to find
token-efficient LLM planning frameworks that are also able to reason about the
complexities of multi-robot coordination. In this work, we compare the task
success rate and token efficiency of four multi-agent communication frameworks
(centralized, decentralized, and two hybrid) as applied to four
coordination-dependent multi-agent 2D task scenarios for increasing numbers of
agents. We find that a hybrid framework achieves better task success rates
across all four tasks and scales better to more agents. We further demonstrate
the hybrid frameworks in 3D simulations where the vision-to-text problem and
dynamical errors are considered. See our project website
https://yongchao98.github.io/MIT-REALM-Multi-Robot/ for prompts, videos, and
code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Complex Motion Plans using Neural ODEs with Safety and
  Stability Guarantees <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.00186v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.00186v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farhad Nawaz, Tianyu Li, Nikolai Matni, Nadia Figueroa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a Dynamical System (DS) approach to learn complex, possibly
periodic motion plans from kinesthetic demonstrations using Neural Ordinary
Differential Equations (NODE). To ensure reactivity and robustness to
disturbances, we propose a novel approach that selects a target point at each
time step for the robot to follow, by combining tools from control theory and
the target trajectory generated by the learned NODE. A correction term to the
NODE model is computed online by solving a quadratic program that guarantees
stability and safety using control Lyapunov functions and control barrier
functions, respectively. Our approach outperforms baseline DS learning
techniques on the LASA handwriting dataset and complex periodic trajectories.
It is also validated on the Franka Emika robot arm to produce stable motions
for wiping and stirring tasks that do not have a single attractor, while being
robust to perturbations and safe around humans and obstacles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Traffic Management Framework for On-Demand Urban Air Mobility Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.07139v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.07139v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Milad Pooladsanj, Ketan Savla, Petros A. Ioannou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Urban Air Mobility (UAM) offers a solution to current traffic congestion by
providing on-demand air mobility in urban areas. Effective traffic management
is crucial for efficient operation of UAM systems, especially for high-demand
scenarios. In this paper, we present a centralized traffic management framework
for on-demand UAM systems. Specifically, we provide a scheduling policy, called
VertiSync, which schedules the aircraft for either servicing trip requests or
rebalancing in the system subject to aircraft safety margins and energy
requirements. We characterize the system-level throughput of VertiSync, which
determines the demand threshold at which passenger waiting times transition
from being stabilized to being increasing over time. We show that the proposed
policy is able to maximize throughput for sufficiently large fleet sizes. We
demonstrate the performance of VertiSync through a case study for the city of
Los Angeles, and show that it significantly reduces passenger waiting times
compared to a first-come first-serve scheduling policy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kinematics-aware Trajectory Generation and Prediction with Latent
  Stochastic Differential Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09317v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09317v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruochen Jiao, Yixuan Wang, Xiangguo Liu, Chao Huang, Qi Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory generation and trajectory prediction are two critical tasks in
autonomous driving, which generate various trajectories for testing during
development and predict the trajectories of surrounding vehicles during
operation, respectively. In recent years, emerging data-driven deep
learning-based methods have shown great promise for these two tasks in learning
various traffic scenarios and improving average performance without assuming
physical models. However, it remains a challenging problem for these methods to
ensure that the generated/predicted trajectories are physically realistic. This
challenge arises because learning-based approaches often function as opaque
black boxes and do not adhere to physical laws. Conversely, existing
model-based methods provide physically feasible results but are constrained by
predefined model structures, limiting their capabilities to address complex
scenarios. To address the limitations of these two types of approaches, we
propose a new method that integrates kinematic knowledge into neural stochastic
differential equations (SDE) and designs a variational autoencoder based on
this latent kinematics-aware SDE (LK-SDE) to generate vehicle motions.
Experimental results demonstrate that our method significantly outperforms both
model-based and learning-based baselines in producing physically realistic and
precisely controllable vehicle trajectories. Additionally, it performs well in
predicting unobservable physical variables in the latent space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, conference paper in motion generation</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-21T00:00:00Z">2024-03-21</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">61</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ODTFormer: Efficient Obstacle Detection and Tracking with Stereo Cameras
  Based on <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianye Ding, Hongyu Li, Huaizu Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Obstacle detection and tracking represent a critical component in robot
autonomous navigation. In this paper, we propose ODTFormer, a Transformer-based
model to address both obstacle detection and tracking problems. For the
detection task, our approach leverages deformable attention to construct a 3D
cost volume, which is decoded progressively in the form of voxel occupancy
grids. We further track the obstacles by matching the voxels between
consecutive frames. The entire model can be optimized in an end-to-end manner.
Through extensive experiments on DrivingStereo and KITTI benchmarks, our model
achieves state-of-the-art performance in the obstacle detection task. We also
report comparable accuracy to state-of-the-art obstacle tracking models while
requiring only a fraction of their computation cost, typically ten-fold to
twenty-fold less. The code and model weights will be publicly released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SDP Synthesis of Maximum Coverage Trees for Probabilistic Planning under
  Control Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naman Aggarwal, Jonathan P. How
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper presents Maximal Covariance Backward Reachable Trees (MAXCOVAR
BRT), which is a multi-query algorithm for planning of dynamic systems under
stochastic motion uncertainty and constraints on the control input with
explicit coverage guarantees. In contrast to existing roadmap-based
probabilistic planning methods that sample belief nodes randomly and draw edges
between them \cite{csbrm_tro2024}, under control constraints, the reachability
of belief nodes needs to be explicitly established and is determined by
checking the feasibility of a non-convex program. Moreover, there is no
explicit consideration of coverage of the roadmap while adding nodes and edges
during the construction procedure for the existing methods. Our contribution is
a novel optimization formulation to add nodes and construct the corresponding
edge controllers such that the generated roadmap results in provably maximal
coverage under control constraints as compared to any other method of adding
nodes and edges. We characterize formally the notion of coverage of a roadmap
in this stochastic domain via introduction of the h-$\operatorname{BRS}$
(Backward Reachable Set of Distributions) of a tree of distributions under
control constraints, and also support our method with extensive simulations on
a 6 DoF model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extended Reality for Enhanced Human-Robot Collaboration: a
  Human-in-the-Loop Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yehor Karpichev, Todd Charter, Homayoun Najjaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of automation has provided an opportunity to achieve higher
efficiency in manufacturing processes, yet it often compromises the flexibility
required to promptly respond to evolving market needs and meet the demand for
customization. Human-robot collaboration attempts to tackle these challenges by
combining the strength and precision of machines with human ingenuity and
perceptual understanding. In this paper, we conceptualize and propose an
implementation framework for an autonomous, machine learning-based manipulator
that incorporates human-in-the-loop principles and leverages Extended Reality
(XR) to facilitate intuitive communication and programming between humans and
robots. Furthermore, the conceptual framework foresees human involvement
directly in the robot learning process, resulting in higher adaptability and
task generalization. The paper highlights key technologies enabling the
proposed framework, emphasizing the importance of developing the digital
ecosystem as a whole. Additionally, we review the existent implementation
approaches of XR in human-robot collaboration, showcasing diverse perspectives
and methodologies. The challenges and future outlooks are discussed, delving
into the major obstacles and potential research avenues of XR for more natural
human-robot interaction and integration in the industrial landscape.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VXP: Voxel-Cross-Pixel Large-scale Image-LiDAR Place Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun-Jin Li, Mariia Gladkova, Yan Xia, Rui Wang, Daniel Cremers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works on the global place recognition treat the task as a retrieval
problem, where an off-the-shelf global descriptor is commonly designed in
image-based and LiDAR-based modalities. However, it is non-trivial to perform
accurate image-LiDAR global place recognition since extracting consistent and
robust global descriptors from different domains (2D images and 3D point
clouds) is challenging. To address this issue, we propose a novel
Voxel-Cross-Pixel (VXP) approach, which establishes voxel and pixel
correspondences in a self-supervised manner and brings them into a shared
feature space. Specifically, VXP is trained in a two-stage manner that first
explicitly exploits local feature correspondences and enforces similarity of
global descriptors. Extensive experiments on the three benchmarks (Oxford
RobotCar, ViViD++ and KITTI) demonstrate our method surpasses the
state-of-the-art cross-modal retrieval by a large margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page https://yunjinli.github.io/projects-vxp/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Co-Optimization of Environment and Policies for Decentralized
  Multi-Agent Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhan Gao, Guang Yang, Amanda Prorok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work views the multi-agent system and its surrounding environment as a
co-evolving system, where the behavior of one affects the other. The goal is to
take both agent actions and environment configurations as decision variables,
and optimize these two components in a coordinated manner to improve some
measure of interest. Towards this end, we consider the problem of decentralized
multi-agent navigation in cluttered environments. By introducing two
sub-objectives of multi-agent navigation and environment optimization, we
propose an $\textit{agent-environment co-optimization}$ problem and develop a
$\textit{coordinated algorithm}$ that alternates between these sub-objectives
to search for an optimal synthesis of agent actions and obstacle configurations
in the environment; ultimately, improving the navigation performance. Due to
the challenge of explicitly modeling the relation between agents, environment
and performance, we leverage policy gradient to formulate a model-free learning
mechanism within the coordinated framework. A formal convergence analysis shows
that our coordinated algorithm tracks the local minimum trajectory of an
associated time-varying non-convex optimization problem. Extensive numerical
results corroborate theoretical findings and show the benefits of
co-optimization over baselines. Interestingly, the results also indicate that
optimized environment configurations are able to offer structural guidance that
is key to de-conflicting agents in motion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Hierarchical Control For Constrained Dynamic Task Assignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charlott Vallon, Alessandro Pinto, Bartolomeo Stellato, Francesco Borrelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel data-driven hierarchical control scheme for
managing a fleet of nonlinear, capacity-constrained autonomous agents in an
iterative environment. We propose a control framework consisting of a
high-level dynamic task assignment and routing layer and low-level motion
planning and tracking layer. Each layer of the control hierarchy uses a
data-driven MPC policy, maintaining bounded computational complexity at each
calculation of a new task assignment or actuation input. We utilize collected
data to iteratively refine estimates of agent capacity usage, and update MPC
policy parameters accordingly. Our approach leverages tools from iterative
learning control to integrate learning at both levels of the hierarchy, and
coordinates learning between levels in order to maintain closed-loop
feasibility and performance improvement of the connected architecture.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Click to Grasp: Zero-Shot Precise Manipulation via Visual Diffusion
  Descriptors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Tsagkas, Jack Rome, Subramanian Ramamoorthy, Oisin Mac Aodha, Chris Xiaoxuan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise manipulation that is generalizable across scenes and objects remains
a persistent challenge in robotics. Current approaches for this task heavily
depend on having a significant number of training instances to handle objects
with pronounced visual and/or geometric part ambiguities. Our work explores the
grounding of fine-grained part descriptors for precise manipulation in a
zero-shot setting by utilizing web-trained text-to-image diffusion-based
generative models. We tackle the problem by framing it as a dense semantic part
correspondence task. Our model returns a gripper pose for manipulating a
specific part, using as reference a user-defined click from a source image of a
visually different instance of the same object. We require no manual grasping
demonstrations as we leverage the intrinsic object geometry and features.
Practical experiments in a real-world tabletop scenario validate the efficacy
of our approach, demonstrating its potential for advancing semantic-aware
robotics manipulation. Web page: https://tsagkas.github.io/click2grasp
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-Based Causal Reasoning for Safe & Robust Next-Best Action
  Selection in Robot Manipulation Tasks <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14488v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14488v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ricardo Cannizzaro, Michael Groom, Jonathan Routley, Robert Osazuwa Ness, Lars Kunze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safe and efficient object manipulation is a key enabler of many real-world
robot applications. However, this is challenging because robot operation must
be robust to a range of sensor and actuator uncertainties. In this paper, we
present a physics-informed causal-inference-based framework for a robot to
probabilistically reason about candidate actions in a block stacking task in a
partially observable setting. We integrate a physics-based simulation of the
rigid-body system dynamics with a causal Bayesian network (CBN) formulation to
define a causal generative probabilistic model of the robot decision-making
process. Using simulation-based Monte Carlo experiments, we demonstrate our
framework's ability to successfully: (1) predict block tower stability with
high accuracy (Pred Acc: 88.6%); and, (2) select an approximate next-best
action for the block stacking task, for execution by an integrated robot
system, achieving 94.2% task success rate. We also demonstrate our framework's
suitability for real-world robot systems by demonstrating successful task
executions with a domestic support robot, with perception and manipulation
sub-system integration. Hence, we show that by embedding physics-based causal
reasoning into robots' decision-making processes, we can make robot task
execution safer, more reliable, and more robust to various types of
uncertainty.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures, submitted to 2024 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bringing Robots Home: The Rise of AI Robots in Consumer Electronics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14449v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14449v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiwei Dong, Yang Liu, Ted Chu, Abdulmotaleb El Saddik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  On March 18, 2024, NVIDIA unveiled Project GR00T, a general-purpose
multimodal generative AI model designed specifically for training humanoid
robots. Preceding this event, Tesla's unveiling of the Optimus Gen 2 humanoid
robot on December 12, 2023, underscored the profound impact robotics is poised
to have on reshaping various facets of our daily lives. While robots have long
dominated industrial settings, their presence within our homes is a burgeoning
phenomenon. This can be attributed, in part, to the complexities of domestic
environments and the challenges of creating robots that can seamlessly
integrate into our daily routines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Consumer Electronics Magazine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring 3D Human Pose Estimation and Forecasting from the Robot's
  Perspective: The HARPER <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Avogaro. Andrea Toaiari, Federico Cunico, Xiangmin Xu, Haralambos Dafas, Alessandro Vinciarelli, Emma Li, Marco Cristani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce HARPER, a novel dataset for 3D body pose estimation and forecast
in dyadic interactions between users and \spot, the quadruped robot
manufactured by Boston Dynamics. The key-novelty is the focus on the robot's
perspective, i.e., on the data captured by the robot's sensors. These make 3D
body pose analysis challenging because being close to the ground captures
humans only partially. The scenario underlying HARPER includes 15 actions, of
which 10 involve physical contact between the robot and users. The Corpus
contains not only the recordings of the built-in stereo cameras of Spot, but
also those of a 6-camera OptiTrack system (all recordings are synchronized).
This leads to ground-truth skeletal representations with a precision lower than
a millimeter. In addition, the Corpus includes reproducible benchmarks on 3D
Human Pose Estimation, Human Pose Forecasting, and Collision Prediction, all
based on publicly available baseline approaches. This enables future HARPER
users to rigorously compare their results with those we provide in this work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Model Learning and Adaptive Tracking Control of Magnetic
  Micro-Robots for Non-Contact Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14414v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14414v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongyi Jia, Shu Miao, Junjian Zhou, Niandong Jiao, Lianqing Liu, Xiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Magnetic microrobots can be navigated by an external magnetic field to
autonomously move within living organisms with complex and unstructured
environments. Potential applications include drug delivery, diagnostics, and
therapeutic interventions. Existing techniques commonly impart magnetic
properties to the target object,or drive the robot to contact and then
manipulate the object, both probably inducing physical damage. This paper
considers a non-contact formulation, where the robot spins to generate a
repulsive field to push the object without physical contact. Under such a
formulation, the main challenge is that the motion model between the input of
the magnetic field and the output velocity of the target object is commonly
unknown and difficult to analyze. To deal with it, this paper proposes a
data-driven-based solution. A neural network is constructed to efficiently
estimate the motion model. Then, an approximate model-based optimal control
scheme is developed to push the object to track a time-varying trajectory,
maintaining the non-contact with distance constraints. Furthermore, a
straightforward planner is introduced to assess the adaptability of non-contact
manipulation in a cluttered unstructured environment. Experimental results are
presented to show the tracking and navigation performance of the proposed
scheme.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures, received by 2024 IEEE International Conference on
  Robotics and Automation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DaCapo: Accelerating Continuous Learning in Autonomous Systems for Video
  Analytics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14353v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14353v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoonsung Kim, Changhun Oh, Jinwoo Hwang, Wonung Kim, Seongryong Oh, Yubin Lee, Hardik Sharma, Amir Yazdanbakhsh, Jongse Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural network (DNN) video analytics is crucial for autonomous systems
such as self-driving vehicles, unmanned aerial vehicles (UAVs), and security
robots. However, real-world deployment faces challenges due to their limited
computational resources and battery power. To tackle these challenges,
continuous learning exploits a lightweight "student" model at deployment
(inference), leverages a larger "teacher" model for labeling sampled data
(labeling), and continuously retrains the student model to adapt to changing
scenarios (retraining). This paper highlights the limitations in
state-of-the-art continuous learning systems: (1) they focus on computations
for retraining, while overlooking the compute needs for inference and labeling,
(2) they rely on power-hungry GPUs, unsuitable for battery-operated autonomous
systems, and (3) they are located on a remote centralized server, intended for
multi-tenant scenarios, again unsuitable for autonomous systems due to privacy,
network availability, and latency concerns. We propose a hardware-algorithm
co-designed solution for continuous learning, DaCapo, that enables autonomous
systems to perform concurrent executions of inference, labeling, and training
in a performant and energy-efficient manner. DaCapo comprises (1) a
spatially-partitionable and precision-flexible accelerator enabling parallel
execution of kernels on sub-accelerators at their respective precisions, and
(2) a spatiotemporal resource allocation algorithm that strategically navigates
the resource-accuracy tradeoff space, facilitating optimal decisions for
resource allocation to achieve maximal accuracy. Our evaluation shows that
DaCapo achieves 6.5% and 5.5% higher accuracy than a state-of-the-art GPU-based
continuous learning systems, Ekya and EOMU, respectively, while consuming 254x
less power.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comparative Study of Real-Time Implementable Cooperative Aerial
  Manipulation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stamatina C. Barakou, Costas S. Tzafestas, Kimon P. Valavanis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This survey paper focuses on quadrotor- and multirotor- based cooperative
aerial manipulation. Emphasis is first given on comparing and evaluating
prototype systems that have been implemented and tested in real-time in diverse
application environments. Underlying modeling and control approaches are also
discussed and compared. The outcome of the survey allows for understanding the
motivation and rationale to develop such systems, their applicability and
implementability in diverse applications and also challenges that need to be
addressed and overcome. Moreover, the survey provides a guide to develop the
next generation of prototype systems based on preferred characteristics,
functionality, operability and application domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to MDPI Drones</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tell Me What You Want (What You Really, Really Want): Addressing the
  Expectation Gap for Goal Conveyance from Humans to Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Leahy, Ho Chit Siu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conveying human goals to autonomous systems (AS) occurs both when the system
is being designed and when it is being operated. The design-step conveyance is
typically mediated by robotics and AI engineers, who must appropriately capture
end-user requirements and concepts of operations, while the operation-step
conveyance is mediated by the design, interfaces, and behavior of the AI.
However, communication can be difficult during both these periods because of
mismatches in the expectations and expertise of the end-user and the
roboticist, necessitating more design cycles to resolve. We examine some of the
barriers in communicating system design requirements, and develop an
augmentation for applied cognitive task analysis (ACTA) methods, that we call
robot task analysis (RTA), pertaining specifically to the development of
autonomous systems. Further, we introduce a top-down view of an underexplored
area of friction between requirements communication -- implied human
expectations -- utilizing a collection of work primarily from experimental
psychology and social sciences. We show how such expectations can be used in
conjunction with task-specific expectations and the system design process for
AS to improve design team communication, alleviate barriers to user rejection,
and reduce the number of design cycles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the End-User Development for Human-Robot Interaction
  (EUD4HRI) workshop at HRI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distilling Reinforcement Learning Policies for Interpretable Robot
  Locomotion: Gradient Boosting Machines and Symbolic Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fernando Acero, Zhibin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in reinforcement learning (RL) have led to remarkable
achievements in robot locomotion capabilities. However, the complexity and
``black-box'' nature of neural network-based RL policies hinder their
interpretability and broader acceptance, particularly in applications demanding
high levels of safety and reliability. This paper introduces a novel approach
to distill neural RL policies into more interpretable forms using Gradient
Boosting Machines (GBMs), Explainable Boosting Machines (EBMs) and Symbolic
Regression. By leveraging the inherent interpretability of generalized additive
models, decision trees, and analytical expressions, we transform opaque neural
network policies into more transparent ``glass-box'' models. We train expert
neural network policies using RL and subsequently distill them into (i) GBMs,
(ii) EBMs, and (iii) symbolic policies. To address the inherent distribution
shift challenge of behavioral cloning, we propose to use the Dataset
Aggregation (DAgger) algorithm with a curriculum of episode-dependent
alternation of actions between expert and distilled policies, to enable
efficient distillation of feedback control policies. We evaluate our approach
on various robot locomotion gaits -- walking, trotting, bounding, and pacing --
and study the importance of different observations in joint actions for
distilled policies using various methods. We train neural expert policies for
205 hours of simulated experience and distill interpretable policies with only
10 minutes of simulated interaction for each gait using the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation and Deployment of LiDAR-based Place Recognition in Dense
  Forests 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haedam Oh, Nived Chebrolu, Matias Mattamala, Leonard Freißmuth, Maurice Fallon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many LiDAR place recognition systems have been developed and tested
specifically for urban driving scenarios. Their performance in natural
environments such as forests and woodlands have been studied less closely. In
this paper, we analyzed the capabilities of four different LiDAR place
recognition systems, both handcrafted and learning-based methods, using LiDAR
data collected with a handheld device and legged robot within dense forest
environments. In particular, we focused on evaluating localization where there
is significant translational and orientation difference between corresponding
LiDAR scan pairs. This is particularly important for forest survey systems
where the sensor or robot does not follow a defined road or path. Extending our
analysis we then incorporated the best performing approach, Logg3dNet, into a
full 6-DoF pose estimation system -- introducing several verification layers
for precise registration. We demonstrated the performance of our methods in
three operational modes: online SLAM, offline multi-mission SLAM map merging,
and relocalization into a prior map. We evaluated these modes using data
captured in forests from three different countries, achieving 80% of correct
loop closures candidates with baseline distances up to 5m, and 60% up to 10m.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exosense: A Vision-Centric Scene Understanding System For Safe
  Exoskeleton Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianeng Wang, Matias Mattamala, Christina Kassab, Lintong Zhang, Maurice Fallon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exoskeletons for daily use by those with mobility impairments are being
developed. They will require accurate and robust scene understanding systems.
Current research has used vision to identify immediate terrain and geometric
obstacles, however these approaches are constrained to detections directly in
front of the user and are limited to classifying a finite range of terrain
types (e.g., stairs, ramps and level-ground). This paper presents Exosense, a
vision-centric scene understanding system which is capable of generating rich,
globally-consistent elevation maps, incorporating both semantic and terrain
traversability information. It features an elastic Atlas mapping framework
associated with a visual SLAM pose graph, embedded with open-vocabulary room
labels from a Vision-Language Model (VLM). The device's design includes a wide
field-of-view (FoV) fisheye multi-camera system to mitigate the challenges
introduced by the exoskeleton walking pattern. We demonstrate the system's
robustness to the challenges of typical periodic walking gaits, and its ability
to construct accurate semantically-rich maps in indoor settings. Additionally,
we showcase its potential for motion planning -- providing a step towards safe
navigation for exoskeletons.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian Optimization for Sample-Efficient Policy Improvement in Robotic
  Manipulation <span class="chip">IROS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrian Röfer, Iman Nematollahi, Tim Welschehold, Wolfram Burgard, Abhinav Valada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sample efficient learning of manipulation skills poses a major challenge in
robotics. While recent approaches demonstrate impressive advances in the type
of task that can be addressed and the sensing modalities that can be
incorporated, they still require large amounts of training data. Especially
with regard to learning actions on robots in the real world, this poses a major
problem due to the high costs associated with both demonstrations and
real-world robot interactions. To address this challenge, we introduce
BOpt-GMM, a hybrid approach that combines imitation learning with own
experience collection. We first learn a skill model as a dynamical system
encoded in a Gaussian Mixture Model from a few demonstrations. We then improve
this model with Bayesian optimization building on a small number of autonomous
skill executions in a sparse reward setting. We demonstrate the sample
efficiency of our approach on multiple complex manipulation skills in both
simulations and real-world experiments. Furthermore, we make the code and
pre-trained models publicly available at http://bopt-gmm. cs.uni-freiburg.de.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, 2 tables, submitted to IROS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DexDribbler: Learning Dexterous Soccer Manipulation via Dynamic
  Supervision <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14300v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14300v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutong Hu, Kehan Wen, Fisher Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning dexterous locomotion policy for legged robots is becoming
increasingly popular due to its ability to handle diverse terrains and resemble
intelligent behaviors. However, joint manipulation of moving objects and
locomotion with legs, such as playing soccer, receive scant attention in the
learning community, although it is natural for humans and smart animals. A key
challenge to solve this multitask problem is to infer the objectives of
locomotion from the states and targets of the manipulated objects. The implicit
relation between the object states and robot locomotion can be hard to capture
directly from the training experience. We propose adding a feedback control
block to compute the necessary body-level movement accurately and using the
outputs as dynamic joint-level locomotion supervision explicitly. We further
utilize an improved ball dynamic model, an extended context-aided estimator,
and a comprehensive ball observer to facilitate transferring policy learned in
simulation to the real world. We observe that our learning scheme can not only
make the policy network converge faster but also enable soccer robots to
perform sophisticated maneuvers like sharp cuts and turns on flat surfaces, a
capability that was lacking in previous methods. Video and code are available
at https://github.com/SysCV/soccer-player
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, submitted to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human Reactions to Incorrect Answers from Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ponkoj Chandra Shill, Md. Azizul Hakim, Muhammad Jahanzeb Khan, Bashira Akter Anima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As robots grow more and more integrated into numerous industries, it is
critical to comprehend how humans respond to their failures. This paper
systematically studies how trust dynamics and system design are affected by
human responses to robot failures. The three-stage survey used in the study
provides a thorough understanding of human-robot interactions. While the second
stage concentrates on interaction details, such as robot precision and error
acknowledgment, the first stage collects demographic data and initial levels of
trust. In the last phase, participants' perceptions are examined after the
encounter, and trust dynamics, forgiveness, and propensity to suggest robotic
technologies are evaluated. Results show that participants' trust in robotic
technologies increased significantly when robots acknowledged their errors or
limitations to participants and their willingness to suggest robots for
activities in the future points to a favorable change in perception,
emphasizing the role that direct engagement has in influencing trust dynamics.
By providing useful advice for creating more sympathetic, responsive, and
reliable robotic systems, the study advances the science of human-robot
interaction and promotes a wider adoption of robotic technologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures, 1 table, Ro-Man 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UAV-Assisted Maritime Search and Rescue: A Holistic Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Messmer, Benjamin Kiefer, Leon Amadeus Varga, Andreas Zell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explore the application of Unmanned Aerial Vehicles (UAVs)
in maritime search and rescue (mSAR) missions, focusing on medium-sized
fixed-wing drones and quadcopters. We address the challenges and limitations
inherent in operating some of the different classes of UAVs, particularly in
search operations. Our research includes the development of a comprehensive
software framework designed to enhance the efficiency and efficacy of SAR
operations. This framework combines preliminary detection onboard UAVs with
advanced object detection at ground stations, aiming to reduce visual strain
and improve decision-making for operators. It will be made publicly available
upon publication. We conduct experiments to evaluate various Region of Interest
(RoI) proposal methods, especially by imposing simulated limited bandwidth on
them, an important consideration when flying remote or offshore operations.
This forces the algorithm to prioritize some predictions over others.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Salzmann, Markus Ryll, Alex Bewley, Matthias Minderer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual relationship detection aims to identify objects and their
relationships in images. Prior methods approach this task by adding separate
relationship modules or decoders to existing object detection architectures.
This separation increases complexity and hinders end-to-end training, which
limits performance. We propose a simple and highly efficient decoder-free
architecture for open-vocabulary visual relationship detection. Our model
consists of a Transformer-based image encoder that represents objects as tokens
and models their relationships implicitly. To extract relationship information,
we introduce an attention mechanism that selects object pairs likely to form a
relationship. We provide a single-stage recipe to train this model on a mixture
of object and relationship detection data. Our approach achieves
state-of-the-art relationship detection performance on Visual Genome and on the
large-vocabulary GQA benchmark at real-time inference speeds. We provide
analyses of zero-shot performance, ablations, and real-world qualitative
examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReFeree: Radar-based efficient global descriptor using a Feature and
  Free space for Place Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Byunghee Choi, Hogyun Kim, Younggun Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radar is highlighted for robust sensing capabilities in adverse weather
conditions (e.g. dense fog, heavy rain, or snowfall). In addition, Radar can
cover wide areas and penetrate small particles. Despite these advantages,
Radar-based place recognition remains in the early stages compared to other
sensors due to its unique characteristics such as low resolution, and
significant noise. In this paper, we propose a Radarbased place recognition
utilizing a descriptor called ReFeree using a feature and free space. Unlike
traditional methods, we overwhelmingly summarize the Radar image. Despite being
lightweight, it contains semi-metric information and is also outstanding from
the perspective of place recognition performance. For concrete validation, we
test a single session from the MulRan dataset and a multi-session from the
Oxford Radar RobotCar and the Boreas dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HCTO: Optimality-Aware LiDAR Inertial Odometry with Hybrid Continuous
  Time Optimization for Compact Wearable Mapping System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14173v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14173v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianping Li, Shenghai Yuan, Muqing Cao, Thien-Minh Nguyen, Kun Cao, Lihua Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compact wearable mapping system (WMS) has gained significant attention due to
their convenience in various applications. Specifically, it provides an
efficient way to collect prior maps for 3D structure inspection and robot-based
"last-mile delivery" in complex environments. However, vibrations in human
motion and the uneven distribution of point cloud features in complex
environments often lead to rapid drift, which is a prevalent issue when
applying existing LiDAR Inertial Odometry (LIO) methods on low-cost WMS. To
address these limitations, we propose a novel LIO for WMSs based on Hybrid
Continuous Time Optimization (HCTO) considering the optimality of Lidar
correspondences. First, HCTO recognizes patterns in human motion
(high-frequency part, low-frequency part, and constant velocity part) by
analyzing raw IMU measurements. Second, HCTO constructs hybrid IMU factors
according to different motion states, which enables robust and accurate
estimation against vibration-induced noise in the IMU measurements. Third, the
best point correspondences are selected using optimal design to achieve
real-time performance and better odometry accuracy. We conduct experiments on
head-mounted WMS datasets to evaluate the performance of our system,
demonstrating significant advantages over state-of-the-art methods. Video
recordings of experiments can be found on the project page of HCTO:
\href{https://github.com/kafeiyin00/HCTO}{https://github.com/kafeiyin00/HCTO}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Large Language Model-based Room-Object Relationships
  Knowledge for Enhancing Multimodal-Input Object Goal Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leyuan Sun, Asako Kanezaki, Guillaume Caron, Yusuke Yoshiyasu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object-goal navigation is a crucial engineering task for the community of
embodied navigation; it involves navigating to an instance of a specified
object category within unseen environments. Although extensive investigations
have been conducted on both end-to-end and modular-based, data-driven
approaches, fully enabling an agent to comprehend the environment through
perceptual knowledge and perform object-goal navigation as efficiently as
humans remains a significant challenge. Recently, large language models have
shown potential in this task, thanks to their powerful capabilities for
knowledge extraction and integration. In this study, we propose a data-driven,
modular-based approach, trained on a dataset that incorporates common-sense
knowledge of object-to-room relationships extracted from a large language
model. We utilize the multi-channel Swin-Unet architecture to conduct
multi-task learning incorporating with multimodal inputs. The results in the
Habitat simulator demonstrate that our framework outperforms the baseline by an
average of 10.6% in the efficiency metric, Success weighted by Path Length
(SPL). The real-world demonstration shows that the proposed approach can
efficiently conduct this task by traversing several rooms. For more details and
real-world demonstrations, please check our project webpage
(https://sunleyuan.github.io/ObjectNav).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>will soon submit to the Elsevier journal, Advanced Engineering
  Informatics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extrinsic Calibration of Multiple LiDARs for a Mobile Robot based on
  Floor Plane And Object Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14161v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14161v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shun Niijima, Atsushi Suzuki, Ryoichi Tsuzaki, Masaya Kinoshita
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mobile robots equipped with multiple light detection and ranging (LiDARs) and
capable of recognizing their surroundings are increasing due to the
minitualization and cost reduction of LiDAR. This paper proposes a target-less
extrinsic calibration method of multiple LiDARs with non-overlapping field of
view (FoV). The proposed method uses accumulated point clouds of floor plane
and objects while in motion. It enables accurate calibration with challenging
configuration of LiDARs that directed towards the floor plane, caused by biased
feature values. Additionally, the method includes a noise removal module that
considers the scanning pattern to address bleeding points, which are noises of
significant source of error in point cloud alignment using high-density LiDARs.
Evaluations through simulation demonstrate that the proposed method achieved
higher accuracy extrinsic calibration with two and four LiDARs than
conventional methods, regardless type of objects. Furthermore, the experiments
using a real mobile robot has shown that our proposed noise removal module can
eliminate noise more precisely than conventional methods, and the estimated
extrinsic parameters have successfully created consistent 3D maps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8pages, 10figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Development of a Compact Robust Passive Transformable Omni-Ball for
  Enhanced Step-Climbing and Vibration Reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazuo Hongo, Takashi Kito, Yasuhisa Kamikawa, Masaya Kinoshita, Yasunori Kawanami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the Passive Transformable Omni-Ball (PTOB), an advanced
omnidirectional wheel engineered to enhance step-climbing performance,
incorporate built-in actuators, diminish vibrations, and fortify structural
integrity. By modifying the omni-ball's structure from two to three segments,
we have achieved improved in-wheel actuation and a reduction in vibrational
feedback. Additionally, we have implemented a sliding mechanism in the follower
wheels to boost the wheel's step-climbing abilities. A prototype with a 127 mm
diameter PTOB was constructed, which confirmed its functionality for
omnidirectional movement and internal actuation. Compared to a traditional
omni-wheel, the PTOB demonstrated a comparable level of vibration while
offering superior capabilities. Extensive testing in varied settings showed
that the PTOB can adeptly handle step obstacles up to 45 mm, equivalent to 35
$\%$ of the wheel's diameter, in both the forward and lateral directions. The
PTOB showcased robust construction and proved to be versatile in navigating
through environments with diverse obstacles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Locomotion via Zero-order Stochastic Nonlinear Model Predictive
  Control with Guard Saltation Matrix 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sotaro Katayama, Noriaki Takasugi, Mitsuhisa Kaneko, Norio Nagatsuka, and Masaya Kinoshita
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a stochastic/robust nonlinear model predictive control
(NMPC) to enhance the robustness of legged locomotion against contact
uncertainties. We integrate the contact uncertainties into the covariance
propagation of stochastic/robust NMPC framework by leveraging the guard
saltation matrix and an extended Kalman filter-like covariance update. We
achieve fast stochastic/robust NMPC computation by utilizing the zero-order
stochastic/robust NMPC algorithm with additional improvements in computational
efficiency concerning the feedback gains. We conducted numerical experiments
and demonstrate that the proposed method can accurately forecast future state
covariance and generate trajectories that satisfies constraints even in the
presence of the contact uncertainties. Hardware experiments on the perceptive
locomotion of a wheeled-legged robot were also carried out, validating the
feasibility of the proposed method in a real-world system with limited on-board
computation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evidential Semantic Mapping in Off-road Environments with
  Uncertainty-aware Bayesian Kernel Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14138v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14138v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyoung Kim, Junwon Seo, Jihong Min
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic mapping with Bayesian Kernel Inference (BKI) has shown promise in
creating semantic maps by effectively leveraging local spatial information.
However, existing semantic mapping methods face challenges in constructing
reliable maps in unstructured outdoor scenarios due to unreliable semantic
predictions. To address this issue, we propose an evidential semantic mapping,
which can enhance reliability in perceptually challenging off-road
environments. We integrate Evidential Deep Learning into the semantic
segmentation network to obtain the uncertainty estimate of semantic prediction.
Subsequently, this semantic uncertainty is incorporated into an
uncertainty-aware BKI, tailored to prioritize more confident semantic
predictions when accumulating semantic information. By adaptively handling
semantic uncertainties, the proposed framework constructs robust
representations of the surroundings even in previously unseen environments.
Comprehensive experiments across various off-road datasets demonstrate that our
framework enhances accuracy and robustness, consistently outperforming existing
methods in scenes with high perceptual uncertainties.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our project website can be found at
  https://kjyoung.github.io/Homepage/#/Projects/Evidential-Semantic-Mapping</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantics from Space: Satellite-Guided Thermal Semantic Segmentation
  Annotation for Aerial Field Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14056v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14056v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Connor Lee, Saraswati Soedarmadji, Matthew Anderson, Anthony J. Clark, Soon-Jo Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new method to automatically generate semantic segmentation
annotations for thermal imagery captured from an aerial vehicle by utilizing
satellite-derived data products alongside onboard global positioning and
attitude estimates. This new capability overcomes the challenge of developing
thermal semantic perception algorithms for field robots due to the lack of
annotated thermal field datasets and the time and costs of manual annotation,
enabling precise and rapid annotation of thermal data from field collection
efforts at a massively-parallelizable scale. By incorporating a
thermal-conditioned refinement step with visual foundation models, our approach
can produce highly-precise semantic segmentation labels using low-resolution
satellite land cover data for little-to-no cost. It achieves 98.5% of the
performance from using costly high-resolution options and demonstrates between
70-160% improvement over popular zero-shot semantic segmentation methods based
on large vision-language models currently used for generating annotations for
RGB imagery. Code will be available at:
https://github.com/connorlee77/aerial-auto-segment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Roadmap Towards Automated and Regulated Robotic Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihao Liu, Mehran Armand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of generative technology opens up possibility for
higher level of automation, and artificial intelligence (AI) embodiment in
robotic systems is imminent. However, due to the blackbox nature of the
generative technology, the generation of the knowledge and workflow scheme is
uncontrolled, especially in a dynamic environment and a complex scene. This
poses challenges to regulations in safety-demanding applications such as
medical scenes. We argue that the unregulated generative processes from AI is
fitted for low level end tasks, but intervention in the form of manual or
automated regulation should happen post-workflow-generation and
pre-robotic-execution. To address this, we propose a roadmap that can lead to
fully automated and regulated robotic systems. In this paradigm, the high level
policies are generated as structured graph data, enabling regulatory oversight
and reusability, while the code base for lower level tasks is generated by
generative models. Our approach aims the transitioning from expert knowledge to
regulated action, akin to the iterative processes of study, practice, scrutiny,
and execution in human tasks. We identify the generative and deterministic
processes in a design cycle, where generative processes serve as a text-based
world simulator and the deterministic processes generate the executable system.
We propose State Machine Seralization Language (SMSL) to be the conversion
point between text simulator and executable workflow control. From there, we
analyze the modules involved based on the current literature, and discuss human
in the loop. As a roadmap, this work identifies the current possible
implementation and future work. This work does not provide an implemented
system but envisions to inspire the researchers working on the direction in the
roadmap. We implement the SMSL and D-SFO paradigm that serve as the starting
point of the roadmap.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GelLink: A Compact Multi-phalanx Finger with Vision-based Tactile
  Sensing and Proprioception <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Ma,  Jialiang,  Zhao, Edward Adelson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compared to fully-actuated robotic end-effectors, underactuated ones are
generally more adaptive, robust, and cost-effective. However, state estimation
for underactuated hands is usually more challenging. Vision-based tactile
sensors, like Gelsight, can mitigate this issue by providing high-resolution
tactile sensing and accurate proprioceptive sensing. As such, we present
GelLink, a compact, underactuated, linkage-driven robotic finger with low-cost,
high-resolution vision-based tactile sensing and proprioceptive sensing
capabilities. In order to reduce the amount of embedded hardware, i.e. the
cameras and motors, we optimize the linkage transmission with a planar linkage
mechanism simulator and develop a planar reflection simulator to simplify the
tactile sensing hardware. As a result, GelLink only requires one motor to
actuate the three phalanges, and one camera to capture tactile signals along
the entire finger. Overall, GelLink is a compact robotic finger that shows
adaptability and robustness when performing grasping tasks. The integration of
vision-based tactile sensors can significantly enhance the capabilities of
underactuated fingers and potentially broaden their future usage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Supplement video: https://www.youtube.com/watch?v=hZwUpAig5C0 . 7
  pages, 9 figures. ICRA 2024 (IEEE International Conference on Robotics and
  Automation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Change: Choreographing Mixed Traffic Through Lateral Control
  and Hierarchical Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14879v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14879v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dawei Wang, Weizi Li, Lei Zhu, Jia Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The management of mixed traffic that consists of robot vehicles (RVs) and
human-driven vehicles (HVs) at complex intersections presents a multifaceted
challenge. Traditional signal controls often struggle to adapt to dynamic
traffic conditions and heterogeneous vehicle types. Recent advancements have
turned to strategies based on reinforcement learning (RL), leveraging its
model-free nature, real-time operation, and generalizability over different
scenarios. We introduce a hierarchical RL framework to manage mixed traffic
through precise longitudinal and lateral control of RVs. Our proposed
hierarchical framework combines the state-of-the-art mixed traffic control
algorithm as a high level decision maker to improve the performance and
robustness of the whole system. Our experiments demonstrate that the framework
can reduce the average waiting time by up to 54% compared to the
state-of-the-art mixed traffic control method. When the RV penetration rate
exceeds 60%, our technique consistently outperforms conventional traffic signal
control programs in terms of the average waiting time for all vehicles at the
intersection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TEeVTOL: Balancing Energy and Time Efficiency in eVTOL Aircraft Path
  Planning Across City-Scale Wind Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14877v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14877v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songyang Liu, Shuai Li, Haochen Li, Weizi Li, Jindong Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electric vertical-takeoff and landing (eVTOL) aircraft, recognized for their
maneuverability and flexibility, offer a promising alternative to our
transportation system. However, the operational effectiveness of these aircraft
faces many challenges, such as the delicate balance between energy and time
efficiency, stemming from unpredictable environmental factors, including wind
fields. Mathematical modeling-based approaches have been adopted to plan
aircraft flight path in urban wind fields with the goal to save energy and time
costs. While effective, they are limited in adapting to dynamic and complex
environments. To optimize energy and time efficiency in eVTOL's flight through
dynamic wind fields, we introduce a novel path planning method leveraging deep
reinforcement learning. We assess our method with extensive experiments,
comparing it to Dijkstra's algorithm -- the theoretically optimal approach for
determining shortest paths in a weighted graph, where weights represent either
energy or time cost. The results show that our method achieves a graceful
balance between energy and time efficiency, closely resembling the
theoretically optimal values for both objectives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Quadruped Locomotion Using Differentiable Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14864v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14864v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunlong Song, Sangbae Kim, Davide Scaramuzza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While most recent advancements in legged robot control have been driven by
model-free reinforcement learning, we explore the potential of differentiable
simulation. Differentiable simulation promises faster convergence and more
stable training by computing low-variant first-order gradients using the robot
model, but so far, its use for legged robot control has remained limited to
simulation. The main challenge with differentiable simulation lies in the
complex optimization landscape of robotic tasks due to discontinuities in
contact-rich environments, e.g., quadruped locomotion. This work proposes a
new, differentiable simulation framework to overcome these challenges. The key
idea involves decoupling the complex whole-body simulation, which may exhibit
discontinuities due to contact, into two separate continuous domains.
Subsequently, we align the robot state resulting from the simplified model with
a more precise, non-differentiable simulator to maintain sufficient simulation
accuracy. Our framework enables learning quadruped walking in minutes using a
single simulated robot without any parallelization. When augmented with GPU
parallelization, our approach allows the quadruped robot to master diverse
locomotion skills, including trot, pace, bound, and gallop, on challenging
terrains in minutes. Additionally, our policy achieves robust locomotion
performance in the real world zero-shot. To the best of our knowledge, this
work represents the first demonstration of using differentiable simulation for
controlling a real quadruped robot. This work provides several important
insights into using differentiable simulations for legged locomotion in the
real world.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-agent Task-Driven Exploration via Intelligent Map Compression and
  Sharing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evangelos Psomiadis, Dipankar Maity, Panagiotis Tsiotras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the task-driven exploration of unknown environments
with mobile sensors communicating compressed measurements. The sensors explore
the area and transmit their compressed data to another robot, assisting it in
reaching a goal location. We propose a novel communication framework and a
tractable multi-agent exploration algorithm to select the sensors' actions. The
algorithm uses a task-driven measure of uncertainty, resulting from map
compression, as a reward function. We validate the efficacy of our algorithm
through numerical simulations conducted on a realistic map and compare it with
two alternative approaches. The results indicate that the proposed algorithm
effectively decreases the time required for the robot to reach its target
without causing excessive load on the communication network.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AMP: Autoregressive Motion Prediction Revisited with Next Token
  Prediction for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13331v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13331v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaosong Jia, Shaoshuai Shi, Zijun Chen, Li Jiang, Wenlong Liao, Tao He, Junchi Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As an essential task in autonomous driving (AD), motion prediction aims to
predict the future states of surround objects for navigation. One natural
solution is to estimate the position of other agents in a step-by-step manner
where each predicted time-step is conditioned on both observed time-steps and
previously predicted time-steps, i.e., autoregressive prediction. Pioneering
works like SocialLSTM and MFP design their decoders based on this intuition.
However, almost all state-of-the-art works assume that all predicted time-steps
are independent conditioned on observed time-steps, where they use a single
linear layer to generate positions of all time-steps simultaneously. They
dominate most motion prediction leaderboards due to the simplicity of training
MLPs compared to autoregressive networks.
  In this paper, we introduce the GPT style next token prediction into motion
forecasting. In this way, the input and output could be represented in a
unified space and thus the autoregressive prediction becomes more feasible.
However, different from language data which is composed of homogeneous units
-words, the elements in the driving scene could have complex spatial-temporal
and semantic relations. To this end, we propose to adopt three factorized
attention modules with different neighbors for information aggregation and
different position encoding styles to capture their relations, e.g., encoding
the transformation between coordinate systems for spatial relativity while
adopting RoPE for temporal relativity. Empirically, by equipping with the
aforementioned tailored designs, the proposed method achieves state-of-the-art
performance in the Waymo Open Motion and Waymo Interaction datasets. Notably,
AMP outperforms other recent autoregressive motion prediction methods: MotionLM
and StateTransformer, which demonstrates the effectiveness of the proposed
designs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TD-MPC2: Scalable, Robust World Models for Continuous Control <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16828v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16828v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicklas Hansen, Hao Su, Xiaolong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  TD-MPC is a model-based reinforcement learning (RL) algorithm that performs
local trajectory optimization in the latent space of a learned implicit
(decoder-free) world model. In this work, we present TD-MPC2: a series of
improvements upon the TD-MPC algorithm. We demonstrate that TD-MPC2 improves
significantly over baselines across 104 online RL tasks spanning 4 diverse task
domains, achieving consistently strong results with a single set of
hyperparameters. We further show that agent capabilities increase with model
and data size, and successfully train a single 317M parameter agent to perform
80 tasks across multiple task domains, embodiments, and action spaces. We
conclude with an account of lessons, opportunities, and risks associated with
large TD-MPC2 agents. Explore videos, models, data, code, and more at
https://tdmpc2.com
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024. Explore videos, models, data, code, and more at
  https://tdmpc2.com</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Modular Aerial System Based on Homogeneous Quadrotors with
  Fault-Tolerant Control <span class="chip">ICRA2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01477v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01477v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengguang Li, Kai Cui, Heinz Koeppl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The standard quadrotor is one of the most popular and widely used aerial
vehicle of recent decades, offering great maneuverability with mechanical
simplicity. However, the under-actuation characteristic limits its
applications, especially when it comes to generating desired wrench with six
degrees of freedom (DOF). Therefore, existing work often compromises between
mechanical complexity and the controllable DOF of the aerial system. To take
advantage of the mechanical simplicity of a standard quadrotor, we propose a
modular aerial system, IdentiQuad, that combines only homogeneous
quadrotor-based modules. Each IdentiQuad can be operated alone like a standard
quadrotor, but at the same time allows task-specific assembly, increasing the
controllable DOF of the system. Each module is interchangeable within its
assembly. We also propose a general controller for different configurations of
assemblies, capable of tolerating rotor failures and balancing the energy
consumption of each module. The functionality and robustness of the system and
its controller are validated using physics-based simulations for different
assembly configurations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instance-aware Exploration-Verification-Exploitation for Instance
  ImageGoal Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17587v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17587v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohan Lei, Min Wang, Wengang Zhou, Li Li, Houqiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a new embodied vision task, Instance ImageGoal Navigation (IIN) aims to
navigate to a specified object depicted by a goal image in an unexplored
environment.
  The main challenge of this task lies in identifying the target object from
different viewpoints while rejecting similar distractors.
  Existing ImageGoal Navigation methods usually adopt the simple
Exploration-Exploitation framework and ignore the identification of specific
instance during navigation.
  In this work, we propose to imitate the human behaviour of ``getting closer
to confirm" when distinguishing objects from a distance.
  Specifically, we design a new modular navigation framework named
Instance-aware Exploration-Verification-Exploitation (IEVE) for instance-level
image goal navigation.
  Our method allows for active switching among the exploration, verification,
and exploitation actions, thereby facilitating the agent in making reasonable
decisions under different situations.
  On the challenging HabitatMatterport 3D semantic (HM3D-SEM) dataset, our
method surpasses previous state-of-the-art work, with a classical segmentation
model (0.684 vs. 0.561 success) or a robust model (0.702 vs. 0.561 success).
Our code will be made publicly available at https://github.com/XiaohanLei/IEVE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning a Depth Covariance Function <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12157v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12157v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Dexheimer, Andrew J. Davison
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose learning a depth covariance function with applications to
geometric vision tasks. Given RGB images as input, the covariance function can
be flexibly used to define priors over depth functions, predictive
distributions given observations, and methods for active point selection. We
leverage these techniques for a selection of downstream tasks: depth
completion, bundle adjustment, and monocular dense visual odometry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023. Project page: https://edexheim.github.io/DepthCov/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep learning reduces sensor requirements for gust rejection on a small
  uncrewed aerial vehicle morphing wing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.03133v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.03133v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin PT. Haughn, Christina Harvey, Daniel J. Inman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a growing need for uncrewed aerial vehicles (UAVs) to operate in
cities. However, the uneven urban landscape and complex street systems cause
large-scale wind gusts that challenge the safe and effective operation of UAVs.
Current gust alleviation methods rely on traditional control surfaces and
computationally expensive modeling to select a control action, leading to a
slower response. Here, we used deep reinforcement learning to create an
autonomous gust alleviation controller for a camber-morphing wing. This method
reduced gust impact by 84%, directly from real-time, on-board pressure signals.
Notably, we found that gust alleviation using signals from only three pressure
taps was statistically indistinguishable from using six signals. This
reduced-sensor fly-by-feel control opens the door to UAV missions in previously
inoperable locations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Human's Gender Perception and Bias toward Non-Humanoid Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.12001v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.12001v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahya Ramezani, Jose Luis Sanchez-Lopez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As non-humanoid robots increasingly permeate various sectors, understanding
their design implications for human acceptance becomes paramount. Despite their
ubiquity, studies on how to improve human interaction are sparse. Our
investigation, conducted through two surveys, addresses this gap. The first
survey emphasizes non-humanoid robots and human perceptions about gender
attributions, suggesting that both design and perceived gender influence
acceptance. Survey 2 investigates the effects of varying gender cues on robot
designs and their consequent impacts on human-robot interactions. Our findings
highlighted that distinct gender cues can bolster or impede interaction
comfort.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Star-Searcher: A Complete and Efficient Aerial System for Autonomous
  Target Search in Complex Unknown Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16348v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16348v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Luo, Zixuan Zhuang, Neng Pan, Chen Feng, Shaojie Shen, Fei Gao, Hui Cheng, Boyu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper tackles the challenge of autonomous target search using unmanned
aerial vehicles (UAVs) in complex unknown environments. To fill the gap in
systematic approaches for this task, we introduce Star-Searcher, an aerial
system featuring specialized sensor suites, mapping, and planning modules to
optimize searching. Path planning challenges due to increased inspection
requirements are addressed through a hierarchical planner with a
visibility-based viewpoint clustering method. This simplifies planning by
breaking it into global and local sub-problems, ensuring efficient global and
local path coverage in real-time. Furthermore, our global path planning employs
a history-aware mechanism to reduce motion inconsistency from frequent map
changes, significantly enhancing search efficiency. We conduct comparisons with
state-of-the-art methods in both simulation and the real world, demonstrating
shorter flight paths, reduced time, and higher target search completeness. Our
approach will be open-sourced for community benefit at
https://github.com/SYSU-STAR/STAR-Searcher.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Aceepted to IEEE RA-L. Code:
  https://github.com/SYSU-STAR/STAR-Searcher. Video:
  https://www.youtube.com/watch?v=08ll_oo_DtU</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models for Multi-Modal Human-Robot Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15174v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15174v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Wang, Stephan Hasler, Daniel Tanneberg, Felix Ocker, Frank Joublin, Antonello Ceravola, Joerg Deigmoeller, Michael Gienger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an innovative large language model (LLM)-based robotic
system for enhancing multi-modal human-robot interaction (HRI). Traditional HRI
systems relied on complex designs for intent estimation, reasoning, and
behavior generation, which were resource-intensive. In contrast, our system
empowers researchers and practitioners to regulate robot behavior through three
key aspects: providing high-level linguistic guidance, creating "atomics" for
actions and expressions the robot can use, and offering a set of examples.
Implemented on a physical robot, it demonstrates proficiency in adapting to
multi-modal inputs and determining the appropriate manner of action to assist
humans with its arms, following researchers' defined guidelines.
Simultaneously, it coordinates the robot's lid, neck, and ear movements with
speech output to produce dynamic, multi-modal expressions. This showcases the
system's potential to revolutionize HRI by shifting from conventional, manual
state-and-flow design methods to an intuitive, guidance-based, and
example-driven approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAkEable: Memory-centered and Affordance-based Task Execution Framework
  for Transferable Mobile Manipulation Skills 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.16899v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.16899v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christoph Pohl, Fabian Reister, Fabian Peller-Konrad, Tamim Asfour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To perform versatile mobile manipulation tasks in human-centered
environments, the ability to efficiently transfer learned tasks and experiences
from one robot to another or across different environments is key. In this
paper, we present MAkEable, a versatile uni- and multi-manual mobile
manipulation framework that facilitates the transfer of capabilities and
knowledge across different tasks, environments, and robots. Our framework
integrates an affordance-based task description into the memory-centric
cognitive architecture of the ARMAR humanoid robot family, which supports the
sharing of experiences and demonstrations for transfer learning. By
representing mobile manipulation actions through affordances, i.e., interaction
possibilities of the robot with its environment, we provide a unifying
framework for the autonomous uni- and multi-manual manipulation of known and
unknown objects in various environments. We demonstrate the applicability of
the framework in real-world experiments for multiple robots, tasks, and
environments. This includes grasping known and unknown objects, object placing,
bimanual object grasping, memory-enabled skill transfer in a drawer opening
scenario across two different humanoid robots, and a pouring task learned from
human demonstration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Driving Animatronic Robot Facial Expression From Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12670v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12670v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boren Li, Hang Li, Hangxin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Animatronic robots aim to enable natural human-robot interaction through
lifelike facial expressions. However, generating realistic, speech-synchronized
robot expressions is challenging due to the complexities of facial biomechanics
and responsive motion synthesis. This paper presents a principled,
skinning-centric approach to drive animatronic robot facial expressions from
speech. The proposed approach employs linear blend skinning (LBS) as the core
representation to guide tightly integrated innovations in embodiment design and
motion synthesis. LBS informs the actuation topology, enables human expression
retargeting, and allows speech-driven facial motion generation. The proposed
approach is capable of generating highly realistic, real-time facial
expressions from speech on an animatronic face, significantly advancing robots'
ability to replicate nuanced human expressions for natural interaction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review. For associated project page, see
  https://library87.github.io/animatronic-face-iros24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ROS-Causal: A ROS-based Causal Analysis Framework for Human-Robot
  Interaction Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16068v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16068v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Castri, Gloria Beraldo, Sariah Mghames, Marc Hanheide, Nicola Bellotto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deploying robots in human-shared spaces requires understanding interactions
among nearby agents and objects. Modelling cause-and-effect relations through
causal inference aids in predicting human behaviours and anticipating robot
interventions. However, a critical challenge arises as existing causal
discovery methods currently lack an implementation inside the ROS ecosystem,
the standard de facto in robotics, hindering effective utilisation in robotics.
To address this gap, this paper introduces ROS-Causal, a ROS-based framework
for onboard data collection and causal discovery in human-robot spatial
interactions. An ad-hoc simulator, integrated with ROS, illustrates the
approach's effectiveness, showcasing the robot onboard generation of causal
models during data collection. ROS-Causal is available on GitHub:
https://github.com/lcastri/roscausal.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the "Causal-HRI: Causal Learning for Human-Robot
  Interaction" workshop at the 2024 ACM/IEEE International Conference on
  Human-Robot Interaction (HRI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Large Language Models to Facilitate Variable Autonomy for
  Human-Robot Teaming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07214v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07214v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Younes Lakhnati, Max Pascher, Jens Gerken
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a rapidly evolving digital landscape autonomous tools and robots are
becoming commonplace. Recognizing the significance of this development, this
paper explores the integration of Large Language Models (LLMs) like Generative
pre-trained transformer (GPT) into human-robot teaming environments to
facilitate variable autonomy through the means of verbal human-robot
communication. In this paper, we introduce a novel framework for such a
GPT-powered multi-robot testbed environment, based on a Unity Virtual Reality
(VR) setting. This system allows users to interact with robot agents through
natural language, each powered by individual GPT cores. By means of OpenAI's
function calling, we bridge the gap between unstructured natural language input
and structure robot actions. A user study with 12 participants explores the
effectiveness of GPT-4 and, more importantly, user strategies when being given
the opportunity to converse in natural language within a multi-robot
environment. Our findings suggest that users may have preconceived expectations
on how to converse with robots and seldom try to explore the actual language
and cognitive capabilities of their robot collaborators. Still, those users who
did explore where able to benefit from a much more natural flow of
communication and human-like back-and-forth. We provide a set of lessons
learned for future research and technical implementations of similar systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Frontiers in Robotics and AI, Variable Autonomy for Human-Robot
  Teaming</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SLIM: Skill Learning with Multiple Critics <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00823v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00823v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Emukpere, Bingbing Wu, Julien Perez, Jean-Michel Renders
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised skill learning aims to acquire useful behaviors that leverage
the underlying dynamics of the environment. Latent variable models, based on
mutual information maximization, have been successful in this task but still
struggle in the context of robotic manipulation. As it requires impacting a
possibly large set of degrees of freedom composing the environment, mutual
information maximization fails alone in producing useful and safe manipulation
behaviors. Furthermore, tackling this by augmenting skill discovery rewards
with additional rewards through a naive combination might fail to produce
desired behaviors. To address this limitation, we introduce SLIM, a
multi-critic learning approach for skill discovery with a particular focus on
robotic manipulation. Our main insight is that utilizing multiple critics in an
actor-critic framework to gracefully combine multiple reward functions leads to
a significant improvement in latent-variable skill discovery for robotic
manipulation while overcoming possible interference occurring among rewards
which hinders convergence to useful skills. Furthermore, in the context of
tabletop manipulation, we demonstrate the applicability of our novel skill
discovery approach to acquire safe and efficient motor primitives in a
hierarchical reinforcement learning fashion and leverage them through planning,
significantly surpassing baseline approaches for skill discovery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ R2SNet: Scalable Domain Adaptation for Object Detection in Cloud-Based
  Robots Ecosystems via Proposal Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11567v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11567v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michele Antonazzi, Matteo Luperto, N. Alberto Borghese, Nicola Basilico
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel approach for scalable domain adaptation in cloud
robotics scenarios where robots rely on third-party AI inference services
powered by large pre-trained deep neural networks. Our method is based on a
downstream proposal-refinement stage running locally on the robots, exploiting
a new lightweight DNN architecture, R2SNet. This architecture aims to mitigate
performance degradation from domain shifts by adapting the object detection
process to the target environment, focusing on relabeling, rescoring, and
suppression of bounding-box proposals. Our method allows for local execution on
robots, addressing the scalability challenges of domain adaptation without
incurring significant computational costs. Real-world results on mobile service
robots performing door detection show the effectiveness of the proposed method
in achieving scalable domain adaptation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoBRA: A Composable Benchmark for Robotics Applications <span class="chip">ICRA'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09337v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09337v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthias Mayer, Jonathan Külz, Matthias Althoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Selecting an optimal robot, its base pose, and trajectory for a given task is
currently mainly done by human expertise or trial and error. To evaluate
automatic approaches to this combined optimization problem, we introduce a
benchmark suite encompassing a unified format for robots, environments, and
task descriptions. Our benchmark suite is especially useful for modular robots,
where the multitude of robots that can be assembled creates a host of
additional parameters to optimize. We include tasks such as machine tending and
welding in synthetic environments and 3D scans of real-world machine shops. All
benchmarks are accessible through https://cobra.cps.cit.tum.de, a platform to
conveniently share, reference, and compare tasks, robot models, and solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 Figures, 5 Tables Final version for IEEE ICRA'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalized Early Stopping in Evolutionary Direct Policy Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.03574v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.03574v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Etor Arza, Leni K. Le Goff, Emma Hart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lengthy evaluation times are common in many optimization problems such as
direct policy search tasks, especially when they involve conducting evaluations
in the physical world, e.g. in robotics applications. Often when evaluating
solution over a fixed time period it becomes clear that the objective value
will not increase with additional computation time (for example when a two
wheeled robot continuously spins on the spot). In such cases, it makes sense to
stop the evaluation early to save computation time. However, most approaches to
stop the evaluation are problem specific and need to be specifically designed
for the task at hand. Therefore, we propose an early stopping method for direct
policy search. The proposed method only looks at the objective value at each
time step and requires no problem specific knowledge. We test the introduced
stopping criterion in five direct policy search environments drawn from games,
robotics and classic control domains, and show that it can save up to 75% of
the computation time. We also compare it with problem specific stopping
criteria and show that it performs comparably, while being more generally
applicable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language and Sketching: An LLM-driven Interactive Multimodal Multitask
  Robot Navigation Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08244v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08244v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiqin Zu, Wenbin Song, Ruiqing Chen, Ze Guo, Fanglei Sun, Zheng Tian, Wei Pan, Jun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The socially-aware navigation system has evolved to adeptly avoid various
obstacles while performing multiple tasks, such as point-to-point navigation,
human-following, and -guiding. However, a prominent gap persists: in
Human-Robot Interaction (HRI), the procedure of communicating commands to
robots demands intricate mathematical formulations. Furthermore, the transition
between tasks does not quite possess the intuitive control and user-centric
interactivity that one would desire. In this work, we propose an LLM-driven
interactive multimodal multitask robot navigation framework, termed LIM2N, to
solve the above new challenge in the navigation field. We achieve this by first
introducing a multimodal interaction framework where language and hand-drawn
inputs can serve as navigation constraints and control objectives. Next, a
reinforcement learning agent is built to handle multiple tasks with the
received information. Crucially, LIM2N creates smooth cooperation among the
reasoning of multimodal input, multitask planning, and adaptation and
processing of the intelligent sensing modules in the complicated system.
Extensive experiments are conducted in both simulation and the real world
demonstrating that LIM2N has superior user needs understanding, alongside an
enhanced interactive experience.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-time Perceptive Motion Control using Control Barrier Functions with
  Analytical Smoothing for Six-Wheeled-Telescopic-Legged Robot Tachyon 3 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11792v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11792v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noriaki Takasugi, Masaya Kinoshita, Yasuhisa Kamikawa, Ryoichi Tsuzaki, Atsushi Sakamoto, Toshimitsu Kai, Yasunori Kawanami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To achieve safe legged locomotion, it is important to generate motion in
real-time considering various constraints in robots and environments. In this
study, we propose a lightweight real-time perspective motion control system for
the newly developed six-wheeled-telescopic-legged robot, Tachyon 3. In the
proposed method, analytically smoothed constraints including Smooth Separating
Axis Theorem (Smooth SAT) as a novel higher order differentiable collision
detection for 3D shapes is applied to the Control Barrier Function (CBF). The
proposed system integrating the CBF achieves online motion generation in a
short control cycle of 1 ms that satisfies joint limitations, environmental
collision avoidance and safe convex foothold constraints. The efficiency of
Smooth SAT is shown from the collision detection time of 1 us or less and the
CBF constraint computation time for Tachyon3 of several us. Furthermore, the
effectiveness of the proposed system is verified through the stair-climbing
motion, integrating online recognition in a simulation and a real machine.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures, This work has been submitted to the IEEE for
  possible publication. Copyright may be transferred without notice, after
  which this version may no longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distilling and Retrieving Generalizable Knowledge for Robot Manipulation
  via Language Corrections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10678v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10678v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lihan Zha, Yuchen Cui, Li-Heng Lin, Minae Kwon, Montserrat Gonzalez Arenas, Andy Zeng, Fei Xia, Dorsa Sadigh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Today's robot policies exhibit subpar performance when faced with the
challenge of generalizing to novel environments. Human corrective feedback is a
crucial form of guidance to enable such generalization. However, adapting to
and learning from online human corrections is a non-trivial endeavor: not only
do robots need to remember human feedback over time to retrieve the right
information in new settings and reduce the intervention rate, but also they
would need to be able to respond to feedback that can be arbitrary corrections
about high-level human preferences to low-level adjustments to skill
parameters. In this work, we present Distillation and Retrieval of Online
Corrections (DROC), a large language model (LLM)-based system that can respond
to arbitrary forms of language feedback, distill generalizable knowledge from
corrections, and retrieve relevant past experiences based on textual and visual
similarity for improving performance in novel settings. DROC is able to respond
to a sequence of online language corrections that address failures in both
high-level task plans and low-level skill primitives. We demonstrate that DROC
effectively distills the relevant information from the sequence of online
corrections in a knowledge base and retrieves that knowledge in settings with
new task or object instances. DROC outperforms other techniques that directly
generate robot code via LLMs by using only half of the total number of
corrections needed in the first round and requires little to no corrections
after two iterations. We show further results, videos, prompts and code on
https://sites.google.com/stanford.edu/droc .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, videos and code links on website
  https://sites.google.com/stanford.edu/droc</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning for Inertial Positioning: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03757v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03757v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changhao Chen, Xianfei Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inertial sensors are widely utilized in smartphones, drones, robots, and IoT
devices, playing a crucial role in enabling ubiquitous and reliable
localization. Inertial sensor-based positioning is essential in various
applications, including personal navigation, location-based security, and
human-device interaction. However, low-cost MEMS inertial sensors' measurements
are inevitably corrupted by various error sources, leading to unbounded drifts
when integrated doubly in traditional inertial navigation algorithms,
subjecting inertial positioning to the problem of error drifts. In recent
years, with the rapid increase in sensor data and computational power, deep
learning techniques have been developed, sparking significant research into
addressing the problem of inertial positioning. Relevant literature in this
field spans across mobile computing, robotics, and machine learning. In this
article, we provide a comprehensive review of deep learning-based inertial
positioning and its applications in tracking pedestrians, drones, vehicles, and
robots. We connect efforts from different fields and discuss how deep learning
can be applied to address issues such as sensor calibration, positioning error
drift reduction, and multi-sensor fusion. This article aims to attract readers
from various backgrounds, including researchers and practitioners interested in
the potential of deep learning-based techniques to solve inertial positioning
problems. Our review demonstrates the exciting possibilities that deep learning
brings to the table and provides a roadmap for future research in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Intelligent Transportation Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Redundancy parameterization and inverse kinematics of 7-DOF revolute
  manipulators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.13122v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.13122v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander J. Elias, John T. Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Seven degree-of-freedom (DOF) robot arms have one redundant DOF which does
not change the motion of the end effector. The redundant DOF offers greater
manipulability of the arm configuration to avoid obstacles and singularities,
but it must be parameterized to fully specify the joint angles for a given end
effector pose. For 7-DOF revolute (7R) manipulators, we introduce a new concept
of generalized shoulder-elbow-wrist (SEW) angle, a generalization of the
conventional SEW angle but with an arbitrary choice of the reference direction
function. The SEW angle is widely used and easy for human operators to
visualize as a rotation of the elbow about the shoulder-wrist line. Since other
redundancy parameterizations including the conventional SEW angle encounter an
algorithmic singularity along a line in the workspace, we introduce a special
choice of the reference direction function called the stereographic SEW angle
which has a singularity only along a half-line, which can be placed out of
reach. We prove that such a singularity is unavoidable for any
parameterization. We also include expressions for the SEW angle Jacobian along
with singularity analysis. Finally, we provide efficient and singularity-robust
inverse kinematics solutions for most known 7R manipulators using the general
SEW angle and the subproblem decomposition method. These solutions are often
closed-form but may sometimes involve a 1D or 2D search in the general case.
Search-based solutions may be converted to finding zeros of a high-order
polynomial. Inverse kinematics solutions, examples, and evaluations are
available in a publicly accessible repository.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 14 figures. Update: Sawyer IK using polynomial method, two
  video extensions, expanded related literature</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Open X-Embodiment: Robotic Learning <span class="highlight-title">Dataset</span>s and RT-X Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08864v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08864v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Open X-Embodiment Collaboration, Abby O'Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi "Jim" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag R Sanketi, Patrick "Tree" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martín-Martín, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large, high-capacity models trained on diverse datasets have shown remarkable
successes on efficiently tackling downstream applications. In domains from NLP
to Computer Vision, this has led to a consolidation of pretrained models, with
general pretrained backbones serving as a starting point for many applications.
Can such a consolidation happen in robotics? Conventionally, robotic learning
methods train a separate model for every application, every robot, and even
every environment. Can we instead train generalist X-robot policy that can be
adapted efficiently to new robots, tasks, and environments? In this paper, we
provide datasets in standardized data formats and models to make it possible to
explore this possibility in the context of robotic manipulation, alongside
experimental results that provide an example of effective X-robot policies. We
assemble a dataset from 22 different robots collected through a collaboration
between 21 institutions, demonstrating 527 skills (160266 tasks). We show that
a high-capacity model trained on this data, which we call RT-X, exhibits
positive transfer and improves the capabilities of multiple robots by
leveraging experience from other platforms. More details can be found on the
project website https://robotics-transformer-x.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://robotics-transformer-x.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TeleMoMa: A Modular and Versatile Teleoperation System for Mobile
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07869v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07869v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivin Dass, Wensi Ai, Yuqian Jiang, Samik Singh, Jiaheng Hu, Ruohan Zhang, Peter Stone, Ben Abbatematteo, Roberto Martín-Martín
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A critical bottleneck limiting imitation learning in robotics is the lack of
data. This problem is more severe in mobile manipulation, where collecting
demonstrations is harder than in stationary manipulation due to the lack of
available and easy-to-use teleoperation interfaces. In this work, we
demonstrate TeleMoMa, a general and modular interface for whole-body
teleoperation of mobile manipulators. TeleMoMa unifies multiple human
interfaces including RGB and depth cameras, virtual reality controllers,
keyboard, joysticks, etc., and any combination thereof. In its more accessible
version, TeleMoMa works using simply vision (e.g., an RGB-D camera), lowering
the entry bar for humans to provide mobile manipulation demonstrations. We
demonstrate the versatility of TeleMoMa by teleoperating several existing
mobile manipulators - PAL Tiago++, Toyota HSR, and Fetch - in simulation and
the real world. We demonstrate the quality of the demonstrations collected with
TeleMoMa by training imitation learning policies for mobile manipulation tasks
involving synchronized whole-body motion. Finally, we also show that TeleMoMa's
teleoperation channel enables teleoperation on site, looking at the robot, or
remote, sending commands and observations through a computer network, and
perform user studies to evaluate how easy it is for novice users to learn to
collect demonstrations with different combinations of human interfaces enabled
by our system. We hope TeleMoMa becomes a helpful tool for the community
enabling researchers to collect whole-body mobile manipulation demonstrations.
For more information and video results,
https://robin-lab.cs.utexas.edu/telemoma-web.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OASIS: Optimal Arrangements for Sensing in SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10698v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10698v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pushyami Kaveti, Matthew Giamou, Hanumant Singh, David M. Rosen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The number and arrangement of sensors on mobile robot dramatically influence
its perception capabilities. Ensuring that sensors are mounted in a manner that
enables accurate detection, localization, and mapping is essential for the
success of downstream control tasks. However, when designing a new robotic
platform, researchers and practitioners alike usually mimic standard
configurations or maximize simple heuristics like field-of-view (FOV) coverage
to decide where to place exteroceptive sensors. In this work, we conduct an
information-theoretic investigation of this overlooked element of robotic
perception in the context of simultaneous localization and mapping (SLAM). We
show how to formalize the sensor arrangement problem as a form of subset
selection under the E-optimality performance criterion. While this formulation
is NP-hard in general, we show that a combination of greedy sensor selection
and fast convex relaxation-based post-hoc verification enables the efficient
recovery of certifiably optimal sensor designs in practice. Results from
synthetic experiments reveal that sensors placed with OASIS outperform
benchmarks in terms of mean squared error of visual SLAM estimates.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-20T00:00:00Z">2024-03-20</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">70</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Natural Language as Polices: Reasoning for Coordinate-Level Embodied
  Control with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13801v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13801v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuke Mikami, Andrew Melnik, Jun Miura, Ville Hautamäki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We demonstrate experimental results with LLMs that address robotics action
planning problems. Recently, LLMs have been applied in robotics action
planning, particularly using a code generation approach that converts complex
high-level instructions into mid-level policy codes. In contrast, our approach
acquires text descriptions of the task and scene objects, then formulates
action planning through natural language reasoning, and outputs coordinate
level control commands, thus reducing the necessity for intermediate
representation code as policies. Our approach is evaluated on a multi-modal
prompt simulation benchmark, demonstrating that our prompt engineering
experiments with natural language reasoning significantly enhance success rates
compared to its absence. Furthermore, our approach illustrates the potential
for natural language descriptions to transfer robotics skills from known tasks
to previously unseen tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Convex Formulation of Frictional Contact for the Material Point Method
  and Rigid Bodies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeshun Zong, Chenfanfu Jiang, Xuchen Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a novel convex formulation that seamlessly
integrates the Material Point Method (MPM) with articulated rigid body dynamics
in frictional contact scenarios. We extend the linear corotational hyperelastic
model into the realm of elastoplasticity and include an efficient return
mapping algorithm. This approach is particularly effective for MPM simulations
involving significant deformation and topology changes, while preserving the
convexity of the optimization problem. Our method ensures global convergence,
enabling the use of large simulation time steps without compromising
robustness. We have validated our approach through rigorous testing and
performance evaluations, highlighting its superior capabilities in managing
complex simulations relevant to robotics. Compared to previous MPM based
robotic simulators, our method significantly improves the stability of contact
resolution -- a critical factor in robot manipulation tasks. We make our method
available in the open-source robotics toolkit, Drake.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Certified Human Trajectory Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadhossein Bahari, Saeed Saadatnejad, Amirhossein Asgari Farsangi, Seyed-Mohsen Moosavi-Dezfooli, Alexandre Alahi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory prediction plays an essential role in autonomous vehicles. While
numerous strategies have been developed to enhance the robustness of trajectory
prediction models, these methods are predominantly heuristic and do not offer
guaranteed robustness against adversarial attacks and noisy observations. In
this work, we propose a certification approach tailored for the task of
trajectory prediction. To this end, we address the inherent challenges
associated with trajectory prediction, including unbounded outputs, and
mutli-modality, resulting in a model that provides guaranteed robustness.
Furthermore, we integrate a denoiser into our method to further improve the
performance. Through comprehensive evaluations, we demonstrate the
effectiveness of the proposed technique across various baselines and using
standard trajectory prediction datasets. The code will be made available
online: https://s-attack.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Embedding Pose Graph, Enabling 3D Foundation Model Capabilities with a
  Compact Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugues Thomas, Jian Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the Embedding Pose Graph (EPG), an innovative method that
combines the strengths of foundation models with a simple 3D representation
suitable for robotics applications. Addressing the need for efficient spatial
understanding in robotics, EPG provides a compact yet powerful approach by
attaching foundation model features to the nodes of a pose graph. Unlike
traditional methods that rely on bulky data formats like voxel grids or point
clouds, EPG is lightweight and scalable. It facilitates a range of robotic
tasks, including open-vocabulary querying, disambiguation, image-based
querying, language-directed navigation, and re-localization in 3D environments.
We showcase the effectiveness of EPG in handling these tasks, demonstrating its
capacity to improve how robots interact with and navigate through complex
spaces. Through both qualitative and quantitative assessments, we illustrate
EPG's strong performance and its ability to outperform existing methods in
re-localization. Our work introduces a crucial step forward in enabling robots
to efficiently understand and operate within large-scale 3D spaces.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Projection-free computation of robust controllable sets with constrained
  zonotopes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abraham P. Vinod, Avishai Weiss, Stefano Di Cairano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of computing robust controllable sets for discrete-time
linear systems with additive uncertainty. We propose a tractable and scalable
approach to inner- and outer-approximate robust controllable sets using
constrained zonotopes, when the additive uncertainty set is a symmetric,
convex, and compact set. Our least-squares-based approach uses novel
closed-form approximations of the Pontryagin difference between a constrained
zonotopic minuend and a symmetric, convex, and compact subtrahend. Unlike
existing approaches, our approach does not rely on convex optimization solvers,
and is projection-free for ellipsoidal and zonotopic uncertainty sets. We also
propose a least-squares-based approach to compute a convex, polyhedral
outer-approximation to constrained zonotopes, and characterize sufficient
conditions under which all these approximations are exact. We demonstrate the
computational efficiency and scalability of our approach in several case
studies, including the design of abort-safe rendezvous trajectories for a
spacecraft in near-rectilinear halo orbit under uncertainty. Our approach can
inner-approximate a 20-step robust controllable set for a 100-dimensional
linear system in under 15 seconds on a standard computer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning for Online Testing of Autonomous Driving Systems:
  a Replication and Extension Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Giamattei, Matteo Biagiola, Roberto Pietrantuono, Stefano Russo, Paolo Tonella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a recent study, Reinforcement Learning (RL) used in combination with
many-objective search, has been shown to outperform alternative techniques
(random search and many-objective search) for online testing of Deep Neural
Network-enabled systems. The empirical evaluation of these techniques was
conducted on a state-of-the-art Autonomous Driving System (ADS). This work is a
replication and extension of that empirical study. Our replication shows that
RL does not outperform pure random test generation in a comparison conducted
under the same settings of the original study, but with no confounding factor
coming from the way collisions are measured. Our extension aims at eliminating
some of the possible reasons for the poor performance of RL observed in our
replication: (1) the presence of reward components providing contrasting or
useless feedback to the RL agent; (2) the usage of an RL algorithm (Q-learning)
which requires discretization of an intrinsically continuous state space.
Results show that our new RL agent is able to converge to an effective policy
that outperforms random testing. Results also highlight other possible
improvements, which open to further investigations on how to best leverage RL
for online ADS testing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DBA-Fusion: Tightly Integrating Deep Dense Visual Bundle Adjustment with
  Multiple Sensors for Large-Scale Localization and Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Zhou, Xingxing Li, Shengyu Li, Xuanbin Wang, Shaoquan Feng, Yuxuan Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual simultaneous localization and mapping (VSLAM) has broad applications,
with state-of-the-art methods leveraging deep neural networks for better
robustness and applicability. However, there is a lack of research in fusing
these learning-based methods with multi-sensor information, which could be
indispensable to push related applications to large-scale and complex
scenarios. In this paper, we tightly integrate the trainable deep dense bundle
adjustment (DBA) with multi-sensor information through a factor graph. In the
framework, recurrent optical flow and DBA are performed among sequential
images. The Hessian information derived from DBA is fed into a generic factor
graph for multi-sensor fusion, which employs a sliding window and supports
probabilistic marginalization. A pipeline for visual-inertial integration is
firstly developed, which provides the minimum ability of metric-scale
localization and mapping. Furthermore, other sensors (e.g., global navigation
satellite system) are integrated for driftless and geo-referencing
functionality. Extensive tests are conducted on both public datasets and
self-collected datasets. The results validate the superior localization
performance of our approach, which enables real-time dense mapping in
large-scale environments. The code has been made open-source
(https://github.com/GREAT-WHU/DBA-Fusion).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Matters for Active Texture Recognition With Vision-Based Tactile
  Sensors <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alina Böhm, Tim Schneider, Boris Belousov, Alap Kshirsagar, Lisa Lin, Katja Doerschner, Knut Drewing, Constantin A. Rothkopf, Jan Peters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores active sensing strategies that employ vision-based
tactile sensors for robotic perception and classification of fabric textures.
We formalize the active sampling problem in the context of tactile fabric
recognition and provide an implementation of information-theoretic exploration
strategies based on minimizing predictive entropy and variance of probabilistic
models. Through ablation studies and human experiments, we investigate which
components are crucial for quick and reliable texture recognition. Along with
the active sampling strategies, we evaluate neural network architectures,
representations of uncertainty, influence of data augmentation, and dataset
variability. By evaluating our method on a previously published Active Clothing
Perception Dataset and on a real robotic system, we establish that the choice
of the active exploration strategy has only a minor influence on the
recognition accuracy, whereas data augmentation and dropout rate play a
significantly larger role. In a comparison study, while humans achieve 66.9%
recognition accuracy, our best approach reaches 90.0% in under 5 touches,
highlighting that vision-based tactile sensors are highly effective for fabric
texture recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 9 figures, accepted at 2024 IEEE International Conference on
  Robotics and Automation (ICRA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Loss Regularizing Robotic Terrain Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shakti Deo Kumar, Sudhanshu Tripathi, Krishna Ujjwal, Sarvada Sakshi Jha, Suddhasil De
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Locomotion mechanics of legged robots are suitable when pacing through
difficult terrains. Recognising terrains for such robots are important to fully
yoke the versatility of their movements. Consequently, robotic terrain
classification becomes significant to classify terrains in real time with high
accuracy. The conventional classifiers suffer from overfitting problem, low
accuracy problem, high variance problem, and not suitable for live dataset. On
the other hand, classifying a growing dataset is difficult for convolution
based terrain classification. Supervised recurrent models are also not
practical for this classification. Further, the existing recurrent
architectures are still evolving to improve accuracy of terrain classification
based on live variable-length sensory data collected from legged robots. This
paper proposes a new semi-supervised method for terrain classification of
legged robots, avoiding preprocessing of long variable-length dataset. The
proposed method has a stacked Long Short-Term Memory architecture, including a
new loss regularization. The proposed method solves the existing problems and
improves accuracy. Comparison with the existing architectures show the
improvements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preliminary draft of the work published in IEEE conference 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DVMNet: Computing Relative Pose for Unseen Objects Beyond Hypotheses <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13683v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13683v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Zhao, Tong Zhang, Zheng Dang, Mathieu Salzmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Determining the relative pose of an object between two images is pivotal to
the success of generalizable object pose estimation. Existing approaches
typically approximate the continuous pose representation with a large number of
discrete pose hypotheses, which incurs a computationally expensive process of
scoring each hypothesis at test time. By contrast, we present a Deep Voxel
Matching Network (DVMNet) that eliminates the need for pose hypotheses and
computes the relative object pose in a single pass. To this end, we map the two
input RGB images, reference and query, to their respective voxelized 3D
representations. We then pass the resulting voxels through a pose estimation
module, where the voxels are aligned and the pose is computed in an end-to-end
fashion by solving a least-squares problem. To enhance robustness, we introduce
a weighted closest voxel algorithm capable of mitigating the impact of noisy
voxels. We conduct extensive experiments on the CO3D, LINEMOD, and Objaverse
datasets, demonstrating that our method delivers more accurate relative pose
estimates for novel objects at a lower computational cost compared to
state-of-the-art methods. Our code is released at:
https://github.com/sailor-z/DVMNet/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reward-Driven Automated Curriculum Learning for Interaction-Aware
  Self-Driving at Unsignalized Intersections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zengqi Peng, Xiao Zhou, Lei Zheng, Yubin Wang, Jun Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present a reward-driven automated curriculum reinforcement
learning approach for interaction-aware self-driving at unsignalized
intersections, taking into account the uncertainties associated with
surrounding vehicles (SVs). These uncertainties encompass the uncertainty of
SVs' driving intention and also the quantity of SVs. To deal with this problem,
the curriculum set is specifically designed to accommodate a progressively
increasing number of SVs. By implementing an automated curriculum selection
mechanism, the importance weights are rationally allocated across various
curricula, thereby facilitating improved sample efficiency and training
outcomes. Furthermore, the reward function is meticulously designed to guide
the agent towards effective policy exploration. Thus the proposed framework
could proactively address the above uncertainties at unsignalized intersections
by employing the automated curriculum learning technique that progressively
increases task difficulty, and this ensures safe self-driving through effective
interaction with SVs. Comparative experiments are conducted in $Highway\_Env$,
and the results indicate that our approach achieves the highest task success
rate, attains strong robustness to initialization parameters of the curriculum
selection module, and exhibits superior adaptability to diverse situational
configurations at unsignalized intersections. Furthermore, the effectiveness of
the proposed method is validated using the high-fidelity CARLA simulator.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LaCE-LHMP: Airflow Modelling-Inspired Long-Term Human Motion Prediction
  By Enhancing Laminar Characteristics in Human Flow <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yufei Zhu, Han Fan, Andrey Rudenko, Martin Magnusson, Erik Schaffernicht, Achim J. Lilienthal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-term human motion prediction (LHMP) is essential for safely operating
autonomous robots and vehicles in populated environments. It is fundamental for
various applications, including motion planning, tracking, human-robot
interaction and safety monitoring. However, accurate prediction of human
trajectories is challenging due to complex factors, including, for example,
social norms and environmental conditions. The influence of such factors can be
captured through Maps of Dynamics (MoDs), which encode spatial motion patterns
learned from (possibly scattered and partial) past observations of motion in
the environment and which can be used for data-efficient, interpretable motion
prediction (MoD-LHMP). To address the limitations of prior work, especially
regarding accuracy and sensitivity to anomalies in long-term prediction, we
propose the Laminar Component Enhanced LHMP approach (LaCE-LHMP). Our approach
is inspired by data-driven airflow modelling, which estimates laminar and
turbulent flow components and uses predominantly the laminar components to make
flow predictions. Based on the hypothesis that human trajectory patterns also
manifest laminar flow (that represents predictable motion) and turbulent flow
components (that reflect more unpredictable and arbitrary motion), LaCE-LHMP
extracts the laminar patterns in human dynamics and uses them for human motion
prediction. We demonstrate the superior prediction performance of LaCE-LHMP
through benchmark comparisons with state-of-the-art LHMP methods, offering an
unconventional perspective and a more intuitive understanding of human movement
patterns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 2024 IEEE International Conference on Robotics and
  Automation (ICRA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From One to Many: How Active Robot Swarm Sizes Influence Human Cognitive
  Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Kaduk, Müge Cavdan, Knut Drewing, Heiko Hamann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In robotics, understanding human interaction with autonomous systems is
crucial for enhancing collaborative technologies. We focus on human-swarm
interaction (HSI), exploring how differently sized groups of active robots
affect operators' cognitive and perceptual reactions over different durations.
We analyze the impact of different numbers of active robots within a 15-robot
swarm on operators' time perception, emotional state, flow experience, and task
difficulty perception. Our findings indicate that managing multiple active
robots when compared to one active robot significantly alters time perception
and flow experience, leading to a faster passage of time and increased flow.
More active robots and extended durations cause increased emotional arousal and
perceived task difficulty, highlighting the interaction between robot the
number of active robots and human cognitive processes. These insights inform
the creation of intuitive human-swarm interfaces and aid in developing swarm
robotic systems aligned with human cognitive structures, enhancing human-robot
collaboration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Motion Generation from Fine-grained Textual Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13518v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13518v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunhang Li, Yansong Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of text2motion is to generate motion sequences from given textual
descriptions, where a model should explore the interactions between natural
language instructions and human body movements. While most existing works are
confined to coarse-grained motion descriptions (e.g., "A man squats."),
fine-grained ones specifying movements of relevant body parts are barely
explored. Models trained with coarse texts may not be able to learn mappings
from fine-grained motion-related words to motion primitives, resulting in the
failure in generating motions from unseen descriptions. In this paper, we build
a large-scale language-motion dataset with fine-grained textual descriptions,
FineHumanML3D, by feeding GPT-3.5-turbo with delicate prompts. Accordingly, we
design a new text2motion model, FineMotionDiffuse, which makes full use of
fine-grained textual information. Our experiments show that FineMotionDiffuse
trained on FineHumanML3D acquires good results in quantitative evaluation. We
also find this model can better generate spatially/chronologically composite
motions by learning the implicit mappings from simple descriptions to the
corresponding basic motions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Iterative Active-Inactive Obstacle Classification for Time-Optimal
  Collision Avoidance <span class="chip">IROS24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13474v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13474v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehmetcan Kaymaz, Nazim Kemal Ure
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time-optimal obstacle avoidance is a prevalent problem encountered in various
fields, including robotics and autonomous vehicles, where the task involves
determining a path for a moving vehicle to reach its goal while navigating
around obstacles within its environment. This problem becomes increasingly
challenging as the number of obstacles in the environment rises. We propose an
iterative active-inactive obstacle approach, which involves identifying a
subset of the obstacles as "active", that considers solely the effect of the
"active" obstacles on the path of the moving vehicle. The remaining obstacles
are considered "inactive" and are not considered in the path planning process.
The obstacles are classified as 'active' on the basis of previous findings
derived from prior iterations. This approach allows for a more efficient
calculation of the optimal path by reducing the number of obstacles that need
to be considered. The effectiveness of the proposed method is demonstrated with
two different dynamic models using the various number of obstacles. The results
show that the proposed method is able to find the optimal path in a timely
manner, while also being able to handle a large number of obstacles in the
environment and the constraints on the motion of the object.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is under review in IROS24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLIPSwarm: Generating Drone Shows from Text <span class="highlight-title">Prompt</span>s with Vision-Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13467v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13467v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo Pueyo, Eduardo Montijano, Ana C. Murillo, Mac Schwager
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces CLIPSwarm, a new algorithm designed to automate the
modeling of swarm drone formations based on natural language. The algorithm
begins by enriching a provided word, to compose a text prompt that serves as
input to an iterative approach to find the formation that best matches the
provided word. The algorithm iteratively refines formations of robots to align
with the textual description, employing different steps for "exploration" and
"exploitation". Our framework is currently evaluated on simple formation
targets, limited to contour shapes. A formation is visually represented through
alpha-shape contours and the most representative color is automatically found
for the input word. To measure the similarity between the description and the
visual representation of the formation, we use CLIP [1], encoding text and
images into vectors and assessing their similarity. Subsequently, the algorithm
rearranges the formation to visually represent the word more effectively,
within the given constraints of available drones. Control actions are then
assigned to the drones, ensuring robotic behavior and collision-free movement.
Experimental results demonstrate the system's efficacy in accurately modeling
robot formations from natural language descriptions. The algorithm's
versatility is showcased through the execution of drone shows in photorealistic
simulation with varying shapes. We refer the reader to the supplementary video
for a visual reference of the results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FACT: Fast and Active Coordinate Initialization for Vision-based Drone
  Swarms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13455v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13455v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Li, Anke Zhao, Yingjian Wang, Ziyi Xu, Xin Zhou, Jinni Zhou, Chao Xu, Fei Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Swarm robots have sparked remarkable developments across a range of fields.
While it is necessary for various applications in swarm robots, a fast and
robust coordinate initialization in vision-based drone swarms remains elusive.
To this end, our paper proposes a complete system to recover a swarm's initial
relative pose on platforms with size, weight, and power (SWaP) constraints. To
overcome limited coverage of field-of-view (FoV), the drones rotate in place to
obtain observations. To tackle the anonymous measurements, we formulate a
non-convex rotation estimation problem and transform it into a semi-definite
programming (SDP) problem, which can steadily obtain global optimal values.
Then we utilize the Hungarian algorithm to recover relative translation and
correspondences between observations and drone identities. To safely acquire
complete observations, we actively search for positions and generate feasible
trajectories to avoid collisions. To validate the practicability of our system,
we conduct experiments on a vision-based drone swarm with only stereo cameras
and inertial measurement units (IMUs) as sensors. The results demonstrate that
the system can robustly get accurate relative poses in real time with limited
onboard computation resources. The source code is released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mobile Robot Localization: a Modular, Odometry-Improving Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13452v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13452v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Mozzarelli, Luca Cattaneo, Matteo Corno, Sergio Matteo Savaresi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the number of works published in recent years, vehicle localization
remains an open, challenging problem. While map-based localization and SLAM
algorithms are getting better and better, they remain a single point of failure
in typical localization pipelines. This paper proposes a modular localization
architecture that fuses sensor measurements with the outputs of off-the-shelf
localization algorithms. The fusion filter estimates model uncertainties to
improve odometry in case absolute pose measurements are lost entirely. The
architecture is validated experimentally on a real robot navigating
autonomously proving a reduction of the position error of more than 90% with
respect to the odometrical estimate without uncertainty estimation in a
two-minute navigation period without position measurements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE European Control Conference 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast-Poly: A Fast Polyhedral Framework For 3D Multi-Object Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Li, Dedong Liu, Lijun Zhao, Yitao Wu, Xian Wu, Jinghan Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Multi-Object Tracking (MOT) captures stable and comprehensive motion
states of surrounding obstacles, essential for robotic perception. However,
current 3D trackers face issues with accuracy and latency consistency. In this
paper, we propose Fast-Poly, a fast and effective filter-based method for 3D
MOT. Building upon our previous work Poly-MOT, Fast-Poly addresses object
rotational anisotropy in 3D space, enhances local computation densification,
and leverages parallelization technique, improving inference speed and
precision. Fast-Poly is extensively tested on two large-scale tracking
benchmarks with Python implementation. On the nuScenes dataset, Fast-Poly
achieves new state-of-the-art performance with 75.8% AMOTA among all methods
and can run at 34.2 FPS on a personal CPU. On the Waymo dataset, Fast-Poly
exhibits competitive accuracy with 63.6% MOTA and impressive inference speed
(35.5 FPS). The source code is publicly available at
https://github.com/lixiaoyu2000/FastPoly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>1st on the NuScenes Tracking benchmark with 75.8 AMOTA and 34.2 FPS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Navigation Map Generation for Mobile Robots in Urban
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Mozzarelli, Simone Specchia, Matteo Corno, Sergio Matteo Savaresi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental prerequisite for safe and efficient navigation of mobile robots
is the availability of reliable navigation maps upon which trajectories can be
planned. With the increasing industrial interest in mobile robotics, especially
in urban environments, the process of generating navigation maps has become of
particular interest, being a labor intensive step of the deployment process.
Automating this step is challenging and becomes even more arduous when the
perception capabilities are limited by cost considerations. This paper proposes
an algorithm to automatically generate navigation maps using a typical
navigation-oriented sensor setup: a single top-mounted 3D LiDAR sensor. The
proposed method is designed and validated with the urban environment as the
main use case: it is shown to be able to produce accurate maps featuring
different terrain types, positive obstacles of different heights as well as
negative obstacles. The algorithm is applied to data collected in a typical
urban environment with a wheeled inverted pendulum robot, showing its
robustness against localization, perception and dynamic uncertainties. The
generated map is validated against a human-made map.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Caching-Augmented Lifelong Multi-Agent Path Finding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yimin Tang, Zhenghong Yu, Yi Zheng, T. K. Satish Kumar, Jiaoyang Li, Sven Koenig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Agent Path Finding (MAPF), which involves finding collision-free paths
for multiple robots, is crucial in various applications. Lifelong MAPF, where
targets are reassigned to agents as soon as they complete their initial
objectives, offers a more accurate approximation of real-world warehouse
planning. In this paper, we present a novel mechanism named Caching-Augmented
Lifelong MAPF (CAL-MAPF), designed to improve the performance of Lifelong MAPF.
We have developed a new map grid type called cache for temporary item storage
and replacement and designed a lock mechanism for it to improve the stability
of the planning solution. This cache mechanism was evaluated using various
cache replacement policies and a spectrum of input task distributions. We
identified three main factors significantly impacting CAL-MAPF performance
through experimentation: suitable input task distribution, high cache hit rate,
and smooth traffic. Overall, CAL-MAPF has demonstrated potential for
performance improvements in certain task distributions, maps and agent
configurations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unifying Local and Global Multimodal Features for Place Recognition in
  Aliased and Low-Texture Environments <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13395v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13395v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alberto García-Hernández, Riccardo Giubilato, Klaus H. Strobl, Javier Civera, Rudolph Triebel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perceptual aliasing and weak textures pose significant challenges to the task
of place recognition, hindering the performance of Simultaneous Localization
and Mapping (SLAM) systems. This paper presents a novel model, called UMF
(standing for Unifying Local and Global Multimodal Features) that 1) leverages
multi-modality by cross-attention blocks between vision and LiDAR features, and
2) includes a re-ranking stage that re-orders based on local feature matching
the top-k candidates retrieved using a global representation. Our experiments,
particularly on sequences captured on a planetary-analogous environment, show
that UMF outperforms significantly previous baselines in those challenging
aliased environments. Since our work aims to enhance the reliability of SLAM in
all situations, we also explore its performance on the widely used RobotCar
dataset, for broader applicability. Code and models are available at
https://github.com/DLR-RM/UMF
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted submission to International Conference on Robotics and
  Automation (ICRA), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Centroidal State Estimation based on the Koopman Embedding for Dynamic
  Legged Locomotion <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahram Khorshidi, Murad Dawood, Maren Bennewitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a novel approach to centroidal state estimation,
which plays a crucial role in predictive model-based control strategies for
dynamic legged locomotion. Our approach uses the Koopman operator theory to
transform the robot's complex nonlinear dynamics into a linear system, by
employing dynamic mode decomposition and deep learning for model construction.
We evaluate both models on their linearization accuracy and capability to
capture both fast and slow dynamic system responses. We then select the most
suitable model for estimation purposes, and integrate it within a moving
horizon estimator. This estimator is formulated as a convex quadratic program,
to facilitate robust, real-time centroidal state estimation. Through extensive
simulation experiments on a quadruped robot executing various dynamic gaits,
our data-driven framework outperforms conventional filtering techniques based
on nonlinear dynamics. Our estimator addresses challenges posed by force/torque
measurement noise in highly dynamic motions and accurately recovers the
centroidal states, demonstrating the adaptability and effectiveness of the
Koopman-based linear representation for complex locomotive behaviors.
Importantly, our model based on dynamic mode decomposition, trained with two
locomotion patterns (trot and jump), successfully estimates the centroidal
states for a different motion (bound) without retraining.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ManiPose: A Comprehensive Benchmark for Pose-aware Object Manipulation
  in Robotics <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiaojun Yu, Ce Hao, Junbo Wang, Wenhai Liu, Liu Liu, Yao Mu, Yang You, Hengxu Yan, Cewu Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic manipulation in everyday scenarios, especially in unstructured
environments, requires skills in pose-aware object manipulation (POM), which
adapts robots' grasping and handling according to an object's 6D pose.
Recognizing an object's position and orientation is crucial for effective
manipulation. For example, if a mug is lying on its side, it's more effective
to grasp it by the rim rather than the handle. Despite its importance, research
in POM skills remains limited, because learning manipulation skills requires
pose-varying simulation environments and datasets. This paper introduces
ManiPose, a pioneering benchmark designed to advance the study of pose-varying
manipulation tasks. ManiPose encompasses: 1) Simulation environments for POM
feature tasks ranging from 6D pose-specific pick-and-place of single objects to
cluttered scenes, further including interactions with articulated objects. 2) A
comprehensive dataset featuring geometrically consistent and
manipulation-oriented 6D pose labels for 2936 real-world scanned rigid objects
and 100 articulated objects across 59 categories. 3) A baseline for POM,
leveraging the inferencing abilities of LLM (e.g., ChatGPT) to analyze the
relationship between 6D pose and task-specific requirements, offers enhanced
pose-aware grasp prediction and motion planning capabilities. Our benchmark
demonstrates notable advancements in pose estimation, pose-aware manipulation,
and real-robot skill transfer, setting new standards for POM research. We will
open-source the ManiPose benchmark with the final version paper, inviting the
community to engage with our resources, available at our
website:https://sites.google.com/view/manipose.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, submitted to 2024 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GeRM: A Generalist Robotic Model with Mixture-of-experts for Quadruped
  Robot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13358v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13358v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Song, Han Zhao, Pengxiang Ding, Can Cui, Shangke Lyu, Yaning Fan, Donglin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-task robot learning holds significant importance in tackling diverse
and complex scenarios. However, current approaches are hindered by performance
issues and difficulties in collecting training datasets. In this paper, we
propose GeRM (Generalist Robotic Model). We utilize offline reinforcement
learning to optimize data utilization strategies to learn from both
demonstrations and sub-optimal data, thus surpassing the limitations of human
demonstrations. Thereafter, we employ a transformer-based VLA network to
process multi-modal inputs and output actions. By introducing the
Mixture-of-Experts structure, GeRM allows faster inference speed with higher
whole model capacity, and thus resolves the issue of limited RL parameters,
enhancing model performance in multi-task learning while controlling
computational costs. Through a series of experiments, we demonstrate that GeRM
outperforms other methods across all tasks, while also validating its
efficiency in both training and inference processes. Additionally, we uncover
its potential to acquire emergent skills. Additionally, we contribute the
QUARD-Auto dataset, collected automatically to support our training approach
and foster advancements in multi-task quadruped robot learning. This work
presents a new paradigm for reducing the cost of collecting robot data and
driving progress in the multi-task learning community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MULAN-WC: Multi-Robot Localization Uncertainty-aware Active NeRF with
  Wireless Coordination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13348v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13348v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiying Wang, Victor Cai, Stephanie Gil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents MULAN-WC, a novel multi-robot 3D reconstruction framework
that leverages wireless signal-based coordination between robots and Neural
Radiance Fields (NeRF). Our approach addresses key challenges in multi-robot 3D
reconstruction, including inter-robot pose estimation, localization uncertainty
quantification, and active best-next-view selection. We introduce a method for
using wireless Angle-of-Arrival (AoA) and ranging measurements to estimate
relative poses between robots, as well as quantifying and incorporating the
uncertainty embedded in the wireless localization of these pose estimates into
the NeRF training loss to mitigate the impact of inaccurate camera poses.
Furthermore, we propose an active view selection approach that accounts for
robot pose uncertainty when determining the next-best views to improve the 3D
reconstruction, enabling faster convergence through intelligent view selection.
Extensive experiments on both synthetic and real-world datasets demonstrate the
effectiveness of our framework in theory and in practice. Leveraging wireless
coordination and localization uncertainty-aware training, MULAN-WC can achieve
high-quality 3d reconstruction which is close to applying the ground truth
camera poses. Furthermore, the quantification of the information gain from a
novel view enables consistent rendering quality improvement with incrementally
captured images by commending the robot the novel view position. Our hardware
experiments showcase the practicality of deploying MULAN-WC to real robotic
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Discretizing SO(2)-Equivariant Features for Robotic Kitting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiadong Zhou, Yadan Zeng, Huixu Dong, I-Ming Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic kitting has attracted considerable attention in logistics and
industrial settings. However, existing kitting methods encounter challenges
such as low precision and poor efficiency, limiting their widespread
applications. To address these issues, we present a novel kitting framework
that improves both the precision and computational efficiency of complex
kitting tasks. Firstly, our approach introduces a fine-grained orientation
estimation technique in the picking module, significantly enhancing orientation
precision while effectively decoupling computational load from orientation
granularity. This approach combines an SO(2)-equivariant network with a group
discretization operation to preciously predict discrete orientation
distributions. Secondly, we develop the Hand-tool Kitting Dataset (HKD) to
evaluate the performance of different solutions in handling
orientation-sensitive kitting tasks. This dataset comprises a diverse
collection of hand tools and synthetically created kits, which reflects the
complexities encountered in real-world kitting scenarios. Finally, a series of
experiments are conducted to evaluate the performance of the proposed method.
The results demonstrate that our approach offers remarkable precision and
enhanced computational efficiency in robotic kitting tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robotics meets Fluid Dynamics: A Characterization of the Induced Airflow
  around a Quadrotor 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonard Bauersfeld, Koen Muller, Dominic Ziegler, Filippo Coletti, Davide Scaramuzza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread adoption of quadrotors for diverse applications, from
agriculture to public safety, necessitates an understanding of the aerodynamic
disturbances they create. This paper introduces a computationally lightweight
model for estimating the time-averaged magnitude of the induced flow below
quadrotors in hover. Unlike related approaches that rely on expensive
computational fluid dynamics (CFD) simulations or time-consuming empirical
measurements, our method leverages classical theory from turbulent flows. By
analyzing over 9 hours of flight data from drones of varying sizes within a
large motion capture system, we show that the combined flow from all propellers
of the drone is well-approximated by a turbulent jet. Through the use of a
novel normalization and scaling, we have developed and experimentally validated
a unified model that describes the mean velocity field of the induced flow for
different drone sizes. The model accurately describes the far-field airflow in
a very large volume below the drone which is difficult to simulate in CFD. Our
model, which requires only the drone's mass, propeller size, and drone size for
calculations, offers a practical tool for dynamic planning in multi-agent
scenarios, ensuring safer operations near humans and optimizing sensor
placements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7+1 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Workload Estimation for Unknown Tasks: A <span class="highlight-title">Survey</span> of Machine Learning
  Under Distribution Shift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13318v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13318v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Josh Bhagat Smith, Julie A. Adams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-robot teams involve humans and robots collaborating to achieve tasks
under various environmental conditions. Successful teaming will require robots
to adapt autonomously to a human teammate's internal state. An important
element of such adaptation is the ability to estimate the human teammates'
workload in unknown situations. Existing workload models use machine learning
to model the relationships between physiological metrics and workload; however,
these methods are susceptible to individual differences and are heavily
influenced by other factors. These methods cannot generalize to unknown tasks,
as they rely on standard machine learning approaches that assume data consists
of independent and identically distributed (IID) samples. This assumption does
not necessarily hold for estimating workload for new tasks. A survey of non-IID
machine learning techniques is presented, where commonly used techniques are
evaluated using three criteria: portability, model complexity, and
adaptability. These criteria are used to argue which techniques are most
applicable for estimating workload for unknown tasks in dynamic, real-time
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Robot Connected Fermat Spiral Coverage <span class="chip">ICAPS24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingtao Tang, Hang Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the Multi-Robot Connected Fermat Spiral (MCFS), a novel
algorithmic framework for Multi-Robot Coverage Path Planning (MCPP) that adapts
Connected Fermat Spiral (CFS) from the computer graphics community to
multi-robot coordination for the first time. MCFS uniquely enables the
orchestration of multiple robots to generate coverage paths that contour around
arbitrarily shaped obstacles, a feature that is notably lacking in traditional
methods. Our framework not only enhances area coverage and optimizes task
performance, particularly in terms of makespan, for workspaces rich in
irregular obstacles but also addresses the challenges of path continuity and
curvature critical for non-holonomic robots by generating smooth paths without
decomposing the workspace. MCFS solves MCPP by constructing a graph of isolines
and transforming MCPP into a combinatorial optimization problem, aiming to
minimize the makespan while covering all vertices. Our contributions include
developing a unified CFS version for scalable and adaptable MCPP, extending it
to MCPP with novel optimization techniques for cost reduction and path
continuity and smoothness, and demonstrating through extensive experiments that
MCFS outperforms existing MCPP methods in makespan, path curvature, coverage
ratio, and overlapping ratio. Our research marks a significant step in MCPP,
showcasing the fusion of computer graphics and automated planning principles to
advance the capabilities of multi-robot systems in complex environments. Our
code is available at https://github.com/reso1/MCFS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to ICAPS24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ POLICEd RL: Learning Closed-Loop Robot Control Policies with Provable
  Satisfaction of Hard Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean-Baptiste Bouvier, Kartik Nagpal, Negar Mehr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we seek to learn a robot policy guaranteed to satisfy state
constraints. To encourage constraint satisfaction, existing RL algorithms
typically rely on Constrained Markov Decision Processes and discourage
constraint violations through reward shaping. However, such soft constraints
cannot offer verifiable safety guarantees. To address this gap, we propose
POLICEd RL, a novel RL algorithm explicitly designed to enforce affine hard
constraints in closed-loop with a black-box environment. Our key insight is to
force the learned policy to be affine around the unsafe set and use this affine
region as a repulsive buffer to prevent trajectories from violating the
constraint. We prove that such policies exist and guarantee constraint
satisfaction. Our proposed framework is applicable to both systems with
continuous and discrete state and action spaces and is agnostic to the choice
of the RL training algorithm. Our results demonstrate the capacity of POLICEd
RL to enforce hard constraints in robotic tasks while significantly
outperforming existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Map-Aware Human Pose Prediction for Robot Follow-Ahead 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyuan Jiang, Burak Susam, Jun-Jee Chao, Volkan Isler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the robot follow-ahead task, a mobile robot is tasked to maintain its
relative position in front of a moving human actor while keeping the actor in
sight. To accomplish this task, it is important that the robot understand the
full 3D pose of the human (since the head orientation can be different than the
torso) and predict future human poses so as to plan accordingly. This
prediction task is especially tricky in a complex environment with junctions
and multiple corridors. In this work, we address the problem of forecasting the
full 3D trajectory of a human in such environments. Our main insight is to show
that one can first predict the 2D trajectory and then estimate the full 3D
trajectory by conditioning the estimator on the predicted 2D trajectory. With
this approach, we achieve results comparable or better than the
state-of-the-art methods three times faster. As part of our contribution, we
present a new dataset where, in contrast to existing datasets, the human motion
is in a much larger area than a single room. We also present a complete robot
system that integrates our human pose forecasting network on the mobile robot
to enable real-time robot follow-ahead and present results from real-world
experiments in multiple buildings on campus. Our project page, including
supplementary material and videos, can be found at:
https://qingyuan-jiang.github.io/iros2024_poseForecasting/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Look Before You Leap: Socially Acceptable High-Speed Ground Robot
  Navigation in Crowded Hallways <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lakshay Sharma, Jonathan P. How
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To operate safely and efficiently, autonomous warehouse/delivery robots must
be able to accomplish tasks while navigating in dynamic environments and
handling the large uncertainties associated with the motions/behaviors of other
robots and/or humans. A key scenario in such environments is the hallway
problem, where robots must operate in the same narrow corridor as human traffic
going in one or both directions. Traditionally, robot planners have tended to
focus on socially acceptable behavior in the hallway scenario at the expense of
performance. This paper proposes a planner that aims to address the consequent
"robot freezing problem" in hallways by allowing for "peek-and-pass" maneuvers.
We then go on to demonstrate in simulation how this planner improves robot time
to goal without violating social norms. Finally, we show initial hardware
demonstrations of this planner in the real world.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Waypoint-Based Reinforcement Learning for Robot Manipulation Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaunak A. Mehta, Soheil Habibian, Dylan P. Losey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot arms should be able to learn new tasks. One framework here is
reinforcement learning, where the robot is given a reward function that encodes
the task, and the robot autonomously learns actions to maximize its reward.
Existing approaches to reinforcement learning often frame this problem as a
Markov decision process, and learn a policy (or a hierarchy of policies) to
complete the task. These policies reason over hundreds of fine-grained actions
that the robot arm needs to take: e.g., moving slightly to the right or
rotating the end-effector a few degrees. But the manipulation tasks that we
want robots to perform can often be broken down into a small number of
high-level motions: e.g., reaching an object or turning a handle. In this paper
we therefore propose a waypoint-based approach for model-free reinforcement
learning. Instead of learning a low-level policy, the robot now learns a
trajectory of waypoints, and then interpolates between those waypoints using
existing controllers. Our key novelty is framing this waypoint-based setting as
a sequence of multi-armed bandits: each bandit problem corresponds to one
waypoint along the robot's motion. We theoretically show that an ideal solution
to this reformulation has lower regret bounds than standard frameworks. We also
introduce an approximate posterior sampling solution that builds the robot's
motion one waypoint at a time. Results across benchmark simulations and two
real-world experiments suggest that this proposed approach learns new tasks
more quickly than state-of-the-art baselines. See videos here:
https://youtu.be/MMEd-lYfq4Y
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UNO Push: Unified Nonprehensile Object Pushing via Non-Parametric
  Estimation and Model Predictive Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaotian Wang, Kejia Ren, Kaiyu Hang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nonprehensile manipulation through precise pushing is an essential skill that
has been commonly challenged by perception and physical uncertainties, such as
those associated with contacts, object geometries, and physical properties. For
this, we propose a unified framework that jointly addresses system modeling,
action generation, and control. While most existing approaches either heavily
rely on a priori system information for analytic modeling, or leverage a large
dataset to learn dynamic models, our framework approximates a system transition
function via non-parametric learning only using a small number of exploratory
actions (ca. 10). The approximated function is then integrated with model
predictive control to provide precise pushing manipulation. Furthermore, we
show that the approximated system transition functions can be robustly
transferred across novel objects while being online updated to continuously
improve the manipulation accuracy. Through extensive experiments on a real
robot platform with a set of novel objects and comparing against a
state-of-the-art baseline, we show that the proposed unified framework is a
light-weight and highly effective approach to enable precise pushing
manipulation all by itself. Our evaluation results illustrate that the system
can robustly ensure millimeter-level precision and can straightforwardly work
on any novel object.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Security in Multi-Robot Systems through Co-Observation
  Planning, Reachability Analysis, and Network Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Yang, Roberto Tron
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses security challenges in multi-robot systems (MRS) where
adversaries may compromise robot control, risking unauthorized access to
forbidden areas. We propose a novel multi-robot optimal planning algorithm that
integrates mutual observations and introduces reachability constraints for
enhanced security. This ensures that, even with adversarial movements,
compromised robots cannot breach forbidden regions without missing scheduled
co-observations. The reachability constraint uses ellipsoidal
over-approximation for efficient intersection checking and gradient
computation. To enhance system resilience and tackle feasibility challenges, we
also introduce sub-teams. These cohesive units replace individual robot
assignments along each route, enabling redundant robots to deviate for
co-observations across different trajectories, securing multiple sub-teams
without requiring modifications. We formulate the cross-trajectory
co-observation plan by solving a network flow coverage problem on the
checkpoint graph generated from the original unsecured MRS trajectories,
providing the same security guarantees against plan-deviation attacks. We
demonstrate the effectiveness and robustness of our proposed algorithm, which
significantly strengthens the security of multi-robot systems in the face of
adversarial threats.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures, submitted to IEEE Transactions on Control of
  Network Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Rule-Compliance Path Planner for Lane-Merge Scenarios Based on
  Responsibility-Sensitive Safety <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengfei Lin, Ehsan Javanmardi, Yuze Jiang, Manabu Tsukada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lane merging is one of the critical tasks for self-driving cars, and how to
perform lane-merge maneuvers effectively and safely has become one of the
important standards in measuring the capability of autonomous driving systems.
However, due to the ambiguity in driving intentions and right-of-way issues,
the lane merging process in autonomous driving remains deficient in terms of
maintaining or ceding the right-of-way and attributing liability, which could
result in protracted durations for merging and problems such as trajectory
oscillation. Hence, we present a rule-compliance path planner (RCPP) for
lane-merge scenarios, which initially employs the extended
responsibility-sensitive safety (RSS) to elucidate the right-of-way, followed
by the potential field-based sigmoid planner for path generation. In the
simulation, we have validated the efficacy of the proposed algorithm. The
algorithm demonstrated superior performance over previous approaches in aspects
such as merging time (Saved 72.3%), path length (reduced 53.4%), and
eliminating the trajectory oscillation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated reinforcement learning for robot motion planning with
  zero-shot generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyuan Yuan, Siyuan Xu, Minghui Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper considers the problem of learning a control policy for robot
motion planning with zero-shot generalization, i.e., no data collection and
policy adaptation is needed when the learned policy is deployed in new
environments. We develop a federated reinforcement learning framework that
enables collaborative learning of multiple learners and a central server, i.e.,
the Cloud, without sharing their raw data. In each iteration, each learner
uploads its local control policy and the corresponding estimated normalized
arrival time to the Cloud, which then computes the global optimum among the
learners and broadcasts the optimal policy to the learners. Each learner then
selects between its local control policy and that from the Cloud for next
iteration. The proposed framework leverages on the derived zero-shot
generalization guarantees on arrival time and safety. Theoretical guarantees on
almost-sure convergence, almost consensus, Pareto improvement and optimality
gap are also provided. Monte Carlo simulation is conducted to evaluate the
proposed framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AMCO: Adaptive Multimodal Coupling of Vision and Proprioception for
  Quadruped Robot Navigation in Outdoor Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Elnoor, Kasun Weerakoon, Adarsh Jagan Sathyamoorthy, Tianrui Guan, Vignesh Rajagopal, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present AMCO, a novel navigation method for quadruped robots that
adaptively combines vision-based and proprioception-based perception
capabilities. Our approach uses three cost maps: general knowledge map;
traversability history map; and current proprioception map; which are derived
from a robot's vision and proprioception data, and couples them to obtain a
coupled traversability cost map for navigation. The general knowledge map
encodes terrains semantically segmented from visual sensing, and represents a
terrain's typically expected traversability. The traversability history map
encodes the robot's recent proprioceptive measurements on a terrain and its
semantic segmentation as a cost map. Further, the robot's present
proprioceptive measurement is encoded as a cost map in the current
proprioception map. As the general knowledge map and traversability history map
rely on semantic segmentation, we evaluate the reliability of the visual
sensory data by estimating the brightness and motion blur of input RGB images
and accordingly combine the three cost maps to obtain the coupled
traversability cost map used for navigation. Leveraging this adaptive coupling,
the robot can depend on the most reliable input modality available. Finally, we
present a novel planner that selects appropriate gaits and velocities for
traversing challenging outdoor environments using the coupled traversability
cost map. We demonstrate AMCO's navigation performance in different real-world
outdoor environments and observe 10.8%-34.9% reduction w.r.t. two stability
metrics, and up to 50% improvement in terms of success rate compared to current
navigation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Contact Model based on Denoising Diffusion to Learn Variable Impedance
  Control for Contact-rich Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13221v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13221v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masashi Okada, Mayumi Komatsu, Tadahiro Taniguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, a novel approach is proposed for learning robot control in
contact-rich tasks such as wiping, by developing Diffusion Contact Model (DCM).
Previous methods of learning such tasks relied on impedance control with
time-varying stiffness tuning by performing Bayesian optimization by
trial-and-error with robots. The proposed approach aims to reduce the cost of
robot operation by predicting the robot contact trajectories from the variable
stiffness inputs and using neural models. However, contact dynamics are
inherently highly nonlinear, and their simulation requires iterative
computations such as convex optimization. Moreover, approximating such
computations by using finite-layer neural models is difficult. To overcome
these limitations, the proposed DCM used the denoising diffusion models that
could simulate the complex dynamics via iterative computations of multi-step
denoising, thus improving the prediction accuracy. Stiffness tuning experiments
conducted in simulated and real environments showed that the DCM achieved
comparable performance to a conventional robot-based optimization method while
reducing the number of robot trials.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "It's Not a Replacement:" Enabling Parent-Robot Collaboration to Support
  In-Home Learning Experiences of Young Children 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui-Ru Ho, Edward Hubbard, Bilge Mutlu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning companion robots for young children are increasingly adopted in
informal learning environments. Although parents play a pivotal role in their
children's learning, very little is known about how parents prefer to
incorporate robots into their children's learning activities. We developed
prototype capabilities for a learning companion robot to deliver educational
prompts and responses to parent-child pairs during reading sessions and
conducted in-home user studies involving 10 families with children aged 3-5.
Our data indicates that parents want to work with robots as collaborators to
augment parental activities to foster children's learning, introducing the
notion of parent-robot collaboration. Our findings offer an empirical
understanding of the needs and challenges of parent-child interaction in
informal learning scenarios and design opportunities for integrating a
companion robot into these interactions. We offer insights into how robots
might be designed to facilitate parent-robot collaboration, including parenting
policies, collaboration patterns, and interaction paradigms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quadcopter Team Configurable Motion Guided by a Quadruped 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14029v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14029v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Ghufran, Sourish Tetakayala, Jack Hughes, Aron Wilson, Hossein Rastgoftar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper focuses on modeling and experimental evaluation of a quadcopter
team configurable coordination guided by a single quadruped robot. We consider
the quadcopter team as particles of a two-dimensional deformable body and
propose a two-dimensional affine transformation model for safe and
collision-free configurable coordination of this heterogeneous robotic system.
The proposed affine transformation is decomposed into translation, that is
specified by the quadruped global position, and configurable motion of the
quadcopters, which is determined by a nonsingular Jacobian matrix so that the
quadcopter team can safely navigate a constrained environment while avoiding
collision. We propose two methods to experimentally evaluate the proposed
heterogeneous robot coordination model. The first method measures real
positions of quadcopters, quadruped, and environmental objects all with respect
to the global coordinate system. On the other hand, the second method measures
position with respect to the local coordinate system fixed on the dog robot
which in turn enables safe planning the Jacobian matrix of the quadcopter team
while the world is virtually approached the robotic system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HRI Curriculum for a Liberal Arts Education 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14025v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14025v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jason R. Wilson, Emily Jensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we discuss the opportunities and challenges of teaching a
human-robot interaction course at an undergraduate liberal arts college. We
provide a sample syllabus adapted from a previous version of a course.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the Designing an Intro to HRI Course Workshop at HRI
  2024 (arXiv:2403.05588)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Crowdsourcing Task Traces for Service Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14014v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14014v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Porfirio, Allison Sauppé, Maya Cakmak, Aws Albarghouthi, Bilge Mutlu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Demonstration is an effective end-user development paradigm for teaching
robots how to perform new tasks. In this paper, we posit that demonstration is
useful not only as a teaching tool, but also as a way to understand and assist
end-user developers in thinking about a task at hand. As a first step toward
gaining this understanding, we constructed a lightweight web interface to
crowdsource step-by-step instructions of common household tasks, leveraging the
imaginations and past experiences of potential end-user developers. As evidence
of the utility of our interface, we deployed the interface on Amazon Mechanical
Turk and collected 207 task traces that span 18 different task categories. We
describe our vision for how these task traces can be operationalized as task
models within end-user development tools and provide a roadmap for future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in the companion proceedings of the 2023 ACM/IEEE
  International Conference on Human-Robot Interaction</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Imitation Learning of Task-Oriented Object Grasping and
  Rearrangement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14000v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14000v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichen Cai, Jianfeng Gao, Christoph Pohl, Tamim Asfour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task-oriented object grasping and rearrangement are critical skills for
robots to accomplish different real-world manipulation tasks. However, they
remain challenging due to partial observations of the objects and shape
variations in categorical objects. In this paper, we propose the Multi-feature
Implicit Model (MIMO), a novel object representation that encodes multiple
spatial features between a point and an object in an implicit neural field.
Training such a model on multiple features ensures that it embeds the object
shapes consistently in different aspects, thus improving its performance in
object shape reconstruction from partial observation, shape similarity measure,
and modeling spatial relations between objects. Based on MIMO, we propose a
framework to learn task-oriented object grasping and rearrangement from single
or multiple human demonstration videos. The evaluations in simulation show that
our approach outperforms the state-of-the-art methods for multi- and
single-view observations. Real-world experiments demonstrate the efficacy of
our approach in one- and few-shot imitation learning of manipulation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Goal-Oriented End-User Programming of Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Porfirio, Mark Roberts, Laura M. Hiatt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-user programming (EUP) tools must balance user control with the robot's
ability to plan and act autonomously. Many existing task-oriented EUP tools
enforce a specific level of control, e.g., by requiring that users hand-craft
detailed sequences of actions, rather than offering users the flexibility to
choose the level of task detail they wish to express. We thereby created a
novel EUP system, Polaris, that in contrast to most existing EUP tools, uses
goal predicates as the fundamental building block of programs. Users can
thereby express high-level robot objectives or lower-level checkpoints at their
choosing, while an off-the-shelf task planner fills in any remaining program
detail. To ensure that goal-specified programs adhere to user expectations of
robot behavior, Polaris is equipped with a Plan Visualizer that exposes the
planner's output to the user before runtime. In what follows, we describe our
design of Polaris and its evaluation with 32 human participants. Our results
support the Plan Visualizer's ability to help users craft higher-quality
programs. Furthermore, there are strong associations between user perception of
the robot and Plan Visualizer usage, and evidence that robot familiarity has a
key role in shaping user experience.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in the proceedings of the 2024 ACM/IEEE International
  Conference on Human-Robot Interaction</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open Access NAO (OAN): a ROS2-based software framework for HRI
  applications with the NAO robot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13960v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13960v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Bono, Kenji Brameld, Luigi D'Alfonso, Giuseppe Fedele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a new software framework for HRI experimentation with the
sixth version of the common NAO robot produced by the United Robotics Group.
Embracing the common demand of researchers for better performance and new
features for NAO, the authors took advantage of the ability to run ROS2 onboard
on the NAO to develop a framework independent of the APIs provided by the
manufacturer. Such a system provides NAO with not only the basic skills of a
humanoid robot such as walking and reproducing movements of interest but also
features often used in HRI such as: speech recognition/synthesis, face and
object detention, and the use of Generative Pre-trained Transformer (GPT)
models for conversation. The developed code is therefore configured as a
ready-to-use but also highly expandable and improvable tool thanks to the
possibilities provided by the ROS community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sensory Glove-Based Surgical Robot User Interface <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13941v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13941v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo Borgioli, Ki-Hwan Oh, Alberto Mangano, Alvaro Ducas, Luciano Ambrosini, Federico Pinto, Paula A Lopez, Jessica Cassiani, Milos Zefran, Liaohai Chen, Pier Cristoforo Giulianotti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic surgery has reached a high level of maturity and has become an
integral part of standard surgical care. However, existing surgeon consoles are
bulky and take up valuable space in the operating room, present challenges for
surgical team coordination, and their proprietary nature makes it difficult to
take advantage of recent technological advances, especially in virtual and
augmented reality. One potential area for further improvement is the
integration of modern sensory gloves into robotic platforms, allowing surgeons
to control robotic arms directly with their hand movements intuitively. We
propose one such system that combines an HTC Vive tracker, a Manus Meta Prime 3
XR sensory glove, and God Vision wireless smart glasses. The system controls
one arm of a da Vinci surgical robot. In addition to moving the arm, the
surgeon can use fingers to control the end-effector of the surgical instrument.
Hand gestures are used to implement clutching and similar functions. In
particular, we introduce clutching of the instrument orientation, a
functionality not available in the da Vinci system. The vibrotactile elements
of the glove are used to provide feedback to the user when gesture commands are
invoked. A preliminary evaluation of the system shows that it has excellent
tracking accuracy and allows surgeons to efficiently perform common surgical
training tasks with minimal practice with the new interface; this suggests that
the interface is highly intuitive. The proposed system is inexpensive, allows
rapid prototyping, and opens opportunities for further innovations in the
design of surgical robot interfaces.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures, 7 tables, submitted to International Conference
  on Intelligent Robots and Systems (IROS)2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safety-Aware Perception for Autonomous Collision Avoidance in Dynamic
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13929v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13929v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan M. Bena, Chongbo Zhao, Quan Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous collision avoidance requires accurate environmental perception;
however, flight systems often possess limited sensing capabilities with
field-of-view (FOV) restrictions. To navigate this challenge, we present a
safety-aware approach for online determination of the optimal sensor-pointing
direction $\psi_\text{d}$ which utilizes control barrier functions (CBFs).
First, we generate a spatial density function $\Phi$ which leverages CBF
constraints to map the collision risk of all local coordinates. Then, we
convolve $\Phi$ with an attitude-dependent sensor FOV quality function to
produce the objective function $\Gamma$ which quantifies the total observed
risk for a given pointing direction. Finally, by finding the global optimizer
for $\Gamma$, we identify the value of $\psi_\text{d}$ which maximizes the
perception of risk within the FOV. We incorporate $\psi_\text{d}$ into a
safety-critical flight architecture and conduct a numerical analysis using
multiple simulated mission profiles. Our algorithm achieves a success rate of
$88-96\%$, constituting a $16-29\%$ improvement compared to the best heuristic
methods. We demonstrate the functionality of our approach via a flight
demonstration using the Crazyflie 2.1 micro-quadrotor. Without a priori
obstacle knowledge, the quadrotor follows a dynamic flight path while
simultaneously calculating and tracking $\psi_\text{d}$ to perceive and avoid
two static obstacles with an average computation time of 371 $\mu$s.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Augmented Reality Demonstrations for Scalable Robot Imitation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13910v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13910v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Yang, Bryce Ikeda, Gedas Bertasius, Daniel Szafir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot Imitation Learning (IL) is a widely used method for training robots to
perform manipulation tasks that involve mimicking human demonstrations to
acquire skills. However, its practicality has been limited due to its
requirement that users be trained in operating real robot arms to provide
demonstrations. This paper presents an innovative solution: an Augmented
Reality (AR)-assisted framework for demonstration collection, empowering
non-roboticist users to produce demonstrations for robot IL using devices like
the HoloLens 2. Our framework facilitates scalable and diverse demonstration
collection for real-world tasks. We validate our approach with experiments on
three classical robotics tasks: reach, push, and pick-and-place. The real robot
performs each task successfully while replaying demonstrations collected via
AR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Out-of-Distribution Generalization of Learned Dynamics by
  Learning Pseudometrics and Constraint Manifolds <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12245v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12245v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yating Lin, Glen Chou, Dmitry Berenson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a method for improving the prediction accuracy of learned robot
dynamics models on out-of-distribution (OOD) states. We achieve this by
leveraging two key sources of structure often present in robot dynamics: 1)
sparsity, i.e., some components of the state may not affect the dynamics, and
2) physical limits on the set of possible motions, in the form of nonholonomic
constraints. Crucially, we do not assume this structure is known a priori, and
instead learn it from data. We use contrastive learning to obtain a distance
pseudometric that uncovers the sparsity pattern in the dynamics, and use it to
reduce the input space when learning the dynamics. We then learn the unknown
constraint manifold by approximating the normal space of possible motions from
the data, which we use to train a Gaussian process (GP) representation of the
constraint manifold. We evaluate our approach on a physical differential-drive
robot and a simulated quadrotor, showing improved prediction accuracy on OOD
data relative to baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept to ICRA 2024, 6 pages + references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ada-NAV: Adaptive Trajectory Length-Based Sample Efficient Policy
  Learning for Robotic Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06192v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06192v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhrij Patel, Kasun Weerakoon, Wesley A. Suttle, Alec Koppel, Brian M. Sadler, Tianyi Zhou, Amrit Singh Bedi, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory length stands as a crucial hyperparameter within reinforcement
learning (RL) algorithms, significantly contributing to the sample inefficiency
in robotics applications. Motivated by the pivotal role trajectory length plays
in the training process, we introduce Ada-NAV, a novel adaptive trajectory
length scheme designed to enhance the training sample efficiency of RL
algorithms in robotic navigation tasks. Unlike traditional approaches that
treat trajectory length as a fixed hyperparameter, we propose to dynamically
adjust it based on the entropy of the underlying navigation policy.
Interestingly, Ada-NAV can be applied to both existing on-policy and off-policy
RL methods, which we demonstrate by empirically validating its efficacy on
three popular RL methods: REINFORCE, Proximal Policy Optimization (PPO), and
Soft Actor-Critic (SAC). We demonstrate through simulated and real-world
robotic experiments that Ada-NAV outperforms conventional methods that employ
constant or randomly sampled trajectory lengths. Specifically, for a fixed
sample budget, Ada-NAV achieves an 18\% increase in navigation success rate, a
20-38\% reduction in navigation path length, and a 9.32\% decrease in elevation
costs. Furthermore, we showcase the versatility of Ada-NAV by integrating it
with the Clearpath Husky robot, illustrating its applicability in complex
outdoor environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 9 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception
  Uncertainties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05840v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05840v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kshitij Sirohi, Daniel Büscher, Wolfram Burgard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The availability of a robust map-based localization system is essential for
the operation of many autonomously navigating vehicles. Since uncertainty is an
inevitable part of perception, it is beneficial for the robustness of the robot
to consider it in typical downstream tasks of navigation stacks. In particular
localization and mapping methods, which in modern systems often employ
convolutional neural networks (CNNs) for perception tasks, require proper
uncertainty estimates. In this work, we present uncertainty-aware Panoptic
Localization and Mapping (uPLAM), which employs pixel-wise uncertainty
estimates for panoptic CNNs as a bridge to fuse modern perception with
classical probabilistic localization and mapping approaches. Beyond the
perception, we introduce an uncertainty-based map aggregation technique to
create accurate panoptic maps, containing surface semantics and landmark
instances. Moreover, we provide cell-wise map uncertainties, and present a
particle filter-based localization method that employs perception
uncertainties. Extensive evaluations show that our proposed incorporation of
uncertainties leads to more accurate maps with reliable uncertainty estimates
and improved localization accuracy. Additionally, we present the Freiburg
Panoptic Driving dataset for evaluating panoptic mapping and localization
methods. We make our code and dataset available at:
\url{http://uplam.cs.uni-freiburg.de}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exposing the Unseen: Exposure Time Emulation for Offline Benchmarking of
  Vision Algorithms <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13139v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13139v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olivier Gamache, Jean-Michel Fortin, Matěj Boxan, Maxime Vaidis, François Pomerleau, Philippe Giguère
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Odometry (VO) is one of the fundamental tasks in computer vision for
robotics. However, its performance is deeply affected by High Dynamic Range
(HDR) scenes, omnipresent outdoor. While new Automatic-Exposure (AE) approaches
to mitigate this have appeared, their comparison in a reproducible manner is
problematic. This stems from the fact that the behavior of AE depends on the
environment, and it affects the image acquisition process. Consequently, AE has
traditionally only been benchmarked in an online manner, making the experiments
non-reproducible. To solve this, we propose a new methodology based on an
emulator that can generate images at any exposure time. It leverages BorealHDR,
a unique multi-exposure stereo dataset collected over 10 km, on 55 trajectories
with challenging illumination conditions. Moreover, it includes
lidar-inertial-based global maps with pose estimation for each image frame as
well as Global Navigation Satellite System (GNSS) data, for comparison. We show
that using these images acquired at different exposure times, we can emulate
realistic images, keeping a Root-Mean-Square Error (RMSE) below 1.78 % compared
to ground truth images. To demonstrate the practicality of our approach for
offline benchmarking, we compared three state-of-the-art AE algorithms on key
elements of Visual Simultaneous Localization And Mapping (VSLAM) pipeline,
against four baselines. Consequently, reproducible evaluation of AE is now
possible, speeding up the development of future approaches. Our code and
dataset are available online at this link:
https://github.com/norlab-ulaval/BorealHDR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, submitted to 2024 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigation of Enhanced Inertial Navigation Algorithms by Functional
  Iteration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.05855v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.05855v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyan Jiang, Maoran Zhu, Yuanxin Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The defects of the traditional strapdown inertial navigation algorithms
become well acknowledged and the corresponding enhanced algorithms have been
quite recently proposed trying to mitigate both theoretical and algorithmic
defects. In this paper, the analytical accuracy evaluation of both the
traditional algorithms and the enhanced algorithms is investigated, against the
true reference for the first time enabled by the functional iteration approach
having provable convergence. The analyses by the help of MATLAB Symbolic
Toolbox show that the resultant error orders of all algorithms under
investigation are consistent with those in the existing literatures, and the
enhanced attitude algorithm notably reduces error orders of the traditional
counterpart, while the impact of the enhanced velocity algorithm on error order
reduction is insignificant. Simulation results agree with analyses that the
superiority of the enhanced algorithm over the traditional one in the
body-frame attitude computation scenario diminishes significantly in the entire
inertial navigation computation scenario, while the functional iteration
approach possesses significant accuracy superiority even under sustained lowly
dynamic conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intention-Aware Planner for Robust and Safe Aerial Tracking <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08854v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08854v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiuyu Ren, Huan Yu, Jiajun Dai, Zhi Zheng, Jun Meng, Li Xu, Chao Xu, Fei Gao, Yanjun Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous target tracking with quadrotors has wide applications in many
scenarios, such as cinematographic follow-up shooting or suspect chasing.
Target motion prediction is necessary when designing the tracking planner.
However, the widely used constant velocity or constant rotation assumption can
not fully capture the dynamics of the target. The tracker may fail when the
target happens to move aggressively, such as sudden turn or deceleration. In
this paper, we propose an intention-aware planner by additionally considering
the intention of the target to enhance safety and robustness in aerial tracking
applications. Firstly, a designated intention prediction method is proposed,
which combines a user-defined potential assessment function and a state
observation function. A reachable region is generated to specifically evaluate
the turning intentions. Then we design an intention-driven hybrid A* method to
predict the future possible positions for the target. Finally, an
intention-aware optimization approach is designed to generate a
spatial-temporal optimal trajectory, allowing the tracker to perceive
unexpected situations from the target. Benchmark comparisons and real-world
experiments are conducted to validate the performance of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 10 figures, submitted to 2024 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Surfer: Progressive Reasoning with World Models for Robotic Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11335v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11335v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengzhen Ren, Kaidong Zhang, Hetao Zheng, Zixuan Li, Yuhang Wen, Fengda Zhu, Mas Ma, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Considering how to make the model accurately understand and follow natural
language instructions and perform actions consistent with world knowledge is a
key challenge in robot manipulation. This mainly includes human fuzzy
instruction reasoning and the following of physical knowledge. Therefore, the
embodied intelligence agent must have the ability to model world knowledge from
training data. However, most existing vision and language robot manipulation
methods mainly operate in less realistic simulator and language settings and
lack explicit modeling of world knowledge. To bridge this gap, we introduce a
novel and simple robot manipulation framework, called Surfer. It is based on
the world model, treats robot manipulation as a state transfer of the visual
scene, and decouples it into two parts: action and scene. Then, the
generalization ability of the model on new instructions and new scenes is
enhanced by explicit modeling of the action and scene prediction in multi-modal
information. In addition to the framework, we also built a robot manipulation
simulator that supports full physics execution based on the MuJoCo physics
engine. It can automatically generate demonstration training data and test
data, effectively reducing labor costs. To conduct a comprehensive and
systematic evaluation of the robot manipulation model in terms of language
understanding and physical execution, we also created a robotic manipulation
benchmark with progressive reasoning tasks, called SeaWave. It contains 4
levels of progressive reasoning tasks and can provide a standardized testing
platform for embedded AI agents in multi-modal environments. On average, Surfer
achieved a success rate of 54.74% on the defined four levels of manipulation
tasks, exceeding the best baseline performance of 47.64%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM3:Large Language Model-based Task and Motion Planning with Motion
  Failure Reasoning <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11552v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11552v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu Wang, Muzhi Han, Ziyuan Jiao, Zeyu Zhang, Ying Nian Wu, Song-Chun Zhu, Hangxin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional Task and Motion Planning (TAMP) approaches rely on manually
crafted interfaces connecting symbolic task planning with continuous motion
generation. These domain-specific and labor-intensive modules are limited in
addressing emerging tasks in real-world settings. Here, we present LLM^3, a
novel Large Language Model (LLM)-based TAMP framework featuring a
domain-independent interface. Specifically, we leverage the powerful reasoning
and planning capabilities of pre-trained LLMs to propose symbolic action
sequences and select continuous action parameters for motion planning.
Crucially, LLM^3 incorporates motion planning feedback through prompting,
allowing the LLM to iteratively refine its proposals by reasoning about motion
failure. Consequently, LLM^3 interfaces between task planning and motion
planning, alleviating the intricate design process of handling domain-specific
messages between them. Through a series of simulations in a box-packing domain,
we quantitatively demonstrate the effectiveness of LLM^3 in solving TAMP
problems and the efficiency in selecting action parameters. Ablation studies
underscore the significant contribution of motion failure reasoning to the
success of LLM^3. Furthermore, we conduct qualitative experiments on a physical
manipulator, demonstrating the practical applicability of our approach in
real-world settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS 2024. Codes available:
  https://github.com/AssassinWS/LLM-TAMP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pseudo-rigid body networks: learning interpretable deformable object
  dynamics from partial observations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.07975v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.07975v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shamil Mamedov, A. René Geist, Jan Swevers, Sebastian Trimpe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate prediction of deformable linear object (DLO) dynamics is challenging
if the task at hand requires a human-interpretable yet computationally fast
model. In this work, we draw inspiration from the pseudo-rigid body method
(PRB) and model a DLO as a serial chain of rigid bodies whose internal state is
unrolled through time by a dynamics network. This dynamics network is trained
jointly with a physics-informed encoder which maps observed motion variables to
the DLO's hidden state. To encourage that the state acquires a physically
meaningful representation, we leverage the forward kinematics of the PRB model
as decoder. We demonstrate in robot experiments that the proposed DLO dynamics
model provides physically interpretable predictions from partial observations
while being on par with black-box models regarding prediction accuracy. The
project code is available at: http://tinyurl.com/prb-networks
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Camera Height Doesn't Change: Unsupervised Training for Metric Monocular
  Road-Scene Depth Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04530v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04530v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Genki Kinoshita, Ko Nishino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a novel training method for making any monocular
depth network learn absolute scale and estimate metric road-scene depth just
from regular training data, i.e., driving videos. We refer to this training
framework as StableCamH. The key idea is to leverage cars found on the road as
sources of scale supervision but to incorporate them in the training robustly.
StableCamH detects and estimates the sizes of cars in the frame and aggregates
scale information extracted from them into a camera height estimate whose
consistency across the entire video sequence is enforced as scale supervision.
This realizes robust unsupervised training of any, otherwise scale-oblivious,
monocular depth network to become not only scale-aware but also metric-accurate
without the need for auxiliary sensors and extra supervision. Extensive
experiments on the KITTI and Cityscapes datasets show the effectiveness of
StableCamH and its state-of-the-art accuracy compared with related methods. We
also show that StableCamH enables training on mixed datasets of different
camera heights, which leads to larger-scale training and thus higher
generalization. Metric depth reconstruction is essential in any road-scene
visual modeling, and StableCamH democratizes its deployment by establishing the
means to train any model as a metric depth estimator.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NDT-Map-Code: A 3D global descriptor for real-time loop closure
  detection in lidar SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.08221v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.08221v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lizhou Liao, Wenlei Yan, Li Sun, Xinhui Bai, Zhenxing You, Hongyuan Yuan, Chunyun Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Loop-closure detection, also known as place recognition, aiming to identify
previously visited locations, is an essential component of a SLAM system.
Existing research on lidar-based loop closure heavily relies on dense point
cloud and 360 FOV lidars. This paper proposes an out-of-the-box NDT (Normal
Distribution Transform) based global descriptor, NDT-Map-Code, designed for
both on-road driving and underground valet parking scenarios. NDT-Map-Code can
be directly extracted from the NDT map without the need for a dense point
cloud, resulting in excellent scalability and low maintenance cost. The NDT
representation is leveraged to identify representative patterns, which are
further encoded according to their spatial location (bearing, range, and
height). Experimental results on the NIO underground parking lot dataset and
the KITTI dataset demonstrate that our method achieves significantly better
performance compared to the state-of-the-art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision-State Fusion: Improving Deep Neural Networks for Autonomous
  Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.06112v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.06112v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elia Cereda, Stefano Bonato, Mirko Nava, Alessandro Giusti, Daniele Palossi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-based deep learning perception fulfills a paramount role in robotics,
facilitating solutions to many challenging scenarios, such as acrobatic
maneuvers of autonomous unmanned aerial vehicles (UAVs) and robot-assisted
high-precision surgery. Control-oriented end-to-end perception approaches,
which directly output control variables for the robot, commonly take advantage
of the robot's state estimation as an auxiliary input. When intermediate
outputs are estimated and fed to a lower-level controller, i.e. mediated
approaches, the robot's state is commonly used as an input only for egocentric
tasks, which estimate physical properties of the robot itself. In this work, we
propose to apply a similar approach for the first time -- to the best of our
knowledge -- to non-egocentric mediated tasks, where the estimated outputs
refer to an external subject. We prove how our general methodology improves the
regression performance of deep convolutional neural networks (CNNs) on a broad
class of non-egocentric 3D pose estimation problems, with minimal computational
cost. By analyzing three highly-different use cases, spanning from grasping
with a robotic arm to following a human subject with a pocket-sized UAV, our
results consistently improve the R\textsuperscript{2} regression metric, up to
+0.51, compared to their stateless baselines. Finally, we validate the in-field
performance of a closed-loop autonomous cm-scale UAV on the human pose
estimation task. Our results show a significant reduction, i.e., 24\% on
average, on the mean absolute error of our stateful CNN, compared to a
State-of-the-Art stateless counterpart.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for publication in the Journal of
  Intelligent & Robotic Systems. \copyright 2024 Springer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributed Pose-graph Optimization with Multi-level Partitioning for
  Collaborative SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01657v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01657v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cunhao Li, Peng Yi, Guanghui Guo, Yiguang Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The back-end module of Distributed Collaborative Simultaneous Localization
and Mapping (DCSLAM) requires solving a nonlinear Pose Graph Optimization (PGO)
under a distributed setting, also known as SE(d)-synchronization. Most existing
distributed graph optimization algorithms employ a simple sequential
partitioning scheme, which may result in unbalanced subgraph dimensions due to
the different geographic locations of each robot, and hence imposes extra
communication load. Moreover, the performance of current Riemannian
optimization algorithms can be further accelerated. In this letter, we propose
a novel distributed pose graph optimization algorithm combining multi-level
partitioning with an accelerated Riemannian optimization method. Firstly, we
employ the multi-level graph partitioning algorithm to preprocess the naive
pose graph to formulate a balanced optimization problem. In addition, inspired
by the accelerated coordinate descent method, we devise an Improved Riemannian
Block Coordinate Descent (IRBCD) algorithm and the critical point obtained is
globally optimal. Finally, we evaluate the effects of four common graph
partitioning approaches on the correlation of the inter-subgraphs, and discover
that the Highest scheme has the best partitioning performance. Also, we
implement simulations to quantitatively demonstrate that our proposed algorithm
outperforms the state-of-the-art distributed pose graph optimization protocols.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Results and Lessons Learned from Autonomous Driving Transportation
  Services in Airfield, Crowded Indoor, and Urban Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01233v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01233v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Doosan Baek, Sanghyun Kim, Seung-Woo Seo, Sang-Hyun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous vehicles have been actively investigated over the past few
decades. Several recent works show the potential of autonomous vehicles in
urban environments with impressive experimental results. However, these works
note that autonomous vehicles are still occasionally inferior to expert drivers
in complex scenarios. Furthermore, they do not focus on the possibilities of
autonomous driving transportation services in other areas beyond urban
environments. This paper presents the research results and lessons learned from
autonomous driving transportation services in airfield, crowded indoor, and
urban environments. We discuss how we address several unique challenges in
these diverse environments. We also offer an overview of remaining challenges
that have not received much attention but must be addressed. This paper aims to
share our unique experience to support researchers who are interested in
exploring autonomous driving transportation services in various real-world
environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LingoQA: Video Question Answering for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ana-Maria Marcu, Long Chen, Jan Hünermann, Alice Karnsund, Benoit Hanotte, Prajwal Chidananda, Saurabh Nair, Vijay Badrinarayanan, Alex Kendall, Jamie Shotton, Elahe Arani, Oleg Sinavski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving has long faced a challenge with public acceptance due to
the lack of explainability in the decision-making process. Video
question-answering (QA) in natural language provides the opportunity for
bridging this gap. Nonetheless, evaluating the performance of Video QA models
has proved particularly tough due to the absence of comprehensive benchmarks.
To fill this gap, we introduce LingoQA, a benchmark specifically for autonomous
driving Video QA. The LingoQA trainable metric demonstrates a 0.95 Spearman
correlation coefficient with human evaluations. We introduce a Video QA dataset
of central London consisting of 419k samples that we release with the paper. We
establish a baseline vision-language model and run extensive ablation studies
to understand its performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Benchmark and dataset are available at
  https://github.com/wayveai/LingoQA/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Designing Library of Skill-Agents for Hardware-Level Reusability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02316v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02316v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Takamatsu, Daichi Saito, Katsushi Ikeuchi, Atsushi Kanehira, Kazuhiro Sasabuchi, Naoki Wake
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To use new robot hardware in a new environment, it is necessary to develop a
control program tailored to that specific robot in that environment.
Considering the reusability of software among robots is crucial to minimize the
effort involved in this process and maximize software reuse across different
robots in different environments. This paper proposes a method to remedy this
process by considering hardware-level reusability, using
Learning-from-observation (LfO) paradigm with a pre-designed skill-agent
library. The LfO framework represents the required actions in
hardware-independent representations, referred to as task models, from
observing human demonstrations, capturing the necessary parameters for the
interaction between the environment and the robot. When executing the desired
actions from the task models, a set of skill agents is employed to convert the
representations into robot commands. This paper focuses on the latter part of
the LfO framework, utilizing the set to generate robot actions from the task
models, and explores a hardware-independent design approach for these skill
agents. These skill agents are described in a hardware-independent manner,
considering the relative relationship between the robot's hand position and the
environment. As a result, it is possible to execute these actions on robots
with different hardware configurations by simply swapping the inverse
kinematics solver. This paper, first, defines a necessary and sufficient
skill-agent set corresponding to cover all possible actions, and considers the
design principles for these skill agents in the library. We provide concrete
examples of such skill agents and demonstrate the practicality of using these
skill agents by showing that the same representations can be executed on two
different robots, Nextage and Fetch, using the proposed skill-agents set.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Working Backwards: Learning to Place by Picking <span class="chip">IROS'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02352v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02352v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oliver Limoyo, Abhisek Konar, Trevor Ablett, Jonathan Kelly, Francois R. Hogan, Gregory Dudek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present placing via picking (PvP), a method to autonomously collect
real-world demonstrations for a family of placing tasks in which objects must
be manipulated to specific contact-constrained locations. With PvP, we approach
the collection of robotic object placement demonstrations by reversing the
grasping process and exploiting the inherent symmetry of the pick and place
problems. Specifically, we obtain placing demonstrations from a set of grasp
sequences of objects initially located at their target placement locations. Our
system can collect hundreds of demonstrations in contact-constrained
environments without human intervention by combining two modules: tactile
regrasping and compliant control for grasps. We train a policy directly from
visual observations through behavioral cloning, using the
autonomously-collected demonstrations. By doing so, the policy can generalize
to object placement scenarios outside of the training environment without
privileged information (e.g., placing a plate picked up from a table). We
validate our approach in home robotic scenarios that include dishwasher loading
and table setting. Our approach yields robotic placing policies that outperform
policies trained with kinesthetic teaching, both in terms of performance and
data efficiency, while requiring no human supervision.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the IEEE/RSJ International Conference on Intelligent
  Robotics and Systems (IROS'24), Abu Dhabi, UAE, Oct 14-18, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PhotoBot: Reference-Guided Interactive Photography via Natural Language <span class="chip">IROS'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11061v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11061v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oliver Limoyo, Jimmy Li, Dmitriy Rivkin, Jonathan Kelly, Gregory Dudek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce PhotoBot, a framework for fully automated photo acquisition
based on an interplay between high-level human language guidance and a robot
photographer. We propose to communicate photography suggestions to the user via
reference images that are selected from a curated gallery. We leverage a visual
language model (VLM) and an object detector to characterize the reference
images via textual descriptions and then use a large language model (LLM) to
retrieve relevant reference images based on a user's language query through
text-based reasoning. To correspond the reference image and the observed scene,
we exploit pre-trained features from a vision transformer capable of capturing
semantic similarity across marked appearance variations. Using these features,
we compute pose adjustments for an RGB-D camera by solving a
perspective-n-point (PnP) problem. We demonstrate our approach using a
manipulator equipped with a wrist camera. Our user studies show that photos
taken by PhotoBot are often more aesthetically pleasing than those taken by
users themselves, as measured by human feedback. We also show that PhotoBot can
generalize to other reference sources such as paintings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the IEEE/RSJ International Conference on Intelligent
  Robotics and Systems (IROS'24), Abu Dhabi, UAE, Oct 14-18, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Social Robots for Sleep Health: A Scoping <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04169v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04169v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor Nikhil Antony, Mengchi Li, Shu-Han Lin, Junxin Li, Chien-Ming Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Poor sleep health is an increasingly concerning public healthcare crisis,
especially when coupled with a dwindling number of health professionals
qualified to combat it. However, there is a growing body of scientific
literature on the use of digital technologies in supporting and sustaining
individuals' healthy sleep habits. Social robots are a relatively recent
technology that has been used to facilitate health care interventions and may
have potential in improving sleep health outcomes, as well. Social robots'
unique characteristics -- such as anthropomorphic physical embodiment or
effective communication methods -- help to engage users and motivate them to
comply with specific interventions, thus improving the interventions' outcomes.
This scoping review aims to evaluate current scientific evidence for employing
social robots in sleep health interventions, identify critical research gaps,
and suggest future directions for developing and using social robots to improve
people's sleep health. Our analysis of the reviewed studies found them limited
due to a singular focus on the older adult population, use of small sample
sizes, limited intervention durations, and other compounding factors.
Nevertheless, the reviewed studies reported several positive outcomes,
highlighting the potential social robots hold in this field. Although our
review found limited clinical evidence for the efficacy of social robots as
purveyors of sleep health interventions, it did elucidate the potential for a
successful future in this domain if current limitations are addressed and more
research is conducted.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LGMCTS: Language-Guided Monte-Carlo Tree Search for Executable Semantic
  Object Rearrangement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15821v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15821v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan Chang, Kai Gao, Kowndinya Boyalakuntla, Alex Lee, Baichuan Huang, Harish Udhaya Kumar, Jinjin Yu, Abdeslam Boularias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel approach to the executable semantic object rearrangement
problem. In this challenge, a robot seeks to create an actionable plan that
rearranges objects within a scene according to a pattern dictated by a natural
language description. Unlike existing methods such as StructFormer and
StructDiffusion, which tackle the issue in two steps by first generating poses
and then leveraging a task planner for action plan formulation, our method
concurrently addresses pose generation and action planning. We achieve this
integration using a Language-Guided Monte-Carlo Tree Search (LGMCTS).
Quantitative evaluations are provided on two simulation datasets, and
complemented by qualitative tests with a real robot.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code and supplementary materials are accessible at
  https://github.com/changhaonan/LG-MCTS</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-19T00:00:00Z">2024-03-19</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">82</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WHAC: World-grounded Humans and Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanqi Yin, Zhongang Cai, Ruisi Wang, Fanzhou Wang, Chen Wei, Haiyi Mei, Weiye Xiao, Zhitao Yang, Qingping Sun, Atsushi Yamashita, Ziwei Liu, Lei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating human and camera trajectories with accurate scale in the world
coordinate system from a monocular video is a highly desirable yet challenging
and ill-posed problem. In this study, we aim to recover expressive parametric
human models (i.e., SMPL-X) and corresponding camera poses jointly, by
leveraging the synergy between three critical players: the world, the human,
and the camera. Our approach is founded on two key observations. Firstly,
camera-frame SMPL-X estimation methods readily recover absolute human depth.
Secondly, human motions inherently provide absolute spatial cues. By
integrating these insights, we introduce a novel framework, referred to as
WHAC, to facilitate world-grounded expressive human pose and shape estimation
(EHPS) alongside camera pose estimation, without relying on traditional
optimization techniques. Additionally, we present a new synthetic dataset,
WHAC-A-Mole, which includes accurately annotated humans and cameras, and
features diverse interactive human motions as well as realistic camera
trajectories. Extensive experiments on both standard and newly established
benchmarks highlight the superiority and efficacy of our framework. We will
make the code and dataset publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Homepage: https://wqyin.github.io/projects/WHAC/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DROID: A Large-Scale In-The-Wild Robot Manipulation <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12945v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12945v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, Peter David Fagan, Joey Hejna, Masha Itkina, Marion Lepert, Yecheng Jason Ma, Patrick Tree Miller, Jimmy Wu, Suneel Belkhale, Shivin Dass, Huy Ha, Arhan Jain, Abraham Lee, Youngwoon Lee, Marius Memmel, Sungjae Park, Ilija Radosavovic, Kaiyuan Wang, Albert Zhan, Kevin Black, Cheng Chi, Kyle Beltran Hatch, Shan Lin, Jingpei Lu, Jean Mercat, Abdul Rehman, Pannag R Sanketi, Archit Sharma, Cody Simpson, Quan Vuong, Homer Rich Walke, Blake Wulfe, Ted Xiao, Jonathan Heewon Yang, Arefeh Yavary, Tony Z. Zhao, Christopher Agia, Rohan Baijal, Mateo Guaman Castro, Daphne Chen, Qiuyu Chen, Trinity Chung, Jaimyn Drake, Ethan Paul Foster, Jensen Gao, David Antonio Herrera, Minho Heo, Kyle Hsu, Jiaheng Hu, Donovon Jackson, Charlotte Le, Yunshuang Li, Kevin Lin, Roy Lin, Zehan Ma, Abhiram Maddukuri, Suvir Mirchandani, Daniel Morton, Tony Nguyen, Abigail O'Neill, Rosario Scalise, Derick Seale, Victor Son, Stephen Tian, Emi Tran, Andrew E. Wang, Yilin Wu, Annie Xie, Jingyun Yang, Patrick Yin, Yunchu Zhang, Osbert Bastani, Glen Berseth, Jeannette Bohg, Ken Goldberg, Abhinav Gupta, Abhishek Gupta, Dinesh Jayaraman, Joseph J Lim, Jitendra Malik, Roberto Martín-Martín, Subramanian Ramamoorthy, Dorsa Sadigh, Shuran Song, Jiajun Wu, Michael C. Yip, Yuke Zhu, Thomas Kollar, Sergey Levine, Chelsea Finn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The creation of large, diverse, high-quality robot manipulation datasets is
an important stepping stone on the path toward more capable and robust robotic
manipulation policies. However, creating such datasets is challenging:
collecting robot manipulation data in diverse environments poses logistical and
safety challenges and requires substantial investments in hardware and human
labour. As a result, even the most general robot manipulation policies today
are mostly trained on data collected in a small number of environments with
limited scene and task diversity. In this work, we introduce DROID (Distributed
Robot Interaction Dataset), a diverse robot manipulation dataset with 76k
demonstration trajectories or 350 hours of interaction data, collected across
564 scenes and 84 tasks by 50 data collectors in North America, Asia, and
Europe over the course of 12 months. We demonstrate that training with DROID
leads to policies with higher performance and improved generalization ability.
We open source the full dataset, policy learning code, and a detailed guide for
reproducing our robot hardware setup.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://droid-dataset.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vid2Robot: End-to-end Video-conditioned Policy Learning with
  Cross-Attention <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12943v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12943v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vidhi Jain, Maria Attarian, Nikhil J Joshi, Ayzaan Wahid, Danny Driess, Quan Vuong, Pannag R Sanketi, Pierre Sermanet, Stefan Welker, Christine Chan, Igor Gilitschenski, Yonatan Bisk, Debidatta Dwibedi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large-scale robotic systems typically rely on textual instructions for
tasks, this work explores a different approach: can robots infer the task
directly from observing humans? This shift necessitates the robot's ability to
decode human intent and translate it into executable actions within its
physical constraints and environment. We introduce Vid2Robot, a novel
end-to-end video-based learning framework for robots. Given a video
demonstration of a manipulation task and current visual observations, Vid2Robot
directly produces robot actions. This is achieved through a unified
representation model trained on a large dataset of human video and robot
trajectory. The model leverages cross-attention mechanisms to fuse prompt video
features to the robot's current state and generate appropriate actions that
mimic the observed task. To further improve policy performance, we propose
auxiliary contrastive losses that enhance the alignment between human and robot
video representations. We evaluate Vid2Robot on real-world robots,
demonstrating a 20% improvement in performance compared to other
video-conditioned policies when using human demonstration videos. Additionally,
our model exhibits emergent capabilities, such as successfully transferring
observed motions from one object to another, and long-horizon composition, thus
showcasing its potential for real-world applications. Project website:
vid2robot.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Robot learning: Imitation Learning, Robot Perception, Sensing &
  Vision, Grasping & Manipulation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic Layering in Room Segmentation via LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12920v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12920v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taehyeon Kim, Byung-Cheol Min
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce Semantic Layering in Room Segmentation via LLMs
(SeLRoS), an advanced method for semantic room segmentation by integrating
Large Language Models (LLMs) with traditional 2D map-based segmentation. Unlike
previous approaches that solely focus on the geometric segmentation of indoor
environments, our work enriches segmented maps with semantic data, including
object identification and spatial relationships, to enhance robotic navigation.
By leveraging LLMs, we provide a novel framework that interprets and organizes
complex information about each segmented area, thereby improving the accuracy
and contextual relevance of room segmentation. Furthermore, SeLRoS overcomes
the limitations of existing algorithms by using a semantic evaluation method to
accurately distinguish true room divisions from those erroneously generated by
furniture and segmentation inaccuracies. The effectiveness of SeLRoS is
verified through its application across 30 different 3D environments. Source
code and experiment videos for this work are available at:
https://sites.google.com/view/selros.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Yell At Your Robot: Improving On-the-Fly from Language Corrections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12910v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12910v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucy Xiaoyang Shi, Zheyuan Hu, Tony Z. Zhao, Archit Sharma, Karl Pertsch, Jianlan Luo, Sergey Levine, Chelsea Finn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hierarchical policies that combine language and low-level control have been
shown to perform impressively long-horizon robotic tasks, by leveraging either
zero-shot high-level planners like pretrained language and vision-language
models (LLMs/VLMs) or models trained on annotated robotic demonstrations.
However, for complex and dexterous skills, attaining high success rates on
long-horizon tasks still represents a major challenge -- the longer the task
is, the more likely it is that some stage will fail. Can humans help the robot
to continuously improve its long-horizon task performance through intuitive and
natural feedback? In this paper, we make the following observation: high-level
policies that index into sufficiently rich and expressive low-level
language-conditioned skills can be readily supervised with human feedback in
the form of language corrections. We show that even fine-grained corrections,
such as small movements ("move a bit to the left"), can be effectively
incorporated into high-level policies, and that such corrections can be readily
obtained from humans observing the robot and making occasional suggestions.
This framework enables robots not only to rapidly adapt to real-time language
feedback, but also incorporate this feedback into an iterative training scheme
that improves the high-level policy's ability to correct errors in both
low-level execution and high-level decision-making purely from verbal feedback.
Our evaluation on real hardware shows that this leads to significant
performance improvement in long-horizon, dexterous manipulation tasks without
the need for any additional teleoperation. Videos and code are available at
https://yay-robot.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://yay-robot.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uoc luong kenh truyen trong he thong da robot su dung SDR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12892v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12892v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Do Hai Son, Nguyen Huu Hung, Pham Duy Hung, Tran Thi Thuy Quynh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study focuses on developing an experimental system for estimating
communication channels in a multi-robot mobile system using software-defined
radio (SDR) devices. The system consists of two mobile robots programmed for
two scenarios: one where the robot remains stationary and another where it
follows a predefined trajectory. Communication within the system is conducted
through orthogonal frequency-division multiplexing (OFDM) to mitigate the
effects of multipath propagation in indoor environments. The system's
performance is evaluated using the bit error rate (BER). Connections related to
robot motion and communication are implemented using Raspberry Pi 3 and BladeRF
x115, respectively. The least squares (LS) technique is employed to estimate
the channel with a bit error rate of approximately 10^(-2).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>in Vietnamese language</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Visual Imitation Learning for Robotic Assisted Feeding Across
  Varied Bowl Configurations and Food Types 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Liu, Amisha Bhaskar, Pratap Tokekar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we introduce a novel visual imitation network with a spatial
attention module for robotic assisted feeding (RAF). The goal is to acquire
(i.e., scoop) food items from a bowl. However, achieving robust and adaptive
food manipulation is particularly challenging. To deal with this, we propose a
framework that integrates visual perception with imitation learning to enable
the robot to handle diverse scenarios during scooping. Our approach, named AVIL
(adaptive visual imitation learning), exhibits adaptability and robustness
across different bowl configurations in terms of material, size, and position,
as well as diverse food types including granular, semi-solid, and liquid, even
in the presence of distractors. We validate the effectiveness of our approach
by conducting experiments on a real robot. We also compare its performance with
a baseline. The results demonstrate improvement over the baseline across all
scenarios, with an enhancement of up to 2.5 times in terms of a success metric.
Notably, our model, trained solely on data from a transparent glass bowl
containing granular cereals, showcases generalization ability when tested
zero-shot on other bowl configurations with different types of food.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LAVA: Long-horizon Visual Action based Food Acquisition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12876v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12876v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amisha Bhaskar, Rui Liu, Vishnu D. Sharma, Guangyao Shi, Pratap Tokekar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic Assisted Feeding (RAF) addresses the fundamental need for individuals
with mobility impairments to regain autonomy in feeding themselves. The goal of
RAF is to use a robot arm to acquire and transfer food to individuals from the
table. Existing RAF methods primarily focus on solid foods, leaving a gap in
manipulation strategies for semi-solid and deformable foods. This study
introduces Long-horizon Visual Action (LAVA) based food acquisition of liquid,
semisolid, and deformable foods. Long-horizon refers to the goal of "clearing
the bowl" by sequentially acquiring the food from the bowl. LAVA employs a
hierarchical policy for long-horizon food acquisition tasks. The framework uses
high-level policy to determine primitives by leveraging ScoopNet. At the
mid-level, LAVA finds parameters for primitives using vision. To carry out
sequential plans in the real world, LAVA delegates action execution which is
driven by Low-level policy that uses parameters received from mid-level policy
and behavior cloning ensuring precise trajectory execution. We validate our
approach on complex real-world acquisition trials involving granular, liquid,
semisolid, and deformable food types along with fruit chunks and soup
acquisition. Across 46 bowls, LAVA acquires much more efficiently than
baselines with a success rate of 89 +/- 4% and generalizes across realistic
plate variations such as different positions, varieties, and amount of food in
the bowl. Code, datasets, videos, and supplementary materials can be found on
our website.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PE-Planner: A Performance-Enhanced Quadrotor Motion Planner for
  Autonomous Flight in Complex and Dynamic Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12865v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12865v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxin Qiu, Qingchen Liu, Jiahu Qin, Dewang Cheng, Yawei Tian, Qichao Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The role of a motion planner is pivotal in quadrotor applications, yet
existing methods often struggle to adapt to complex environments, limiting
their ability to achieve fast, safe, and robust flight. In this letter, we
introduce a performance-enhanced quadrotor motion planner designed for
autonomous flight in complex environments including dense obstacles, dynamic
obstacles, and unknown disturbances. The global planner generates an initial
trajectory through kinodynamic path searching and refines it using B-spline
trajectory optimization. Subsequently, the local planner takes into account the
quadrotor dynamics, estimated disturbance, global reference trajectory, control
cost, time cost, and safety constraints to generate real-time control inputs,
utilizing the framework of model predictive contouring control. Both
simulations and real-world experiments corroborate the heightened robustness,
safety, and speed of the proposed motion planner. Additionally, our motion
planner achieves flights at more than 6.8 m/s in a challenging and complex
racing scenario.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ D-Cubed: Latent Diffusion Trajectory Optimisation for Dexterous
  Deformable Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12861v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12861v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Yamada, Shaohong Zhong, Jack Collins, Ingmar Posner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mastering dexterous robotic manipulation of deformable objects is vital for
overcoming the limitations of parallel grippers in real-world applications.
Current trajectory optimisation approaches often struggle to solve such tasks
due to the large search space and the limited task information available from a
cost function. In this work, we propose D-Cubed, a novel trajectory
optimisation method using a latent diffusion model (LDM) trained from a
task-agnostic play dataset to solve dexterous deformable object manipulation
tasks. D-Cubed learns a skill-latent space that encodes short-horizon actions
in the play dataset using a VAE and trains a LDM to compose the skill latents
into a skill trajectory, representing a long-horizon action trajectory in the
dataset. To optimise a trajectory for a target task, we introduce a novel
gradient-free guided sampling method that employs the Cross-Entropy method
within the reverse diffusion process. In particular, D-Cubed samples a small
number of noisy skill trajectories using the LDM for exploration and evaluates
the trajectories in simulation. Then, D-Cubed selects the trajectory with the
lowest cost for the subsequent reverse process. This effectively explores
promising solution areas and optimises the sampled trajectories towards a
target task throughout the reverse diffusion process. Through empirical
evaluation on a public benchmark of dexterous deformable object manipulation
tasks, we demonstrate that D-Cubed outperforms traditional trajectory
optimisation and competitive baseline approaches by a significant margin. We
further demonstrate that trajectories found by D-Cubed readily transfer to a
real-world LEAP hand on a folding task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://applied-ai-lab.github.io/D-cubed/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Equivariant Ensembles and Regularization for Reinforcement Learning in
  Map-based Path Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mirco Theile, Hongpeng Cao, Marco Caccamo, Alberto L. Sangiovanni-Vincentelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In reinforcement learning (RL), exploiting environmental symmetries can
significantly enhance efficiency, robustness, and performance. However,
ensuring that the deep RL policy and value networks are respectively
equivariant and invariant to exploit these symmetries is a substantial
challenge. Related works try to design networks that are equivariant and
invariant by construction, limiting them to a very restricted library of
components, which in turn hampers the expressiveness of the networks. This
paper proposes a method to construct equivariant policies and invariant value
functions without specialized neural network components, which we term
equivariant ensembles. We further add a regularization term for adding
inductive bias during training. In a map-based path planning case study, we
show how equivariant ensembles and regularization benefit sample efficiency and
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted for possible publication. A video can be found here:
  https://youtu.be/L6NOdvU7n7s</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RASP: A Drone-based Reconfigurable Actuation and Sensing Platform
  Towards Ambient Intelligent Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghui Zhao, Junxi Xia, Kaiyuan Hou, Yanchen Liu, Stephen Xia, Xiaofan Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Realizing consumer-grade drones that are as useful as robot vacuums
throughout our homes or personal smartphones in our daily lives requires drones
to sense, actuate, and respond to general scenarios that may arise. Towards
this vision, we propose RASP, a modular and reconfigurable sensing and
actuation platform that allows drones to autonomously swap onboard sensors and
actuators in only 25 seconds, allowing a single drone to quickly adapt to a
diverse range of tasks. RASP consists of a mechanical layer to physically swap
sensor modules, an electrical layer to maintain power and communication lines
to the sensor/actuator, and a software layer to maintain a common interface
between the drone and any sensor module in our platform. Leveraging recent
advances in large language and visual language models, we further introduce the
architecture, implementation, and real-world deployments of a personal
assistant system utilizing RASP. We demonstrate that RASP can enable a diverse
range of useful tasks in home, office, lab, and other indoor settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Interplay Between Symmetries and Impact Effects on Hybrid Mechanical
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12842v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12842v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Clark, Leonardo Colombo, Anthony Bloch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hybrid systems are dynamical systems with continuous-time and discrete-time
components in their dynamics. When hybrid systems are defined on a principal
bundle we are able to define two classes of impacts for the discrete-time
transition of the dynamics: interior impacts and exterior impacts. In this
paper we define hybrid systems on principal bundles, study the underlying
geometry on the switching surface where impacts occur and we find conditions
for which both exterior and interior impacts are preserved by the mechanical
connection induced in the principal bundle.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages. To be presented at a conference. Comments welcome</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Opti-Acoustic Semantic SLAM with Unknown Objects in Underwater
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kurran Singh, Jungseok Hong, Nicholas R. Rypkema, John J. Leonard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advances in semantic Simultaneous Localization and Mapping
(SLAM) for terrestrial and aerial applications, underwater semantic SLAM
remains an open and largely unaddressed research problem due to the unique
sensing modalities and the object classes found underwater. This paper presents
an object-based semantic SLAM method for underwater environments that can
identify, localize, classify, and map a wide variety of marine objects without
a priori knowledge of the object classes present in the scene. The method
performs unsupervised object segmentation and object-level feature aggregation,
and then uses opti-acoustic sensor fusion for object localization.
Probabilistic data association is used to determine observation to landmark
correspondences. Given such correspondences, the method then jointly optimizes
landmark and vehicle position estimates. Indoor and outdoor underwater datasets
with a wide variety of objects and challenging acoustic and lighting conditions
are collected for evaluation and made publicly available. Quantitative and
qualitative results show the proposed method achieves reduced trajectory error
compared to baseline methods, and is able to obtain comparable map accuracy to
a baseline closed-set method that requires hand-labeled data of all objects in
the scene.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AnySkill: Learning Open-Vocabulary Physical Skill for Interactive Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12835v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12835v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jieming Cui, Tengyu Liu, Nian Liu, Yaodong Yang, Yixin Zhu, Siyuan Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional approaches in physics-based motion generation, centered around
imitation learning and reward shaping, often struggle to adapt to new
scenarios. To tackle this limitation, we propose AnySkill, a novel hierarchical
method that learns physically plausible interactions following open-vocabulary
instructions. Our approach begins by developing a set of atomic actions via a
low-level controller trained via imitation learning. Upon receiving an
open-vocabulary textual instruction, AnySkill employs a high-level policy that
selects and integrates these atomic actions to maximize the CLIP similarity
between the agent's rendered images and the text. An important feature of our
method is the use of image-based rewards for the high-level policy, which
allows the agent to learn interactions with objects without manual reward
engineering. We demonstrate AnySkill's capability to generate realistic and
natural motion sequences in response to unseen instructions of varying lengths,
marking it the first method capable of open-vocabulary physical skill learning
for interactive humanoid agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Introducing Combi-Stations in Robotic Mobile Fulfilment Systems: A
  Queueing-Theory-Based Efficiency Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12798v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12798v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Xie, Sonja Otten
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of digital commerce, the surge in online shopping and the
expectation for rapid delivery have placed unprecedented demands on warehouse
operations. The traditional method of order fulfilment, where human order
pickers traverse large storage areas to pick items, has become a bottleneck,
consuming valuable time and resources. Robotic Mobile Fulfilment Systems (RMFS)
offer a solution by using robots to transport storage racks directly to
human-operated picking stations, eliminating the need for pickers to travel.
This paper introduces combi-stations, a novel type of station that enables both
item picking and replenishment, as opposed to traditional separate stations. We
analyse the efficiency of combi-stations using queueing theory and demonstrate
their potential to streamline warehouse operations. Our results suggest that
combi-stations can reduce the number of robots required for stability and
significantly reduce order turnover time, indicating a promising direction for
future warehouse automation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures. arXiv admin note: text overlap with
  arXiv:1912.01782</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BTGenBot: Behavior Tree Generation for Robotic Tasks with Lightweight
  LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Andrea Izzo, Gianluca Bardaro, Matteo Matteucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach to generating behavior trees for robots
using lightweight large language models (LLMs) with a maximum of 7 billion
parameters. The study demonstrates that it is possible to achieve satisfying
results with compact LLMs when fine-tuned on a specific dataset. The key
contributions of this research include the creation of a fine-tuning dataset
based on existing behavior trees using GPT-3.5 and a comprehensive comparison
of multiple LLMs (namely llama2, llama-chat, and code-llama) across nine
distinct tasks. To be thorough, we evaluated the generated behavior trees using
static syntactical analysis, a validation system, a simulated environment, and
a real robot. Furthermore, this work opens the possibility of deploying such
solutions directly on the robot, enhancing its practical applicability.
Findings from this study demonstrate the potential of LLMs with a limited
number of parameters in generating effective and efficient robot behaviors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Some geometric and topological data-driven methods in robot motion path
  planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boris Goldfarb
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion path planning is an intrinsically geometric problem which is central
for design of robot systems. Since the early years of AI, robotics together
with computer vision have been the areas of computer science that drove its
development. Many questions that arise, such as existence, optimality, and
diversity of motion paths in the configuration space that describes feasible
robot configurations, are of topological nature. The recent advances in
topological data analysis and related metric geometry, topology and
combinatorics have provided new tools to address these engineering tasks. We
will survey some questions, issues, recent work and promising directions in
data-driven geometric and topological methods with some emphasis on the use of
discrete Morse theory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 6 figures, to appear in a book project on Topology,
  Geometry and AI in the EMS Series in Industrial and Applied Mathematics,
  edited by Michael Farber and Jes\'us Gonz\'alez</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shared Autonomy via Variable Impedance Control and Virtual Potential
  Fields for Encoding Human Demonstration <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12720v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12720v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shail Jadav, Johannes Heidersberger, Christian Ott, Dongheui Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article introduces a framework for complex human-robot collaboration
tasks, such as the co-manufacturing of furniture. For these tasks, it is
essential to encode tasks from human demonstration and reproduce these skills
in a compliant and safe manner. Therefore, two key components are addressed in
this work: motion generation and shared autonomy. We propose a motion generator
based on a time-invariant potential field, capable of encoding wrench profiles,
complex and closed-loop trajectories, and additionally incorporates obstacle
avoidance. Additionally, the paper addresses shared autonomy (SA) which enables
synergetic collaboration between human operators and robots by dynamically
allocating authority. Variable impedance control (VIC) and force control are
employed, where impedance and wrench are adapted based on the human-robot
autonomy factor derived from interaction forces. System passivity is ensured by
an energy-tank based task passivation strategy. The framework's efficacy is
validated through simulations and an experimental study employing a Franka
Emika Research 3 robot.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WaterVG: Waterway Visual Grounding based on Text-Guided Vision and
  mmWave Radar 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runwei Guan, Liye Jia, Fengyufan Yang, Shanliang Yao, Erick Purwanto, Xiaohui Zhu, Eng Gee Lim, Jeremy Smith, Ka Lok Man, Yutao Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The perception of waterways based on human intent holds significant
importance for autonomous navigation and operations of Unmanned Surface
Vehicles (USVs) in water environments. Inspired by visual grounding, in this
paper, we introduce WaterVG, the first visual grounding dataset designed for
USV-based waterway perception based on human intention prompts. WaterVG
encompasses prompts describing multiple targets, with annotations at the
instance level including bounding boxes and masks. Notably, WaterVG includes
11,568 samples with 34,950 referred targets, which integrates both visual and
radar characteristics captured by monocular camera and millimeter-wave (mmWave)
radar, enabling a finer granularity of text prompts. Furthermore, we propose a
novel multi-modal visual grounding model, Potamoi, which is a multi-modal and
multi-task model based on the one-stage paradigm with a designed Phased
Heterogeneous Modality Fusion (PHMF) structure, including Adaptive Radar
Weighting (ARW) and Multi-Head Slim Cross Attention (MHSCA). In specific, MHSCA
is a low-cost and efficient fusion module with a remarkably small parameter
count and FLOPs, elegantly aligning and fusing scenario context information
captured by two sensors with linguistic features, which can effectively address
tasks of referring expression comprehension and segmentation based on
fine-grained prompts. Comprehensive experiments and evaluations have been
conducted on WaterVG, where our Potamoi archives state-of-the-art performances
compared with counterparts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Manipulation of Deformable Objects using Imitation Learning with
  Adaptation to Hardware Constraints <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12685v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12685v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Hannus, Tran Nguyen Le, David Blanco-Mulero, Ville Kyrki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation Learning (IL) is a promising paradigm for learning dynamic
manipulation of deformable objects since it does not depend on
difficult-to-create accurate simulations of such objects. However, the
translation of motions demonstrated by a human to a robot is a challenge for
IL, due to differences in the embodiments and the robot's physical limits.
These limits are especially relevant in dynamic manipulation where high
velocities and accelerations are typical. To address this problem, we propose a
framework that first maps a dynamic demonstration into a motion that respects
the robot's constraints using a constrained Dynamic Movement Primitive. Second,
the resulting object state is further optimized by quasi-static refinement
motions to optimize task performance metrics. This allows both efficiently
altering the object state by dynamic motions and stable small-scale
refinements. We evaluate the framework in the challenging task of bag opening,
designing the system BILBO: Bimanual dynamic manipulation using Imitation
Learning for Bag Opening. Our results show that BILBO can successfully open a
wide range of crumpled bags, using a demonstration with a single bag. See
supplementary material at https://sites.google.com/view/bilbo-bag.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to 2024 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS 2024). 8 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IFFNeRF: Initialisation Free and Fast 6DoF pose estimation from a single
  image and a NeRF model <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Bortolon, Theodore Tsesmelis, Stuart James, Fabio Poiesi, Alessio Del Bue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce IFFNeRF to estimate the six degrees-of-freedom (6DoF) camera
pose of a given image, building on the Neural Radiance Fields (NeRF)
formulation. IFFNeRF is specifically designed to operate in real-time and
eliminates the need for an initial pose guess that is proximate to the sought
solution. IFFNeRF utilizes the Metropolis-Hasting algorithm to sample surface
points from within the NeRF model. From these sampled points, we cast rays and
deduce the color for each ray through pixel-level view synthesis. The camera
pose can then be estimated as the solution to a Least Squares problem by
selecting correspondences between the query image and the resulting bundle. We
facilitate this process through a learned attention mechanism, bridging the
query image embedding with the embedding of parameterized rays, thereby
matching rays pertinent to the image. Through synthetic and real evaluation
settings, we show that our method can improve the angular and translation error
accuracy by 80.1% and 67.3%, respectively, compared to iNeRF while performing
at 34fps on consumer hardware and not requiring the initial pose guess.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted ICRA 2024, Project page:
  https://mbortolon97.github.io/iffnerf/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In-Hand Following of Deformable Linear Objects Using Dexterous Fingers
  with Tactile Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12676v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12676v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingrui Yu, Boyuan Liang, Xiang Zhang, Xinghao Zhu, Xiang Li, Masayoshi Tomizuka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most research on deformable linear object (DLO) manipulation assumes rigid
grasping. However, beyond rigid grasping and re-grasping, in-hand following is
also an essential skill that humans use to dexterously manipulate DLOs, which
requires continuously changing the grasp point by in-hand sliding while holding
the DLO to prevent it from falling. Achieving such a skill is very challenging
for robots without using specially designed but not versatile end-effectors.
Previous works have attempted using generic parallel grippers, but their
robustness is unsatisfactory owing to the conflict between following and
holding, which is hard to balance with a one-degree-of-freedom gripper. In this
work, inspired by how humans use fingers to follow DLOs, we explore the usage
of a generic dexterous hand with tactile sensing to imitate human skills and
achieve robust in-hand DLO following. To enable the hardware system to function
in the real world, we develop a framework that includes Cartesian-space
arm-hand control, tactile-based in-hand 3-D DLO pose estimation, and
task-specific motion design. Experimental results demonstrate the significant
superiority of our method over using parallel grippers, as well as its great
robustness, generalizability, and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Driving Animatronic Robot Facial Expression From Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boren Li, Hang Li, Hangxin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Animatronic robots aim to enable natural human-robot interaction through
lifelike facial expressions. However, generating realistic, speech-synchronized
robot expressions is challenging due to the complexities of facial biomechanics
and responsive motion synthesis. This paper presents a principled,
skinning-centric approach to drive animatronic robot facial expressions from
speech. The proposed approach employs linear blend skinning (LBS) as the core
representation to guide tightly integrated innovations in embodiment design and
motion synthesis. LBS informs the actuation topology, enables human expression
retargeting, and allows speech-driven facial motion generation. The proposed
approach is capable of generating highly realistic, real-time facial
expressions from speech on an animatronic face, significantly advancing robots'
ability to replicate nuanced human expressions for natural interaction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PointGrasp: Point Cloud-based Grasping for Tendon-driven Soft Robotic
  Glove Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Hu, Shirui Lyu, Eojin Rho, Daekyum Kim, Shan Luo, Letizia Gionfrida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Controlling hand exoskeletons to assist individuals with grasping tasks poses
a challenge due to the difficulty in understanding user intentions. We propose
that most daily grasping tasks during activities of daily living (ADL) can be
deduced by analyzing object geometries (simple and complex) from 3D point
clouds. The study introduces PointGrasp, a real-time system designed for
identifying household scenes semantically, aiming to support and enhance
assistance during ADL for tailored end-to-end grasping tasks. The system
comprises an RGB-D camera with an inertial measurement unit and a
microprocessor integrated into a tendon-driven soft robotic glove. The RGB-D
camera processes 3D scenes at a rate exceeding 30 frames per second. The
proposed pipeline demonstrates an average RMSE of 0.8 $\pm$ 0.39 cm for simple
and 0.11 $\pm$ 0.06 cm for complex geometries. Within each mode, it identifies
and pinpoints reachable objects. This system shows promise in end-to-end
vision-driven robotic-assisted rehabilitation manual tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 8 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Looking for the Human in HRI Teaching: User-Centered Course Design for
  Tech-Savvy Students 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Doernbach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Top-down, user-centered thinking is not typically a strength of all students,
especially tech-savvy computer science-related ones. We propose Human-Robot
Interaction (HRI) introductory courses as a highly suitable opportunity to
foster these important skills since the HRI discipline includes a focus on
humans as users. Our HRI course therefore contains elements like scenario-based
design of laboratory projects, discussing and merging ideas and other
self-empowerment techniques. Participants describe, implement and present
everyday scenarios using Pepper robots and our customized open-source visual
programming tool. We observe that students obtain a good grasp of the taught
topics and improve their user-centered thinking skills.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the Designing an Intro to HRI Course Workshop at HRI
  2024 (arXiv:2403.05588)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FootstepNet: an Efficient Actor-Critic Method for Fast On-line Bipedal
  Footstep Planning and Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clément Gaspard, Grégoire Passault, Mélodie Daniel, Olivier Ly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing a humanoid locomotion controller is challenging and classically
split up in sub-problems. Footstep planning is one of those, where the sequence
of footsteps is defined. Even in simpler environments, finding a minimal
sequence, or even a feasible sequence, yields a complex optimization problem.
In the literature, this problem is usually addressed by search-based algorithms
(e.g. variants of A*). However, such approaches are either computationally
expensive or rely on hand-crafted tuning of several parameters. In this work,
at first, we propose an efficient footstep planning method to navigate in local
environments with obstacles, based on state-of-the art Deep Reinforcement
Learning (DRL) techniques, with very low computational requirements for on-line
inference. Our approach is heuristic-free and relies on a continuous set of
actions to generate feasible footsteps. In contrast, other methods necessitate
the selection of a relevant discrete set of actions. Second, we propose a
forecasting method, allowing to quickly estimate the number of footsteps
required to reach different candidates of local targets. This approach relies
on inherent computations made by the actor-critic DRL architecture. We
demonstrate the validity of our approach with simulation results, and by a
deployment on a kid-size humanoid robot during the RoboCup 2023 competition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M2DA: Multi-Modal Fusion <span class="highlight-title">Transformer</span> Incorporating Driver Attention for
  Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongyang Xu, Haokun Li, Qingfan Wang, Ziying Song, Lei Chen, Hanming Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end autonomous driving has witnessed remarkable progress. However, the
extensive deployment of autonomous vehicles has yet to be realized, primarily
due to 1) inefficient multi-modal environment perception: how to integrate data
from multi-modal sensors more efficiently; 2) non-human-like scene
understanding: how to effectively locate and predict critical risky agents in
traffic scenarios like an experienced driver. To overcome these challenges, in
this paper, we propose a Multi-Modal fusion transformer incorporating Driver
Attention (M2DA) for autonomous driving. To better fuse multi-modal data and
achieve higher alignment between different modalities, a novel
Lidar-Vision-Attention-based Fusion (LVAFusion) module is proposed. By
incorporating driver attention, we empower the human-like scene understanding
ability to autonomous vehicles to identify crucial areas within complex
scenarios precisely and ensure safety. We conduct experiments on the CARLA
simulator and achieve state-of-the-art performance with less data in
closed-loop benchmarks. Source codes are available at
https://anonymous.4open.science/r/M2DA-4772.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-View Active Sensing for Human-Robot Interaction via Hierarchically
  Connected Tree 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanjiong Ying, Xian Huang, Wei Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Comprehensive perception of human beings is the prerequisite to ensure the
safety of human-robot interaction. Currently, prevailing visual sensing
approach typically involves a single static camera, resulting in a restricted
and occluded field of view. In our work, we develop an active vision system
using multiple cameras to dynamically capture multi-source RGB-D data. An
integrated human sensing strategy based on a hierarchically connected tree
structure is proposed to fuse localized visual information. Constituting the
tree model are the nodes representing keypoints and the edges representing
keyparts, which are consistently interconnected to preserve the structural
constraints during multi-source fusion. Utilizing RGB-D data and HRNet, the 3D
positions of keypoints are analytically estimated, and their presence is
inferred through a sliding widow of confidence scores. Subsequently, the point
clouds of reliable keyparts are extracted by drawing occlusion-resistant masks,
enabling fine registration between data clouds and cylindrical model following
the hierarchical order. Experimental results demonstrate that our method
enhances keypart recognition recall from 69.20% to 90.10%, compared to
employing a single static camera. Furthermore, in overcoming challenges related
to localized and occluded perception, the robotic arm's obstacle avoidance
capabilities are effectively improved.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High-Fidelity SLAM Using Gaussian Splatting with Rendering-Guided
  Densification and Regularized Optimization <span class="chip">IROS24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Sun, Malcolm Mielle, Achim J. Lilienthal, Martin Magnusson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a dense RGBD SLAM system based on 3D Gaussian Splatting that
provides metrically accurate pose tracking and visually realistic
reconstruction. To this end, we first propose a Gaussian densification strategy
based on the rendering loss to map unobserved areas and refine reobserved
areas. Second, we introduce extra regularization parameters to alleviate the
forgetting problem in the continuous mapping problem, where parameters tend to
overfit the latest frame and result in decreasing rendering quality for
previous frames. Both mapping and tracking are performed with Gaussian
parameters by minimizing re-rendering loss in a differentiable way. Compared to
recent neural and concurrently developed gaussian splatting RGBD SLAM
baselines, our method achieves state-of-the-art results on the synthetic
dataset Replica and competitive results on the real-world dataset TUM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IROS24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ To Help or Not to Help: LLM-based Attentive Support for Human-Robot
  Group Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Tanneberg, Felix Ocker, Stephan Hasler, Joerg Deigmoeller, Anna Belardinelli, Chao Wang, Heiko Wersing, Bernhard Sendhoff, Michael Gienger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How can a robot provide unobtrusive physical support within a group of
humans? We present Attentive Support, a novel interaction concept for robots to
support a group of humans. It combines scene perception, dialogue acquisition,
situation understanding, and behavior generation with the common-sense
reasoning capabilities of Large Language Models (LLMs). In addition to
following user instructions, Attentive Support is capable of deciding when and
how to support the humans, and when to remain silent to not disturb the group.
With a diverse set of scenarios, we show and evaluate the robot's attentive
behavior, which supports and helps the humans when required, while not
disturbing if no help is needed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TON-VIO: Online Time Offset Modeling Networks for Robust Temporal
  Alignment in High Dynamic Motion VIO 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoran Xiong, Guoqing Liu, Qi Wu, Songpengcheng Xia, Tong Hua, Kehui Ma, Zhen Sun, Yan Xiang, Ling Pei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal misalignment (time offset) between sensors is common in low cost
visual-inertial odometry (VIO) systems. Such temporal misalignment introduces
inconsistent constraints for state estimation, leading to a significant
positioning drift especially in high dynamic motion scenarios. In this article,
we focus on online temporal calibration to reduce the positioning drift caused
by the time offset for high dynamic motion VIO. For the time offset observation
model, most existing methods rely on accurate state estimation or stable visual
tracking. For the prediction model, current methods oversimplify the time
offset as a constant value with white Gaussian noise. However, these ideal
conditions are seldom satisfied in real high dynamic scenarios, resulting in
the poor performance. In this paper, we introduce online time offset modeling
networks (TON) to enhance real-time temporal calibration. TON improves the
accuracy of time offset observation and prediction modeling. Specifically, for
observation modeling, we propose feature velocity observation networks to
enhance velocity computation for features in unstable visual tracking
conditions. For prediction modeling, we present time offset prediction networks
to learn its evolution pattern. To highlight the effectiveness of our method,
we integrate the proposed TON into both optimization-based and filter-based VIO
systems. Simulation and real-world experiments are conducted to demonstrate the
enhanced performance of our approach. Additionally, to contribute to the VIO
community, we will open-source the code of our method on:
https://github.com/Franky-X/FVON-TPN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Under-actuated Robotic Gripper with Multiple Grasping Modes Inspired by
  Human Finger 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihao Li, Tingbo Liao, Hassen Nigatu, Haotian Guo, Guodong Lu, Huixu Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Under-actuated robot grippers as a pervasive tool of robots have become a
considerable research focus. Despite their simplicity of mechanical design and
control strategy, they suffer from poor versatility and weak adaptability,
making widespread applications limited. To better relieve relevant research
gaps, we present a novel 3-finger linkage-based gripper that realizes
retractable and reconfigurable multi-mode grasps driven by a single motor.
Firstly, inspired by the changes that occurred in the contact surface with a
human finger moving, we artfully design a slider-slide rail mechanism as the
phalanx to achieve retraction of each finger, allowing for better performance
in the enveloping grasping mode. Secondly, a reconfigurable structure is
constructed to broaden the grasping range of objects' dimensions for the
proposed gripper. By adjusting the configuration and gesture of each finger,
the gripper can achieve five grasping modes. Thirdly, the proposed gripper is
just actuated by a single motor, yet it can be capable of grasping and
reconfiguring simultaneously. Finally, various experiments on grasps of
slender, thin, and large-volume objects are implemented to evaluate the
performance of the proposed gripper in practical scenarios, which demonstrates
the excellent grasping capabilities of the gripper.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Theoretical Modeling and Bio-inspired Trajectory Optimization of A
  Multiple-locomotion Origami Robot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12471v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12471v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keqi Zhu, Haotian Guo, Wei Yu, Hassen Nigatu, Tong Li, Huixu Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research on mobile robots has focused on increasing their adaptability
to unpredictable and unstructured environments using soft materials and
structures. However, the determination of key design parameters and control
over these compliant robots are predominantly iterated through experiments,
lacking a solid theoretical foundation. To improve their efficiency, this paper
aims to provide mathematics modeling over two locomotion, crawling and
swimming. Specifically, a dynamic model is first devised to reveal the
influence of the contact surfaces' frictional coefficients on displacements in
different motion phases. Besides, a swimming kinematics model is provided using
coordinate transformation, based on which, we further develop an algorithm that
systematically plans human-like swimming gaits, with maximum thrust obtained.
The proposed algorithm is highly generalizable and has the potential to be
applied in other soft robots with multiple joints. Simulation experiments have
been conducted to illustrate the effectiveness of the proposed modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diagrammatic Instructions to Specify Spatial Objectives and Constraints
  with Applications to Mobile Base Placement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qilin Sun, Weiming Zhi, Tianyi Zhang, Matthew Johnson-Roberson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Spatial Diagrammatic Instructions (SDIs), an approach
for human operators to specify objectives and constraints that are related to
spatial regions in the working environment. Human operators are enabled to
sketch out regions directly on camera images that correspond to the objectives
and constraints. These sketches are projected to 3D spatial coordinates, and
continuous Spatial Instruction Maps (SIMs) are learned upon them. These maps
can then be integrated into optimization problems for tasks of robots. In
particular, we demonstrate how Spatial Diagrammatic Instructions can be applied
to solve the Base Placement Problem of mobile manipulators, which concerns the
best place to put the manipulator to facilitate a certain task. Human operators
can specify, via sketch, spatial regions of interest for a manipulation task
and permissible regions for the mobile manipulator to be at. Then, an
optimization problem that maximizes the manipulator's reachability, or
coverage, over the designated regions of interest while remaining in the
permissible regions is solved. We provide extensive empirical evaluations, and
show that our formulation of Spatial Instruction Maps provides accurate
representations of user-specified diagrammatic instructions. Furthermore, we
demonstrate that our diagrammatic approach to the Mobile Base Placement Problem
enables higher quality solutions and faster run-time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Object RANSAC: Efficient Plane Clustering Method in a Clutter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12449v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12449v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seunghyeon Lim, Youngjae Yoo, Jun Ki Lee, Byoung-Tak Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel method for plane clustering specialized in
cluttered scenes using an RGB-D camera and validate its effectiveness through
robot grasping experiments. Unlike existing methods, which focus on large-scale
indoor structures, our approach -- Multi-Object RANSAC emphasizes cluttered
environments that contain a wide range of objects with different scales. It
enhances plane segmentation by generating subplanes in Deep Plane Clustering
(DPC) module, which are then merged with the final planes by post-processing.
DPC rearranges the point cloud by voting layers to make subplane clusters,
trained in a self-supervised manner using pseudo-labels generated from RANSAC.
Multi-Object RANSAC demonstrates superior plane instance segmentation
performances over other recent RANSAC applications. We conducted an experiment
on robot suction-based grasping, comparing our method with vision-based
grasping network and RANSAC applications. The results from this real-world
scenario showed its remarkable performance surpassing the baseline methods,
highlighting its potential for advanced scene understanding and manipulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniDexFPM: Universal Dexterous Functional Pre-grasp Manipulation Via
  Diffusion Policy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianhao Wu, Yunchong Gan, Mingdong Wu, Jingbo Cheng, Yaodong Yang, Yixin Zhu, Hao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objects in the real world are often not naturally positioned for functional
grasping, which usually requires repositioning and reorientation before they
can be grasped, a process known as pre-grasp manipulation. However, effective
learning of universal dexterous functional pre-grasp manipulation necessitates
precise control over relative position, relative orientation, and contact
between the hand and object, while generalizing to diverse dynamic scenarios
with varying objects and goal poses. We address the challenge by using
teacher-student learning. We propose a novel mutual reward that incentivizes
agents to jointly optimize three key criteria. Furthermore, we introduce a
pipeline that leverages a mixture-of-experts strategy to learn diverse
manipulation policies, followed by a diffusion policy to capture complex action
distributions from these experts. Our method achieves a success rate of 72.6%
across 30+ object categories encompassing 1400+ objects and 10k+ goal poses.
Notably, our method relies solely on object pose information for universal
dexterous functional pre-grasp manipulation by using extrinsic dexterity and
adjusting from feedback. Additional experiments under noisy object pose
observation showcase the robustness of our method and its potential for
real-world applications. The demonstrations can be viewed at
https://unidexfpm.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bin Packing Optimization via Deep Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baoying Wang, Huixu Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Bin Packing Problem (BPP) has attracted enthusiastic research interest
recently, owing to widespread applications in logistics and warehousing
environments. It is truly essential to optimize the bin packing to enable more
objects to be packed into boxes. Object packing order and placement strategy
are the two crucial optimization objectives of the BPP. However, existing
optimization methods for BPP, such as the genetic algorithm (GA), emerge as the
main issues in highly computational cost and relatively low accuracy, making it
difficult to implement in realistic scenarios. To well relieve the research
gaps, we present a novel optimization methodology of two-dimensional (2D)-BPP
and three-dimensional (3D)-BPP for objects with regular shapes via deep
reinforcement learning (DRL), maximizing the space utilization and minimizing
the usage number of boxes. First, an end-to-end DRL neural network constructed
by a modified Pointer Network consisting of an encoder, a decoder and an
attention module is proposed to achieve the optimal object packing order.
Second, conforming to the top-down operation mode, the placement strategy based
on a height map is used to arrange the ordered objects in the boxes, preventing
the objects from colliding with boxes and other objects in boxes. Third, the
reward and loss functions are defined as the indicators of the compactness,
pyramid, and usage number of boxes to conduct the training of the DRL neural
network based on an on-policy actor-critic framework. Finally, a series of
experiments are implemented to compare our method with conventional packing
methods, from which we conclude that our method outperforms these packing
methods in both packing accuracy and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OV9D: Open-Vocabulary Category-Level 9D Object Pose and Size Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhao Cai, Yisheng He, Weihao Yuan, Siyu Zhu, Zilong Dong, Liefeng Bo, Qifeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies a new open-set problem, the open-vocabulary category-level
object pose and size estimation. Given human text descriptions of arbitrary
novel object categories, the robot agent seeks to predict the position,
orientation, and size of the target object in the observed scene image. To
enable such generalizability, we first introduce OO3D-9D, a large-scale
photorealistic dataset for this task. Derived from OmniObject3D, OO3D-9D is the
largest and most diverse dataset in the field of category-level object pose and
size estimation. It includes additional annotations for the symmetry axis of
each category, which help resolve symmetric ambiguity. Apart from the
large-scale dataset, we find another key to enabling such generalizability is
leveraging the strong prior knowledge in pre-trained visual-language foundation
models. We then propose a framework built on pre-trained DinoV2 and
text-to-image stable diffusion models to infer the normalized object coordinate
space (NOCS) maps of the target instances. This framework fully leverages the
visual semantic prior from DinoV2 and the aligned visual and language knowledge
within the text-to-image diffusion model, which enables generalization to
various text descriptions of novel categories. Comprehensive quantitative and
qualitative experiments demonstrate that the proposed open-vocabulary method,
trained on our large-scale synthesized data, significantly outperforms the
baseline and can effectively generalize to real-world images of unseen
categories. The project page is at https://ov9d.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online Multi-Agent Pickup and Delivery with Task Deadlines <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hiroya Makino, Seigo Ito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Managing delivery deadlines in automated warehouses and factories is crucial
for maintaining customer satisfaction and ensuring seamless production. This
study introduces the problem of online multi-agent pickup and delivery with
task deadlines (MAPD-D), which is an advanced variant of the online MAPD
problem incorporating delivery deadlines. MAPD-D presents a dynamic
deadline-driven approach that includes task deadlines, with tasks being added
at any time (online), thus challenging conventional MAPD frameworks. To tackle
MAPD-D, we propose a novel algorithm named deadline-aware token passing (D-TP).
The D-TP algorithm is designed to calculate pickup deadlines and assign tasks
while balancing execution cost and deadline proximity. Additionally, we
introduce the D-TP with task swaps (D-TPTS) method to further reduce task
tardiness, enhancing flexibility and efficiency via task-swapping strategies.
Numerical experiments were conducted in simulated warehouse environments to
showcase the effectiveness of the proposed methods. Both D-TP and D-TPTS
demonstrate significant reductions in task tardiness compared to existing
methods, thereby contributing to efficient operations in automated warehouses
and factories with delivery deadlines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures, submitted to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CaDRE: Controllable and Diverse Generation of Safety-Critical Driving
  Scenarios using Real-World Trajectories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peide Huang, Wenhao Ding, Jonathan Francis, Bingqing Chen, Ding Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulation is an indispensable tool in the development and testing of
autonomous vehicles (AVs), offering an efficient and safe alternative to road
testing by allowing the exploration of a wide range of scenarios. Despite its
advantages, a significant challenge within simulation-based testing is the
generation of safety-critical scenarios, which are essential to ensure that AVs
can handle rare but potentially fatal situations. This paper addresses this
challenge by introducing a novel generative framework, CaDRE, which is
specifically designed for generating diverse and controllable safety-critical
scenarios using real-world trajectories. Our approach optimizes for both the
quality and diversity of scenarios by employing a unique formulation and
algorithm that integrates real-world data, domain knowledge, and black-box
optimization techniques. We validate the effectiveness of our framework through
extensive testing in three representative types of traffic scenarios. The
results demonstrate superior performance in generating diverse and high-quality
scenarios with greater sample efficiency than existing reinforcement learning
and sampling-based methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Robots That Know When They Need Help: Affordance-Based
  Uncertainty for Large Language Model Planners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13198v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13198v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James F. Mullen Jr., Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) showcase many desirable traits for intelligent
and helpful robots. However, they are also known to hallucinate predictions.
This issue is exacerbated in consumer robotics where LLM hallucinations may
result in robots confidently executing plans that are contrary to user goals,
relying more frequently on human assistance, or preventing the robot from
asking for help at all. In this work, we present LAP, a novel approach for
utilizing off-the-shelf LLM's, alongside scene and object Affordances, in
robotic Planners that minimize harmful hallucinations and know when to ask for
help. Our key finding is that calculating and leveraging a scene affordance
score, a measure of whether a given action is possible in the provided scene,
helps to mitigate hallucinations in LLM predictions and better align the LLM's
confidence measure with the probability of success. We specifically propose and
test three different affordance scores, which can be used independently or in
tandem to improve performance across different use cases. The most successful
of these individual scores involves prompting an LLM to determine if a given
action is possible and safe in the given scene and uses the LLM's response to
compute the score. Through experiments in both simulation and the real world,
on tasks with a variety of ambiguities, we show that LAP significantly
increases success rate and decreases the amount of human intervention required
relative to prior art. For example, in our real-world testing paradigm, LAP
decreases the human help rate of previous methods by over 33% at a success rate
of 70%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reflectivity Is All You Need!: Advancing LiDAR Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13188v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13188v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kasi Viswanath, Peng Jiang, Srikanth Saripalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR semantic segmentation frameworks predominantly leverage geometry-based
features to differentiate objects within a scan. While these methods excel in
scenarios with clear boundaries and distinct shapes, their performance declines
in environments where boundaries are blurred, particularly in off-road
contexts. To address this, recent strides in 3D segmentation algorithms have
focused on harnessing raw LiDAR intensity measurements to improve prediction
accuracy. Despite these efforts, current learning-based models struggle to
correlate the intricate connections between raw intensity and factors such as
distance, incidence angle, material reflectivity, and atmospheric conditions.
Building upon our prior work, this paper delves into the advantages of
employing calibrated intensity (also referred to as reflectivity) within
learning-based LiDAR semantic segmentation frameworks. We initially establish
that incorporating reflectivity as an input enhances the existing LiDAR
semantic segmentation model. Furthermore, we present findings that enable the
model to learn to calibrate intensity can boost its performance. Through
extensive experimentation on the off-road dataset Rellis-3D, we demonstrate
notable improvements. Specifically, converting intensity to reflectivity
results in a 4% increase in mean Intersection over Union (mIoU) when compared
to using raw intensity in Off-road scenarios. Additionally, we also investigate
the possible benefits of using calibrated intensity in semantic segmentation in
urban environments (SemanticKITTI) and cross-sensor domain adaptation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ User-customizable Shared Control for Fine Teleoperation via Virtual
  Reality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Luo, Mark Zolotas, Drake Moore, Taskin Padir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Shared control can ease and enhance a human operator's ability to teleoperate
robots, particularly for intricate tasks demanding fine control over multiple
degrees of freedom. However, the arbitration process dictating how much
autonomous assistance to administer in shared control can confuse novice
operators and impede their understanding of the robot's behavior. To overcome
these adverse side-effects, we propose a novel formulation of shared control
that enables operators to tailor the arbitration to their unique capabilities
and preferences. Unlike prior approaches to customizable shared control where
users could indirectly modify the latent parameters of the arbitration function
by issuing a feedback command, we instead make these parameters observable and
directly editable via a virtual reality (VR) interface. We present our
user-customizable shared control method for a teleoperation task in SE(3),
known as the buzz wire game. A user study is conducted with participants
teleoperating a robotic arm in VR to complete the game. The experiment spanned
two weeks per subject to investigate longitudinal trends. Our findings reveal
that users allowed to interactively tune the arbitration parameters across
trials generalize well to adaptations in the task, exhibiting improvements in
precision and fluency over direct teleoperation and conventional shared
control.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Designing Consistent Covariance Recovery from a Deep Learning Visual
  Odometry Engine <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13170v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13170v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jagatpreet Singh Nir, Dennis Giaya, Hanumant Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning techniques have significantly advanced in providing accurate
visual odometry solutions by leveraging large datasets. However, generating
uncertainty estimates for these methods remains a challenge. Traditional sensor
fusion approaches in a Bayesian framework are well-established, but deep
learning techniques with millions of parameters lack efficient methods for
uncertainty estimation.
  This paper addresses the issue of uncertainty estimation for pre-trained
deep-learning models in monocular visual odometry. We propose formulating a
factor graph on an implicit layer of the deep learning network to recover
relative covariance estimates, which allows us to determine the covariance of
the Visual Odometry (VO) solution. We showcase the consistency of the deep
learning engine's covariance approximation with an empirical analysis of the
covariance model on the EUROC datasets to demonstrate the correctness of our
formulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meta-Learning for Fast Adaptation in Intent Inferral on a Robotic Hand
  Orthosis for Stroke 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13147v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13147v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Leandro La Rotta, Jingxi Xu, Ava Chen, Lauren Winterbottom, Wenxi Chen, Dawn Nilsen, Joel Stein, Matei Ciocarlie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose MetaEMG, a meta-learning approach for fast adaptation in intent
inferral on a robotic hand orthosis for stroke. One key challenge in machine
learning for assistive and rehabilitative robotics with disabled-bodied
subjects is the difficulty of collecting labeled training data. Muscle tone and
spasticity often vary significantly among stroke subjects, and hand function
can even change across different use sessions of the device for the same
subject. We investigate the use of meta-learning to mitigate the burden of data
collection needed to adapt high-capacity neural networks to a new session or
subject. Our experiments on real clinical data collected from five stroke
subjects show that MetaEMG can improve the intent inferral accuracy with a
small session- or subject-specific dataset and very few fine-tuning epochs. To
the best of our knowledge, we are the first to formulate intent inferral on
stroke subjects as a meta-learning problem and demonstrate fast adaptation to a
new session or subject for controlling a robotic hand orthosis with EMG
signals.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interactive Robot-Environment Self-Calibration via Compliant Exploratory
  Actions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Podshara Chanrungmaneekul, Kejia Ren, Joshua T. Grace, Aaron M. Dollar, Kaiyu Hang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Calibrating robots into their workspaces is crucial for manipulation tasks.
Existing calibration techniques often rely on sensors external to the robot
(cameras, laser scanners, etc.) or specialized tools. This reliance complicates
the calibration process and increases the costs and time requirements.
Furthermore, the associated setup and measurement procedures require
significant human intervention, which makes them more challenging to operate.
Using the built-in force-torque sensors, which are nowadays a default component
in collaborative robots, this work proposes a self-calibration framework where
robot-environmental spatial relations are automatically estimated through
compliant exploratory actions by the robot itself. The self-calibration
approach converges, verifies its own accuracy, and terminates upon completion,
autonomously purely through interactive exploration of the environment's
geometries. Extensive experiments validate the effectiveness of our
self-calibration approach in accurately establishing the robot-environment
spatial relationships without the need for additional sensing equipment or any
human intervention.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wearable Roller Rings to Enable Robot Dexterous In-Hand Manipulation
  through Active Surfaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hayden Webb, Podshara Chanrungmaneekul, Shenli Yuan, Kaiyu Hang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-hand manipulation is a crucial ability for reorienting and repositioning
objects within grasps. The main challenges are not only the complexity in the
computational models, but also the risks of grasp instability caused by active
finger motions, such as rolling, sliding, breaking, and remaking contacts.
Based on the idea of manipulation without lifting a finger, this paper presents
the development of Roller Rings (RR), a modular robotic attachment with active
surfaces that is wearable by both robot and human hands. By installing and
angling the RRs on grasping systems, such that their spatial motions are not
co-linear, we derive a general differential motion model for the object
actuated by the active surfaces. Our motion model shows that complete in-hand
manipulation skill sets can be provided by as few as only 2 RRs through
non-holonomic object motions, while more RRs can enable enhanced manipulation
dexterity with fewer motion constraints. Through extensive experiments, we wear
RRs on both a robot hand and a human hand to evaluate their manipulation
capabilities, and show that the RRs can be employed to manipulate arbitrary
object shapes to provide dexterous in-hand manipulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Better Call SAL: Towards Learning to Segment Anything in Lidar 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aljoša Ošep, Tim Meinhardt, Francesco Ferroni, Neehar Peri, Deva Ramanan, Laura Leal-Taixé
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose $\texttt{SAL}$ ($\texttt{S}$egment $\texttt{A}$nything in
$\texttt{L}$idar) method consisting of a text-promptable zero-shot model for
segmenting and classifying any object in Lidar, and a pseudo-labeling engine
that facilitates model training without manual supervision. While the
established paradigm for $\textit{Lidar Panoptic Segmentation}$ (LPS) relies on
manual supervision for a handful of object classes defined a priori, we utilize
2D vision foundation models to generate 3D supervision "for free". Our
pseudo-labels consist of instance masks and corresponding CLIP tokens, which we
lift to Lidar using calibrated multi-modal data. By training our model on these
labels, we distill the 2D foundation models into our Lidar $\texttt{SAL}$
model. Even without manual labels, our model achieves $91\%$ in terms of
class-agnostic segmentation and $44\%$ in terms of zero-shot LPS of the fully
supervised state-of-the-art. Furthermore, we outperform several baselines that
do not distill but only lift image features to 3D. More importantly, we
demonstrate that $\texttt{SAL}$ supports arbitrary class prompts, can be easily
extended to new datasets, and shows significant potential to improve with
increasing amounts of self-labeled data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cooperative Modular Manipulation with Numerous Cable-Driven Robots for
  Assistive Construction and Gap Crossing <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13124v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13124v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Murphy, Joao C. V. Soares, Justin K. Yim, Dustin Nottage, Ahmet Soylemezoglu, Joao Ramos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Soldiers in the field often need to cross negative obstacles, such as rivers
or canyons, to reach goals or safety. Military gap crossing involves on-site
temporary bridges construction. However, this procedure is conducted with
dangerous, time and labor intensive operations, and specialized machinery. We
envision a scalable robotic solution inspired by advancements in
force-controlled and Cable Driven Parallel Robots (CDPRs); this solution can
address the challenges inherent in this transportation problem, achieving fast,
efficient, and safe deployment and field operations. We introduce the embodied
vision in Co3MaNDR, a solution to the military gap crossing problem, a
distributed robot consisting of several modules simultaneously pulling on a
central payload, controlling the cables' tensions to achieve complex
objectives, such as precise trajectory tracking or force amplification.
Hardware experiments demonstrate teleoperation of a payload, trajectory
following, and the sensing and amplification of operators' applied physical
forces during slow operations. An operator was shown to manipulate a 27.2 kg
(60 lb) payload with an average force utilization of 14.5\% of its weight.
Results indicate that the system can be scaled up to heavier payloads without
compromising performance or introducing superfluous complexity. This research
lays a foundation to expand CDPR technology to uncoordinated and unstable
mobile platforms in unknown environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures. Submit to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Neural Network-based Multi-agent Reinforcement Learning for
  Resilient Distributed Coordination of Multi-Robot Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony Goeckner, Yueyuan Sui, Nicolas Martinet, Xinliang Li, Qi Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing multi-agent coordination techniques are often fragile and vulnerable
to anomalies such as agent attrition and communication disturbances, which are
quite common in the real-world deployment of systems like field robotics. To
better prepare these systems for the real world, we present a graph neural
network (GNN)-based multi-agent reinforcement learning (MARL) method for
resilient distributed coordination of a multi-robot system. Our method,
Multi-Agent Graph Embedding-based Coordination (MAGEC), is trained using
multi-agent proximal policy optimization (PPO) and enables distributed
coordination around global objectives under agent attrition, partial
observability, and limited or disturbed communications. We use a multi-robot
patrolling scenario to demonstrate our MAGEC method in a ROS 2-based simulator
and then compare its performance with prior coordination approaches. Results
demonstrate that MAGEC outperforms existing methods in several experiments
involving agent attrition and communication disturbance, and provides
competitive results in scenarios without such anomalies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Digital Twin-Driven Reinforcement Learning for Obstacle Avoidance in
  Robot Manipulators: A Self-Improving Online Training Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13090v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13090v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzhu Sun, Mien Van, Stephen McIlvanna, Nguyen Minh Nhat, Kabirat Olayemi, Jack Close, Seán McLoone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution and growing automation of collaborative robots introduce more
complexity and unpredictability to systems, highlighting the crucial need for
robot's adaptability and flexibility to address the increasing complexities of
their environment. In typical industrial production scenarios, robots are often
required to be re-programmed when facing a more demanding task or even a few
changes in workspace conditions. To increase productivity, efficiency and
reduce human effort in the design process, this paper explores the potential of
using digital twin combined with Reinforcement Learning (RL) to enable robots
to generate self-improving collision-free trajectories in real time. The
digital twin, acting as a virtual counterpart of the physical system, serves as
a 'forward run' for monitoring, controlling, and optimizing the physical system
in a safe and cost-effective manner. The physical system sends data to
synchronize the digital system through the video feeds from cameras, which
allows the virtual robot to update its observation and policy based on real
scenarios. The bidirectional communication between digital and physical systems
provides a promising platform for hardware-in-the-loop RL training through
trial and error until the robot successfully adapts to its new environment. The
proposed online training framework is demonstrated on the Unfactory Xarm5
collaborative robot, where the robot end-effector aims to reach the target
position while avoiding obstacles. The experiment suggest that proposed
framework is capable of performing policy online training, and that there
remains significant room for improvement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Subgoal Diffuser: Coarse-to-fine Subgoal Generation to Guide Model
  Predictive Control for Robot Manipulation <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Huang, Yating Lin, Fan Yang, Dmitry Berenson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manipulation of articulated and deformable objects can be difficult due to
their compliant and under-actuated nature. Unexpected disturbances can cause
the object to deviate from a predicted state, making it necessary to use
Model-Predictive Control (MPC) methods to plan motion. However, these methods
need a short planning horizon to be practical. Thus, MPC is ill-suited for
long-horizon manipulation tasks due to local minima. In this paper, we present
a diffusion-based method that guides an MPC method to accomplish long-horizon
manipulation tasks by dynamically specifying sequences of subgoals for the MPC
to follow. Our method, called Subgoal Diffuser, generates subgoals in a
coarse-to-fine manner, producing sparse subgoals when the task is easily
accomplished by MPC and more dense subgoals when the MPC method needs more
guidance. The density of subgoals is determined dynamically based on a learned
estimate of reachability, and subgoals are distributed to focus on challenging
parts of the task. We evaluate our method on two robot manipulation tasks and
find it improves the planning performance of an MPC method, and also
outperforms prior diffusion-based methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Current-Based Impedance Control for Interacting with Mobile Manipulators <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13079v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13079v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jelmer de Wolde, Luzia Knoedler, Gianluca Garofalo, Javier Alonso-Mora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As robots shift from industrial to human-centered spaces, adopting mobile
manipulators, which expand workspace capabilities, becomes crucial. In these
settings, seamless interaction with humans necessitates compliant control. Two
common methods for safe interaction, admittance, and impedance control, require
force or torque sensors, often absent in lower-cost or lightweight robots. This
paper presents an adaption of impedance control that can be used on
current-controlled robots without the use of force or torque sensors and its
application for compliant control of a mobile manipulator. A calibration method
is designed that enables estimation of the actuators' current/torque ratios and
frictions, used by the adapted impedance controller, and that can handle model
errors. The calibration method and the performance of the designed controller
are experimentally validated using the Kinova GEN3 Lite arm. Results show that
the calibration method is consistent and that the designed controller for the
arm is compliant while also being able to track targets with five-millimeter
precision when no interaction is present. Additionally, this paper presents two
operational modes for interacting with the mobile manipulator: one for guiding
the robot around the workspace through interacting with the arm and another for
executing a tracking task, both maintaining compliance to external forces.
These operational modes were tested in real-world experiments, affirming their
practical applicability and effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 13 figures, under review for IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TAPTR: Tracking Any Point with <span class="highlight-title">Transformer</span>s as Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13042v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13042v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyang Li, Hao Zhang, Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a simple and strong framework for Tracking Any
Point with TRansformers (TAPTR). Based on the observation that point tracking
bears a great resemblance to object detection and tracking, we borrow designs
from DETR-like algorithms to address the task of TAP. In the proposed
framework, in each video frame, each tracking point is represented as a point
query, which consists of a positional part and a content part. As in DETR, each
query (its position and content feature) is naturally updated layer by layer.
Its visibility is predicted by its updated content feature. Queries belonging
to the same tracking point can exchange information through self-attention
along the temporal dimension. As all such operations are well-designed in
DETR-like algorithms, the model is conceptually very simple. We also adopt some
useful designs such as cost volume from optical flow models and develop simple
designs to provide long temporal information while mitigating the feature
drifting issue. Our framework demonstrates strong performance with
state-of-the-art performance on various TAP datasets with faster inference
speed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VIHE: Virtual In-Hand Eye <span class="highlight-title">Transformer</span> for 3D Robotic Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11461v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11461v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiyao Wang, Yutian Lei, Shiyu Jin, Gregory D. Hager, Liangjun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce the Virtual In-Hand Eye Transformer (VIHE), a
novel method designed to enhance 3D manipulation capabilities through
action-aware view rendering. VIHE autoregressively refines actions in multiple
stages by conditioning on rendered views posed from action predictions in the
earlier stages. These virtual in-hand views provide a strong inductive bias for
effectively recognizing the correct pose for the hand, especially for
challenging high-precision tasks such as peg insertion. On 18 manipulation
tasks in RLBench simulated environments, VIHE achieves a new state-of-the-art,
with a 12% absolute improvement, increasing from 65% to 77% over the existing
state-of-the-art model using 100 demonstrations per task. In real-world
scenarios, VIHE can learn manipulation tasks with just a handful of
demonstrations, highlighting its practical utility. Videos and code
implementation can be found at our project site: https://vihe-3d.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DynamicGlue: Epipolar and Time-Informed Data Association in Dynamic
  Environments using Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11370v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11370v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theresa Huber, Simon Schaefer, Stefan Leutenegger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The assumption of a static environment is common in many geometric computer
vision tasks like SLAM but limits their applicability in highly dynamic scenes.
Since these tasks rely on identifying point correspondences between input
images within the static part of the environment, we propose a graph neural
network-based sparse feature matching network designed to perform robust
matching under challenging conditions while excluding keypoints on moving
objects. We employ a similar scheme of attentional aggregation over graph edges
to enhance keypoint representations as state-of-the-art feature-matching
networks but augment the graph with epipolar and temporal information and
vastly reduce the number of graph edges. Furthermore, we introduce a
self-supervised training scheme to extract pseudo labels for image pairs in
dynamic environments from exclusively unprocessed visual-inertial data. A
series of experiments show the superior performance of our network as it
excludes keypoints on moving objects compared to state-of-the-art feature
matching networks while still achieving similar results regarding conventional
matching metrics. When integrated into a SLAM system, our network significantly
improves performance, especially in highly dynamic scenes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic
  Manipulations With Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06420v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06420v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangliang Chen, Yutian Lei, Shiyu Jin, Ying Zhang, Liangjun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has demonstrated its capability in solving
various tasks but is notorious for its low sample efficiency. In this paper, we
propose RLingua, a framework that can leverage the internal knowledge of large
language models (LLMs) to reduce the sample complexity of RL in robotic
manipulations. To this end, we first present a method for extracting the prior
knowledge of LLMs by prompt engineering so that a preliminary rule-based robot
controller for a specific task can be generated in a user-friendly manner.
Despite being imperfect, the LLM-generated robot controller is utilized to
produce action samples during rollouts with a decaying probability, thereby
improving RL's sample efficiency. We employ TD3, the widely-used RL baseline
method, and modify the actor loss to regularize the policy learning towards the
LLM-generated controller. RLingua also provides a novel method of improving the
imperfect LLM-generated robot controllers by RL. We demonstrate that RLingua
can significantly reduce the sample complexity of TD3 in four robot tasks of
panda_gym and achieve high success rates in 12 sampled sparsely rewarded robot
tasks in RLBench, where the standard TD3 fails. Additionally, We validated
RLingua's effectiveness in real-world robot experiments through Sim2Real,
demonstrating that the learned policies are effectively transferable to real
robot tasks. Further details about our work are available at our project
website https://rlingua.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SmartRefine: A Scenario-Adaptive Refinement Framework for Efficient
  Motion Prediction <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11492v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11492v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Zhou, Hao Shao, Letian Wang, Steven L. Waslander, Hongsheng Li, Yu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting the future motion of surrounding agents is essential for
autonomous vehicles (AVs) to operate safely in dynamic, human-robot-mixed
environments. Context information, such as road maps and surrounding agents'
states, provides crucial geometric and semantic information for motion behavior
prediction. To this end, recent works explore two-stage prediction frameworks
where coarse trajectories are first proposed, and then used to select critical
context information for trajectory refinement. However, they either incur a
large amount of computation or bring limited improvement, if not both. In this
paper, we introduce a novel scenario-adaptive refinement strategy, named
SmartRefine, to refine prediction with minimal additional computation.
Specifically, SmartRefine can comprehensively adapt refinement configurations
based on each scenario's properties, and smartly chooses the number of
refinement iterations by introducing a quality score to measure the prediction
quality and remaining refinement potential of each scenario. SmartRefine is
designed as a generic and flexible approach that can be seamlessly integrated
into most state-of-the-art motion prediction models. Experiments on Argoverse
(1 & 2) show that our method consistently improves the prediction accuracy of
multiple state-of-the-art prediction models. Specifically, by adding
SmartRefine to QCNet, we outperform all published ensemble-free works on the
Argoverse 2 leaderboard (single agent track) at submission. Comprehensive
studies are also conducted to ablate design choices and explore the mechanism
behind multi-iteration refinement. Codes are available at
https://github.com/opendilab/SmartRefine/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version for CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Asynchronous Distributed Smoothing and Mapping via On-Manifold Consensus
  ADMM <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12320v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12320v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel McGann, Kyle Lassak, Michael Kaess
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we present a fully distributed, asynchronous, and general
purpose optimization algorithm for Consensus Simultaneous Localization and
Mapping (CSLAM). Multi-robot teams require that agents have timely and accurate
solutions to their state as well as the states of the other robots in the team.
To optimize this solution we develop a CSLAM back-end based on Consensus ADMM
called MESA (Manifold, Edge-based, Separable ADMM). MESA is fully distributed
to tolerate failures of individual robots, asynchronous to tolerate
communication delays and outages, and general purpose to handle any CSLAM
problem formulation. We demonstrate that MESA exhibits superior convergence
rates and accuracy compare to existing state-of-the art CSLAM back-end
optimizers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ada-NAV: Adaptive Trajectory Length-Based Sample Efficient Policy
  Learning for Robotic Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06192v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06192v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhrij Patel, Kasun Weerakoon, Wesley A. Suttle, Alec Koppel, Brian M. Sadler, Tianyi Zhou, Amrit Singh Bedi, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory length stands as a crucial hyperparameter within reinforcement
learning (RL) algorithms, significantly contributing to the sample inefficiency
in robotics applications. Motivated by the pivotal role trajectory length plays
in the training process, we introduce Ada-NAV, a novel adaptive trajectory
length scheme designed to enhance the training sample efficiency of RL
algorithms in robotic navigation tasks. Unlike traditional approaches that
treat trajectory length as a fixed hyperparameter, we propose to dynamically
adjust it based on the entropy of the underlying navigation policy.
Interestingly, Ada-NAV can be applied to both existing on-policy and off-policy
RL methods, which we demonstrate by empirically validating its efficacy on
three popular RL methods: REINFORCE, Proximal Policy Optimization (PPO), and
Soft Actor-Critic (SAC). We demonstrate through simulated and real-world
robotic experiments that Ada-NAV outperforms conventional methods that employ
constant or randomly sampled trajectory lengths. Specifically, for a fixed
sample budget, Ada-NAV achieves an 18\% increase in navigation success rate, a
20-38\% reduction in navigation path length, and a 9.32\% decrease in elevation
costs. Furthermore, we showcase the versatility of Ada-NAV by integrating it
with the Clearpath Husky robot, illustrating its applicability in complex
outdoor environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 9 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Robotics Meets Wireless Communications: An Introductory Tutorial 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.02021v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.02021v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Bonilla Licea, Mounir Ghogho, Martin Saska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The importance of ground Mobile Robots (MRs) and Unmanned Aerial Vehicles
(UAVs) within the research community, industry, and society is growing fast.
Many of these agents are nowadays equipped with communication systems that are,
in some cases, essential to successfully achieve certain tasks. In this
context, we have begun to witness the development of a new interdisciplinary
research field at the intersection of robotics and communications. This
research field has been boosted by the intention of integrating UAVs within the
5G and 6G communication networks. This research will undoubtedly lead to many
important applications in the near future. Nevertheless, one of the main
obstacles to the development of this research area is that most researchers
address these problems by oversimplifying either the robotics or the
communications aspect. This impedes the ability of reaching the full potential
of this new interdisciplinary research area. In this tutorial, we present some
of the modelling tools necessary to address problems involving both robotics
and communication from an interdisciplinary perspective. As an illustrative
example of such problems, we focus in this tutorial on the issue of
communication-aware trajectory planning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 192 references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CognitiveOS: Large Multimodal Model based System to Endow Any Type of
  Robot with Generative AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.16205v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.16205v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Artem Lykov, Mikhail Konenkov, Koffivi Fidèle Gbagbe, Mikhail Litvinov, Denis Davletshin, Aleksey Fedoseev, Miguel Altamirano Cabrera, Robinroy Peter, Dzmitry Tsetserukou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces CognitiveOS, the first operating system designed for
cognitive robots capable of functioning across diverse robotic platforms.
CognitiveOS is structured as a multi-agent system comprising modules built upon
a transformer architecture, facilitating communication through an internal
monologue format. These modules collectively empower the robot to tackle
intricate real-world tasks. The paper delineates the operational principles of
the system along with descriptions of its nine distinct modules. The modular
design endows the system with distinctive advantages over traditional
end-to-end methodologies, notably in terms of adaptability and scalability. The
system's modules are configurable, modifiable, or deactivatable depending on
the task requirements, while new modules can be seamlessly integrated. This
system serves as a foundational resource for researchers and developers in the
cognitive robotics domain, alleviating the burden of constructing a cognitive
robot system from scratch. Experimental findings demonstrate the system's
advanced task comprehension and adaptability across varied tasks, robotic
platforms, and module configurations, underscoring its potential for real-world
applications. Moreover, in the category of Reasoning it outperformed
CognitiveDog (by 15%) and RT2 (by 31%), achieving the highest to date rate of
77%. We provide a code repository and dataset for the replication of
CognitiveOS: link will be provided in camera-ready submission.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper is submitted to the IEEE conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Fast and Optimal Learning-based Path Planning Method for Planetary
  Rovers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.04792v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.04792v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Ji, Yang Liu, Guanghu Xie, Boyu Ma, Zongwu Xie, Baoshi Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intelligent autonomous path planning is crucial to improve the exploration
efficiency of planetary rovers. In this paper, we propose a learning-based
method to quickly search for optimal paths in an elevation map, which is called
NNPP. The NNPP model learns semantic information about start and goal
locations, as well as map representations, from numerous pre-annotated optimal
path demonstrations, and produces a probabilistic distribution over each pixel
representing the likelihood of it belonging to an optimal path on the map. More
specifically, the paper computes the traversal cost for each grid cell from the
slope, roughness and elevation difference obtained from the DEM. Subsequently,
the start and goal locations are encoded using a Gaussian distribution and
different location encoding parameters are analyzed for their effect on model
performance. After training, the NNPP model is able to perform path planning on
novel maps. Experiments show that the guidance field generated by the NNPP
model can significantly reduce the search time for optimal paths under the same
hardware conditions, and the advantage of NNPP increases with the scale of the
map.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-task real-robot data with gaze attention for dual-arm fine
  manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07603v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07603v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heecheol Kim, Yoshiyuki Ohmura, Yasuo Kuniyoshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of robotic manipulation, deep imitation learning is recognized
as a promising approach for acquiring manipulation skills. Additionally,
learning from diverse robot datasets is considered a viable method to achieve
versatility and adaptability. In such research, by learning various tasks,
robots achieved generality across multiple objects. However, such multi-task
robot datasets have mainly focused on single-arm tasks that are relatively
imprecise, not addressing the fine-grained object manipulation that robots are
expected to perform in the real world. This paper introduces a dataset of
diverse object manipulations that includes dual-arm tasks and/or tasks
requiring fine manipulation. To this end, we have generated dataset with 224k
episodes (150 hours, 1,104 language instructions) which includes dual-arm fine
tasks such as bowl-moving, pencil-case opening or banana-peeling, and this data
is publicly available. Additionally, this dataset includes visual attention
signals as well as dual-action labels, a signal that separates actions into a
robust reaching trajectory and precise interaction with objects, and language
instructions to achieve robust and precise object manipulation. We applied the
dataset to our Dual-Action and Attention (DAA), a model designed for
fine-grained dual arm manipulation tasks and robust against covariate shifts.
The model was tested with over 7k total trials in real robot manipulation
tasks, demonstrating its capability in fine manipulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, The dataset is available at
  https://sites.google.com/view/multi-task-fine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PGA: Personalizing Grasping Agents with Single Human-Robot Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12547v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12547v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junghyun Kim, Gi-Cheon Kang, Jaein Kim, Seoyun Yang, Minjoon Jung, Byoung-Tak Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language-Conditioned Robotic Grasping (LCRG) aims to develop robots that
comprehend and grasp objects based on natural language instructions. While the
ability to understand personal objects like my wallet facilitates more natural
interaction with human users, current LCRG systems only allow generic language
instructions, e.g., the black-colored wallet next to the laptop. To this end,
we introduce a task scenario GraspMine alongside a novel dataset aimed at
pinpointing and grasping personal objects given personal indicators via
learning from a single human-robot interaction, rather than a large labeled
dataset. Our proposed method, Personalized Grasping Agent (PGA), addresses
GraspMine by leveraging the unlabeled image data of the user's environment,
called Reminiscence. Specifically, PGA acquires personal object information by
a user presenting a personal object with its associated indicator, followed by
PGA inspecting the object by rotating it. Based on the acquired information,
PGA pseudo-labels objects in the Reminiscence by our proposed label propagation
algorithm. Harnessing the information acquired from the interactions and the
pseudo-labeled objects in the Reminiscence, PGA adapts the object grounding
model to grasp personal objects. This results in significant efficiency while
previous LCRG systems rely on resource-intensive human annotations --
necessitating hundreds of labeled data to learn my wallet. Moreover, PGA
outperforms baseline methods across all metrics and even shows comparable
performance compared to the fully-supervised method, which learns from 9k
annotated data samples. We further validate PGA's real-world applicability by
employing a physical robot to execute GrsapMine. Code and data are publicly
available at https://github.com/JHKim-snu/PGA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Goal-conditioned dual-action imitation learning for dexterous dual-arm
  robot manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09749v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09749v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heecheol Kim, Yoshiyuki Ohmura, Yasuo Kuniyoshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-horizon dexterous robot manipulation of deformable objects, such as
banana peeling, is a problematic task because of the difficulties in object
modeling and a lack of knowledge about stable and dexterous manipulation
skills. This paper presents a goal-conditioned dual-action (GC-DA) deep
imitation learning (DIL) approach that can learn dexterous manipulation skills
using human demonstration data. Previous DIL methods map the current sensory
input and reactive action, which often fails because of compounding errors in
imitation learning caused by the recurrent computation of actions. The method
predicts reactive action only when the precise manipulation of the target
object is required (local action) and generates the entire trajectory when
precise manipulation is not required (global action). This dual-action
formulation effectively prevents compounding error in the imitation learning
using the trajectory-based global action while responding to unexpected changes
in the target object during the reactive local action. The proposed method was
tested in a real dual-arm robot and successfully accomplished the
banana-peeling task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, published in Transactions on Robotics (T-RO)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Drones Guiding Drones: Cooperative Navigation of a Less-Equipped Micro
  Aerial Vehicle in Cluttered Environments <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09786v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09786v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Václav Pritzl, Matouš Vrba, Yurii Stasinchuk, Vít Krátký, Jiří Horyna, Petr Štěpán, Martin Saska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable deployment of Unmanned Aerial Vehicles (UAVs) in cluttered unknown
environments requires accurate sensors for Global Navigation Satellite System
(GNSS)-denied localization and obstacle avoidance. Such a requirement limits
the usage of cheap and micro-scale vehicles with constrained payload capacity
if industrial-grade reliability and precision are required. This paper
investigates the possibility of offloading the necessity to carry heavy sensors
to another member of the UAV team while preserving the desired capability of
the smaller robot intended for exploring narrow passages. A novel cooperative
guidance framework offloading the sensing requirements from a minimalistic
secondary UAV to a superior primary UAV is proposed. The primary UAV constructs
a dense occupancy map of the environment and plans collision-free paths for
both UAVs to ensure reaching the desired secondary UAV's goals even in areas
not accessible by the bigger robot. The primary UAV guides the secondary UAV to
follow the planned path while tracking the UAV using Light Detection and
Ranging (LiDAR)-based relative localization. The proposed approach was verified
in real-world experiments with a heterogeneous team of a 3D LiDAR-equipped
primary UAV and a micro-scale camera-equipped secondary UAV moving autonomously
through unknown cluttered GNSS-denied environments with the proposed framework
running fully on board the UAVs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, submitted to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MonoForce: <span class="highlight-title">Self-supervised</span> Learning of Physics-aware Model for
  Predicting Robot-terrain Interaction <span class="chip">IROS-2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09007v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09007v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruslan Agishev, Karel Zimmermann, Vladimír Kubelka, Martin Pecka, Tomáš Svoboda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While autonomous navigation of mobile robots on rigid terrain is a
well-explored problem, navigating on deformable terrain such as tall grass or
bushes remains a challenge. To address it, we introduce an explainable,
physics-aware and end-to-end differentiable model which predicts the outcome of
robot-terrain interaction from camera images, both on rigid and non-rigid
terrain. The proposed MonoForce model consists of a black-box module which
predicts robot-terrain interaction forces from onboard cameras, followed by a
white-box module, which transforms these forces and a control signals into
predicted trajectories, using only the laws of classical mechanics. The
differentiable white-box module allows backpropagating the predicted trajectory
errors into the black-box module, serving as a self-supervised loss that
measures consistency between the predicted forces and ground-truth trajectories
of the robot. Experimental evaluation on a public dataset and our data has
shown that while the prediction capabilities are comparable to state-of-the-art
algorithms on rigid terrain, MonoForce shows superior accuracy on non-rigid
terrain such as tall grass or bushes. To facilitate the reproducibility of our
results, we release both the code and datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, IROS-2024 submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Dense and Accurate Radar Perception Via Efficient Cross-Modal
  Diffusion Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08460v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08460v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruibin Zhang, Donglai Xue, Yuhan Wang, Ruixu Geng, Fei Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Millimeter wave (mmWave) radars have attracted significant attention from
both academia and industry due to their capability to operate in extreme
weather conditions. However, they face challenges in terms of sparsity and
noise interference, which hinder their application in the field of micro aerial
vehicle (MAV) autonomous navigation. To this end, this paper proposes a novel
approach to dense and accurate mmWave radar point cloud construction via
cross-modal learning. Specifically, we introduce diffusion models, which
possess state-of-the-art performance in generative modeling, to predict
LiDAR-like point clouds from paired raw radar data. We also incorporate the
most recent diffusion model inference accelerating techniques to ensure that
the proposed method can be implemented on MAVs with limited computing
resources.We validate the proposed method through extensive benchmark
comparisons and real-world experiments, demonstrating its superior performance
and generalization ability. Code and pretrained models will be available at
https://github.com/ZJU-FAST-Lab/Radar-Diffusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, submitted to RA-L</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HeLiPR: Heterogeneous LiDAR <span class="highlight-title">Dataset</span> for inter-LiDAR Place Recognition
  under Spatiotemporal Variations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14590v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14590v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minwoo Jung, Wooseong Yang, Dongjae Lee, Hyeonjae Gil, Giseop Kim, Ayoung Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Place recognition is crucial for robot localization and loop closure in
simultaneous localization and mapping (SLAM). Light Detection and Ranging
(LiDAR), known for its robust sensing capabilities and measurement consistency
even in varying illumination conditions, has become pivotal in various fields,
surpassing traditional imaging sensors in certain applications. Among various
types of LiDAR, spinning LiDARs are widely used, while non-repetitive scanning
patterns have recently been utilized in robotics applications. Some LiDARs
provide additional measurements such as reflectivity, Near Infrared (NIR), and
velocity from Frequency modulated continuous wave (FMCW) LiDARs. Despite these
advances, there is a lack of comprehensive datasets reflecting the broad
spectrum of LiDAR configurations for place recognition. To tackle this issue,
our paper proposes the HeLiPR dataset, curated especially for place recognition
with heterogeneous LiDARs, embodying spatiotemporal variations. To the best of
our knowledge, the HeLiPR dataset is the first heterogeneous LiDAR dataset
supporting inter-LiDAR place recognition with both non-repetitive and spinning
LiDARs, accommodating different field of view (FOV)s and varying numbers of
rays. The dataset covers diverse environments, from urban cityscapes to
high-dynamic freeways, over a month, enhancing adaptability and robustness
across scenarios. Notably, HeLiPR includes trajectories parallel to MulRan
sequences, making it valuable for research in heterogeneous LiDAR place
recognition and long-term studies. The dataset is accessible at
https://sites.google.com/view/heliprdataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 9 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chasing Day and Night: Towards Robust and Efficient All-Day Object
  Detection Guided by an Event Camera <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09297v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09297v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahang Cao, Xu Zheng, Yuanhuiyi Lyu, Jiaxu Wang, Renjing Xu, Lin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to detect objects in all lighting (i.e., normal-, over-, and
under-exposed) conditions is crucial for real-world applications, such as
self-driving.Traditional RGB-based detectors often fail under such varying
lighting conditions.Therefore, recent works utilize novel event cameras to
supplement or guide the RGB modality; however, these methods typically adopt
asymmetric network structures that rely predominantly on the RGB modality,
resulting in limited robustness for all-day detection. In this paper, we
propose EOLO, a novel object detection framework that achieves robust and
efficient all-day detection by fusing both RGB and event modalities. Our EOLO
framework is built based on a lightweight spiking neural network (SNN) to
efficiently leverage the asynchronous property of events. Buttressed by it, we
first introduce an Event Temporal Attention (ETA) module to learn the high
temporal information from events while preserving crucial edge information.
Secondly, as different modalities exhibit varying levels of importance under
diverse lighting conditions, we propose a novel Symmetric RGB-Event Fusion
(SREF) module to effectively fuse RGB-Event features without relying on a
specific modality, thus ensuring a balanced and adaptive fusion for all-day
detection. In addition, to compensate for the lack of paired RGB-Event datasets
for all-day training and evaluation, we propose an event synthesis approach
based on the randomized optical flow that allows for directly generating the
event frame from a single exposure image. We further build two new datasets,
E-MSCOCO and E-VOC based on the popular benchmarks MSCOCO and PASCAL VOC.
Extensive experiments demonstrate that our EOLO outperforms the
state-of-the-art detectors,e.g.,RENet,by a substantial margin (+3.74% mAP50) in
all lighting conditions.Our code and datasets will be available at
https://vlislab22.github.io/EOLO/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safe Planning through Incremental Decomposition of Signal Temporal Logic
  Specifications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10554v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10554v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parv Kapoor, Eunsuk Kang, Romulo Meira-Goes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory planning is a critical process that enables autonomous systems to
safely navigate complex environments. Signal temporal logic (STL)
specifications are an effective way to encode complex temporally extended
objectives for trajectory planning in cyber-physical systems (CPS). However,
planning from these specifications using existing techniques scale
exponentially with the number of nested operators and the horizon of
specification. Additionally, performance is exacerbated at runtime due to
limited computational budgets and compounding modeling errors. Decomposing a
complex specification into smaller subtasks and incrementally planning for them
can remedy these issues. In this work, we present a way to decompose STL
requirements temporally to improve planning efficiency and performance. The key
insight in our work is to encode all specifications as a set of reachability
and invariance constraints and scheduling these constraints sequentially at
runtime. Our proposed technique outperforms the state-of-the-art trajectory
synthesis techniques for both linear and non linear dynamical systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Nasa Formal Methods (NFM) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reinforcement Learning with Latent State Inference for Autonomous
  On-ramp Merging under Observation Delay 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11852v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11852v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amin Tabrizian, Zhitong Huang, Peng Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach to address the challenging problem of
autonomous on-ramp merging, where a self-driving vehicle needs to seamlessly
integrate into a flow of vehicles on a multi-lane highway. We introduce the
Lane-keeping, Lane-changing with Latent-state Inference and Safety Controller
(L3IS) agent, designed to perform the on-ramp merging task safely without
comprehensive knowledge about surrounding vehicles' intents or driving styles.
We also present an augmentation of this agent called AL3IS that accounts for
observation delays, allowing the agent to make more robust decisions in
real-world environments with vehicle-to-vehicle (V2V) communication delays. By
modeling the unobservable aspects of the environment through latent states,
such as other drivers' intents, our approach enhances the agent's ability to
adapt to dynamic traffic conditions, optimize merging maneuvers, and ensure
safe interactions with other vehicles. We demonstrate the effectiveness of our
method through extensive simulations generated from real traffic data and
compare its performance with existing approaches. L3IS shows a 99.90% success
rate in a challenging on-ramp merging case generated from the real US Highway
101 data. We further perform a sensitivity analysis on AL3IS to evaluate its
robustness against varying observation delays, which demonstrates an acceptable
performance of 93.84% success rate in 1-second V2V communication delay.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Think, Act, and Ask: Open-World Interactive Personalized Robot
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07968v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07968v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinpei Dai, Run Peng, Sikai Li, Joyce Chai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-Shot Object Navigation (ZSON) enables agents to navigate towards
open-vocabulary objects in unknown environments. The existing works of ZSON
mainly focus on following individual instructions to find generic object
classes, neglecting the utilization of natural language interaction and the
complexities of identifying user-specific objects. To address these
limitations, we introduce Zero-shot Interactive Personalized Object Navigation
(ZIPON), where robots need to navigate to personalized goal objects while
engaging in conversations with users. To solve ZIPON, we propose a new
framework termed Open-woRld Interactive persOnalized Navigation (ORION), which
uses Large Language Models (LLMs) to make sequential decisions to manipulate
different modules for perception, navigation and communication. Experimental
results show that the performance of interactive agents that can leverage user
feedback exhibits significant improvement. However, obtaining a good balance
between task completion and the efficiency of navigation and interaction
remains challenging for all methods. We further provide more findings on the
impact of diverse user feedback forms on the agents' performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Video URL: https://www.youtube.com/watch?v=rN5S8QIhhQc Code URL:
  https://github.com/sled-group/navchat</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sensor Fault Detection and Compensation with Performance Prescription
  for Robotic Manipulators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.19157v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.19157v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S. Mohammadreza Ebrahimi, Farid Norouzi, Hossein Dastres, Reza Faieghi, Mehdi Naderi, Milad Malekzadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on sensor fault detection and compensation for robotic
manipulators. The proposed method features a new adaptive observer and a new
terminal sliding mode control law established on a second-order integral
sliding surface. The method enables sensor fault detection without the need to
know the bounds on fault value and/or its derivative. It also enables fast and
fixed-time fault-tolerant control whose performance can be prescribed
beforehand by defining funnel bounds on the tracking error. The ultimate
boundedness of the estimation errors for the proposed observer and the
fixed-time stability of the control system are shown using Lyapunov stability
analysis. The effectiveness of the proposed method is verified using numerical
simulations on two different robotic manipulators, and the results are compared
with existing methods. Our results demonstrate performance gains obtained by
the proposed method compared to the existing results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Granger-Causal Hierarchical Skill Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.09509v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.09509v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caleb Chuck, Kevin Black, Aditya Arjun, Yuke Zhu, Scott Niekum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) has demonstrated promising results in learning
policies for complex tasks, but it often suffers from low sample efficiency and
limited transferability. Hierarchical RL (HRL) methods aim to address the
difficulty of learning long-horizon tasks by decomposing policies into skills,
abstracting states, and reusing skills in new tasks. However, many HRL methods
require some initial task success to discover useful skills, which
paradoxically may be very unlikely without access to useful skills. On the
other hand, reward-free HRL methods often need to learn far too many skills to
achieve proper coverage in high-dimensional domains. In contrast, we introduce
the Chain of Interaction Skills (COInS) algorithm, which focuses on
controllability in factored domains to identify a small number of task-agnostic
skills that still permit a high degree of control. COInS uses learned detectors
to identify interactions between state factors and then trains a chain of
skills to control each of these factors successively. We evaluate COInS on a
robotic pushing task with obstacles-a challenging domain where other RL and HRL
methods fall short. We also demonstrate the transferability of skills learned
by COInS, using variants of Breakout, a common RL benchmark, and show 2-3x
improvement in both sample efficiency and final performance compared to
standard RL baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted TMLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Locomotion Generation for a Rat Robot based on Environmental Changes via
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11788v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11788v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinhui Shan, Yuhong Huang, Zhenshan Bing, Zitao Zhang, Xiangtong Yao, Kai Huang, Alois Knoll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research focuses on developing reinforcement learning approaches for the
locomotion generation of small-size quadruped robots. The rat robot NeRmo is
employed as the experimental platform. Due to the constrained volume,
small-size quadruped robots typically possess fewer and weaker sensors,
resulting in difficulty in accurately perceiving and responding to
environmental changes. In this context, insufficient and imprecise feedback
data from sensors makes it difficult to generate adaptive locomotion based on
reinforcement learning. To overcome these challenges, this paper proposes a
novel reinforcement learning approach that focuses on extracting effective
perceptual information to enhance the environmental adaptability of small-size
quadruped robots. According to the frequency of a robot's gait stride, key
information of sensor data is analyzed utilizing sinusoidal functions derived
from Fourier transform results. Additionally, a multifunctional reward
mechanism is proposed to generate adaptive locomotion in different tasks.
Extensive simulations are conducted to assess the effectiveness of the proposed
reinforcement learning approach in generating rat robot locomotion in various
environments. The experiment results illustrate the capability of the proposed
approach to maintain stable locomotion of a rat robot across different
terrains, including ramps, stairs, and spiral stairs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OSDaR23: Open Sensor Data for Rail 2023 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03001v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03001v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rustam Tagiew, Martin Köppel, Karsten Schwalbe, Patrick Denzler, Philipp Neumaier, Tobias Klockau, Martin Boekhoff, Pavel Klasek, Roman Tilly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To achieve a driverless train operation on mainline railways, actual and
potential obstacles for the train's driveway must be detected automatically by
appropriate sensor systems. Machine learning algorithms have proven to be
powerful tools for this task during the last years. However, these algorithms
require large amounts of high-quality annotated data containing
railway-specific objects as training data. Unfortunately, all of the publicly
available datasets that tackle this requirement are restricted in some way.
Therefore, this paper presents OSDaR23, a multi-sensor dataset of 45
subsequences acquired in Hamburg, Germany, in September 2021, that was created
to foster driverless train operation on mainline railways. The sensor setup
consists of multiple calibrated and synchronized infrared (IR) and visual (RGB)
cameras, lidars, a radar, and position and acceleration sensors mounted on the
front of a rail vehicle. In addition to the raw data, the dataset contains
204091 polyline, polygonal, rectangle, and cuboid annotations in total for 20
different object classes. It is the first publicly available multi-sensor
dataset annotated with a variety of object classes that are relevant for the
railway context. OSDaR23, available at data.fid-move.de/dataset/osdar23, can
also be used for tasks beyond collision prediction, which are listed in this
paper.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 11 images, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Optimization-based Control for Whole-body Loco-manipulation
  of Heavy Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.00112v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.00112v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alberto Rigo, Muqun Hu, Satyandra K. Gupta, Quan Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the field of legged robotics has seen growing interest in
enhancing the capabilities of these robots through the integration of
articulated robotic arms. However, achieving successful loco-manipulation,
especially involving interaction with heavy objects, is far from
straightforward, as object manipulation can introduce substantial disturbances
that impact the robot's locomotion. This paper presents a novel framework for
legged loco-manipulation that considers whole-body coordination through a
hierarchical optimization-based control framework. First, an online
manipulation planner computes the manipulation forces and manipulated object
task-based reference trajectory. Then, pose optimization aligns the robot's
trajectory with kinematic constraints. The resultant robot reference trajectory
is executed via a linear MPC controller incorporating the desired manipulation
forces into its prediction model. Our approach has been validated in simulation
and hardware experiments, highlighting the necessity of whole-body optimization
compared to the baseline locomotion MPC when interacting with heavy objects.
Experimental results with Unitree Aliengo, equipped with a custom-made robotic
arm, showcase its ability to lift and carry an 8kg payload and manipulate
doors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Agile Locomotion and Adaptive Behaviors via RL-augmented MPC 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09442v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09442v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyu Chen, Quan Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of legged robots, adaptive behavior involves adaptive
balancing and adaptive swing foot reflection. While adaptive balancing
counteracts perturbations to the robot, adaptive swing foot reflection helps
the robot to navigate intricate terrains without foot entrapment. In this
paper, we manage to bring both aspects of adaptive behavior to quadruped
locomotion by combining RL and MPC while improving the robustness and agility
of blind legged locomotion. This integration leverages MPC's strength in
predictive capabilities and RL's adeptness in drawing from past experiences.
Unlike traditional locomotion controls that separate stance foot control and
swing foot trajectory, our innovative approach unifies them, addressing their
lack of synchronization. At the heart of our contribution is the synthesis of
stance foot control with swing foot reflection, improving agility and
robustness in locomotion with adaptive behavior. A hallmark of our approach is
robust blind stair climbing through swing foot reflection. Moreover, we
intentionally designed the learning module as a general plugin for different
robot platforms. We trained the policy and implemented our approach on the
Unitree A1 robot, achieving impressive results: a peak turn rate of 8.5 rad/s,
a peak running speed of 3 m/s, and steering at a speed of 2.5 m/s. Remarkably,
this framework also allows the robot to maintain stable locomotion while
bearing an unexpected load of 10 kg, or 83\% of its body mass. We further
demonstrate the generalizability and robustness of the same policy where it
realizes zero-shot transfer to different robot platforms like Go1 and AlienGo
robots for load carrying. Code is made available for the use of the research
community at https://github.com/DRCL-USC/RL_augmented_MPC.git
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LANCAR: Leveraging Language for Context-Aware Robot Locomotion in
  Unstructured Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00481v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00481v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chak Lam Shek, Xiyang Wu, Wesley A. Suttle, Carl Busart, Erin Zaroukian, Dinesh Manocha, Pratap Tokekar, Amrit Singh Bedi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Navigating robots through unstructured terrains is challenging, primarily due
to the dynamic environmental changes. While humans adeptly navigate such
terrains by using context from their observations, creating a similar
context-aware navigation system for robots is difficult. The essence of the
issue lies in the acquisition and interpretation of contextual information, a
task complicated by the inherent ambiguity of human language. In this work, we
introduce LANCAR, which addresses this issue by combining a context translator
with reinforcement learning (RL) agents for context-aware locomotion. LANCAR
allows robots to comprehend contextual information through Large Language
Models (LLMs) sourced from human observers and convert this information into
actionable contextual embeddings. These embeddings, combined with the robot's
sensor data, provide a complete input for the RL agent's policy network. We
provide an extensive evaluation of LANCAR under different levels of contextual
ambiguity and compare with alternative methods. The experimental results
showcase the superior generalizability and adaptability across different
terrains. Notably, LANCAR shows at least a 7.4% increase in episodic reward
over the best alternatives, highlighting its potential to enhance robotic
navigation in unstructured environments. More details and experiment videos
could be found in http://raaslab.org/projects/LLM_Context_Estimation/.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-18T00:00:00Z">2024-03-18</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">75</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BEVCar: Camera-Radar Fusion for BEV Map and Object Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Schramm, Niclas Vödisch, Kürsat Petek, B Ravi Kiran, Senthil Yogamani, Wolfram Burgard, Abhinav Valada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic scene segmentation from a bird's-eye-view (BEV) perspective plays a
crucial role in facilitating planning and decision-making for mobile robots.
Although recent vision-only methods have demonstrated notable advancements in
performance, they often struggle under adverse illumination conditions such as
rain or nighttime. While active sensors offer a solution to this challenge, the
prohibitively high cost of LiDARs remains a limiting factor. Fusing camera data
with automotive radars poses a more inexpensive alternative but has received
less attention in prior research. In this work, we aim to advance this
promising avenue by introducing BEVCar, a novel approach for joint BEV object
and map segmentation. The core novelty of our approach lies in first learning a
point-based encoding of raw radar data, which is then leveraged to efficiently
initialize the lifting of image features into the BEV space. We perform
extensive experiments on the nuScenes dataset and demonstrate that BEVCar
outperforms the current state of the art. Moreover, we show that incorporating
radar information significantly enhances robustness in challenging
environmental conditions and improves segmentation performance for distant
objects. To foster future research, we provide the weather split of the
nuScenes dataset used in our experiments, along with our code and trained
models at http://bevcar.cs.uni-freiburg.de.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerating Model Predictive Control for Legged Robots through
  Distributed Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Amatucci, Giulio Turrisi, Angelo Bratta, Victor Barasuol, Claudio Semini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach to enhance Model Predictive Control
(MPC) for legged robots through Distributed Optimization. Our method focuses on
decomposing the robot dynamics into smaller, parallelizable subsystems, and
utilizing the Alternating Direction Method of Multipliers (ADMM) to ensure
consensus among them. Each subsystem is managed by its own \gls{ocp}, with ADMM
facilitating consistency between their optimizations. This approach not only
decreases the computational time but also allows for effective scaling with
more complex robot configurations, facilitating the integration of additional
subsystems such as articulated arms on a quadruped robot. We demonstrate,
through numerical evaluations, the convergence of our approach on two systems
with increasing complexity. In addition, we showcase that our approach
converges towards the same solution when compared to a state-of-the-art
centralized whole-body MPC implementation. Moreover, we quantitatively compare
the computational efficiency of our method to the centralized approach,
revealing up to a 75\% reduction in computational time. Overall, our approach
offers a promising avenue for accelerating MPC solutions for legged robots,
paving the way for more effective utilization of the computational performance
of modern hardware.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SMT-Based Dynamic Multi-Robot Task Allocation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victoria Marie Tuck, Pei-Wei Chen, Georgios Fainekos, Bardh Hoxha, Hideki Okamoto, S. Shankar Sastry, Sanjit A. Seshia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Robot Task Allocation (MRTA) is a problem that arises in many
application domains including package delivery, warehouse robotics, and
healthcare. In this work, we consider the problem of MRTA for a dynamic stream
of tasks with task deadlines and capacitated agents (capacity for more than one
simultaneous task). Previous work commonly focuses on the static case, uses
specialized algorithms for restrictive task specifications, or lacks
guarantees. We propose an approach to Dynamic MRTA for capacitated robots that
is based on Satisfiability Modulo Theories (SMT) solving and addresses these
concerns. We show our approach is both sound and complete, and that the SMT
encoding is general, enabling extension to a broader class of task
specifications. We show how to leverage the incremental solving capabilities of
SMT solvers, keeping learned information when allocating new tasks arriving
online, and to solve non-incrementally, which we provide runtime comparisons
of. Additionally, we provide an algorithm to start with a smaller but
potentially incomplete encoding that can iteratively be adjusted to the
complete encoding. We evaluate our method on a parameterized set of benchmarks
encoding multi-robot delivery created from a graph abstraction of a
hospital-like environment. The effectiveness of our approach is demonstrated
using a range of encodings, including quantifier-free theories of uninterpreted
functions and linear or bitvector arithmetic across multiple solvers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 6 figures, to be published in NASA Formal Methods Symposium
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hardware Design and Learning-Based Software Architecture of
  Musculoskeletal Wheeled Robot Musashi-W for Real-World Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kento Kawaharazuka, Akihiro Miki, Masahiro Bando, Temma Suzuki, Yoshimoto Ribayashi, Yasunori Toshimitsu, Yuya Nagamatsu, Kei Okada, and Masayuki Inaba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Various musculoskeletal humanoids have been developed so far. While these
humanoids have the advantage of their flexible and redundant bodies that mimic
the human body, they are still far from being applied to real-world tasks. One
of the reasons for this is the difficulty of bipedal walking in a flexible
body. Thus, we developed a musculoskeletal wheeled robot, Musashi-W, by
combining a wheeled base and musculoskeletal upper limbs for real-world
applications. Also, we constructed its software system by combining static and
dynamic body schema learning, reflex control, and visual recognition. We show
that the hardware and software of Musashi-W can make the most of the advantages
of the musculoskeletal upper limbs, through several tasks of cleaning by human
teaching, carrying a heavy object considering muscle addition, and setting a
table through dynamic cloth manipulation with variable stiffness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Humanoids2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PITA: Physics-Informed Trajectory Autoencoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes Fischer, Kevin Rösch, Martin Lauer, Christoph Stiller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Validating robotic systems in safety-critical appli-cations requires testing
in many scenarios including rare edgecases that are unlikely to occur,
requiring to complement real-world testing with testing in simulation.
Generative models canbe used to augment real-world datasets with generated data
toproduce edge case scenarios by sampling in a learned latentspace.
Autoencoders can learn said latent representation for aspecific domain by
learning to reconstruct the input data froma lower-dimensional intermediate
representation. However, theresulting trajectories are not necessarily
physically plausible, butinstead typically contain noise that is not present in
the inputtrajectory. To resolve this issue, we propose the novel
Physics-Informed Trajectory Autoencoder (PITA) architecture, whichincorporates
a physical dynamics model into the loss functionof the autoencoder. This
results in smooth trajectories that notonly reconstruct the input trajectory
but also adhere to thephysical model. We evaluate PITA on a real-world dataset
ofvehicle trajectories and compare its performance to a normalautoencoder and a
state-of-the-art action-space autoencoder.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MASSTAR: A Multi-Modal and Large-Scale Scene <span class="highlight-title">Dataset</span> with a Versatile
  Toolchain for Surface Prediction and Completion <span class="chip">IROS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guiyong Zheng, Jinqi Jiang, Chen Feng, Shaojie Shen, Boyu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surface prediction and completion have been widely studied in various
applications. Recently, research in surface completion has evolved from small
objects to complex large-scale scenes. As a result, researchers have begun
increasing the volume of data and leveraging a greater variety of data
modalities including rendered RGB images, descriptive texts, depth images, etc,
to enhance algorithm performance. However, existing datasets suffer from a
deficiency in the amounts of scene-level models along with the corresponding
multi-modal information. Therefore, a method to scale the datasets and generate
multi-modal information in them efficiently is essential. To bridge this
research gap, we propose MASSTAR: a Multi-modal lArge-scale Scene dataset with
a verSatile Toolchain for surfAce pRediction and completion. We develop a
versatile and efficient toolchain for processing the raw 3D data from the
environments. It screens out a set of fine-grained scene models and generates
the corresponding multi-modal data. Utilizing the toolchain, we then generate
an example dataset composed of over a thousand scene-level models with partial
real-world data added. We compare MASSTAR with the existing datasets, which
validates its superiority: the ability to efficiently extract high-quality
models from complex scenarios to expand the dataset. Additionally, several
representative surface completion algorithms are benchmarked on MASSTAR, which
reveals that existing algorithms can hardly deal with scene-level completion.
We will release the source code of our toolchain and the dataset. For more
details, please see our project page at https://sysu-star.github.io/MASSTAR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS2024. Code: https://github.com/SYSU-STAR/MASSTAR.
  Project Page: https://github.com/SYSU-STAR/MASSTAR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NEDS-SLAM: A Novel Neural Explicit Dense Semantic SLAM Framework using
  3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Ji, Yang Liu, Guanghu Xie, Boyu Ma, Zongwu Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose NEDS-SLAM, an Explicit Dense semantic SLAM system based on 3D
Gaussian representation, that enables robust 3D semantic mapping, accurate
camera tracking, and high-quality rendering in real-time. In the system, we
propose a Spatially Consistent Feature Fusion model to reduce the effect of
erroneous estimates from pre-trained segmentation head on semantic
reconstruction, achieving robust 3D semantic Gaussian mapping. Additionally, we
employ a lightweight encoder-decoder to compress the high-dimensional semantic
features into a compact 3D Gaussian representation, mitigating the burden of
excessive memory consumption. Furthermore, we leverage the advantage of 3D
Gaussian splatting, which enables efficient and differentiable novel view
rendering, and propose a Virtual Camera View Pruning method to eliminate
outlier GS points, thereby effectively enhancing the quality of scene
representations. Our NEDS-SLAM method demonstrates competitive performance over
existing dense semantic SLAM methods in terms of mapping and tracking accuracy
on Replica and ScanNet datasets, while also showing excellent capabilities in
3D dense semantic mapping.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FE-DeTr: Keypoint Detection and Tracking in Low-quality Image Frames
  with Events <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11662v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11662v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyuan Wang, Kuangyi Chen, Wen Yang, Lei Yu, Yannan Xing, Huai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Keypoint detection and tracking in traditional image frames are often
compromised by image quality issues such as motion blur and extreme lighting
conditions. Event cameras offer potential solutions to these challenges by
virtue of their high temporal resolution and high dynamic range. However, they
have limited performance in practical applications due to their inherent noise
in event data. This paper advocates fusing the complementary information from
image frames and event streams to achieve more robust keypoint detection and
tracking. Specifically, we propose a novel keypoint detection network that
fuses the textural and structural information from image frames with the
high-temporal-resolution motion information from event streams, namely FE-DeTr.
The network leverages a temporal response consistency for supervision, ensuring
stable and efficient keypoint detection. Moreover, we use a spatio-temporal
nearest-neighbor search strategy for robust keypoint tracking. Extensive
experiments are conducted on a new dataset featuring both image frames and
event data captured under extreme conditions. The experimental results confirm
the superior performance of our method over both existing frame-based and
event-based methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, Accepted by ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combining Local and Global Perception for Autonomous Navigation on
  Nano-UAVs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11661v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11661v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Lamberti, Georg Rutishauser, Francesco Conti, Luca Benini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A critical challenge in deploying unmanned aerial vehicles (UAVs) for
autonomous tasks is their ability to navigate in an unknown environment. This
paper introduces a novel vision-depth fusion approach for autonomous navigation
on nano-UAVs. We combine the visual-based PULP-Dronet convolutional neural
network for semantic information extraction, i.e., serving as the global
perception, with 8x8px depth maps for close-proximity maneuvers, i.e., the
local perception. When tested in-field, our integration strategy highlights the
complementary strengths of both visual and depth sensory information. We
achieve a 100% success rate over 15 flights in a complex navigation scenario,
encompassing straight pathways, static obstacle avoidance, and 90{\deg} turns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures, 1 table, 1 video</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion-Based Environment-Aware Trajectory Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theodor Westny, Björn Olofsson, Erik Frisk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to predict the future trajectories of traffic participants is
crucial for the safe and efficient operation of autonomous vehicles. In this
paper, a diffusion-based generative model for multi-agent trajectory prediction
is proposed. The model is capable of capturing the complex interactions between
traffic participants and the environment, accurately learning the multimodal
nature of the data. The effectiveness of the approach is assessed on
large-scale datasets of real-world traffic scenarios, showing that our model
outperforms several well-established methods in terms of prediction accuracy.
By the incorporation of differential motion constraints on the model output, we
illustrate that our model is capable of generating a diverse set of realistic
future trajectories. Through the use of an interaction-aware guidance signal,
we further demonstrate that the model can be adapted to predict the behavior of
less cooperative agents, emphasizing its practical applicability under
uncertain traffic conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Accurate and Real-time Relative Pose Estimation from Triple
  Point-line Images by Decoupling Rotation and Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11639v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11639v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zewen Xu, Yijia He, Hao Wei, Bo Xu, BinJian Xie, Yihong Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Line features are valid complements for point features in man-made
environments. 3D-2D constraints provided by line features have been widely used
in Visual Odometry (VO) and Structure-from-Motion (SfM) systems. However, how
to accurately solve three-view relative motion only with 2D observations of
points and lines in real time has not been fully explored. In this paper, we
propose a novel three-view pose solver based on rotation-translation decoupled
estimation. First, a high-precision rotation estimation method based on normal
vector coplanarity constraints that consider the uncertainty of observations is
proposed, which can be solved by Levenberg-Marquardt (LM) algorithm
efficiently. Second, a robust linear translation constraint that minimizes the
degree of the rotation components and feature observation components in
equations is elaborately designed for estimating translations accurately.
Experiments on synthetic data and real-world data show that the proposed
approach improves both rotation and translation accuracy compared to the
classical trifocal-tensor-based method and the state-of-the-art two-view
algorithm in outdoor and indoor environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthesizing multi-log grasp poses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arvid Fälldin, Erik Wallin, Tommy Löfstedt, Martin Servin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-object grasping is a challenging task. It is important for energy and
cost-efficient operation of industrial crane manipulators, such as those used
to collect tree logs off the forest floor and onto forest machines. In this
work, we used synthetic data from physics simulations to explore how
data-driven modeling can be used to infer multi-object grasp poses from images.
We showed that convolutional neural networks can be trained specifically for
synthesizing multi-object grasps. Using RGB-Depth images and instance
segmentation masks as input, a U-Net model outputs grasp maps with
corresponding grapple orientation and opening width. Given an observation of a
pile of logs, the model can be used to synthesize and rate the possible grasp
poses and select the most suitable one, with the possibility to respect
changing operational constraints such as lift capacity and reach. When tested
on previously unseen data, the proposed model found successful grasp poses with
an accuracy of 95%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Frontier-Based Exploration for Multi-Robot Rendezvous in
  Communication-Restricted Unknown Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mauro Tellaroli, Matteo Luperto, Michele Antonazzi, Nicola Basilico
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-robot rendezvous and exploration are fundamental challenges in the
domain of mobile robotic systems. This paper addresses multi-robot rendezvous
within an initially unknown environment where communication is only possible
after the rendezvous. Traditionally, exploration has been focused on rapidly
mapping the environment, often leading to suboptimal rendezvous performance in
later stages. We adapt a standard frontier-based exploration technique to
integrate exploration and rendezvous into a unified strategy, with a mechanism
that allows robots to re-visit previously explored regions thus enhancing
rendezvous opportunities. We validate our approach in 3D realistic simulations
using ROS, showcasing its effectiveness in achieving faster rendezvous times
compared to exploration strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AGRNav: Efficient and Energy-Saving Autonomous Navigation for Air-Ground
  Robots in Occlusion-Prone Environments <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junming Wang, Zekai Sun, Xiuxian Guan, Tianxiang Shen, Zongyuan Zhang, Tianyang Duan, Dong Huang, Shixiong Zhao, Heming Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The exceptional mobility and long endurance of air-ground robots are raising
interest in their usage to navigate complex environments (e.g., forests and
large buildings). However, such environments often contain occluded and unknown
regions, and without accurate prediction of unobserved obstacles, the movement
of the air-ground robot often suffers a suboptimal trajectory under existing
mapping-based and learning-based navigation methods. In this work, we present
AGRNav, a novel framework designed to search for safe and energy-saving
air-ground hybrid paths. AGRNav contains a lightweight semantic scene
completion network (SCONet) with self-attention to enable accurate obstacle
predictions by capturing contextual information and occlusion area features.
The framework subsequently employs a query-based method for low-latency updates
of prediction results to the grid map. Finally, based on the updated map, the
hierarchical path planner efficiently searches for energy-saving paths for
navigation. We validate AGRNav's performance through benchmarks in both
simulated and real-world environments, demonstrating its superiority over
classical and state-of-the-art methods. The open-source code is available at
https://github.com/jmwang0117/AGRNav.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3DGS-Calib: 3D Gaussian Splatting for Multimodal SpatioTemporal
  Calibration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quentin Herau, Moussab Bennehar, Arthur Moreau, Nathan Piasco, Luis Roldao, Dzmitry Tsishkou, Cyrille Migniot, Pascal Vasseur, Cédric Demonceaux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable multimodal sensor fusion algorithms re- quire accurate
spatiotemporal calibration. Recently, targetless calibration techniques based
on implicit neural representations have proven to provide precise and robust
results. Nevertheless, such methods are inherently slow to train given the high
compu- tational overhead caused by the large number of sampled points required
for volume rendering. With the recent introduction of 3D Gaussian Splatting as
a faster alternative to implicit representation methods, we propose to leverage
this new ren- dering approach to achieve faster multi-sensor calibration. We
introduce 3DGS-Calib, a new calibration method that relies on the speed and
rendering accuracy of 3D Gaussian Splatting to achieve multimodal
spatiotemporal calibration that is accurate, robust, and with a substantial
speed-up compared to methods relying on implicit neural representations. We
demonstrate the superiority of our proposal with experimental results on
sequences from KITTI-360, a widely used driving dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ R2SNet: Scalable Domain Adaptation for Object Detection in Cloud-Based
  Robots Ecosystems via Proposal Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11567v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11567v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michele Antonazzi, Matteo Luperto, N. Alberto Borghese, Nicola Basilico
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel approach for scalable domain adaptation in cloud
robotics scenarios where robots rely on third-party AI inference services
powered by large pre-trained deep neural networks. Our method is based on a
downstream proposal-refinement stage running locally on the robots, exploiting
a new lightweight DNN architecture, R2SNet. This architecture aims to mitigate
performance degradation from domain shifts by adapting the object detection
process to the target environment, focusing on relabeling, rescoring, and
suppression of bounding-box proposals. Our method allows for local execution on
robots, addressing the scalability challenges of domain adaptation without
incurring significant computational costs. Real-world results on mobile service
robots performing door detection show the effectiveness of the proposed method
in achieving scalable domain adaptation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM^3:Large Language Model-based Task and Motion Planning with Motion
  Failure Reasoning <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu Wang, Muzhi Han, Ziyuan Jiao, Zeyu Zhang, Ying Nian Wu, Song-Chun Zhu, Hangxin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional Task and Motion Planning (TAMP) approaches rely on manually
crafted interfaces connecting symbolic task planning with continuous motion
generation. These domain-specific and labor-intensive modules are limited in
addressing emerging tasks in real-world settings. Here, we present LLM^3, a
novel Large Language Model (LLM)-based TAMP framework featuring a
domain-independent interface. Specifically, we leverage the powerful reasoning
and planning capabilities of pre-trained LLMs to propose symbolic action
sequences and select continuous action parameters for motion planning.
Crucially, LLM^3 incorporates motion planning feed- back through prompting,
allowing the LLM to iteratively refine its proposals by reasoning about motion
failure. Consequently, LLM^3 interfaces between task planning and motion
planning, alleviating the intricate design process of handling domain- specific
messages between them. Through a series of simulations in a box-packing domain,
we quantitatively demonstrate the effectiveness of LLM^3 in solving TAMP
problems and the efficiency in selecting action parameters. Ablation studies
un- derscore the significant contribution of motion failure reasoning to the
success of LLM^3. Furthermore, we conduct qualitative experiments on a physical
manipulator, demonstrating the practical applicability of our approach in
real-world settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS 2024. Codes available:
  https://github.com/AssassinWS/LLM-TAMP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SSAP: A Shape-Sensitive Adversarial Patch for Comprehensive Disruption
  of Monocular Depth Estimation in Autonomous Navigation Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11515v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11515v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amira Guesmi, Muhammad Abdullah Hanif, Ihsen Alouani, Bassem Ouni, Muhammad Shafique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular depth estimation (MDE) has advanced significantly, primarily
through the integration of convolutional neural networks (CNNs) and more
recently, Transformers. However, concerns about their susceptibility to
adversarial attacks have emerged, especially in safety-critical domains like
autonomous driving and robotic navigation. Existing approaches for assessing
CNN-based depth prediction methods have fallen short in inducing comprehensive
disruptions to the vision system, often limited to specific local areas. In
this paper, we introduce SSAP (Shape-Sensitive Adversarial Patch), a novel
approach designed to comprehensively disrupt monocular depth estimation (MDE)
in autonomous navigation applications. Our patch is crafted to selectively
undermine MDE in two distinct ways: by distorting estimated distances or by
creating the illusion of an object disappearing from the system's perspective.
Notably, our patch is shape-sensitive, meaning it considers the specific shape
and scale of the target object, thereby extending its influence beyond
immediate proximity. Furthermore, our patch is trained to effectively address
different scales and distances from the camera. Experimental results
demonstrate that our approach induces a mean depth estimation error surpassing
0.5, impacting up to 99% of the targeted region for CNN-based MDE models.
Additionally, we investigate the vulnerability of Transformer-based MDE models
to patch-based attacks, revealing that SSAP yields a significant error of 0.59
and exerts substantial influence over 99% of the target region on these models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Preference Inference: An Image Sequence-Based Preference
  Reasoning in Tabletop Object Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11513v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11513v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joonhyung Lee, Sangbeom Park, Yongin Kwon, Jemin Lee, Minwook Ahn, Sungjoon Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In robotic object manipulation, human preferences can often be influenced by
the visual attributes of objects, such as color and shape. These properties
play a crucial role in operating a robot to interact with objects and align
with human intention. In this paper, we focus on the problem of inferring
underlying human preferences from a sequence of raw visual observations in
tabletop manipulation environments with a variety of object types, named Visual
Preference Inference (VPI). To facilitate visual reasoning in the context of
manipulation, we introduce the Chain-of-Visual-Residuals (CoVR) method. CoVR
employs a prompting mechanism that describes the difference between the
consecutive images (i.e., visual residuals) and incorporates such texts with a
sequence of images to infer the user's preference. This approach significantly
enhances the ability to understand and adapt to dynamic changes in its visual
environment during manipulation tasks. Furthermore, we incorporate such texts
along with a sequence of images to infer the user's preferences. Our method
outperforms baseline methods in terms of extracting human preferences from
visual sequences in both simulation and real-world environments. Code and
videos are available at:
\href{https://joonhyung-lee.github.io/vpi/}{https://joonhyung-lee.github.io/vpi/}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sim-to-Real Grasp Detection with Global-to-Local RGB-D Adaptation <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11511v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11511v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxiang Ma, Ran Qin, Modi shi, Boyang Gao, Di Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on the sim-to-real issue of RGB-D grasp detection and
formulates it as a domain adaptation problem. In this case, we present a
global-to-local method to address hybrid domain gaps in RGB and depth data and
insufficient multi-modal feature alignment. First, a self-supervised rotation
pre-training strategy is adopted to deliver robust initialization for RGB and
depth networks. We then propose a global-to-local alignment pipeline with
individual global domain classifiers for scene features of RGB and depth images
as well as a local one specifically working for grasp features in the two
modalities. In particular, we propose a grasp prototype adaptation module,
which aims to facilitate fine-grained local feature alignment by dynamically
updating and matching the grasp prototypes from the simulation and real-world
scenarios throughout the training process. Due to such designs, the proposed
method substantially reduces the domain shift and thus leads to consistent
performance improvements. Extensive experiments are conducted on the
GraspNet-Planar benchmark and physical environment, and superior results are
achieved which demonstrate the effectiveness of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MCD: Diverse Large-Scale Multi-Campus <span class="highlight-title">Dataset</span> for Robot Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thien-Minh Nguyen, Shenghai Yuan, Thien Hoang Nguyen, Pengyu Yin, Haozhi Cao, Lihua Xie, Maciej Wozniak, Patric Jensfelt, Marko Thiel, Justin Ziegenbein, Noel Blunder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perception plays a crucial role in various robot applications. However,
existing well-annotated datasets are biased towards autonomous driving
scenarios, while unlabelled SLAM datasets are quickly over-fitted, and often
lack environment and domain variations. To expand the frontier of these fields,
we introduce a comprehensive dataset named MCD (Multi-Campus Dataset),
featuring a wide range of sensing modalities, high-accuracy ground truth, and
diverse challenging environments across three Eurasian university campuses. MCD
comprises both CCS (Classical Cylindrical Spinning) and NRE (Non-Repetitive
Epicyclic) lidars, high-quality IMUs (Inertial Measurement Units), cameras, and
UWB (Ultra-WideBand) sensors. Furthermore, in a pioneering effort, we introduce
semantic annotations of 29 classes over 59k sparse NRE lidar scans across three
domains, thus providing a novel challenge to existing semantic segmentation
research upon this largely unexplored lidar modality. Finally, we propose, for
the first time to the best of our knowledge, continuous-time ground truth based
on optimization-based registration of lidar-inertial data on large survey-grade
prior maps, which are also publicly released, each several times the size of
existing ones. We conduct a rigorous evaluation of numerous state-of-the-art
algorithms on MCD, report their performance, and highlight the challenges
awaiting solutions from the research community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by The IEEE/CVF Conference on Computer Vision and Pattern
  Recognition 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SmartRefine: An Scenario-Adaptive Refinement Framework for Efficient
  Motion Prediction <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Zhou, Hao Shao, Letian Wang, Steven L. Waslander, Hongsheng Li, Yu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting the future motion of surrounding agents is essential for
autonomous vehicles (AVs) to operate safely in dynamic, human-robot-mixed
environments. Context information, such as road maps and surrounding agents'
states, provides crucial geometric and semantic information for motion behavior
prediction. To this end, recent works explore two-stage prediction frameworks
where coarse trajectories are first proposed, and then used to select critical
context information for trajectory refinement. However, they either incur a
large amount of computation or bring limited improvement, if not both. In this
paper, we introduce a novel scenario-adaptive refinement strategy, named
SmartRefine, to refine prediction with minimal additional computation.
Specifically, SmartRefine can comprehensively adapt refinement configurations
based on each scenario's properties, and smartly chooses the number of
refinement iterations by introducing a quality score to measure the prediction
quality and remaining refinement potential of each scenario. SmartRefine is
designed as a generic and flexible approach that can be seamlessly integrated
into most state-of-the-art motion prediction models. Experiments on Argoverse
(1 & 2) show that our method consistently improves the prediction accuracy of
multiple state-of-the-art prediction models. Specifically, by adding
SmartRefine to QCNet, we outperform all published ensemble-free works on the
Argoverse 2 leaderboard (single agent track) at submission. Comprehensive
studies are also conducted to ablate design choices and explore the mechanism
behind multi-iteration refinement. Codes are available at
https://github.com/opendilab/SmartRefine/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version for CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can LLMs Generate Human-Like Wayfinding Instructions? Towards
  Platform-Agnostic Embodied Instruction Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11487v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11487v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishnu Sashank Dorbala, Sanjoy Chowdhury, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel approach to automatically synthesize "wayfinding
instructions" for an embodied robot agent. In contrast to prior approaches that
are heavily reliant on human-annotated datasets designed exclusively for
specific simulation platforms, our algorithm uses in-context learning to
condition an LLM to generate instructions using just a few references. Using an
LLM-based Visual Question Answering strategy, we gather detailed information
about the environment which is used by the LLM for instruction synthesis. We
implement our approach on multiple simulation platforms including Matterport3D,
AI Habitat and ThreeDWorld, thereby demonstrating its platform-agnostic nature.
We subjectively evaluate our approach via a user study and observe that 83.3%
of users find the synthesized instructions accurately capture the details of
the environment and show characteristics similar to those of human-generated
instructions. Further, we conduct zero-shot navigation with multiple approaches
on the REVERIE dataset using the generated instructions, and observe very close
correlation with the baseline on standard success metrics (< 1% change in SR),
quantifying the viability of generated instructions in replacing
human-annotated data. To the best of our knowledge, ours is the first
LLM-driven approach capable of generating "human-like" instructions in a
platform-agnostic manner, without requiring any form of training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 Pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robot Navigation in Unknown and Cluttered Workspace with Dynamical
  System Modulation in Starshaped Roadmap 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Chen, Haichao Liu, Yulin Li, Jianghua Duan, Lei Zhu, Jun Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel reactive motion planning framework for navigating
robots in unknown and cluttered 2D workspace. Typical existing methods are
developed by enforcing the robot staying in free regions represented by the
locally extracted ellipse or polygon. Instead, we navigate the robot in free
space with an alternate starshaped decomposition, which is calculated directly
from real-time sensor data. Additionally, a roadmap is constructed
incrementally to maintain the connectivity information of the starshaped
regions. Compared to the roadmap built upon connected polygons or ellipses in
the conventional approaches, the concave starshaped region is better suited to
capture the natural distribution of sensor data, so that the perception
information can be fully exploited for robot navigation. In this sense,
conservative and myopic behaviors are avoided with the proposed approach, and
intricate obstacle configurations can be suitably accommodated in unknown and
cluttered environments. Then, we design a heuristic exploration algorithm on
the roadmap to determine the frontier points of the starshaped regions, from
which short-term goals are selected to attract the robot towards the goal
configuration. It is noteworthy that, a recovery mechanism is developed on the
roadmap that is triggered once a non-extendable short-term goal is reached.
This mechanism renders it possible to deal with dead-end situations that can be
typically encountered in unknown and cluttered environments. Furthermore, safe
and smooth motion within the starshaped regions is generated by employing the
Dynamical System Modulation (DSM) approach on the constructed roadmap. Through
comprehensive evaluation in both simulations and real-world experiments, the
proposed method outperforms the benchmark methods in terms of success rate and
traveling time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ALDM-Grasping: Diffusion-aided Zero-Shot Sim-to-Real Transfer for Robot
  Grasping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11459v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11459v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Li, Zihao Wu, Huaqin Zhao, Tianze Yang, Zhengliang Liu, Peng Shu, Jin Sun, Ramviyas Parasuraman, Tianming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To tackle the "reality gap" encountered in Sim-to-Real transfer, this study
proposes a diffusion-based framework that minimizes inconsistencies in grasping
actions between the simulation settings and realistic environments. The process
begins by training an adversarial supervision layout-to-image diffusion
model(ALDM). Then, leverage the ALDM approach to enhance the simulation
environment, rendering it with photorealistic fidelity, thereby optimizing
robotic grasp task training. Experimental results indicate this framework
outperforms existing models in both success rates and adaptability to new
environments through improvements in the accuracy and reliability of visual
grasping actions under a variety of conditions. Specifically, it achieves a
75\% success rate in grasping tasks under plain backgrounds and maintains a
65\% success rate in more complex scenarios. This performance demonstrates this
framework excels at generating controlled image content based on text
descriptions, identifying object grasp points, and demonstrating zero-shot
learning in complex, unseen scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Demystifying Deep Reinforcement Learning-Based Autonomous Vehicle
  Decision-Making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11432v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11432v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanxi Wan, Pei Li, Arpan Kusari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advent of universal function approximators in the domain of
reinforcement learning, the number of practical applications leveraging deep
reinforcement learning (DRL) has exploded. Decision-making in automated driving
tasks has emerged as a chief application among them, taking the sensor data or
the higher-order kinematic variables as the input and providing a discrete
choice or continuous control output. However, the black-box nature of the
models presents an overwhelming limitation that restricts the real-world
deployment of DRL in autonomous vehicles (AVs). Therefore, in this research
work, we focus on the interpretability of an attention-based DRL framework. We
use a continuous proximal policy optimization-based DRL algorithm as the
baseline model and add a multi-head attention framework in an open-source AV
simulation environment. We provide some analytical techniques for discussing
the interpretability of the trained models in terms of explainability and
causality for spatial and temporal correlations. We show that the weights in
the first head encode the positions of the neighboring vehicles while the
second head focuses on the leader vehicle exclusively. Also, the ego vehicle's
action is causally dependent on the vehicles in the target lane spatially and
temporally. Through these findings, we reliably show that these techniques can
help practitioners decipher the results of the DRL algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted for peer-review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Expert Composer Policy: Scalable Skill Repertoire for Quadruped Robots <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guilherme Christmann, Ying-Sheng Luo, Wei-Chao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose the expert composer policy, a framework to reliably expand the
skill repertoire of quadruped agents. The composer policy links pair of experts
via transitions to a sampled target state, allowing experts to be composed
sequentially. Each expert specializes in a single skill, such as a locomotion
gait or a jumping motion. Instead of a hierarchical or mixture-of-experts
architecture, we train a single composer policy in an independent process that
is not conditioned on the other expert policies. By reusing the same composer
policy, our approach enables adding new experts without affecting existing
ones, enabling incremental repertoire expansion and preserving original motion
quality. We measured the transition success rate of 72 transition pairs and
achieved an average success rate of 99.99\%, which is over 10\% higher than the
baseline random approach, and outperforms other state-of-the-art methods. Using
domain randomization during training we ensure a successful transfer to the
real world, where we achieve an average transition success rate of 97.22\%
(N=360) in our experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Uncertainty: Risk-Aware Active View Acquisition for Safe Robot
  Navigation and 3D Scene Understanding with FisherRF 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyi Liu, Wen Jiang, Boshu Lei, Vivek Pandey, Kostas Daniilidis, Nader Motee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work proposes a novel approach to bolster both the robot's risk
assessment and safety measures while deepening its understanding of 3D scenes,
which is achieved by leveraging Radiance Field (RF) models and 3D Gaussian
Splatting. To further enhance these capabilities, we incorporate additional
sampled views from the environment with the RF model. One of our key
contributions is the introduction of Risk-aware Environment Masking (RaEM),
which prioritizes crucial information by selecting the next-best-view that
maximizes the expected information gain. This targeted approach aims to
minimize uncertainties surrounding the robot's path and enhance the safety of
its navigation. Our method offers a dual benefit: improved robot safety and
increased efficiency in risk-aware 3D scene reconstruction and understanding.
Extensive experiments in real-world scenarios demonstrate the effectiveness of
our proposed approach, highlighting its potential to establish a robust and
safety-focused framework for active robot exploration and 3D scene
understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Systematic <span class="highlight-title">Review</span> of XR-based Remote Human-Robot Interaction Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xian Wang, Luyao Shen, Lik-Hang Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This survey provides an exhaustive review of the applications of extended
reality (XR) technologies in the field of remote human-computer interaction
(HRI). We developed a systematic search strategy based on the PRISMA
methodology. From the initial 2,561 articles selected, 100 research papers that
met our inclusion criteria were included. We categorized and summarized the
domain in detail, delving into XR technologies, including augmented reality
(AR), virtual reality (VR), and mixed reality (MR), and their applications in
facilitating intuitive and effective remote control and interaction with
robotic systems.The survey highlights existing articles on the application of
XR technologies, user experience enhancement, and various interaction designs
for XR in remote HRI, providing insights into current trends and future
directions. We also identified potential gaps and opportunities for future
research to improve remote HRI systems through XR technology to guide and
inform future XR and robotics research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Benefits of GPU Sample-Based Stochastic Predictive Controllers
  for Legged Locomotion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giulio Turrisi, Valerio Modugno, Lorenzo Amatucci, Dimitrios Kanoulas, Claudio Semini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quadrupedal robots excel in mobility, navigating complex terrains with
agility. However, their complex control systems present challenges that are
still far from being fully addressed. In this paper, we introduce the use of
Sample-Based Stochastic control strategies for quadrupedal robots, as an
alternative to traditional optimal control laws. We show that Sample-Based
Stochastic methods, supported by GPU acceleration, can be effectively applied
to real quadruped robots. In particular, in this work, we focus on achieving
gait frequency adaptation, a notable challenge in quadrupedal locomotion for
gradient-based methods. To validate the effectiveness of Sample-Based
Stochastic controllers we test two distinct approaches for quadrupedal robots
and compare them against a conventional gradient-based Model Predictive Control
system. Our findings, validated both in simulation and on a real 21Kg Aliengo
quadruped, demonstrate that our method is on par with a traditional Model
Predictive Control strategy when the robot is subject to zero or moderate
disturbance, while it surpasses gradient-based methods in handling sustained
external disturbances, thanks to the straightforward gait adaptation strategy
that is possible to achieve within their formulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reachability-based Trajectory Design via Exact Formulation of Implicit
  Neural Signed Distance Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12280v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12280v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Michaux, Qingyi Chen, Challen Enninful Adu, Jinsun Liu, Ram Vasudevan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating receding-horizon motion trajectories for autonomous vehicles in
real-time while also providing safety guarantees is challenging. This is
because a future trajectory needs to be planned before the previously computed
trajectory is completely executed. This becomes even more difficult if the
trajectory is required to satisfy continuous-time collision-avoidance
constraints while accounting for a large number of obstacles. To address these
challenges, this paper proposes a novel real-time, receding-horizon motion
planning algorithm named REachability-based trajectory Design via Exact
Formulation of Implicit NEural signed Distance functions (REDEFINED). REDEFINED
first applies offline reachability analysis to compute zonotope-based reachable
sets that overapproximate the motion of the ego vehicle. During online
planning, REDEFINED leverages zonotope arithmetic to construct a neural
implicit representation that computes the exact signed distance between a
parameterized swept volume of the ego vehicle and obstacle vehicles. REDEFINED
then implements a novel, real-time optimization framework that utilizes the
neural network to construct a collision avoidance constraint. REDEFINED is
compared to a variety of state-of-the-art techniques and is demonstrated to
successfully enable the vehicle to safely navigate through complex
environments. Code, data, and video demonstrations can be found at
https://roahmlab.github.io/redefined/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Networked Feature Selection with Randomized Algorithm for Robot
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vivek Pandey, Arash Amini, Guangyi Liu, Ufuk Topcu, Qiyu Sun, Kostas Daniilidis, Nader Motee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the problem of sparse selection of visual features for localizing
a team of robots navigating an unknown environment, where robots can exchange
relative position measurements with neighbors. We select a set of the most
informative features by anticipating their importance in robots localization by
simulating trajectories of robots over a prediction horizon. Through
theoretical proofs, we establish a crucial connection between graph Laplacian
and the importance of features. We show that strong network connectivity
translates to uniformity in feature importance, which enables uniform random
sampling of features and reduces the overall computational complexity. We
leverage a scalable randomized algorithm for sparse sums of positive
semidefinite matrices to efficiently select the set of the most informative
features and significantly improve the probabilistic performance bounds.
Finally, we support our findings with extensive simulations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Human-Autonomous Agents Interaction Using <span class="highlight-title">Pre-Train</span>ed
  Language and Visual Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linus Nwankwo, Elmar Rueckert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we extended the method proposed in [17] to enable humans to
interact naturally with autonomous agents through vocal and textual
conversations. Our extended method exploits the inherent capabilities of
pre-trained large language models (LLMs), multimodal visual language models
(VLMs), and speech recognition (SR) models to decode the high-level natural
language conversations and semantic understanding of the robot's task
environment, and abstract them to the robot's actionable commands or queries.
We performed a quantitative evaluation of our framework's natural vocal
conversation understanding with participants from different racial backgrounds
and English language accents. The participants interacted with the robot using
both spoken and textual instructional commands. Based on the logged interaction
data, our framework achieved 87.55% vocal commands decoding accuracy, 86.27%
commands execution success, and an average latency of 0.89 seconds from
receiving the participants' vocal chat commands to initiating the robot's
actual physical action. The video demonstrations of this paper can be found at
https://linusnep.github.io/MTCC-IRoNL/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IKSPARK: An Inverse Kinematics Solver using Semidefinite Relaxation and
  Rank Minimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangting Wu, Roberto Tron
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inverse kinematics (IK) is a fundamental problem frequently occurred in robot
control and motion planning. However, the problem is nonconvex because the
kinematic map between the configuration and task spaces is generally nonlinear,
which makes it challenging for fast and accurate solutions. The problem can be
more complicated with the existence of different physical constraints imposed
by the robot structure. In this paper, we develop an inverse kinematics solver
named IKSPARK (Inverse Kinematics using Semidefinite Programming And RanK
minimization) that can find solutions for robots with various structures,
including open/closed kinematic chains, spherical, revolute, and/or prismatic
joints. The solver works in the space of rotation matrices of the link
reference frames and involves solving only convex semidefinite problems (SDPs).
Specifically, the IK problem is formulated as an SDP with an additional rank-1
constraint on symmetric matrices with constant traces. The solver first solves
this SDP disregarding the rank constraint to get a start point and then finds
the rank-1 solution iteratively via a rank minimization algorithm with proven
local convergence. Compared to other work that performs SDP relaxation for IK
problems, our formulation is simpler, and uses variables with smaller sizes. We
validate our approach via simulations on different robots, comparing against a
standard IK method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HRI in Indian Education: Challenges Opportunities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chinmaya Mishra, Anuj Nandanwar, Sashikala Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the recent advancements in the field of robotics and the increased focus
on having general-purpose robots widely available to the general public, it has
become increasingly necessary to pursue research into Human-robot interaction
(HRI). While there have been a lot of works discussing frameworks for teaching
HRI in educational institutions with a few institutions already offering
courses to students, a consensus on the course content still eludes the field.
In this work, we highlight a few challenges and opportunities while designing
an HRI course from an Indian perspective. These topics warrant further
deliberations as they have a direct impact on the design of HRI courses and
wider implications for the entire field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the Designing an Intro to HRI Course Workshop at HRI
  2024 (arXiv:2403.05588)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Architectural-Scale Artistic Brush Painting with a Hybrid Cable Robot <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gerry Chen, Tristan Al-Haddad, Frank Dellaert, Seth Hutchinson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot art presents an opportunity to both showcase and advance
state-of-the-art robotics through the challenging task of creating art.
Creating large-scale artworks in particular engages the public in a way that
small-scale works cannot, and the distinct qualities of brush strokes
contribute to an organic and human-like quality. Combining the large scale of
murals with the strokes of the brush medium presents an especially impactful
result, but also introduces unique challenges in maintaining precise, dextrous
motion control of the brush across such a large workspace. In this work, we
present the first robot to our knowledge that can paint architectural-scale
murals with a brush. We create a hybrid robot consisting of a cable-driven
parallel robot and 4 degree of freedom (DoF) serial manipulator to paint a 27m
by 3.7m mural on windows spanning 2-stories of a building. We discuss our
approach to achieving both the scale and accuracy required for brush-painting a
mural through a combination of novel mechanical design elements, coordinated
planning and control, and on-site calibration algorithms with experimental
validations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages IEEE conference format, submitted to IROS 2024,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bootstrapping Reinforcement Learning with Imitation for Vision-Based
  Agile Flight 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxu Xing, Angel Romero, Leonard Bauersfeld, Davide Scaramuzza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We combine the effectiveness of Reinforcement Learning (RL) and the
efficiency of Imitation Learning (IL) in the context of vision-based,
autonomous drone racing. We focus on directly processing visual input without
explicit state estimation. While RL offers a general framework for learning
complex controllers through trial and error, it faces challenges regarding
sample efficiency and computational demands due to the high dimensionality of
visual inputs. Conversely, IL demonstrates efficiency in learning from visual
demonstrations but is limited by the quality of those demonstrations and faces
issues like covariate shift. To overcome these limitations, we propose a novel
training framework combining RL and IL's advantages. Our framework involves
three stages: initial training of a teacher policy using privileged state
information, distilling this policy into a student policy using IL, and
performance-constrained adaptive RL fine-tuning. Our experiments in both
simulated and real-world environments demonstrate that our approach achieves
superior performance and robustness than IL or RL alone in navigating a
quadrotor through a racing course using only visual information without
explicit state estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The POLAR Traverse <span class="highlight-title">Dataset</span>: A <span class="highlight-title">Dataset</span> of Stereo Camera Images Simulating
  Traverses across Lunar Polar Terrain under Extreme Lighting Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12194v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12194v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Margaret Hansen, Uland Wong, Terrence Fong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the POLAR Traverse Dataset: a dataset of high-fidelity stereo pair
images of lunar-like terrain under polar lighting conditions designed to
simulate a straight-line traverse. Images from individual traverses with
different camera heights and pitches were recorded at 1 m intervals by moving a
suspended stereo bar across a test bed filled with regolith simulant and shaped
to mimic lunar south polar terrain. Ground truth geometry and camera position
information was also recorded. This dataset is intended for developing and
testing software algorithms that rely on stereo or monocular camera images,
such as visual odometry, for use in the lunar polar environment, as well as to
provide insight into the expected lighting conditions in lunar polar regions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures, 3 tables. Associated dataset can be found at
  https://ti.arc.nasa.gov/dataset/PolarTrav/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continual Domain Randomization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12193v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12193v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Josip Josifovski, Sayantan Auddy, Mohammadhossein Malmir, Justus Piater, Alois Knoll, Nicolás Navarro-Guerrero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain Randomization (DR) is commonly used for sim2real transfer of
reinforcement learning (RL) policies in robotics. Most DR approaches require a
simulator with a fixed set of tunable parameters from the start of the
training, from which the parameters are randomized simultaneously to train a
robust model for use in the real world. However, the combined randomization of
many parameters increases the task difficulty and might result in sub-optimal
policies. To address this problem and to provide a more flexible training
process, we propose Continual Domain Randomization (CDR) for RL that combines
domain randomization with continual learning to enable sequential training in
simulation on a subset of randomization parameters at a time. Starting from a
model trained in a non-randomized simulation where the task is easier to solve,
the model is trained on a sequence of randomizations, and continual learning is
employed to remember the effects of previous randomizations. Our robotic
reaching and grasping tasks experiments show that the model trained in this
fashion learns effectively in simulation and performs robustly on the real
robot while matching or outperforming baselines that employ combined
randomization or sequential randomization without continual learning. Our code
and videos are available at https://continual-dr.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under peer review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safety Implications of Explainable Artificial Intelligence in End-to-End
  Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahin Atakishiyev, Mohammad Salameh, Randy Goebel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The end-to-end learning pipeline is gradually creating a paradigm shift in
the ongoing development of highly autonomous vehicles, largely due to advances
in deep learning, the availability of large-scale training datasets, and
improvements in integrated sensor devices. However, a lack of interpretability
in real-time decisions with contemporary learning methods impedes user trust
and attenuates the widespread deployment and commercialization of such
vehicles. Moreover, the issue is exacerbated when these cars are involved in or
cause traffic accidents. Such drawback raises serious safety concerns from
societal and legal perspectives. Consequently, explainability in end-to-end
autonomous driving is essential to enable the safety of vehicular automation.
However, the safety and explainability aspects of autonomous driving have
generally been investigated disjointly by researchers in today's state of the
art. In this paper, we aim to bridge the gaps between these topics and seek to
answer the following research question: When and how can explanations improve
safety of autonomous driving? In this regard, we first revisit established
safety and state-of-the-art explainability techniques in autonomous driving.
Furthermore, we present three critical case studies and show the pivotal role
of explanations in enhancing self-driving safety. Finally, we describe our
empirical investigation and reveal potential value, limitations, and caveats
with practical explainable AI methods on their role of assuring safety and
transparency for vehicle autonomy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sim2Real Manipulation on Unknown Objects with Tactile-based
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12170v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12170v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Entong Su, Chengzhe Jia, Yuzhe Qin, Wenxuan Zhou, Annabella Macaluso, Binghao Huang, Xiaolong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using tactile sensors for manipulation remains one of the most challenging
problems in robotics. At the heart of these challenges is generalization: How
can we train a tactile-based policy that can manipulate unseen and diverse
objects? In this paper, we propose to perform Reinforcement Learning with only
visual tactile sensing inputs on diverse objects in a physical simulator. By
training with diverse objects in simulation, it enables the policy to
generalize to unseen objects. However, leveraging simulation introduces the
Sim2Real transfer problem. To mitigate this problem, we study different tactile
representations and evaluate how each affects real-robot manipulation results
after transfer. We conduct our experiments on diverse real-world objects and
show significant improvements over baselines for the pivoting task. Our project
page is available at https://tactilerl.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intelligent Execution through Plan Analysis <span class="chip">IROS 21</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Borrajo, Manuela Veloso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intelligent robots need to generate and execute plans. In order to deal with
the complexity of real environments, planning makes some assumptions about the
world. When executing plans, the assumptions are usually not met. Most works
have focused on the negative impact of this fact and the use of replanning
after execution failures. Instead, we focus on the positive impact, or
opportunities to find better plans. When planning, the proposed technique finds
and stores those opportunities. Later, during execution, the monitoring system
can use them to focus perception and repair the plan, instead of replanning
from scratch. Experiments in several paradigmatic robotic tasks show how the
approach outperforms standard replanning strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at IROS 21, 6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ergonomic Optimization in Worker-Robot Bimanual Object Handover:
  Implementing REBA Using Reinforcement Learning in Virtual Reality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12149v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12149v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mani Amani, Reza Akhavian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robots can serve as safety catalysts on construction job sites by taking over
hazardous and repetitive tasks while alleviating the risks associated with
existing manual workflows. Research on the safety of physical human-robot
interaction (pHRI) is traditionally focused on addressing the risks associated
with potential collisions. However, it is equally important to ensure that the
workflows involving a collaborative robot are inherently safe, even though they
may not result in an accident. For example, pHRI may require the human
counterpart to use non-ergonomic body postures to conform to the robot hardware
and physical configurations. Frequent and long-term exposure to such situations
may result in chronic health issues. Safety and ergonomics assessment measures
can be understood by robots if they are presented in algorithmic fashions so
optimization for body postures is attainable. While frameworks such as Rapid
Entire Body Assessment (REBA) have been an industry standard for many decades,
they lack a rigorous mathematical structure which poses challenges in using
them immediately for pHRI safety optimization purposes. Furthermore, learnable
approaches have limited robustness outside of their training data, reducing
generalizability. In this paper, we propose a novel framework that approaches
optimization through Reinforcement Learning, ensuring precise, online ergonomic
scores as compared to approximations, while being able to generalize and tune
the regiment to any human and any task. To ensure practicality, the training is
done in virtual reality utilizing Inverse Kinematics to simulate human movement
mechanics. Experimental findings are compared to ergonomically naive object
handover heuristics and indicate promising results where the developed
framework can find the optimal object handover coordinates in pHRI contexts for
manual material handling exemplary situations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Safety Science</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StereoNavNet: Learning to Navigate using Stereo Cameras with Auxiliary
  Occupancy Voxels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12039v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12039v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyu Li, Taskin Padir, Huaizu Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual navigation has received significant attention recently. Most of the
prior works focus on predicting navigation actions based on semantic features
extracted from visual encoders. However, these approaches often rely on large
datasets and exhibit limited generalizability. In contrast, our approach draws
inspiration from traditional navigation planners that operate on geometric
representations, such as occupancy maps. We propose StereoNavNet (SNN), a novel
visual navigation approach employing a modular learning framework comprising
perception and policy modules. Within the perception module, we estimate an
auxiliary 3D voxel occupancy grid from stereo RGB images and extract geometric
features from it. These features, along with user-defined goals, are utilized
by the policy module to predict navigation actions. Through extensive empirical
evaluation, we demonstrate that SNN outperforms baseline approaches in terms of
success rates, success weighted by path length, and navigation error.
Furthermore, SNN exhibits better generalizability, characterized by maintaining
leading performance when navigating across previously unseen environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aligning Learning with Communication in Shared Autonomy <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12023v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12023v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Hoegerman, Shahabedin Sagheb, Benjamin A. Christie, Dylan P. Losey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assistive robot arms can help humans by partially automating their desired
tasks. Consider an adult with motor impairments controlling an assistive robot
arm to eat dinner. The robot can reduce the number of human inputs -- and how
precise those inputs need to be -- by recognizing what the human wants (e.g., a
fork) and assisting for that task (e.g., moving towards the fork). Prior
research has largely focused on learning the human's task and providing
meaningful assistance. But as the robot learns and assists, we also need to
ensure that the human understands the robot's intent (e.g., does the human know
the robot is reaching for a fork?). In this paper, we study the effects of
communicating learned assistance from the robot back to the human operator. We
do not focus on the specific interfaces used for communication. Instead, we
develop experimental and theoretical models of a) how communication changes the
way humans interact with assistive robot arms, and b) how robots can harness
these changes to better align with the human's intent. We first conduct online
and in-person user studies where participants operate robots that provide
partial assistance, and we measure how the human's inputs change with and
without communication. With communication, we find that humans are more likely
to intervene when the robot incorrectly predicts their intent, and more likely
to release control when the robot correctly understands their task. We then use
these findings to modify an established robot learning algorithm so that the
robot can correctly interpret the human's inputs when communication is present.
Our results from a second in-person user study suggest that this combination of
communication and learning outperforms assistive systems that isolate either
learning or communication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, under review for IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SceneSense: Diffusion Models for 3D Occupancy Synthesis from Partial
  Observation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11985v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11985v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alec Reed, Brendan Crowe, Doncey Albin, Lorin Achey, Bradley Hayes, Christoffer Heckman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When exploring new areas, robotic systems generally exclusively plan and
execute controls over geometry that has been directly measured. When entering
space that was previously obstructed from view such as turning corners in
hallways or entering new rooms, robots often pause to plan over the newly
observed space. To address this we present SceneScene, a real-time 3D diffusion
model for synthesizing 3D occupancy information from partial observations that
effectively predicts these occluded or out of view geometries for use in future
planning and control frameworks. SceneSense uses a running occupancy map and a
single RGB-D camera to generate predicted geometry around the platform at
runtime, even when the geometry is occluded or out of view. Our architecture
ensures that SceneSense never overwrites observed free or occupied space. By
preserving the integrity of the observed map, SceneSense mitigates the risk of
corrupting the observed space with generative predictions. While SceneSense is
shown to operate well using a single RGB-D camera, the framework is flexible
enough to extend to additional modalities. SceneSense operates as part of any
system that generates a running occupancy map `out of the box', removing
conditioning from the framework. Alternatively, for maximum performance in new
modalities, the perception backbone can be replaced and the model retrained for
inference in new applications. Unlike existing models that necessitate multiple
views and offline scene synthesis, or are focused on filling gaps in observed
data, our findings demonstrate that SceneSense is an effective approach to
estimating unobserved local occupancy information at runtime. Local occupancy
predictions from SceneSense are shown to better represent the ground truth
occupancy distribution during the test exploration trajectories than the
running occupancy map.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inferring Belief States in Partially-Observable Human-Robot Teams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11955v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11955v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jack Kolb, Karen M. Feigh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the real-time estimation of human situation awareness using
observations from a robot teammate with limited visibility. In human factors
and human-autonomy teaming, it is recognized that individuals navigate their
environments using an internal mental simulation, or mental model. The mental
model informs cognitive processes including situation awareness, contextual
reasoning, and task planning. In teaming domains, the mental model includes a
team model of each teammate's beliefs and capabilities, enabling fluent
teamwork without the need for explicit communication. However, little work has
applied team models to human-robot teaming. We compare the performance of two
current methods at estimating user situation awareness over varying visibility
conditions. Our results indicate that the methods are largely resilient to
low-visibility conditions in our domain, however opportunities exist to improve
their overall performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review, project page: https://jackkolb.com/tmm-hri</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Dynamical Systems Encoding Non-Linearity within Space Curvature 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11948v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11948v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bernardo Fichera, Aude Billard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamical Systems (DS) are an effective and powerful means of shaping
high-level policies for robotics control. They provide robust and reactive
control while ensuring the stability of the driving vector field. The
increasing complexity of real-world scenarios necessitates DS with a higher
degree of non-linearity, along with the ability to adapt to potential changes
in environmental conditions, such as obstacles. Current learning strategies for
DSs often involve a trade-off, sacrificing either stability guarantees or
offline computational efficiency in order to enhance the capabilities of the
learned DS. Online local adaptation to environmental changes is either not
taken into consideration or treated as a separate problem. In this paper, our
objective is to introduce a method that enhances the complexity of the learned
DS without compromising efficiency during training or stability guarantees.
Furthermore, we aim to provide a unified approach for seamlessly integrating
the initially learned DS's non-linearity with any local non-linearities that
may arise due to changes in the environment. We propose a geometrical approach
to learn asymptotically stable non-linear DS for robotics control. Each DS is
modeled as a harmonic damped oscillator on a latent manifold. By learning the
manifold's Euclidean embedded representation, our approach encodes the
non-linearity of the DS within the curvature of the space. Having an explicit
embedded representation of the manifold allows us to showcase obstacle
avoidance by directly inducing local deformations of the space. We demonstrate
the effectiveness of our methodology through two scenarios: first, the 2D
learning of synthetic vector fields, and second, the learning of 3D robotic
end-effector motions in real-world settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single-Agent Actor Critic for Decentralized Cooperative Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11914v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11914v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengchao Yan, Lukas König, Wolfram Burgard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active traffic management incorporating autonomous vehicles (AVs) promises a
future with diminished congestion and enhanced traffic flow. However,
developing algorithms for real-world application requires addressing the
challenges posed by continuous traffic flow and partial observability. To
bridge this gap and advance the field of active traffic management towards
greater decentralization, we introduce a novel asymmetric actor-critic model
aimed at learning decentralized cooperative driving policies for autonomous
vehicles using single-agent reinforcement learning. Our approach employs
attention neural networks with masking to handle the dynamic nature of
real-world traffic flow and partial observability. Through extensive
evaluations against baseline controllers across various traffic scenarios, our
model shows great potential for improving traffic flow at diverse bottleneck
locations within the road system. Additionally, we explore the challenge
associated with the conservative driving behaviors of autonomous vehicles that
adhere strictly to traffic regulations. The experiment results illustrate that
our proposed cooperative policy can mitigate potential traffic slowdowns
without compromising safety.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visuo-Tactile <span class="highlight-title">Pretrain</span>ing for Cable Plugging <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11898v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11898v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abraham George, Selam Gano, Pranav Katragadda, Amir Barati Farimani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tactile information is a critical tool for fine-grain manipulation. As
humans, we rely heavily on tactile information to understand objects in our
environments and how to interact with them. We use touch not only to perform
manipulation tasks but also to learn how to perform these tasks. Therefore, to
create robotic agents that can learn to complete manipulation tasks at a human
or super-human level of performance, we need to properly incorporate tactile
information into both skill execution and skill learning. In this paper, we
investigate how we can incorporate tactile information into imitation learning
platforms to improve performance on complex tasks. To do this, we tackle the
challenge of plugging in a USB cable, a dexterous manipulation task that relies
on fine-grain visuo-tactile serving. By incorporating tactile information into
imitation learning frameworks, we are able to train a robotic agent to plug in
a USB cable - a first for imitation learning. Additionally, we explore how
tactile information can be used to train non-tactile agents through a
contrastive-loss pretraining process. Our results show that by pretraining with
tactile information, the performance of a non-tactile agent can be
significantly improved, reaching a level on par with visuo-tactile agents.
  For demonstration videos and access to our codebase, see the project website:
https://sites.google.com/andrew.cmu.edu/visuo-tactile-cable-plugging/home
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, submitted to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Bayesian Future Fusion for <span class="highlight-title">Self-Supervised</span>, High-Resolution,
  Off-Road Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11876v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11876v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubhra Aich, Wenshan Wang, Parv Maheshwari, Matthew Sivaprakasam, Samuel Triest, Cherie Ho, Jason M. Gregory, John G. Rogers III, Sebastian Scherer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The limited sensing resolution of resource-constrained off-road vehicles
poses significant challenges towards reliable off-road autonomy. To overcome
this limitation, we propose a general framework based on fusing the future
information (i.e. future fusion) for self-supervision. Recent approaches
exploit this future information alongside the hand-crafted heuristics to
directly supervise the targeted downstream tasks (e.g. traversability
estimation). However, in this paper, we opt for a more general line of
development - time-efficient completion of the highest resolution (i.e. 2cm per
pixel) BEV map in a self-supervised manner via future fusion, which can be used
for any downstream tasks for better longer range prediction. To this end,
first, we create a high-resolution future-fusion dataset containing pairs of
(RGB / height) raw sparse and noisy inputs and map-based dense labels. Next, to
accommodate the noise and sparsity of the sensory information, especially in
the distal regions, we design an efficient realization of the Bayes filter onto
the vanilla convolutional network via the recurrent mechanism. Equipped with
the ideas from SOTA generative models, our Bayesian structure effectively
predicts high-quality BEV maps in the distal regions. Extensive evaluation on
both the quality of completion and downstream task on our future-fusion dataset
demonstrates the potential of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context-aware LLM-based Safe Control Against Latent Risks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11863v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11863v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Khanh Luu, Xiyu Deng, Anh Van Ho, Yorie Nakahira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is challenging for autonomous control systems to perform complex tasks in
the presence of latent risks. Motivated by this challenge, this paper proposes
an integrated framework that involves Large Language Models (LLMs), stochastic
gradient descent (SGD), and optimization-based control. In the first phrase,
the proposed framework breaks down complex tasks into a sequence of smaller
subtasks, whose specifications account for contextual information and latent
risks. In the second phase, these subtasks and their parameters are refined
through a dual process involving LLMs and SGD. LLMs are used to generate rough
guesses and failure explanations, and SGD is used to fine-tune parameters. The
proposed framework is tested using simulated case studies of robots and
vehicles. The experiments demonstrate that the proposed framework can mediate
actions based on the context and latent risks and learn complex behaviors
efficiently.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OpenOcc: Open Vocabulary 3D Scene Reconstruction via Occupancy
  Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haochen Jiang, Yueming Xu, Yihan Zeng, Hang Xu, Wei Zhang, Jianfeng Feng, Li Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D reconstruction has been widely used in autonomous navigation fields of
mobile robotics. However, the former research can only provide the basic
geometry structure without the capability of open-world scene understanding,
limiting advanced tasks like human interaction and visual navigation. Moreover,
traditional 3D scene understanding approaches rely on expensive labeled 3D
datasets to train a model for a single task with supervision. Thus, geometric
reconstruction with zero-shot scene understanding i.e. Open vocabulary 3D
Understanding and Reconstruction, is crucial for the future development of
mobile robots. In this paper, we propose OpenOcc, a novel framework unifying
the 3D scene reconstruction and open vocabulary understanding with neural
radiance fields. We model the geometric structure of the scene with occupancy
representation and distill the pre-trained open vocabulary model into a 3D
language field via volume rendering for zero-shot inference. Furthermore, a
novel semantic-aware confidence propagation (SCP) method has been proposed to
relieve the issue of language field representation degeneracy caused by
inconsistent measurements in distilled features. Experimental results show that
our approach achieves competitive performance in 3D scene understanding tasks,
especially for small and long-tail objects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ForzaETH Race Stack -- Scaled Autonomous Head-to-Head Racing on Fully
  Commercial off-the-Shelf Hardware 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Baumann, Edoardo Ghignone, Jonas Kühne, Niklas Bastuck, Jonathan Becker, Nadine Imholz, Tobias Kränzlin, Tian Yi Lim, Michael Lötscher, Luca Schwarzenbach, Luca Tognoni, Christian Vogt, Andrea Carron, Michele Magno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous racing in robotics combines high-speed dynamics with the necessity
for reliability and real-time decision-making. While such racing pushes
software and hardware to their limits, many existing full-system solutions
necessitate complex, custom hardware and software, and usually focus on
Time-Trials rather than full unrestricted Head-to-Head racing, due to financial
and safety constraints. This limits their reproducibility, making advancements
and replication feasible mostly for well-resourced laboratories with
comprehensive expertise in mechanical, electrical, and robotics fields.
Researchers interested in the autonomy domain but with only partial experience
in one of these fields, need to spend significant time with familiarization and
integration. The ForzaETH Race Stack addresses this gap by providing an
autonomous racing software platform designed for F1TENTH, a 1:10 scaled
Head-to-Head autonomous racing competition, which simplifies replication by
using commercial off-the-shelf hardware. This approach enhances the competitive
aspect of autonomous racing and provides an accessible platform for research
and development in the field. The ForzaETH Race Stack is designed with
modularity and operational ease of use in mind, allowing customization and
adaptability to various environmental conditions, such as track friction and
layout. Capable of handling both Time-Trials and Head-to-Head racing, the stack
has demonstrated its effectiveness, robustness, and adaptability in the field
by winning the official F1TENTH international competition multiple times.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3DGS-Calib: 3D Gaussian Splatting for Multimodal SpatioTemporal
  Calibration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quentin Herau, Moussab Bennehar, Arthur Moreau, Nathan Piasco, Luis Roldao, Dzmitry Tsishkou, Cyrille Migniot, Pascal Vasseur, Cédric Demonceaux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable multimodal sensor fusion algorithms require accurate spatiotemporal
calibration. Recently, targetless calibration techniques based on implicit
neural representations have proven to provide precise and robust results.
Nevertheless, such methods are inherently slow to train given the high
computational overhead caused by the large number of sampled points required
for volume rendering. With the recent introduction of 3D Gaussian Splatting as
a faster alternative to implicit representation methods, we propose to leverage
this new rendering approach to achieve faster multi-sensor calibration. We
introduce 3DGS-Calib, a new calibration method that relies on the speed and
rendering accuracy of 3D Gaussian Splatting to achieve multimodal
spatiotemporal calibration that is accurate, robust, and with a substantial
speed-up compared to methods relying on implicit neural representations. We
demonstrate the superiority of our proposal with experimental results on
sequences from KITTI-360, a widely used driving dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM^3:Large Language Model-based Task and Motion Planning with Motion
  Failure Reasoning <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu Wang, Muzhi Han, Ziyuan Jiao, Zeyu Zhang, Ying Nian Wu, Song-Chun Zhu, Hangxin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional Task and Motion Planning (TAMP) approaches rely on manually
crafted interfaces connecting symbolic task planning with continuous motion
generation. These domain-specific and labor-intensive modules are limited in
addressing emerging tasks in real-world settings. Here, we present LLM^3, a
novel Large Language Model (LLM)-based TAMP framework featuring a
domain-independent interface. Specifically, we leverage the powerful reasoning
and planning capabilities of pre-trained LLMs to propose symbolic action
sequences and select continuous action parameters for motion planning.
Crucially, LLM^3 incorporates motion planning feedback through prompting,
allowing the LLM to iteratively refine its proposals by reasoning about motion
failure. Consequently, LLM^3 interfaces between task planning and motion
planning, alleviating the intricate design process of handling domain-specific
messages between them. Through a series of simulations in a box-packing domain,
we quantitatively demonstrate the effectiveness of LLM^3 in solving TAMP
problems and the efficiency in selecting action parameters. Ablation studies
underscore the significant contribution of motion failure reasoning to the
success of LLM^3. Furthermore, we conduct qualitative experiments on a physical
manipulator, demonstrating the practical applicability of our approach in
real-world settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS 2024. Codes available:
  https://github.com/AssassinWS/LLM-TAMP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robustness Evaluation of Localization Techniques for Autonomous Racing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07658v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07658v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tian Yi Lim, Edoardo Ghignone, Nicolas Baumann, Michele Magno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces SynPF, an MCL-based algorithm tailored for high-speed
racing environments. Benchmarked against Cartographer, a state-of-the-art
pose-graph SLAM algorithm, SynPF leverages synergies from previous
particle-filtering methods and synthesizes them for the high-performance racing
domain. Our extensive in-field evaluations reveal that while Cartographer
excels under nominal conditions, it struggles when subjected to wheel-slip, a
common phenomenon in a racing scenario due to varying grip levels and
aggressive driving behaviour. Conversely, SynPF demonstrates robustness in
these challenging conditions and a low-latency computation time of 1.25 ms on
on-board computers without a GPU. Using the F1TENTH platform, a 1:10 scaled
autonomous racing vehicle, this work not only highlights the vulnerabilities of
existing algorithms in high-speed scenarios, tested up until 7.6 m/s, but also
emphasizes the potential of SynPF as a viable alternative, especially in
deteriorating odometry conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the Design, Automation and Test in Europe Conference 2024
  as an extended abstract</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Collaborative Aquatic Positioning System Utilising Multi-beam Sonar and
  Depth Sensors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10397v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10397v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        {Xueliang Cheng, Barry Lennox, Keir Groves
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate positioning of remotely operated underwater vehicles (ROVs) in
confined environments is crucial for inspection and mapping tasks and is also a
prerequisite for autonomous operations. Presently, there are no positioning
systems available that are suited for real-world use in confined underwater
environments, unconstrained by environmental lighting and water turbidity
levels and have sufficient accuracy for long-term, reliable and repeatable
navigation. This shortage presents a significant barrier to enhancing the
capabilities of ROVs in such scenarios. This paper introduces an innovative
positioning system for ROVs operating in confined, cluttered underwater
settings, achieved through the collaboration of an omnidirectional surface
vehicle and an ROV. A formulation is proposed and evaluated in the simulation
against ground truth. The experimental results from the simulation form a proof
of principle of the proposed system and also demonstrate its deployability.
Unlike many previous approaches, the system does not rely on fixed
infrastructure or tracking of features in the environment and can cover large
enclosed areas without additional equipment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Efficient Model-Based Approach on Learning Agile Motor Skills without
  Reinforcement <span class="chip">ICRA2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01962v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01962v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haojie Shi, Tingguang Li, Qingxu Zhu, Jiapeng Sheng, Lei Han, Max Q. -H. Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning-based methods have improved locomotion skills of quadruped robots
through deep reinforcement learning. However, the sim-to-real gap and low
sample efficiency still limit the skill transfer. To address this issue, we
propose an efficient model-based learning framework that combines a world model
with a policy network. We train a differentiable world model to predict future
states and use it to directly supervise a Variational Autoencoder (VAE)-based
policy network to imitate real animal behaviors. This significantly reduces the
need for real interaction data and allows for rapid policy updates. We also
develop a high-level network to track diverse commands and trajectories. Our
simulated results show a tenfold sample efficiency increase compared to
reinforcement learning methods such as PPO. In real-world testing, our policy
achieves proficient command-following performance with only a two-minute data
collection period and generalizes well to new speeds and paths.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICRA2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SparseDFF: Sparse-View Feature Distillation for One-Shot Dexterous
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16838v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16838v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianxu Wang, Haotong Zhang, Congyue Deng, Yang You, Hao Dong, Yixin Zhu, Leonidas Guibas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans demonstrate remarkable skill in transferring manipulation abilities
across objects of varying shapes, poses, and appearances, a capability rooted
in their understanding of semantic correspondences between different instances.
To equip robots with a similar high-level comprehension, we present SparseDFF,
a novel DFF for 3D scenes utilizing large 2D vision models to extract semantic
features from sparse RGBD images, a domain where research is limited despite
its relevance to many tasks with fixed-camera setups. SparseDFF generates
view-consistent 3D DFFs, enabling efficient one-shot learning of dexterous
manipulations by mapping image features to a 3D point cloud. Central to
SparseDFF is a feature refinement network, optimized with a contrastive loss
between views and a point-pruning mechanism for feature continuity. This
facilitates the minimization of feature discrepancies w.r.t. end-effector
parameters, bridging demonstrations and target manipulations. Validated in
real-world scenarios with a dexterous hand, SparseDFF proves effective in
manipulating both rigid and deformable objects, demonstrating significant
generalization capabilities across object and scene variations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LeTO: Learning Constrained Visuomotor Policy with Differentiable
  Trajectory Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.17500v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.17500v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengtong Xu, Yu She
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces LeTO, a method for learning constrained visuomotor
policy via differentiable trajectory optimization. Our approach uniquely
integrates a differentiable optimization layer into the neural network. By
formulating the optimization layer as a trajectory optimization problem, we
enable the model to end-to-end generate actions in a safe and controlled
fashion without extra modules. Our method allows for the introduction of
constraints information during the training process, thereby balancing the
training objectives of satisfying constraints, smoothing the trajectories, and
minimizing errors with demonstrations. This "gray box" method marries the
optimization-based safety and interpretability with the powerful
representational abilities of neural networks. We quantitatively evaluate LeTO
in simulation and on the real robot. In simulation, LeTO achieves a success
rate comparable to state-of-the-art imitation learning methods, but the
generated trajectories are of less uncertainty, higher quality, and smoother.
In real-world experiments, we deployed LeTO to handle constraints-critical
tasks. The results show the effectiveness of LeTO comparing with
state-of-the-art imitation learning approaches. We release our code at
https://github.com/ZhengtongXu/LeTO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Reward: Learning Rewards via Conditional Video Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14134v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14134v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Huang, Guangqi Jiang, Yanjie Ze, Huazhe Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning rewards from expert videos offers an affordable and effective
solution to specify the intended behaviors for reinforcement learning tasks. In
this work, we propose Diffusion Reward, a novel framework that learns rewards
from expert videos via conditional video diffusion models for solving complex
visual RL problems. Our key insight is that lower generative diversity is
observed when conditioned on expert trajectories. Diffusion Reward is
accordingly formalized by the negative of conditional entropy that encourages
productive exploration of expert-like behaviors. We show the efficacy of our
method over 10 robotic manipulation tasks from MetaWorld and Adroit with visual
input and sparse reward. Moreover, Diffusion Reward could even solve unseen
tasks successfully and effectively, largely surpassing baseline methods.
Project page and code: https://diffusion-reward.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page and code: https://diffusion-reward.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Value of Assistance for Grasping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14402v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14402v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Masarwy, Yuval Goshen, David Dovrat, Sarah Keren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In multiple realistic settings, a robot is tasked with grasping an object
without knowing its exact pose and relies on a probabilistic estimation of the
pose to decide how to attempt the grasp. We support settings in which it is
possible to provide the robot with an observation of the object before a grasp
is attempted but this possibility is limited and there is a need to decide
which sensing action would be most beneficial. We support this decision by
offering a novel Value of Assistance (VOA) measure for assessing the expected
effect a specific observation will have on the robot's ability to complete its
task. We evaluate our suggested measure in simulated and real-world
collaborative grasping settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast LiDAR Informed Visual Search in Unseen Indoor Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14150v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14150v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Gupta, Kyle Morgenstein, Steven Ortega, Luis Sentis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the problem of planning for visual search without prior
map information. We leverage the pixel-wise environment perception problem
where one is given wide Field of View 2D scan data and must perform LiDAR
segmentation to contextually label points in the surroundings. These pixel
classifications provide an informed prior on which to plan next best viewpoints
during visual search tasks. We present LIVES: LiDAR Informed Visual Search, a
method aimed at finding objects of interest in unknown indoor environments. A
robust map-free classifier is trained from expert data collected using a simple
cart platform equipped with a map-based classifier. An autonomous exploration
planner takes the contextual data from scans and uses that prior to plan
viewpoints more likely to yield detection of the search target. We propose a
utility function that accounts for traditional metrics like information gain
and path cost and for the contextual information. LIVES is baselined against
several existing exploration methods in simulation to verify its performance.
It is validated in real-world experiments with single and multiple search
objects with a Spot robot in two unseen environments. Videos of experiments,
implementation details and open source code can be found at
https://sites.google.com/view/lives-2024/home.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages + references. 6 figures. 1 algorithm. 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High-Gain Disturbance Observer for Robust Trajectory Tracking of
  Quadrotors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.19115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.19115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadreza Izadi, Reza Faieghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a simple method to boost the robustness of quadrotors in
trajectory tracking. The presented method features a high-gain disturbance
observer (HGDO) that provides disturbance estimates in real-time. The estimates
are then used in a trajectory control law to compensate for disturbance
effects. We present theoretical convergence results showing that the proposed
HGDO can quickly converge to an adjustable neighborhood of actual disturbance
values. We will then integrate the disturbance estimates with a typical robust
trajectory controller, namely sliding mode control (SMC), and present Lyapunov
stability analysis to establish the boundedness of trajectory tracking errors.
However, our stability analysis can be easily extended to other Lyapunov-based
controllers to develop different HGDO-based controllers with formal stability
guarantees. We evaluate the proposed HGDO-based control method using both
simulation and laboratory experiments in various scenarios and in the presence
of external disturbances. Our results indicate that the addition of HGDO to a
quadrotor trajectory controller can significantly improve the accuracy and
precision of trajectory tracking in the presence of external disturbances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RLIF: Interactive Imitation Learning as Reinforcement Learning <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12996v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12996v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianlan Luo, Perry Dong, Yuexiang Zhai, Yi Ma, Sergey Levine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although reinforcement learning methods offer a powerful framework for
automatic skill acquisition, for practical learning-based control problems in
domains such as robotics, imitation learning often provides a more convenient
and accessible alternative. In particular, an interactive imitation learning
method such as DAgger, which queries a near-optimal expert to intervene online
to collect correction data for addressing the distributional shift challenges
that afflict na\"ive behavioral cloning, can enjoy good performance both in
theory and practice without requiring manually specified reward functions and
other components of full reinforcement learning methods. In this paper, we
explore how off-policy reinforcement learning can enable improved performance
under assumptions that are similar but potentially even more practical than
those of interactive imitation learning. Our proposed method uses reinforcement
learning with user intervention signals themselves as rewards. This relaxes the
assumption that intervening experts in interactive imitation learning should be
near-optimal and enables the algorithm to learn behaviors that improve over the
potential suboptimal human expert. We also provide a unified framework to
analyze our RL method and DAgger; for which we present the asymptotic analysis
of the suboptimal gap for both methods as well as the non-asymptotic sample
complexity bound of our method. We then evaluate our method on challenging
high-dimensional continuous control simulation benchmarks as well as real-world
robotic vision-based manipulation tasks. The results show that it strongly
outperforms DAgger-like approaches across the different tasks, especially when
the intervening experts are suboptimal. Code and videos can be found on the
project website: https://rlif-page.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D Reconstruction in Noisy Agricultural Environments: A Bayesian
  Optimization Perspective for View Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00145v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00145v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Athanasios Bacharis, Konstantinos D. Polyzos, Henry J. Nelson, Georgios B. Giannakis, Nikolaos Papanikolopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D reconstruction is a fundamental task in robotics that gained attention due
to its major impact in a wide variety of practical settings, including
agriculture, underwater, and urban environments. This task can be carried out
via view planning (VP), which aims to optimally place a certain number of
cameras in positions that maximize the visual information, improving the
resulting 3D reconstruction. Nonetheless, in most real-world settings, existing
environmental noise can significantly affect the performance of 3D
reconstruction. To that end, this work advocates a novel geometric-based
reconstruction quality function for VP, that accounts for the existing noise of
the environment, without requiring its closed-form expression. With no analytic
expression of the objective function, this work puts forth an adaptive Bayesian
optimization algorithm for accurate 3D reconstruction in the presence of noise.
Numerical tests on noisy agricultural environments showcase the merits of the
proposed approach for 3D reconstruction with even a small number of available
cameras.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Touch-GS: Visual-Tactile Supervised 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09875v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09875v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aiden Swann, Matthew Strong, Won Kyung Do, Gadiel Sznaier Camps, Mac Schwager, Monroe Kennedy III
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose a novel method to supervise 3D Gaussian Splatting
(3DGS) scenes using optical tactile sensors. Optical tactile sensors have
become widespread in their use in robotics for manipulation and object
representation; however, raw optical tactile sensor data is unsuitable to
directly supervise a 3DGS scene. Our representation leverages a Gaussian
Process Implicit Surface to implicitly represent the object, combining many
touches into a unified representation with uncertainty. We merge this model
with a monocular depth estimation network, which is aligned in a two stage
process, coarsely aligning with a depth camera and then finely adjusting to
match our touch data. For every training image, our method produces a
corresponding fused depth and uncertainty map. Utilizing this additional
information, we propose a new loss function, variance weighted depth supervised
loss, for training the 3DGS scene model. We leverage the DenseTact optical
tactile sensor and RealSense RGB-D camera to show that combining touch and
vision in this manner leads to quantitatively and qualitatively better results
than vision or touch alone in a few-view scene syntheses on opaque as well as
on reflective and transparent objects. Please see our project page at
http://armlabstanford.github.io/touch-gs
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AO-Grasp: Articulated Object Grasp Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15928v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15928v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlota Parés Morlans, Claire Chen, Yijia Weng, Michelle Yi, Yuying Huang, Nick Heppert, Linqi Zhou, Leonidas Guibas, Jeannette Bohg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce AO-Grasp, a grasp proposal method that generates 6 DoF grasps
that enable robots to interact with articulated objects, such as opening and
closing cabinets and appliances. AO-Grasp consists of two main contributions:
the AO-Grasp Model and the AO-Grasp Dataset. Given a segmented partial point
cloud of a single articulated object, the AO-Grasp Model predicts the best
grasp points on the object with an Actionable Grasp Point Predictor. Then, it
finds corresponding grasp orientations for each of these points, resulting in
stable and actionable grasp proposals. We train the AO-Grasp Model on our new
AO-Grasp Dataset, which contains 78K actionable parallel-jaw grasps on
synthetic articulated objects. In simulation, AO-Grasp achieves a 45.0 % grasp
success rate, whereas the highest performing baseline achieves a 35.0% success
rate. Additionally, we evaluate AO-Grasp on 120 real-world scenes of objects
with varied geometries, articulation axes, and joint states, where AO-Grasp
produces successful grasps on 67.5% of scenes, while the baseline only produces
successful grasps on 33.3% of scenes. To the best of our knowledge, AO-Grasp is
the first method for generating 6 DoF grasps on articulated objects directly
from partial point clouds without requiring part detection or hand-designed
grasp heuristics. Project website: https://stanford-iprl-lab.github.io/ao-grasp
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://stanford-iprl-lab.github.io/ao-grasp</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Continuous Control with Geometric Regularity from Robot
  Intrinsic Symmetry <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.16316v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.16316v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengchao Yan, Baohe Zhang, Yuan Zhang, Joschka Boedecker, Wolfram Burgard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geometric regularity, which leverages data symmetry, has been successfully
incorporated into deep learning architectures such as CNNs, RNNs, GNNs, and
Transformers. While this concept has been widely applied in robotics to address
the curse of dimensionality when learning from high-dimensional data, the
inherent reflectional and rotational symmetry of robot structures has not been
adequately explored. Drawing inspiration from cooperative multi-agent
reinforcement learning, we introduce novel network structures for single-agent
control learning that explicitly capture these symmetries. Moreover, we
investigate the relationship between the geometric prior and the concept of
Parameter Sharing in multi-agent reinforcement learning. Last but not the
least, we implement the proposed framework in online and offline learning
methods to demonstrate its ease of use. Through experiments conducted on
various challenging continuous control tasks on simulators and real robots, we
highlight the significant potential of the proposed geometric regularity in
enhancing robot learning capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comparison of Motion Encoding Frameworks on Human Manipulation Actions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.13024v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.13024v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lennart Jahn, Florentin Wörgötter, Tomas Kulvicius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Movement generation, and especially generalisation to unseen situations,
plays an important role in robotics. Different types of movement generation
methods exist such as spline based methods, dynamical system based methods, and
methods based on Gaussian mixture models (GMMs). Using a large, new dataset on
human manipulations, in this paper we provide a highly detailed comparison of
five fundamentally different and widely used movement encoding and generation
frameworks: dynamic movement primitives (DMPs), time based Gaussian mixture
regression (tbGMR), stable estimator of dynamical systems (SEDS), Probabilistic
Movement Primitives (ProMP) and Optimal Control Primitives (OCP). We compare
these frameworks with respect to their movement encoding efficiency,
reconstruction accuracy, and movement generalisation capabilities. The new
dataset consists of nine object manipulation actions performed by 12 humans:
pick and place, put on top/take down, put inside/take out, hide/uncover, and
push/pull with a total of 7,652 movement examples. Our analysis shows that for
movement encoding and reconstruction DMPs and OCPs are the most efficient with
respect to the number of parameters and reconstruction accuracy, if a
sufficient number of kernels is used. In case of movement generalisation to new
start- and end-point situations, DMPs, OCPs and task parameterized GMM (TP-GMM,
movement generalisation framework based on tbGMR) lead to similar performance,
which ProMPs only achieve when using many demonstrations for learning. All
models outperform SEDS, which additionally proves to be difficult to fit.
Furthermore we observe that TP-GMM and SEDS suffer from problems reaching the
end-points of generalizations.These different quantitative results will help
selecting the most appropriate models and designing trajectory representations
in an improved task-dependent way in future robotic applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exposing the Unseen: Exposure Time Emulation for Offline Benchmarking of
  Vision Algorithms <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13139v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13139v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olivier Gamache, Jean-Michel Fortin, Matěj Boxan, Maxime Vaidis, François Pomerleau, Philippe Giguère
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Odometry (VO) is one of the fundamental tasks in computer vision for
robotics. However, its performance is deeply affected by High Dynamic Range
(HDR) scenes, omnipresent outdoor. While new Automatic-Exposure (AE) approaches
to mitigate this have appeared, their comparison in a reproducible manner is
problematic. This stems from the fact that the behavior of AE depends on the
environment, and it affects the image acquisition process. Consequently, AE has
traditionally only been benchmarked in an online manner, making the experiments
non-reproducible. To solve this, we propose a new methodology based on an
emulator that can generate images at any exposure time. It leverages BorealHDR,
a unique multi-exposure stereo dataset collected over 10 km, on 55 trajectories
with challenging illumination conditions. Moreover, it includes
lidar-inertial-based global maps with pose estimation for each image frame as
well as Global Navigation Satellite System (GNSS) data, for comparison. We show
that using these images acquired at different exposure times, we can emulate
realistic images, keeping a Root-Mean-Square Error (RMSE) below 1.78 % compared
to ground truth images. To demonstrate the practicality of our approach for
offline benchmarking, we compared three state-of-the-art AE algorithms on key
elements of Visual Simultaneous Localization And Mapping (VSLAM) pipeline,
against four baselines. Consequently, reproducible evaluation of AE is now
possible, speeding up the development of future approaches. Our code and
dataset are available online at this link:
https://github.com/norlab-ulaval/BorealHDR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures, submitted to 2024 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAINS: A Magnetic Field Aided Inertial Navigation System for Indoor
  Positioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02599v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02599v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuan Huang, Gustaf Hendeby, Hassen Fourati, Christophe Prieur, Isaac Skog
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A Magnetic field Aided Inertial Navigation System (MAINS) for indoor
navigation is proposed in this paper. MAINS leverages an array of magnetometers
to measure spatial variations in the magnetic field, which are then used to
estimate the displacement and orientation changes of the system, thereby aiding
the inertial navigation system (INS). Experiments show that MAINS significantly
outperforms the stand-alone INS, demonstrating a remarkable two orders of
magnitude reduction in position error. Furthermore, when compared to the
state-of-the-art magnetic-field-aided navigation approach, the proposed method
exhibits slightly improved horizontal position accuracy. On the other hand, it
has noticeably larger vertical error on datasets with large magnetic field
variations. However, one of the main advantages of MAINS compared to the
state-of-the-art is that it enables flexible sensor configurations. The
experimental results show that the position error after 2 minutes of navigation
in most cases is less than 3 meters when using an array of 30 magnetometers.
Thus, the proposed navigation solution has the potential to solve one of the
key challenges faced with current magnetic-field simultaneous localization and
mapping (SLAM) solutions: the very limited allowable length of the exploration
phase during which unvisited areas are mapped.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Sensors Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Studying speed-accuracy trade-offs in best-of-n collective
  decision-making through heterogeneous mean-field modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13694v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13694v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreagiovanni Reina, Thierry Njougouo, Elio Tuci, Timoteo Carletti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To succeed in their objectives, groups of individuals must be able to make
quick and accurate collective decisions on the best option among a set of
alternatives with different qualities. Group-living animals aim to do that all
the time. Plants and fungi are thought to do so too. Swarms of autonomous
robots can also be programmed to make best-of-n decisions for solving tasks
collaboratively. Ultimately, humans critically need it and so many times they
should be better at it. Thanks to their mathematical tractability, simple
models like the voter model and the local majority rule model have proven
useful to describe the dynamics of such collective decision-making processes.
To reach a consensus, individuals change their opinion by interacting with
neighbors in their social network. At least among animals and robots, options
with a better quality are exchanged more often and therefore spread faster than
lower-quality options, leading to the collective selection of the best option.
With our work, we study the impact of individuals making errors in pooling
others' opinions caused, for example, by the need to reduce the cognitive load.
Our analysis is grounded on the introduction of a model that generalizes the
two existing models (local majority rule and voter model), showing a
speed-accuracy trade-off regulated by the cognitive effort of individuals. We
also investigate the impact of the interaction network topology on the
collective dynamics. To do so, we extend our model and, by using the
heterogeneous mean-field approach, we show the presence of another
speed-accuracy trade-off regulated by network connectivity. An interesting
result is that reduced network connectivity corresponds to an increase in
collective decision accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transferring Foundation Models for Generalizable Robotic Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05716v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05716v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiange Yang, Wenhui Tan, Chuhao Jin, Keling Yao, Bei Liu, Jianlong Fu, Ruihua Song, Gangshan Wu, Limin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improving the generalization capabilities of general-purpose robotic
manipulation agents in the real world has long been a significant challenge.
Existing approaches often rely on collecting large-scale robotic data which is
costly and time-consuming, such as the RT-1 dataset. However, due to
insufficient diversity of data, these approaches typically suffer from limiting
their capability in open-domain scenarios with new objects and diverse
environments. In this paper, we propose a novel paradigm that effectively
leverages language-reasoning segmentation mask generated by internet-scale
foundation models, to condition robot manipulation tasks. By integrating the
mask modality, which incorporates semantic, geometric, and temporal correlation
priors derived from vision foundation models, into the end-to-end policy model,
our approach can effectively and robustly perceive object pose and enable
sample-efficient generalization learning, including new object instances,
semantic categories, and unseen backgrounds. We first introduce a series of
foundation models to ground natural language demands across multiple tasks.
Secondly, we develop a two-stream 2D policy model based on imitation learning,
which processes raw images and object masks to predict robot actions with a
local-global perception manner. Extensive realworld experiments conducted on a
Franka Emika robot arm demonstrate the effectiveness of our proposed paradigm
and policy architecture. Demos can be found in our submitted video, and more
comprehensive ones can be found in link1 or link2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-17T00:00:00Z">2024-03-17</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">27</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Driving Style Alignment for LLM-powered Driver Agent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruoxuan Yang, Xinyue Zhang, Anais Fernandez-Laaksonen, Xin Ding, Jiangtao Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, LLM-powered driver agents have demonstrated considerable potential
in the field of autonomous driving, showcasing human-like reasoning and
decision-making abilities.However, current research on aligning driver agent
behaviors with human driving styles remains limited, partly due to the scarcity
of high-quality natural language data from human driving behaviors.To address
this research gap, we propose a multi-alignment framework designed to align
driver agents with human driving styles through demonstrations and feedback.
Notably, we construct a natural language dataset of human driver behaviors
through naturalistic driving experiments and post-driving interviews, offering
high-quality human demonstrations for LLM alignment. The framework's
effectiveness is validated through simulation experiments in the CARLA urban
traffic simulator and further corroborated by human evaluations. Our research
offers valuable insights into designing driving agents with diverse driving
styles.The implementation of the framework and details of the dataset can be
found at the link.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3DGS-ReLoc: 3D Gaussian Splatting for Map Representation and Visual
  ReLocalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11367v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11367v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Jiang, Gaurav Pandey, Srikanth Saripalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel system designed for 3D mapping and visual
relocalization using 3D Gaussian Splatting. Our proposed method uses LiDAR and
camera data to create accurate and visually plausible representations of the
environment. By leveraging LiDAR data to initiate the training of the 3D
Gaussian Splatting map, our system constructs maps that are both detailed and
geometrically accurate. To mitigate excessive GPU memory usage and facilitate
rapid spatial queries, we employ a combination of a 2D voxel map and a KD-tree.
This preparation makes our method well-suited for visual localization tasks,
enabling efficient identification of correspondences between the query image
and the rendered image from the Gaussian Splatting map via normalized
cross-correlation (NCC). Additionally, we refine the camera pose of the query
image using feature-based matching and the Perspective-n-Point (PnP) technique.
The effectiveness, adaptability, and precision of our system are demonstrated
through extensive evaluation on the KITTI360 dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging the Gap between Discrete Agent Strategies in Game Theory and
  Continuous Motion Planning in Dynamic Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongrui Zheng, Zhijun Zhuang, Stephanie Wu, Shuo Yang, Rahul Mangharam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating competitive strategies and performing continuous motion planning
simultaneously in an adversarial setting is a challenging problem. In addition,
understanding the intent of other agents is crucial to deploying autonomous
systems in adversarial multi-agent environments. Existing approaches either
discretize agent action by grouping similar control inputs, sacrificing
performance in motion planning, or plan in uninterpretable latent spaces,
producing hard-to-understand agent behaviors. This paper proposes an agent
strategy representation via Policy Characteristic Space that maps the agent
policies to a pre-specified low-dimensional space. Policy Characteristic Space
enables the discretization of agent policy switchings while preserving
continuity in control. Also, it provides intepretability of agent policies and
clear intentions of policy switchings. Then, regret-based game-theoretic
approaches can be applied in the Policy Characteristic Space to obtain high
performance in adversarial environments. Our proposed method is assessed by
conducting experiments in an autonomous racing scenario using scaled vehicles.
Statistical evidence shows that our method significantly improves the win rate
of ego agent and the method also generalizes well to unseen environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to RA-L</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Simulation-Based Model Preconditions for Fast Action
  Parameter Optimization with Multiple Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. Yunus Seker, Oliver Kroemer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimizing robotic action parameters is a significant challenge for
manipulation tasks that demand high levels of precision and generalization.
Using a model-based approach, the robot must quickly reason about the outcomes
of different actions using a predictive model to find a set of parameters that
will have the desired effect. The model may need to capture the behaviors of
rigid and deformable objects, as well as objects of various shapes and sizes.
Predictive models often need to trade-off speed for prediction accuracy and
generalization. This paper proposes a framework that leverages the strengths of
multiple predictive models, including analytical, learned, and simulation-based
models, to enhance the efficiency and accuracy of action parameter
optimization. Our approach uses Model Deviation Estimators (MDEs) to determine
the most suitable predictive model for any given state-action parameters,
allowing the robot to select models to make fast and precise predictions. We
extend the MDE framework by not only learning sim-to-real MDEs, but also
sim-to-sim MDEs. Our experiments show that these sim-to-sim MDEs provide
significantly faster parameter optimization as well as a basis for efficiently
learning sim-to-real MDEs through finetuning. The ease of collecting sim-to-sim
training data also allows the robot to learn MDEs based directly on visual
inputs and local material properties.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pioneering SE(2)-Equivariant Trajectory Planning for Automated Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steffen Hagedorn, Marcel Milich, Alexandru P. Condurache
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Planning the trajectory of the controlled ego vehicle is a key challenge in
automated driving. As for human drivers, predicting the motions of surrounding
vehicles is important to plan the own actions. Recent motion prediction methods
utilize equivariant neural networks to exploit geometric symmetries in the
scene. However, no existing method combines motion prediction and trajectory
planning in a joint step while guaranteeing equivariance under
roto-translations of the input space. We address this gap by proposing a
lightweight equivariant planning model that generates multi-modal joint
predictions for all vehicles and selects one mode as the ego plan. The
equivariant network design improves sample efficiency, guarantees output
stability, and reduces model parameters. We further propose equivariant route
attraction to guide the ego vehicle along a high-level route provided by an
off-the-shelf GPS navigation system. This module creates a momentum from
embedded vehicle positions toward the route in latent space while keeping the
equivariance property. Route attraction enables goal-oriented behavior without
forcing the vehicle to stick to the exact route. We conduct experiments on the
challenging nuScenes dataset to investigate the capability of our planner. The
results show that the planned trajectory is stable under roto-translations of
the input scene which demonstrates the equivariance of our model. Despite using
only a small split of the dataset for training, our method improves L2 distance
at 3 s by 20.6 % and surpasses the state of the art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Sample Long Range Path Planning under Sensing Uncertainty for
  Off-Road Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matt Schmittle, Rohan Baijal, Brian Hou, Siddhartha Srinivasa, Byron Boots
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We focus on the problem of long-range dynamic replanning for off-road
autonomous vehicles, where a robot plans paths through a previously unobserved
environment while continuously receiving noisy local observations. An effective
approach for planning under sensing uncertainty is determinization, where one
converts a stochastic world into a deterministic one and plans under this
simplification. This makes the planning problem tractable, but the cost of
following the planned path in the real world may be different than in the
determinized world. This causes collisions if the determinized world
optimistically ignores obstacles, or causes unnecessarily long routes if the
determinized world pessimistically imagines more obstacles. We aim to be robust
to uncertainty over potential worlds while still achieving the efficiency
benefits of determinization. We evaluate algorithms for dynamic replanning on a
large real-world dataset of challenging long-range planning problems from the
DARPA RACER program. Our method, Dynamic Replanning via Evaluating and
Aggregating Multiple Samples (DREAMS), outperforms other determinization-based
approaches in terms of combined traversal time and collision cost.
https://sites.google.com/cs.washington.edu/dreams/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ManipVQA: Injecting Robotic Affordance and Physically Grounded
  Information into Multi-Modal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11289v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11289v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Huang, Iaroslav Ponomarenko, Zhengkai Jiang, Xiaoqi Li, Xiaobin Hu, Peng Gao, Hongsheng Li, Hao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of Multimodal Large Language Models (MLLMs) with robotic
systems has significantly enhanced the ability of robots to interpret and act
upon natural language instructions. Despite these advancements, conventional
MLLMs are typically trained on generic image-text pairs, lacking essential
robotics knowledge such as affordances and physical knowledge, which hampers
their efficacy in manipulation tasks. To bridge this gap, we introduce
ManipVQA, a novel framework designed to endow MLLMs with Manipulation-centric
knowledge through a Visual Question-Answering format. This approach not only
encompasses tool detection and affordance recognition but also extends to a
comprehensive understanding of physical concepts. Our approach starts with
collecting a varied set of images displaying interactive objects, which
presents a broad range of challenges in tool object detection, affordance, and
physical concept predictions. To seamlessly integrate this robotic-specific
knowledge with the inherent vision-reasoning capabilities of MLLMs, we adopt a
unified VQA format and devise a fine-tuning strategy that preserves the
original vision-reasoning abilities while incorporating the new robotic
insights. Empirical evaluations conducted in robotic simulators and across
various vision task benchmarks demonstrate the robust performance of ManipVQA.
Code and dataset will be made publicly available at
https://github.com/SiyuanHuang95/ManipVQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and dataset will be made publicly available at
  https://github.com/SiyuanHuang95/ManipVQA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid Feedback for Three-dimensional Convex Obstacle Avoidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mayur Sawant, Ilia Polushin, Abdelhamid Tayebi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a hybrid feedback control scheme for the autonomous robot
navigation problem in three-dimensional environments with arbitrarily-shaped
convex obstacles. The proposed hybrid control strategy, which consists in
switching between the move-to-target mode and the obstacle-avoidance mode,
guarantees global asymptotic stability of the target location in the
obstacle-free workspace. We also provide a procedure for the implementation of
the proposed hybrid controller in a priori unknown environments and validate
its effectiveness through simulation results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zutu: A Platform for Localization and Navigation of Swarm Robots Using
  Virtual Grids <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Prateek, Pawan Wadhwani, Reshesh Kumar Pathak, Mayur Bhosale, Dr. A Helen Victoria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Swarm robots, which are inspired from the way insects behave collectively in
order to achieve a common goal, have become a major part of research with
applications involving search and rescue, area exploration, surveillance etc.
In this paper, we present a swarm of robots that do not require individual
extrinsic sensors to sense the environment but instead use a single central
camera to locate and map the swarm. The robots can be easily built using
readily available components with the main chassis being 3D printed, making the
system low-cost, low-maintenance, and easy to replicate. We describe Zutu's
hardware and software architecture, the algorithms to map the robots to the
real world, and some experiments conducted using four of our robots.
Eventually, we conclude the possible applications of our system in research,
education, and industries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 7th International Conference on Robotics and Automation
  Engineering, ICRAE 2022, Singapore, November 18 - November 20, 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compact 3D Gaussian Splatting For Dense Visual SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianchen Deng, Yaohui Chen, Leyan Zhang, Jianfei Yang, Shenghai Yuan, Danwei Wang, Weidong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has shown that 3D Gaussian-based SLAM enables high-quality
reconstruction, accurate pose estimation, and real-time rendering of scenes.
However, these approaches are built on a tremendous number of redundant 3D
Gaussian ellipsoids, leading to high memory and storage costs, and slow
training speed. To address the limitation, we propose a compact 3D Gaussian
Splatting SLAM system that reduces the number and the parameter size of
Gaussian ellipsoids. A sliding window-based masking strategy is first proposed
to reduce the redundant ellipsoids. Then we observe that the covariance matrix
(geometry) of most 3D Gaussian ellipsoids are extremely similar, which
motivates a novel geometry codebook to compress 3D Gaussian geometric
attributes, i.e., the parameters. Robust and accurate pose estimation is
achieved by a global bundle adjustment method with reprojection loss. Extensive
experiments demonstrate that our method achieves faster training and rendering
speed while maintaining the state-of-the-art (SOTA) quality of the scene
representation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STAIR: Semantic-Targeted Active Implicit Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liren Jin, Haofei Kuang, Yue Pan, Cyrill Stachniss, Marija Popović
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many autonomous robotic applications require object-level understanding when
deployed. Actively reconstructing objects of interest, i.e. objects with
specific semantic meanings, is therefore relevant for a robot to perform
downstream tasks in an initially unknown environment. In this work, we propose
a novel framework for semantic-targeted active reconstruction using posed RGB-D
measurements and 2D semantic labels as input. The key components of our
framework are a semantic implicit neural representation and a compatible
planning utility function based on semantic rendering and uncertainty
estimation, enabling adaptive view planning to target objects of interest. Our
planning approach achieves better reconstruction performance in terms of mesh
and novel view rendering quality compared to implicit reconstruction baselines
that do not consider semantics for view planning. Our framework further
outperforms a state-of-the-art semantic-targeted active reconstruction pipeline
based on explicit maps, justifying our choice of utilising implicit neural
representations to tackle semantic-targeted active reconstruction problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continuous Jumping of a Parallel Wire-Driven Monopedal Robot RAMIEL
  Using Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kento Kawaharazuka, Temma Suzuki, Kei Okada, Masayuki Inaba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We have developed a parallel wire-driven monopedal robot, RAMIEL, which has
both speed and power due to the parallel wire mechanism and a long acceleration
distance. RAMIEL is capable of jumping high and continuously, and so has high
performance in traveling. On the other hand, one of the drawbacks of a minimal
parallel wire-driven robot without joint encoders is that the current joint
velocities estimated from the wire lengths oscillate due to the elongation of
the wires, making the values unreliable. Therefore, despite its high
performance, the control of the robot is unstable, and in 10 out of 16 jumps,
the robot could only jump up to two times continuously. In this study, we
propose a method to realize a continuous jumping motion by reinforcement
learning in simulation, and its application to the actual robot. Because the
joint velocities oscillate with the elongation of the wires, they are not used
directly, but instead are inferred from the time series of joint angles. At the
same time, noise that imitates the vibration caused by the elongation of the
wires is added for transfer to the actual robot. The results show that the
system can be applied to the actual robot RAMIEL as well as to the stable
continuous jumping motion in simulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Humanoids2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning-Based Wiping Behavior of Low-Rigidity Robots Considering
  Various Surface Materials and Task Definitions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11198v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11198v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kento Kawaharazuka, Naoaki Kanazawa, Kei Okada, Masayuki Inaba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wiping behavior is a task of tracing the surface of an object while feeling
the force with the palm of the hand. It is necessary to adjust the force and
posture appropriately considering the various contact conditions felt by the
hand. Several studies have been conducted on the wiping motion, however, these
studies have only dealt with a single surface material, and have only
considered the application of the amount of appropriate force, lacking
intelligent movements to ensure that the force is applied either evenly to the
entire surface or to a certain area. Depending on the surface material, the
hand posture and pressing force should be varied appropriately, and this is
highly dependent on the definition of the task. Also, most of the movements are
executed by high-rigidity robots that are easy to model, and few movements are
executed by robots that are low-rigidity but therefore have a small risk of
damage due to excessive contact. So, in this study, we develop a method of
motion generation based on the learned prediction of contact force during the
wiping motion of a low-rigidity robot. We show that MyCobot, which is made of
low-rigidity resin, can appropriately perform wiping behaviors on a plane with
multiple surface materials based on various task definitions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Humanoids2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Adaptive Cooperation: Model-Based Shared Control Using
  LQ-Differential Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Balint Varga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel model-based adaptive shared control to allow
for the identification and design challenge for shared-control systems, in
which humans and automation share control tasks. The main challenge is the
adaptive behavior of the human in such shared control interactions.
Consequently, merely identifying human behavior without considering automation
is insufficient and often leads to inadequate automation design. Therefore,
this paper proposes a novel solution involving online identification of the
human and the adaptation of shared control using Linear-Quadratic differential
games. The effectiveness of the proposed online adaptation is analyzed in
simulations and compared with a non-adaptive shared control from the state of
the art. Finally, the proposed approach is tested through human-in-the-loop
experiments, highlighting its suitability for real-time applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PyroTrack: Belief-Based Deep Reinforcement Learning Path Planning for
  Aerial Wildfire Monitoring in Partially Observable Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahand Khoshdel, Qi Luo, Fatemeh Afghah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by agility, 3D mobility, and low-risk operation compared to
human-operated management systems of autonomous unmanned aerial vehicles
(UAVs), this work studies UAV-based active wildfire monitoring where a UAV
detects fire incidents in remote areas and tracks the fire frontline. A UAV
path planning solution is proposed considering realistic wildfire management
missions, where a single low-altitude drone with limited power and flight time
is available. Noting the limited field of view of commercial low-altitude UAVs,
the problem formulates as a partially observable Markov decision process
(POMDP), in which wildfire progression outside the field of view causes
inaccurate state representation that prevents the UAV from finding the optimal
path to track the fire front in limited time. Common deep reinforcement
learning (DRL)-based trajectory planning solutions require diverse
drone-recorded wildfire data to generalize pre-trained models to real-time
systems, which is not currently available at a diverse and standard scale. To
narrow down the gap caused by partial observability in the space of possible
policies, a belief-based state representation with broad, extensive simulated
data is proposed where the beliefs (i.e., ignition probabilities of different
grid areas) are updated using a Bayesian framework for the cells within the
field of view. The performance of the proposed solution in terms of the ratio
of detected fire cells and monitored ignited area (MIA) is evaluated in a
complex fire scenario with multiple rapidly growing fire batches, indicating
that the belief state representation outperforms the observation state
representation both in fire coverage and the distance to fire frontline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, Accepted in American Control Conference (ACC) 2024, July
  10-12th, Toronto, ON, Canada</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models Powered Context-aware Motion Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11057v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11057v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoji Zheng, Lixiu Wu, Zhijie Yan, Yuanrong Tang, Hao Zhao, Chen Zhong, Bokui Chen, Jiangtao Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion prediction is among the most fundamental tasks in autonomous driving.
Traditional methods of motion forecasting primarily encode vector information
of maps and historical trajectory data of traffic participants, lacking a
comprehensive understanding of overall traffic semantics, which in turn affects
the performance of prediction tasks. In this paper, we utilized Large Language
Models (LLMs) to enhance the global traffic context understanding for motion
prediction tasks. We first conducted systematic prompt engineering, visualizing
complex traffic environments and historical trajectory information of traffic
participants into image prompts -- Transportation Context Map (TC-Map),
accompanied by corresponding text prompts. Through this approach, we obtained
rich traffic context information from the LLM. By integrating this information
into the motion prediction model, we demonstrate that such context can enhance
the accuracy of motion predictions. Furthermore, considering the cost
associated with LLMs, we propose a cost-effective deployment strategy:
enhancing the accuracy of motion prediction tasks at scale with 0.7\%
LLM-augmented datasets. Our research offers valuable insights into enhancing
the understanding of traffic scenes of LLMs and the motion prediction
performance of autonomous driving.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages,4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantifying the biomimicry gap in biohybrid robot-fish pairs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.08978v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.08978v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vaios Papaspyros, Guy Theraulaz, Clément Sire, Francesco Mondada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biohybrid systems in which robotic lures interact with animals have become
compelling tools for probing and identifying the mechanisms underlying
collective animal behavior. One key challenge lies in the transfer of social
interaction models from simulations to reality, using robotics to validate the
modeling hypotheses. This challenge arises in bridging what we term the
"biomimicry gap", which is caused by imperfect robotic replicas, communication
cues and physics constraints not incorporated in the simulations, that may
elicit unrealistic behavioral responses in animals. In this work, we used a
biomimetic lure of a rummy-nose tetra fish (Hemigrammus rhodostomus) and a
neural network (NN) model for generating biomimetic social interactions.
Through experiments with a biohybrid pair comprising a fish and the robotic
lure, a pair of real fish, and simulations of pairs of fish, we demonstrate
that our biohybrid system generates social interactions mirroring those of
genuine fish pairs. Our analyses highlight that: 1) the lure and NN maintain
minimal deviation in real-world interactions compared to simulations and
fish-only experiments, 2) our NN controls the robot efficiently in real-time,
and 3) a comprehensive validation is crucial to bridge the biomimicry gap,
ensuring realistic biohybrid systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Fine Pinch-Grasp Skills using Tactile Sensing from A Few
  Real-world Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04619v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04619v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofeng Mao, Yucheng Xu, Ruoshi Wen, Mohammadreza Kasaei, Wanming Yu, Efi Psomopoulou, Nathan F. Lepora, Zhibin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning for robot dexterous manipulation, especially with a real
robot setup, typically requires a large number of demonstrations. In this
paper, we present a data-efficient learning from demonstration framework which
exploits the use of rich tactile sensing data and achieves fine bimanual pinch
grasping. Specifically, we employ a convolutional autoencoder network that can
effectively extract and encode high-dimensional tactile information. Further,
We develop a framework that achieves efficient multi-sensor fusion for
imitation learning, allowing the robot to learn contact-aware sensorimotor
skills from demonstrations. Our comparision study against the framework without
using encoded tactile features highlighted the effectiveness of incorporating
rich contact information, which enabled dexterous bimanual grasping with active
contact searching. Extensive experiments demonstrated the robustness of the
fine pinch grasp policy directly learned from few-shot demonstration, including
grasping of the same object with different initial poses, generalizing to ten
unseen new objects, robust and firm grasping against external pushes, as well
as contact-aware and reactive re-grasping in case of dropping objects under
very large perturbations. Furthermore, the saliency map analysis method is used
to describe weight distribution across various modalities during pinch
grasping, confirming the effectiveness of our framework at leveraging
multimodal information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SWTrack: Multiple Hypothesis Sliding Window 3D Multi-Object Tracking <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17892v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17892v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sandro Papais, Robert Ren, Steven Waslander
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern robotic systems are required to operate in dense dynamic environments,
requiring highly accurate real-time track identification and estimation. For 3D
multi-object tracking, recent approaches process a single measurement frame
recursively with greedy association and are prone to errors in ambiguous
association decisions. Our method, Sliding Window Tracker (SWTrack), yields
more accurate association and state estimation by batch processing many frames
of sensor data while being capable of running online in real-time. The most
probable track associations are identified by evaluating all possible track
hypotheses across the temporal sliding window. A novel graph optimization
approach is formulated to solve the multidimensional assignment problem with
lifted graph edges introduced to account for missed detections and graph
sparsity enforced to retain real-time efficiency. We evaluate our SWTrack
implementation$^{2}$ on the NuScenes autonomous driving dataset to demonstrate
improved tracking performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Laboratory Automation Through Robot Skill Learning For
  Sample Scraping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.14875v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.14875v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriella Pizzuto, Hetong Wang, Hatem Fakhruldeen, Bei Peng, Kevin S. Luck, Andrew I. Cooper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of laboratory robotics for autonomous experiments offers an
attractive route to alleviate scientists from tedious tasks while accelerating
material discovery for topical issues such as climate change and
pharmaceuticals. While some experimental workflows can already benefit from
automation, sample preparation is still carried out manually due to the high
level of motor function and dexterity required when dealing with different
tools, chemicals, and glassware. A fundamental workflow in chemical fields is
crystallisation, where one application is polymorph screening, i.e., obtaining
a three dimensional molecular structure from a crystal. For this process, it is
of utmost importance to recover as much of the sample as possible since
synthesising molecules is both costly in time and money. To this aim, chemists
scrape vials to retrieve sample contents prior to imaging plate transfer.
Automating this process is challenging as it goes beyond robotic insertion
tasks due to a fundamental requirement of having to execute fine-granular
movements within a constrained environment (sample vial). Motivated by how
human chemists carry out this process of scraping powder from vials, our work
proposes a model-free reinforcement learning method for learning a scraping
policy, leading to a fully autonomous sample scraping procedure. We first
create a scenario-specific simulation environment with a Panda Franka Emika
robot using a laboratory scraper that is inserted into a simulated vial, to
demonstrate how a scraping policy can be learned successfully in simulation. We
then train and evaluate our method on a real robotic manipulator in laboratory
settings, and show that our method can autonomously scrape powder across
various setups.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Motion Planning Algorithm in a Figure Eight Track 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05570v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05570v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cristian Jardon, Brian Sheppard, Veet Zaveri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We design a motion planning algorithm to coordinate the movements of two
robots along a figure eight track, in such a way that no collisions occur. We
use a topological approach to robot motion planning that relates instabilities
in motion planning algorithms to topological features of configuration spaces.
The topological complexity of a configuration space is an invariant that
measures the complexity of motion planning algorithms. We show that the
topological complexity of our problem is 3 and construct an explicit algorithm
with three continuous instructions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 45 figures, First published in PUMP Journal of
  Undergraduate Research. This research paper was completed under the
  supervision of Prof. Hellen Colman at Wilbur Wright College</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Representing Robot Geometry as Distance Fields: Applications to
  Whole-body Manipulation <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.00533v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.00533v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Li, Yan Zhang, Amirreza Razmjoo, Sylvain Calinon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose a novel approach to represent robot geometry as
distance fields (RDF) that extends the principle of signed distance fields
(SDFs) to articulated kinematic chains. Our method employs a combination of
Bernstein polynomials to encode the signed distance for each robot link with
high accuracy and efficiency while ensuring the mathematical continuity and
differentiability of SDFs. We further leverage the kinematics chain of the
robot to produce the SDF representation in joint space, allowing robust
distance queries in arbitrary joint configurations. The proposed RDF
representation is differentiable and smooth in both task and joint spaces,
enabling its direct integration to optimization problems. Additionally, the
0-level set of the robot corresponds to the robot surface, which can be
seamlessly integrated into whole-body manipulation tasks. We conduct various
experiments in both simulations and with 7-axis Franka Emika robots, comparing
against baseline methods, and demonstrating its effectiveness in collision
avoidance and whole-body manipulation tasks. Project page:
https://sites.google.com/view/lrdf/home
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE International Conference on Robotics and Automation, ICRA, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Parallel Self-assembly for a Multi-USV System on Water Surface with
  Obstacles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.00085v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.00085v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lianxin Zhang, Yihan Huang, Zhongzhong Cao, Yang Jiao, Huihuan Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parallel self-assembly is an efficient approach to accelerate the assembly
process for modular robots. However, these approaches cannot accommodate
complicated environments with obstacles, which restricts their applications.
This paper considers the surrounding stationary obstacles and proposes a
parallel self-assembly planning algorithm named SAPOA. With this algorithm,
modular robots can avoid immovable obstacles when performing docking actions,
which adapts the parallel self-assembly process to complex scenes. To validate
the efficiency and scalability, we have designed 25 distinct grid maps with
different obstacle configurations to simulate the algorithm. From the results
compared to the existing parallel self-assembly algorithms, our algorithm shows
a significantly higher success rate, which is more than 80%. For verification
in real-world applications, a multi-agent hardware testbed system is developed.
The algorithm is successfully deployed on four omnidirectional unmanned surface
vehicles, CuBoats. The navigation strategy that translates the discrete
planner, SAPOA, to the continuous controller on the CuBoats is presented. The
algorithm's feasibility and flexibility were demonstrated through successful
self-assembly experiments on 5 maps with varying obstacle configurations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Physics and Background Attributes Impact Video <span class="highlight-title">Transformer</span>s in
  Robotic Manipulation: A Case Study on Planar Pushing <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02044v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02044v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shutong Jin, Ruiyu Wang, Muhammad Zahid, Florian T. Pokorny
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As model and dataset sizes continue to scale in robot learning, the need to
understand what is the specific factor in the dataset that affects model
performance becomes increasingly urgent to ensure cost-effective data
collection and model performance. In this work, we empirically investigate how
physics attributes (color, friction coefficient, shape) and scene background
characteristics, such as the complexity and dynamics of interactions with
background objects, influence the performance of Video Transformers in
predicting planar pushing trajectories. We aim to investigate three primary
questions: How do physics attributes and background scene characteristics
influence model performance? What kind of changes in attributes are most
detrimental to model generalization? What proportion of fine-tuning data is
required to adapt models to novel scenarios? To facilitate this research, we
present CloudGripper-Push-1K, a large real-world vision-based robot pushing
dataset comprising 1278 hours and 460,000 videos of planar pushing interactions
with objects with different physics and background attributes. We also propose
Video Occlusion Transformer (VOT), a generic modular video-transformer-based
trajectory prediction framework which features 3 choices of 2D-spatial encoders
as the subject of our case study. Dataset and codes will be available at
https://cloudgripper.org.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at IEEE/RSJ IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Impact Angle Guidance via First-Order Optimization under
  Nonconvex Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00398v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00398v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gyubin Park, Jiwoo Choi, Da Hoon Jeong, Jong-Han Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most of the optimal guidance problems can be formulated as nonconvex
optimization problems, which can be solved indirectly by relaxation,
convexification, or linearization. Although these methods are guaranteed to
converge to the global optimum of the modified problems, the obtained solution
may not guarantee global optimality or even the feasibility of the original
nonconvex problems. In this paper, we propose a computational optimal guidance
approach that directly handles the nonconvex constraints encountered in
formulating the guidance problems. The proposed computational guidance approach
alternately solves the least squares problems and projects the solution onto
nonconvex feasible sets, which rapidly converges to feasible suboptimal
solutions or sometimes to the globally optimal solutions. The proposed
algorithm is verified via a series of numerical simulations on impact angle
guidance problems under state dependent maneuver vector constraints, and it is
demonstrated that the proposed algorithm provides superior guidance performance
than conventional techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at 2024 American Control Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CalibFormer: A <span class="highlight-title">Transformer</span>-based Automatic LiDAR-Camera Calibration
  Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15241v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15241v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Xiao, Yao Li, Chengzhen Meng, Xingchen Li, Jianmin Ji, Yanyong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The fusion of LiDARs and cameras has been increasingly adopted in autonomous
driving for perception tasks. The performance of such fusion-based algorithms
largely depends on the accuracy of sensor calibration, which is challenging due
to the difficulty of identifying common features across different data
modalities. Previously, many calibration methods involved specific targets
and/or manual intervention, which has proven to be cumbersome and costly.
Learning-based online calibration methods have been proposed, but their
performance is barely satisfactory in most cases. These methods usually suffer
from issues such as sparse feature maps, unreliable cross-modality association,
inaccurate calibration parameter regression, etc. In this paper, to address
these issues, we propose CalibFormer, an end-to-end network for automatic
LiDAR-camera calibration. We aggregate multiple layers of camera and LiDAR
image features to achieve high-resolution representations. A multi-head
correlation module is utilized to identify correlations between features more
accurately. Lastly, we employ transformer architectures to estimate accurate
calibration parameters from the correlation information. Our method achieved a
mean translation error of $0.8751 \mathrm{cm}$ and a mean rotation error of
$0.0562 ^{\circ}$ on the KITTI dataset, surpassing existing state-of-the-art
methods and demonstrating strong robustness, accuracy, and generalization
capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with
  Multi-Step On-Policy Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.03351v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.03351v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Lei, Zhengmao He, Chenhao Lu, Kaizhe Hu, Yang Gao, Huazhe Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combining offline and online reinforcement learning (RL) is crucial for
efficient and safe learning. However, previous approaches treat offline and
online learning as separate procedures, resulting in redundant designs and
limited performance. We ask: Can we achieve straightforward yet effective
offline and online learning without introducing extra conservatism or
regularization? In this study, we propose Uni-o4, which utilizes an on-policy
objective for both offline and online learning. Owning to the alignment of
objectives in two phases, the RL agent can transfer between offline and online
learning seamlessly. This property enhances the flexibility of the learning
paradigm, allowing for arbitrary combinations of pretraining, fine-tuning,
offline, and online learning. In the offline phase, specifically, Uni-o4
leverages diverse ensemble policies to address the mismatch issues between the
estimated behavior policy and the offline dataset. Through a simple offline
policy evaluation (OPE) approach, Uni-o4 can achieve multi-step policy
improvement safely. We demonstrate that by employing the method above, the
fusion of these two paradigms can yield superior offline initialization as well
as stable and rapid online fine-tuning capabilities. Through real-world robot
tasks, we highlight the benefits of this paradigm for rapid deployment in
challenging, previously unseen real-world environments. Additionally, through
comprehensive evaluations using numerous simulated benchmarks, we substantiate
that our method achieves state-of-the-art performance in both offline and
offline-to-online fine-tuning learning. Our website:
https://lei-kun.github.io/uni-o4/ .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our website: https://lei-kun.github.io/uni-o4/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-16T00:00:00Z">2024-03-16</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">43</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Resilient Fleet Management for Energy-Aware Intra-Factory Logistics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11034v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11034v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mithun Goutham, Stephanie Stockar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel fleet management strategy for battery-powered
robot fleets tasked with intra-factory logistics in an autonomous manufacturing
facility. In this environment, repetitive material handling operations are
subject to real-world uncertainties such as blocked passages, and equipment or
robot malfunctions. In such cases, centralized approaches enhance resilience by
immediately adjusting the task allocation between the robots. To overcome the
computational expense, a two-step methodology is proposed where the nominal
problem is solved a priori using a Monte Carlo Tree Search algorithm for task
allocation, resulting in a nominal search tree. When a disruption occurs, the
nominal search tree is rapidly updated a posteriori with costs to the new
problem while simultaneously generating feasible solutions. Computational
experiments prove the real-time capability of the proposed algorithm for
various scenarios and compare it with the case where the search tree is not
used and the decentralized approach that does not attempt task reassignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript was accepted to the 2024 American Control Conference
  (ACC) which will be held Wednesday through Friday, July 10-12, 2024 in
  Toronto, ON, Canada. arXiv admin note: text overlap with arXiv:2304.11444</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantifying the Sim2real Gap for GPS and IMU Sensors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11000v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11000v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ishaan Mahajan, Huzaifa Unjhawala, Harry Zhang, Zhenhao Zhou, Aaron Young, Alexis Ruiz, Stefan Caldararu, Nevindu Batagoda, Sriram Ashokkumar, Dan Negrut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulation can and should play a critical role in the development and testing
of algorithms for autonomous agents. What might reduce its impact is the
``sim2real'' gap -- the algorithm response differs between operation in
simulated versus real-world environments. This paper introduces an approach to
evaluate this gap, focusing on the accuracy of sensor simulation --
specifically IMU and GPS -- in velocity estimation tasks for autonomous agents.
Using a scaled autonomous vehicle, we conduct 40 real-world experiments across
diverse environments then replicate the experiments in simulation with five
distinct sensor noise models. We note that direct comparison of raw simulation
and real sensor data fails to quantify the sim2real gap for robotics
applications. We demonstrate that by using a state of the art state-estimation
package as a ``judge'', and by evaluating the performance of this
state-estimator in both real and simulated scenarios, we can isolate the
sim2real discrepancies stemming from sensor simulations alone. The dataset
generated is open-source and publicly available for unfettered use.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Scalable and Parallelizable Digital Twin Framework for Sustainable
  Sim2Real Transition of Multi-Agent Reinforcement Learning Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10996v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10996v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chinmay Vilas Samak, Tanmay Vilas Samak, Venkat Krovi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents a sustainable multi-agent deep reinforcement learning
framework capable of selectively scaling parallelized training workloads
on-demand, and transferring the trained policies from simulation to reality
using minimal hardware resources. We introduce AutoDRIVE Ecosystem as an
enabling digital twin framework to train, deploy, and transfer cooperative as
well as competitive multi-agent reinforcement learning policies from simulation
to reality. Particularly, we first investigate an intersection traversal
problem of 4 cooperative vehicles (Nigel) that share limited state information
in single as well as multi-agent learning settings using a common policy
approach. We then investigate an adversarial autonomous racing problem of 2
vehicles (F1TENTH) using an individual policy approach. In either set of
experiments, a decentralized learning architecture was adopted, which allowed
robust training and testing of the policies in stochastic environments. The
agents were provided with realistically sparse observation spaces, and were
restricted to sample control actions that implicitly satisfied the imposed
kinodynamic and safety constraints. The experimental results for both problem
statements are reported in terms of quantitative metrics and qualitative
remarks for training as well as deployment phases. We also discuss agent and
environment parallelization techniques adopted to efficiently accelerate MARL
training, while analyzing their computational performance. Finally, we
demonstrate a resource-aware transition of the trained policies from simulation
to reality using the proposed digital twin framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2309.10007</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SSUP-HRI: Social Signaling in Urban Public Human-Robot Interaction
  <span class="highlight-title">dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10994v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10994v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fanjun Bu, Wendy Ju
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces our dataset featuring human-robot interactions (HRI) in
urban public environments. This dataset is rich with social signals that we
believe can be modeled to help understand naturalistic human-robot interaction.
Our dataset currently comprises approximately 15 hours of video footage
recorded from the robots' perspectives, within which we annotated a total of
274 observable interactions featuring a wide range of naturalistic human-robot
interactions. The data was collected by two mobile trash barrel robots deployed
in Astor Place, New York City, over the course of a week. We invite the HRI
community to access and utilize our dataset. To the best of our knowledge, this
is the first dataset showcasing robot deployments in a complete public,
non-controlled setting involving urban residents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Workshop on Social Signal Modelling (SS4HRI '24) at HRI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inverse Submodular Maximization with Application to Human-in-the-Loop
  Multi-Robot Multi-Objective Coverage Control <span class="chip">IROS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10991v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10991v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyao Shi, Gaurav S. Sukhatme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a new type of inverse combinatorial optimization, Inverse
Submodular Maximization (ISM), for human-in-the-loop multi-robot coordination.
  Forward combinatorial optimization, defined as the process of solving a
combinatorial problem given the reward (cost)-related parameters, is widely
used in multi-robot coordination. In the standard pipeline, the reward
(cost)-related parameters are designed offline by domain experts first and then
these parameters are utilized for coordinating robots online. What if we need
to change these parameters by non-expert human supervisors who watch over the
robots during tasks to adapt to some new requirements? We are interested in the
case where human supervisors can suggest what actions to take, and the robots
need to change the internal parameters based on such suggestions. We study such
problems from the perspective of inverse combinatorial optimization, i.e., the
process of finding parameters given solutions to the problem. Specifically, we
propose a new formulation for ISM, in which we aim to find a new set of
parameters that minimally deviate from the current parameters and can make the
greedy algorithm output actions the same as those suggested by humans. We show
that such problems can be formulated as a Mixed Integer Quadratic Program
(MIQP). However, MIQP involves exponentially many binary variables, making it
intractable for the existing solver when the problem size is large. We propose
a new algorithm under the Branch $\&$ Bound paradigm to solve such problems. In
numerical simulations, we demonstrate how to use ISM in multi-robot
multi-objective coverage control, and we show that the proposed algorithm
achieves significant advantages in running time and peak memory usage compared
to directly using an existing solver.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IROS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Spatial Calibration of Near-Field MIMO Radar With Respect to
  Optical Sensors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vanessa Wirth, Johanna Bräunig, Danti Khouri, Florian Gutsche, Martin Vossiek, Tim Weyrich, Marc Stamminger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite an emerging interest in MIMO radar, the utilization of its
complementary strengths in combination with optical sensors has so far been
limited to far-field applications, due to the challenges that arise from mutual
sensor calibration in the near field. In fact, most related approaches in the
autonomous industry propose target-based calibration methods using corner
reflectors that have proven to be unsuitable for the near field. In contrast,
we propose a novel, joint calibration approach for optical RGB-D sensors and
MIMO radars that is designed to operate in the radar's near-field range, within
decimeters from the sensors. Our pipeline consists of a bespoke calibration
target, allowing for automatic target detection and localization, followed by
the spatial calibration of the two sensor coordinate systems through target
registration. We validate our approach using two different depth sensing
technologies from the optical domain. The experiments show the efficiency and
accuracy of our calibration for various target displacements, as well as its
robustness of our localization in terms of signal ambiguities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Co-Design of Canonical Underactuated Systems for Increased
  Certifiable Stability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10966v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10966v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Girlanda, Lasse Shala, Shivesh Kumar, Frank Kirchner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimal behaviours of a system to perform a specific task can be achieved by
leveraging the coupling between trajectory optimization, stabilization, and
design optimization. This approach is particularly advantageous for
underactuated systems, which are systems that have fewer actuators than degrees
of freedom and thus require for more elaborate control systems. This paper
proposes a novel co-design algorithm, namely Robust Trajectory Control with
Design optimization (RTC-D). An inner optimization layer (RTC) simultaneously
performs direct transcription (DIRTRAN) to find a nominal trajectory while
computing optimal hyperparameters for a stabilizing time-varying linear
quadratic regulator (TVLQR). RTC-D augments RTC with a design optimization
layer, maximizing the system's robustness through a time-varying Lyapunov-based
region of attraction (ROA) analysis. This analysis provides a formal guarantee
of stability for a set of off-nominal states. The proposed algorithm has been
tested on two different underactuated systems: the torque-limited simple
pendulum and the cart-pole. Extensive simulations of off-nominal initial
conditions demonstrate improved robustness, while real-system experiments show
increased insensitivity to torque disturbances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Copr. 2024 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works. PREPRINT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agonist-Antagonist Pouch Motors: Bidirectional Soft Actuators Enhanced
  by Thermally Responsive Peltier Elements <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10955v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10955v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trevor Exley, Rashmi Wijesundara, Nathan Tan, Akshay Sunkara, Xinyu He, Shuopu Wang, Bonnie Chan, Aditya Jain, Luis Espinosa, Amir Jafari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we introduce a novel Mylar-based pouch motor design that
leverages the reversible actuation capabilities of Peltier junctions to enable
agonist-antagonist muscle mimicry in soft robotics. Addressing the limitations
of traditional silicone-based materials, such as leakage and phase-change fluid
degradation, our pouch motors filled with Novec 7000 provide a durable and
leak-proof solution for geometric modeling. The integration of flexible Peltier
junctions offers a significant advantage over conventional Joule heating
methods by allowing active and reversible heating and cooling cycles. This
innovation not only enhances the reliability and longevity of soft robotic
applications but also broadens the scope of design possibilities, including the
development of agonist-antagonist artificial muscles, grippers with can
manipulate through flexion and extension, and an anchor-slip style simple
crawler design. Our findings indicate that this approach could lead to more
efficient, versatile, and durable robotic systems, marking a significant
advancement in the field of soft robotics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IROS 2024, 7 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TVIM: Thermo-Active Variable Impedance Module: Evaluating Shear-Mode
  Capabilities of Polycaprolactone <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10951v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10951v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trevor Exley, Rashmi Wijesundara, Shuopu Wang, Arian Moridani, Amir Jafari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce an advanced thermo-active variable impedance
module which builds upon our previous innovation in thermal-based impedance
adjustment for actuation systems. Our initial design harnessed the
temperature-responsive, viscoelastic properties of Polycaprolactone (PCL) to
modulate stiffness and damping, facilitated by integrated flexible Peltier
elements. While effective, the reliance on compressing and the inherent stress
relaxation characteristics of PCL led to suboptimal response times in impedance
adjustments. Addressing these limitations, the current iteration of our module
pivots to a novel 'shear-mode' operation. By conducting comprehensive shear
rheology analyses on PCL, we have identified a configuration that eliminates
the viscoelastic delay, offering a faster response with improved heat transfer
efficiency. A key advantage of our module lies in its scalability and
elimination of additional mechanical actuators for impedance adjustment. The
compactness and efficiency of thermal actuation through Peltier elements allow
for significant downsizing, making these thermal, variable impedance modules
exceptionally well-suited for applications where space constraints and actuator
weight are critical considerations. This development represents a significant
leap forward in the design of variable impedance actuators, offering a more
versatile, responsive, and compact solution for a wide range of robotic and
biomechanical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS 2024, 7 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-to-Sim Adaptation via High-Fidelity Simulation to Control a
  Wheeled-Humanoid Robot with Unknown Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10948v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10948v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donghoon Baek, Youngwoo Sim, Amartya Purushottam, Saurabh Gupta, Joao Ramos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model-based controllers using a linearized model around the system's
equilibrium point is a common approach in the control of a wheeled humanoid due
to their less computational load and ease of stability analysis. However,
controlling a wheeled humanoid robot while it lifts an unknown object presents
significant challenges, primarily due to the lack of knowledge in object
dynamics. This paper presents a framework designed for predicting the new
equilibrium point explicitly to control a wheeled-legged robot with unknown
dynamics. We estimated the total mass and center of mass of the system from its
response to initially unknown dynamics, then calculated the new equilibrium
point accordingly. To avoid using additional sensors (e.g., force torque
sensor) and reduce the effort of obtaining expensive real data, a data-driven
approach is utilized with a novel real-to-sim adaptation. A more accurate
nonlinear dynamics model, offering a closer representation of real-world
physics, is injected into a rigid-body simulation for real-to-sim adaptation.
The nonlinear dynamics model parameters were optimized using Particle Swarm
Optimization. The efficacy of this framework was validated on a physical
wheeled inverted pendulum, a simplified model of a wheeled-legged robot. The
experimental results indicate that employing a more precise analytical model
with optimized parameters significantly reduces the gap between simulation and
reality, thus improving the efficiency of a model-based controller in
controlling a wheeled robot with unknown dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViSaRL: Visual Reinforcement Learning Guided by Human Saliency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10940v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10940v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony Liang, Jesse Thomason, Erdem Bıyık
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training robots to perform complex control tasks from high-dimensional pixel
input using reinforcement learning (RL) is sample-inefficient, because image
observations are comprised primarily of task-irrelevant information. By
contrast, humans are able to visually attend to task-relevant objects and
areas. Based on this insight, we introduce Visual Saliency-Guided Reinforcement
Learning (ViSaRL). Using ViSaRL to learn visual representations significantly
improves the success rate, sample efficiency, and generalization of an RL agent
on diverse tasks including DeepMind Control benchmark, robot manipulation in
simulation and on a real robot. We present approaches for incorporating
saliency into both CNN and Transformer-based encoders. We show that visual
representations learned using ViSaRL are robust to various sources of visual
perturbations including perceptual noise and scene variations. ViSaRL nearly
doubles success rate on the real-robot tasks compared to the baseline which
does not use saliency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quaternion-Based Sliding Mode Control for Six Degrees of Freedom Flight
  Control of Quadrotors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10934v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10934v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amin Yazdanshenas, Reza Faieghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite extensive research on sliding mode control (SMC) design for
quadrotors, the existing approaches suffer from certain limitations. Euler
angle-based SMC formulations suffer from poor performance in high-pitch or
-roll maneuvers. Quaternion-based SMC approaches have unwinding issues and
complex architecture. Coordinate-free methods are slow and only almost globally
stable. This paper presents a new six degrees of freedom SMC flight controller
to address the above limitations. We use a cascaded architecture with a
position controller in the outer loop and a quaternion-based attitude
controller in the inner loop. The position controller generates the desired
trajectory for the attitude controller using a coordinate-free approach. The
quaternion-based attitude controller uses the natural characteristics of the
quaternion hypersphere, featuring a simple structure while providing global
stability and avoiding unwinding issues. We compare our controller with three
other common control methods conducting challenging maneuvers like flip-over
and high-speed trajectory tracking in the presence of model uncertainties and
disturbances. Our controller consistently outperforms the benchmark approaches
with less control effort and actuator saturation, offering highly effective and
efficient flight control.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning-Based Design of Off-Policy Gaussian Controllers: Integrating
  Model Predictive Control and Gaussian Process Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10932v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10932v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiva Kumar Tekumatla, Varun Gampa, Siavash Farzan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an off-policy Gaussian Predictive Control (GPC) framework
aimed at solving optimal control problems with a smaller computational
footprint, thereby facilitating real-time applicability while ensuring critical
safety considerations. The proposed controller imitates classical control
methodologies by modeling the optimization process through a Gaussian process
and employs Gaussian Process Regression to learn from the Model Predictive
Control (MPC) algorithm. Notably, the Gaussian Process setup does not
incorporate a built-in model, enhancing its applicability to a broad range of
control problems. We applied this framework experimentally to a differential
drive mobile robot, tasking it with trajectory tracking and obstacle avoidance.
Leveraging the off-policy aspect, the controller demonstrated adaptability to
diverse trajectories and obstacle behaviors. Simulation experiments confirmed
the effectiveness of the proposed GPC method, emphasizing its ability to learn
the dynamics of optimal control strategies. Consequently, our findings
highlight the significant potential of off-policy Gaussian Predictive Control
in achieving real-time optimal control for handling of robotic systems in
safety-critical scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACC 2024. 8 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PAAMP: Polytopic Action-Set And Motion Planning For Long Horizon Dynamic
  Motion Planning via Mixed Integer Linear Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10924v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10924v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshay Jaitly, Siavash Farzan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimization methods for long-horizon, dynamically feasible motion planning
in robotics tackle challenging non-convex and discontinuous optimization
problems. Traditional methods often falter due to the nonlinear characteristics
of these problems. We introduce a technique that utilizes learned
representations of the system, known as Polytopic Action Sets, to efficiently
compute long-horizon trajectories. By employing a suitable sequence of
Polytopic Action Sets, we transform the long-horizon dynamically feasible
motion planning problem into a Linear Program. This reformulation enables us to
address motion planning as a Mixed Integer Linear Program (MILP). We
demonstrate the effectiveness of a Polytopic Action-Set and Motion Planning
(PAAMP) approach by identifying swing-up motions for a torque-constrained
pendulum within approximately 0.75 milliseconds. This approach is well-suited
for solving complex motion planning and long-horizon Constraint Satisfaction
Problems (CSPs) in dynamic and underactuated systems such as legged and aerial
robots.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 10 figures, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robotic Task Success Evaluation Under Multi-modal Non-Parametric Object
  Pose Uncertainty <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10874v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10874v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lakshadeep Naik, Thorbjørn Mosekjær Iversen, Aljaz Kramberger, Norbert Krüger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate 6D object pose estimation is essential for various robotic tasks.
Uncertain pose estimates can lead to task failures; however, a certain degree
of error in the pose estimates is often acceptable. Hence, by quantifying
errors in the object pose estimate and acceptable errors for task success,
robots can make informed decisions. This is a challenging problem as both the
object pose uncertainty and acceptable error for the robotic task are often
multi-modal and cannot be parameterized with commonly used uni-modal
distributions. In this paper, we introduce a framework for evaluating robotic
task success under object pose uncertainty, representing both the estimated
error space of the object pose and the acceptable error space for task success
using multi-modal non-parametric probability distributions. The proposed
framework pre-computes the acceptable error space for task success using
dynamic simulations and subsequently integrates the pre-computed acceptable
error space over the estimated error space of the object pose to predict the
likelihood of the task success. We evaluated the proposed framework on two
mobile manipulation tasks. Our results show that by representing the estimated
and the acceptable error space using multi-modal non-parametric distributions,
we achieve higher task success rates and fewer failures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning with Options 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayoub Ghriss, Masashi Sugiyama, Alessandro Lazaric
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current thesis aims to explore the reinforcement learning field and build
on existing methods to produce improved ones to tackle the problem of learning
in high-dimensional and complex environments. It addresses such goals by
decomposing learning tasks in a hierarchical fashion known as Hierarchical
Reinforcement Learning.
  We start in the first chapter by getting familiar with the Markov Decision
Process framework and presenting some of its recent techniques that the
following chapters use. We then proceed to build our Hierarchical Policy
learning as an answer to the limitations of a single primitive policy. The
hierarchy is composed of a manager agent at the top and employee agents at the
lower level.
  In the last chapter, which is the core of this thesis, we attempt to learn
lower-level elements of the hierarchy independently of the manager level in
what is known as the "Eigenoption". Based on the graph structure of the
environment, Eigenoptions allow us to build agents that are aware of the
geometric and dynamic properties of the environment. Their decision-making has
a special property: it is invariant to symmetric transformations of the
environment, allowing as a consequence to greatly reduce the complexity of the
learning task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Master Thesis 2018, MVA ENS Paris-Saclay, Tokyo RIKEN AIP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GAgent: An Adaptive Rigid-Soft Gripping Agent with Vision Language
  Models for Complex Lighting Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10850v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10850v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuowei Li, Miao Zhang, Xiaotian Lin, Meng Yin, Shuai Lu, Xueqian Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces GAgent: an Gripping Agent designed for open-world
environments that provides advanced cognitive abilities via VLM agents and
flexible grasping abilities with variable stiffness soft grippers. GAgent
comprises three primary components - Prompt Engineer module, Visual-Language
Model (VLM) core and Workflow module. These three modules enhance gripper
success rates by recognizing objects and materials and accurately estimating
grasp area even under challenging lighting conditions. As part of creativity,
researchers also created a bionic hybrid soft gripper with variable stiffness
capable of gripping heavy loads while still gently engaging objects. This
intelligent agent, featuring VLM-based cognitive processing with bionic design,
shows promise as it could potentially benefit UAVs in various scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MSI-NeRF: Linking Omni-Depth with View Synthesis through Multi-Sphere
  Image aided Generalizable Neural Radiance Field 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10840v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10840v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongyu Yan, Guanyu Huang, Fengyu Quan, Haoyao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Panoramic observation using fisheye cameras is significant in robot
perception, reconstruction, and remote operation. However, panoramic images
synthesized by traditional methods lack depth information and can only provide
three degrees-of-freedom (3DoF) rotation rendering in virtual reality
applications. To fully preserve and exploit the parallax information within the
original fisheye cameras, we introduce MSI-NeRF, which combines deep learning
omnidirectional depth estimation and novel view rendering. We first construct a
multi-sphere image as a cost volume through feature extraction and warping of
the input images. It is then processed by geometry and appearance decoders,
respectively. Unlike methods that regress depth maps directly, we further build
an implicit radiance field using spatial points and interpolated 3D feature
vectors as input. In this way, we can simultaneously realize omnidirectional
depth estimation and 6DoF view synthesis. Our method is trained in a
semi-self-supervised manner. It does not require target view images and only
uses depth data for supervision. Our network has the generalization ability to
reconstruct unknown scenes efficiently using only four images. Experimental
results show that our method outperforms existing methods in depth estimation
and novel view synthesis tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, Submitted to IEEE/RSJ International Conference on
  Intelligent Robots and Systems 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Reinforcement Learning-based Large-scale Robot Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhong Cao, Rui Zhao, Yizhuo Wang, Bairan Xiang, Guillaume Sartoretti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose a deep reinforcement learning (DRL) based reactive
planner to solve large-scale Lidar-based autonomous robot exploration problems
in 2D action space. Our DRL-based planner allows the agent to reactively plan
its exploration path by making implicit predictions about unknown areas, based
on a learned estimation of the underlying transition model of the environment.
To this end, our approach relies on learned attention mechanisms for their
powerful ability to capture long-term dependencies at different spatial scales
to reason about the robot's entire belief over known areas. Our approach relies
on ground truth information (i.e., privileged learning) to guide the
environment estimation during training, as well as on a graph rarefaction
algorithm, which allows models trained in small-scale environments to scale to
large-scale ones. Simulation results show that our model exhibits better
exploration efficiency (12% in path length, 6% in makespan) and lower planning
time (60%) than the state-of-the-art planners in a 130m x 100m benchmark
scenario. We also validate our learned model on hardware.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>\c{opyright} 20XX IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ H3-Mapping: Quasi-Heterogeneous Feature Grids for Real-time Dense
  Mapping Using Hierarchical Hybrid Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10821v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10821v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxing Jiang, Yiming Luo, Boyu Zhou, Shaojie Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, implicit online dense mapping methods have achieved
high-quality reconstruction results, showcasing great potential in robotics,
AR/VR, and digital twins applications. However, existing methods struggle with
slow texture modeling which limits their real-time performance. To address
these limitations, we propose a NeRF-based dense mapping method that enables
faster and higher-quality reconstruction. To improve texture modeling, we
introduce quasi-heterogeneous feature grids, which inherit the fast querying
ability of uniform feature grids while adapting to varying levels of texture
complexity. Besides, we present a gradient-aided coverage-maximizing strategy
for keyframe selection that enables the selected keyframes to exhibit a closer
focus on rich-textured regions and a broader scope for weak-textured areas.
Experimental results demonstrate that our method surpasses existing NeRF-based
approaches in texture fidelity, geometry accuracy, and time consumption. The
code for our method will be available at:
https://github.com/SYSU-STAR/H3-Mapping.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 11 figures, submitted to IEEE Robotics and Automation
  Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DarkGS: Learning Neural Illumination and 3D Gaussians Relighting for
  Robotic Exploration in the Dark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Zhang, Kaining Huang, Weiming Zhi, Matthew Johnson-Roberson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans have the remarkable ability to construct consistent mental models of
an environment, even under limited or varying levels of illumination. We wish
to endow robots with this same capability. In this paper, we tackle the
challenge of constructing a photorealistic scene representation under poorly
illuminated conditions and with a moving light source. We approach the task of
modeling illumination as a learning problem, and utilize the developed
illumination model to aid in scene reconstruction. We introduce an innovative
framework that uses a data-driven approach, Neural Light Simulators (NeLiS), to
model and calibrate the camera-light system. Furthermore, we present DarkGS, a
method that applies NeLiS to create a relightable 3D Gaussian scene model
capable of real-time, photorealistic rendering from novel viewpoints. We show
the applicability and robustness of our proposed simulator and system in a
variety of real-world environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Trajectory Forecasting and Generation with Conditional Flow
  Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10809v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10809v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sean Ye, Matthew Gombolay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory prediction and generation are vital for autonomous robots
navigating dynamic environments. While prior research has typically focused on
either prediction or generation, our approach unifies these tasks to provide a
versatile framework and achieve state-of-the-art performance. Diffusion models,
which are currently state-of-the-art for learned trajectory generation in
long-horizon planning and offline reinforcement learning tasks, rely on a
computationally intensive iterative sampling process. This slow process impedes
the dynamic capabilities of robotic systems. In contrast, we introduce
Trajectory Conditional Flow Matching (T-CFM), a novel data-driven approach that
utilizes flow matching techniques to learn a solver time-varying vector field
for efficient and fast trajectory generation. We demonstrate the effectiveness
of T-CFM on three separate tasks: adversarial tracking, real-world aircraft
trajectory forecasting, and long-horizon planning. Our model outperforms
state-of-the-art baselines with an increase of 35% in predictive accuracy and
142% increase in planning performance. Notably, T-CFM achieves up to
100$\times$ speed-up compared to diffusion-based models without sacrificing
accuracy, which is crucial for real-time decision making in robotics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Words to Routes: Applying Large Language Models to Vehicle Routing <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhehui Huang, Guangyao Shi, Gaurav S. Sukhatme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs have shown impressive progress in robotics (e.g., manipulation and
navigation) with natural language task descriptions. The success of LLMs in
these tasks leads us to wonder: What is the ability of LLMs to solve vehicle
routing problems (VRPs) with natural language task descriptions? In this work,
we study this question in three steps. First, we construct a dataset with 21
types of single- or multi-vehicle routing problems. Second, we evaluate the
performance of LLMs across four basic prompt paradigms of text-to-code
generation, each involving different types of text input. We find that the
basic prompt paradigm, which generates code directly from natural language task
descriptions, performs the best for GPT-4, achieving 56% feasibility, 40%
optimality, and 53% efficiency. Third, based on the observation that LLMs may
not be able to provide correct solutions at the initial attempt, we propose a
framework that enables LLMs to refine solutions through self-reflection,
including self-debugging and self-verification. With GPT-4, our proposed
framework achieves a 16% increase in feasibility, a 7% increase in optimality,
and a 15% increase in efficiency. Moreover, we examine the sensitivity of GPT-4
to task descriptions, specifically focusing on how its performance changes when
certain details are omitted from the task descriptions, yet the core meaning is
preserved. Our findings reveal that such omissions lead to a notable decrease
in performance: 4% in feasibility, 4% in optimality, and 5% in efficiency.
Website: https://sites.google.com/view/words-to-routes/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Robotics and Automation Society (IROS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion-Reinforcement Learning Hierarchical Motion Planning in
  Adversarial Multi-agent Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Wu, Sean Ye, Manisha Natarajan, Matthew C. Gombolay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning- (RL-)based motion planning has recently shown the
potential to outperform traditional approaches from autonomous navigation to
robot manipulation. In this work, we focus on a motion planning task for an
evasive target in a partially observable multi-agent adversarial
pursuit-evasion games (PEG). These pursuit-evasion problems are relevant to
various applications, such as search and rescue operations and surveillance
robots, where robots must effectively plan their actions to gather intelligence
or accomplish mission tasks while avoiding detection or capture themselves. We
propose a hierarchical architecture that integrates a high-level diffusion
model to plan global paths responsive to environment data while a low-level RL
algorithm reasons about evasive versus global path-following behavior. Our
approach outperforms baselines by 51.2% by leveraging the diffusion model to
guide the RL algorithm for more efficient exploration and improves the
explanability and predictability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE Robotics and Automation
  Letters (RA-L) for possible publication. Copyright may be transferred without
  notice, after which this version may no longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identifying Optimal Launch Sites of High-Altitude Latex-Balloons using
  Bayesian Optimisation for the Task of Station-Keeping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jack Saunders, Sajad Saeedi, Adam Hartshorne, Binbin Xu, Özgur Şimşek, Alan Hunter, Wenbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Station-keeping tasks for high-altitude balloons show promise in areas such
as ecological surveys, atmospheric analysis, and communication relays. However,
identifying the optimal time and position to launch a latex high-altitude
balloon is still a challenging and multifaceted problem. For example, tasks
such as forest fire tracking place geometric constraints on the launch location
of the balloon. Furthermore, identifying the most optimal location also heavily
depends on atmospheric conditions. We first illustrate how reinforcement
learning-based controllers, frequently used for station-keeping tasks, can
exploit the environment. This exploitation can degrade performance on unseen
weather patterns and affect station-keeping performance when identifying an
optimal launch configuration. Valuing all states equally in the region, the
agent exploits the region's geometry by flying near the edge, leading to risky
behaviours. We propose a modification which compensates for this exploitation
and finds this leads to, on average, higher steps within the target region on
unseen data. Then, we illustrate how Bayesian Optimisation (BO) can identify
the optimal launch location to perform station-keeping tasks, maximising the
expected undiscounted return from a given rollout. We show BO can find this
launch location in fewer steps compared to other optimisation methods. Results
indicate that, surprisingly, the most optimal location to launch from is not
commonly within the target region. Please find further information about our
project at https://sites.google.com/view/bo-lauch-balloon/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DPPE: Dense Pose Estimation in a Plenoxels Environment using Gradient
  Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10773v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10773v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Kolios, Yeganeh Bahoo, Sajad Saeedi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present DPPE, a dense pose estimation algorithm that functions over a
Plenoxels environment. Recent advances in neural radiance field techniques have
shown that it is a powerful tool for environment representation. More recent
neural rendering algorithms have significantly improved both training duration
and rendering speed. Plenoxels introduced a fully-differentiable radiance field
technique that uses Plenoptic volume elements contained in voxels for
rendering, offering reduced training times and better rendering accuracy, while
also eliminating the neural net component. In this work, we introduce a 6-DoF
monocular RGB-only pose estimation procedure for Plenoxels, which seeks to
recover the ground truth camera pose after a perturbation. We employ a
variation on classical template matching techniques, using stochastic gradient
descent to optimize the pose by minimizing errors in re-rendering. In
particular, we examine an approach that takes advantage of the rapid rendering
speed of Plenoxels to numerically approximate part of the pose gradient, using
a central differencing technique. We show that such methods are effective in
pose estimation. Finally, we perform ablations over key components of the
problem space, with a particular focus on image subsampling and Plenoxel grid
resolution. Project website: https://sites.google.com/view/dppe
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Task-Driven Manipulation with Reconfigurable Parallel Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10768v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10768v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Morton, Mark Cutkosky, Marco Pavone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ReachBot, a proposed robotic platform, employs extendable booms as limbs for
mobility in challenging environments, such as martian caves. When attached to
the environment, ReachBot acts as a parallel robot, with reconfiguration driven
by the ability to detach and re-place the booms. This ability enables
manipulation-focused scientific objectives: for instance, through operating
tools, or handling and transporting samples. To achieve these capabilities, we
develop a two-part solution, optimizing for robustness against task uncertainty
and stochastic failure modes. First, we present a mixed-integer stance planner
to determine the positioning of ReachBot's booms to maximize the task wrench
space about the nominal point(s). Second, we present a convex tension planner
to determine boom tensions for the desired task wrenches, accounting for the
probabilistic nature of microspine grasping. We demonstrate improvements in key
robustness metrics from the field of dexterous manipulation, and show a large
increase in the volume of the manipulation workspace. Finally, we employ
Monte-Carlo simulation to validate the robustness of these methods,
demonstrating good performance across a range of randomized tasks and
environments, and generalization to cable-driven morphologies. We make our code
available at our project webpage,
https://stanfordasl.github.io/reachbot_manipulation/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NARRATE: Versatile Language Architecture for Optimal Control in Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seif Ismail, Antonio Arbues, Ryan Cotterell, René Zurbrügg, Carmen Amo Alonso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The impressive capabilities of Large Language Models (LLMs) have led to
various efforts to enable robots to be controlled through natural language
instructions, opening exciting possibilities for human-robot interaction The
goal is for the motor-control task to be performed accurately, efficiently and
safely while also enjoying the flexibility imparted by LLMs to specify and
adjust the task through natural language. In this work, we demonstrate how a
careful layering of an LLM in combination with a Model Predictive Control (MPC)
formulation allows for accurate and flexible robotic control via natural
language while taking into consideration safety constraints. In particular, we
rely on the LLM to effectively frame constraints and objective functions as
mathematical expressions, which are later used in the motor-control module via
MPC. The transparency of the optimization formulation allows for
interpretability of the task and enables adjustments through human feedback. We
demonstrate the validity of our method through extensive experiments on
long-horizon reasoning, contact-rich, and multi-object interaction tasks. Our
evaluations show that NARRATE outperforms current existing methods on these
benchmarks and effectively transfers to the real world on two different
embodiments. Videos, Code and Prompts at narrate-mpc.github.io
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scheduling Drone and Mobile Charger via Hybrid-Action Deep Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jizhe Dou, Haotian Zhang, Guodong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently there has been a growing interest in industry and academia,
regarding the use of wireless chargers to prolong the operational longevity of
unmanned aerial vehicles (commonly knowns as drones). In this paper we consider
a charger-assisted drone application: a drone is deployed to observe a set
points of interest, while a charger can move to recharge the drone's battery.
We focus on the route and charging schedule of the drone and the mobile
charger, to obtain high observation utility with the shortest possible time,
while ensuring the drone remains operational during task execution.
Essentially, this proposed drone-charger scheduling problem is a multi-stage
decision-making process, in which the drone and the mobile charger act as two
agents who cooperate to finish a task. The discrete-continuous hybrid action
space of the two agents poses a significant challenge in our problem. To
address this issue, we present a hybrid-action deep reinforcement learning
framework, called HaDMC, which uses a standard policy learning algorithm to
generate latent continuous actions. Motivated by representation learning, we
specifically design and train an action decoder. It involves two pipelines to
convert the latent continuous actions into original discrete and continuous
actions, by which the drone and the charger can directly interact with
environment. We embed a mutual learning scheme in model training, emphasizing
the collaborative rather than individual actions. We conduct extensive
numerical experiments to evaluate HaDMC and compare it with state-of-the-art
deep reinforcement learning approaches. The experimental results show the
effectiveness and efficiency of our solution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CORN: Contact-based Object Representation for Nonprehensile Manipulation
  of General Unseen Objects <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoonyoung Cho, Junhyek Han, Yoontae Cho, Beomjoon Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nonprehensile manipulation is essential for manipulating objects that are too
thin, large, or otherwise ungraspable in the wild. To sidestep the difficulty
of contact modeling in conventional modeling-based approaches, reinforcement
learning (RL) has recently emerged as a promising alternative. However,
previous RL approaches either lack the ability to generalize over diverse
object shapes, or use simple action primitives that limit the diversity of
robot motions. Furthermore, using RL over diverse object geometry is
challenging due to the high cost of training a policy that takes in
high-dimensional sensory inputs. We propose a novel contact-based object
representation and pretraining pipeline to tackle this. To enable massively
parallel training, we leverage a lightweight patch-based transformer
architecture for our encoder that processes point clouds, thus scaling our
training across thousands of environments. Compared to learning from scratch,
or other shape representation baselines, our representation facilitates both
time- and data-efficient learning. We validate the efficacy of our overall
system by zero-shot transferring the trained policy to novel real-world
objects. Code and videos are available at
https://sites.google.com/view/contact-non-prehensile.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fully Distributed Cooperative Multi-agent Underwater Obstacle Avoidance
  Under Dog Walking Paradigm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kanzhong Yao, Ognjen Marjanovic, Simon Watson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Navigation in cluttered underwater environments is challenging, especially
when there are constraints on communication and self-localisation. Part of the
fully distributed underwater navigation problem has been resolved by
introducing multi-agent robot teams, however when the environment becomes
cluttered, the problem remains unresolved. In this paper, we first studied the
connection between everyday activity of dog walking and the cooperative
underwater obstacle avoidance problem. Inspired by this analogy, we propose a
novel dog walking paradigm and implement it in a multi-agent underwater system.
Simulations were conducted across various scenarios, with performance
benchmarked against traditional methods utilising Image-Based Visual Servoing
in a multi-agent setup. Results indicate that our dog walking-inspired paradigm
significantly enhances cooperative behavior among agents and outperforms the
existing approach in navigating through obstacles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ iDb-RRT: Sampling-based Kinodynamic Motion Planning with Motion
  Primitives and Trajectory Optimization <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joaquim Ortiz-Haro, Wolfgang Hönig, Valentin N. Hartmann, Marc Toussaint, Ludovic Righetti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapidly-exploring Random Trees (RRT) and its variations have emerged as a
robust and efficient tool for finding collision-free paths in robotic systems.
However, adding dynamic constraints makes the motion planning problem
significantly harder, as it requires solving two-value boundary problems
(computationally expensive) or propagating random control inputs
(uninformative). Alternatively, Iterative Discontinuity Bounded A* (iDb-A*),
introduced in our previous study, combines search and optimization iteratively.
The search step connects short trajectories (motion primitives) while allowing
a bounded discontinuity between the motion primitives, which is later repaired
in the trajectory optimization step.
  Building upon these foundations, in this paper, we present iDb-RRT, a
sampling-based kinodynamic motion planning algorithm that combines motion
primitives and trajectory optimization within the RRT framework. iDb-RRT is
probabilistically complete and can be implemented in forward or bidirectional
mode. We have tested our algorithm across a benchmark suite comprising 30
problems, spanning 8 different systems, and shown that iDb-RRT can find
solutions up to 10x faster than previous methods, especially in complex
scenarios that require long trajectories or involve navigating through narrow
passages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, submitted to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Convex Hull Cheapest Insertion Heuristic for Precedence Constrained
  Traveling Salesperson Problems or Sequential Ordering Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05333v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05333v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mithun Goutham, Stephanie Stockar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The convex hull cheapest insertion heuristic is a well-known method that
efficiently generates good solutions to the Traveling Salesperson Problem.
However, this heuristic has not been adapted to account for precedence
constraints that restrict the order in which locations can be visited. Such
constraints result in the precedence constrained traveling salesperson problem
or the sequential ordering problem, which are commonly encountered in
applications where items have to be picked up before they are delivered. In
this paper, we present an adapted version of this heuristic that accounts for
precedence constraints in the problem definition. This algorithm is compared
with the widely used Nearest Neighbor heuristic on the TSPLIB benchmark data
with added precedence constraints. It is seen that the proposed algorithm is
particularly well suited to cases where delivery nodes are centrally
positioned, with pickup nodes located in the periphery, outperforming the
Nearest Neighbor algorithm in 97\% of the examined instances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2302.06582</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guided Data Augmentation for Offline Reinforcement Learning and
  Imitation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18247v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18247v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas E. Corrado, Yuxiao Qu, John U. Balis, Adam Labiosa, Josiah P. Hanna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In offline reinforcement learning (RL), an RL agent learns to solve a task
using only a fixed dataset of previously collected data. While offline RL has
been successful in learning real-world robot control policies, it typically
requires large amounts of expert-quality data to learn effective policies that
generalize to out-of-distribution states. Unfortunately, such data is often
difficult and expensive to acquire in real-world tasks. Several recent works
have leveraged data augmentation (DA) to inexpensively generate additional
data, but most DA works apply augmentations in a random fashion and ultimately
produce highly suboptimal augmented experience. In this work, we propose Guided
Data Augmentation (GuDA), a human-guided DA framework that generates
expert-quality augmented data. The key insight behind GuDA is that while it may
be difficult to demonstrate the sequence of actions required to produce expert
data, a user can often easily characterize when an augmented trajectory segment
represents progress toward task completion. Thus, a user can restrict the space
of possible augmentations to automatically reject suboptimal augmented data. To
extract a policy from GuDA, we use off-the-shelf offline reinforcement learning
and behavior cloning algorithms. We evaluate GuDA on a physical robot soccer
task as well as simulated D4RL navigation tasks, a simulated autonomous driving
task, and a simulated soccer task. Empirically, GuDA enables learning given a
small initial dataset of potentially suboptimal experience and outperforms a
random DA strategy as well as a model-based DA strategy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Inertial Parameter Identification of Unknown Object with
  Humanoid Robot using Real-to-Sim Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09810v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09810v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donghoon Baek, Bo Peng, Saurabh Gupta, Joao Ramos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a fast learning-based inertial parameters estimation framework
capable of understanding the dynamics of an unknown object to enable a humanoid
(or manipulator) to more safely and accurately interact with its surrounding
environments. Unlike most relevant literature, our framework doesn't require to
use of a force/torque sensor, vision system, and a long-horizon trajectory. To
achieve fast inertia parameter estimation, a time-series data-driven regression
model is utilized rather than solving a constrained optimization problem. Due
to the challenge of obtaining a large number of the ground truth of inertia
parameters in the real world, we acquire a reliable dataset in a high-fidelity
simulation that is developed using a real-to-sim adaptation. The adaptation
method we introduced consists of two components: 1) \textit{Robot System
Identification} and 2) \textit{Gaussian Processes}. We demonstrate our method
with a 4-DOF single manipulator of a wheeled humanoid robot, SATYRR. Results
show that our method can identify the inertial parameters of various unknown
objects quickly while maintaining sufficient accuracy compared to other
methods. Manipulation and locomotion experiments were also carried out to show
the benefit of using the estimated inertia parameters from control perspective.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Memory-Consistent Neural Networks for Imitation Learning <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06171v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06171v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaustubh Sridhar, Souradeep Dutta, Dinesh Jayaraman, James Weimer, Insup Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning considerably simplifies policy synthesis compared to
alternative approaches by exploiting access to expert demonstrations. For such
imitation policies, errors away from the training samples are particularly
critical. Even rare slip-ups in the policy action outputs can compound quickly
over time, since they lead to unfamiliar future states where the policy is
still more likely to err, eventually causing task failures. We revisit simple
supervised ``behavior cloning'' for conveniently training the policy from
nothing more than pre-recorded demonstrations, but carefully design the model
class to counter the compounding error phenomenon. Our ``memory-consistent
neural network'' (MCNN) outputs are hard-constrained to stay within clearly
specified permissible regions anchored to prototypical ``memory'' training
samples. We provide a guaranteed upper bound for the sub-optimality gap induced
by MCNN policies. Using MCNNs on 10 imitation learning tasks, with MLP,
Transformer, and Diffusion backbones, spanning dexterous robotic manipulation
and driving, proprioceptive inputs and visual inputs, and varying sizes and
types of demonstration data, we find large and consistent gains in performance,
validating that MCNNs are better-suited than vanilla deep neural networks for
imitation learning applications. Website:
https://sites.google.com/view/mcnn-imitation
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024. 26 pages (9 main pages)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Self-Supervised</span> Learning for Joint Pushing and Grasping Policies in
  Highly Cluttered Environments <span class="chip">ICRA2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.02511v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.02511v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongliang Wang, Kamal Mokhtar, Cock Heemskerk, Hamidreza Kasaei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robots often face situations where grasping a goal object is desirable but
not feasible due to other present objects preventing the grasp action. We
present a deep Reinforcement Learning approach to learn grasping and pushing
policies for manipulating a goal object in highly cluttered environments to
address this problem. In particular, a dual Reinforcement Learning model
approach is proposed, which presents high resilience in handling complicated
scenes, reaching an average of 98% task completion using primitive objects in a
simulation environment. To evaluate the performance of the proposed approach,
we performed two extensive sets of experiments in packed objects and a pile of
object scenarios with a total of 1000 test runs in simulation. Experimental
results showed that the proposed method worked very well in both scenarios and
outperformed the recent state-of-the-art approaches. Demo video, trained
models, and source code for the results reproducibility purpose are publicly
available. https://sites.google.com/view/pushandgrasp/home
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for publication at the ICRA2024
  conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ARTEMIS: AI-driven Robotic Triage Labeling and Emergency Medical
  Information System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08865v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08865v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Revanth Krishna Senthilkumaran, Mridu Prashanth, Hrishikesh Viswanath, Sathvika Kotha, Kshitij Tiwari, Aniket Bera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mass casualty incidents (MCIs) pose a significant challenge to emergency
medical services by overwhelming available resources and personnel. Effective
victim assessment is the key to minimizing casualties during such a crisis. We
introduce ARTEMIS, an AI-driven Robotic Triage Labeling and Emergency Medical
Information System, to aid first responders in MCI events. It leverages speech
processing, natural language processing, and deep learning to help with acuity
classification. This is deployed on a quadruped that performs victim
localization and preliminary injury severity assessment. First responders
access victim information through a Graphical User Interface that is updated in
real-time. To validate our proposed algorithmic triage protocol, we used the
Unitree Go1 quadruped. The robot identifies humans, interacts with them, gets
vitals and information, and assigns an acuity label. Simulations of an MCI in
software and a controlled environment outdoors were conducted. The system
achieved a triage-level classification precision of over 74% on average and 99%
for the most critical victims, i.e. level 1 acuity, outperforming
state-of-the-art deep learning-based triage labeling systems. In this paper, we
showcase the potential of human-robot interaction in assisting medical
personnel in MCI events.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PINSAT: Parallelized Interleaving of Graph Search and Trajectory
  Optimization for Kinodynamic Motion Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.08948v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.08948v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ramkumar Natarajan, Shohin Mukherjee, Howie Choset, Maxim Likhachev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory optimization is a widely used technique in robot motion planning
for letting the dynamics and constraints on the system shape and synthesize
complex behaviors. Several previous works have shown its benefits in
high-dimensional continuous state spaces and under differential constraints.
However, long time horizons and planning around obstacles in non-convex spaces
pose challenges in guaranteeing convergence or finding optimal solutions. As a
result, discrete graph search planners and sampling-based planers are preferred
when facing obstacle-cluttered environments. A recently developed algorithm
called INSAT effectively combines graph search in the low-dimensional subspace
and trajectory optimization in the full-dimensional space for global
kinodynamic planning over long horizons. Although INSAT successfully reasoned
about and solved complex planning problems, the numerous expensive calls to an
optimizer resulted in large planning times, thereby limiting its practical use.
Inspired by the recent work on edge-based parallel graph search, we present
PINSAT, which introduces systematic parallelization in INSAT to achieve lower
planning times and higher success rates, while maintaining significantly lower
costs over relevant baselines. We demonstrate PINSAT by evaluating it on 6 DoF
kinodynamic manipulation planning with obstacles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preprocessing-based Kinodynamic Motion Planning Framework for
  Intercepting Projectiles using a Robot Manipulator <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.08022v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.08022v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ramkumar Natarajan, Hanlan Yang, Qintong Xie, Yash Oza, Manash Pratim Das, Fahad Islam, Muhammad Suhail Saleem, Howie Choset, Maxim Likhachev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We are interested in studying sports with robots and starting with the
problem of intercepting a projectile moving toward a robot manipulator equipped
with a shield. To successfully perform this task, the robot needs to (i) detect
the incoming projectile, (ii) predict the projectile's future motion, (iii)
plan a minimum-time rapid trajectory that can evade obstacles and intercept the
projectile, and (iv) execute the planned trajectory. These four steps must be
performed under the manipulator's dynamic limits and extreme time constraints
(<350ms in our setting) to successfully intercept the projectile. In addition,
we want these trajectories to be smooth to reduce the robot's joint torques and
the impulse on the platform on which it is mounted. To this end, we propose a
kinodynamic motion planning framework that preprocesses smooth trajectories
offline to allow real-time collision-free executions online. We present an
end-to-end pipeline along with our planning framework, including perception,
prediction, and execution modules. We evaluate our framework experimentally in
simulation and show that it has a higher blocking success rate than the
baselines. Further, we deploy our pipeline on a robotic system comprising an
industrial arm (ABB IRB-1600) and an onboard stereo camera (ZED 2i), which
achieves a 78% success rate in projectile interceptions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the IEEE International Conference on Robotics and
  Automation (ICRA) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Powered Descent Guidance via First-Order Optimization with Expansive
  Projection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00397v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00397v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiwoo Choi, Jong-Han Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a first-order method for solving optimal powered
descent guidance (PDG) problems, that directly handles the nonconvex
constraints associated with the maximum and minimum thrust bounds with varying
mass and the pointing angle constraints on thrust vectors. This issue has been
conventionally circumvented via lossless convexification (LCvx), which lifts a
nonconvex feasible set to a higher-dimensional convex set, and via linear
approximation of another nonconvex feasible set defined by exponential
functions. However, this approach sometimes results in an infeasible solution
when the solution obtained from the higher-dimensional space is projected back
to the original space, especially when the problem involves a nonoptimal time
of flight. Additionally, the Taylor series approximation introduces an
approximation error that grows with both flight time and deviation from the
reference trajectory. In this paper, we introduce a first-order approach that
makes use of orthogonal projections onto nonconvex sets, allowing expansive
projection (ExProj). We show that 1) this approach produces a feasible solution
with better performance even for the nonoptimal time of flight cases for which
conventional techniques fail to generate achievable trajectories and 2) the
proposed method compensates for the linearization error that arises from Taylor
series approximation, thus generating a superior guidance solution with less
fuel consumption. We provide numerical examples featuring quantitative
assessments to elucidate the effectiveness of the proposed methodology,
particularly in terms of fuel consumption and flight time. Our analysis
substantiates the assertion that the proposed approach affords enhanced
flexibility in devising viable trajectories for a diverse array of planetary
soft landing scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SoftMAC: Differentiable Soft Body Simulation with Forecast-based Contact
  Model and Two-way Coupling with Articulated Rigid Bodies and Clothes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03297v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03297v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Min Liu, Gang Yang, Siyuan Luo, Lin Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentiable physics simulation provides an avenue to tackle previously
intractable challenges through gradient-based optimization, thereby greatly
improving the efficiency of solving robotics-related problems. To apply
differentiable simulation in diverse robotic manipulation scenarios, a key
challenge is to integrate various materials in a unified framework. We present
SoftMAC, a differentiable simulation framework that couples soft bodies with
articulated rigid bodies and clothes. SoftMAC simulates soft bodies with the
continuum-mechanics-based Material Point Method (MPM). We provide a novel
forecast-based contact model for MPM, which effectively reduces penetration
without introducing other artifacts like unnatural rebound. To couple MPM
particles with deformable and non-volumetric clothes meshes, we also propose a
penetration tracing algorithm that reconstructs the signed distance field in
local area. Diverging from previous works, SoftMAC simulates the complete
dynamics of each modality and incorporates them into a cohesive system with an
explicit and differentiable coupling mechanism. The feature empowers SoftMAC to
handle a broader spectrum of interactions, such as soft bodies serving as
manipulators and engaging with underactuated systems. We conducted
comprehensive experiments to validate the effectiveness and accuracy of the
proposed differentiable pipeline in downstream robotic manipulation
applications. Supplementary materials and videos are available on our project
website at https://sites.google.com/view/softmac.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ${\tt MORALS}$: Analysis of High-Dimensional Robot Controllers via
  Topological Tools in a Latent Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03246v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03246v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ewerton R. Vieira, Aravind Sivaramakrishnan, Sumanth Tangirala, Edgar Granados, Konstantin Mischaikow, Kostas E. Bekris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating the region of attraction (${\tt RoA}$) for a robot controller is
essential for safe application and controller composition. Many existing
methods require a closed-form expression that limit applicability to
data-driven controllers. Methods that operate only over trajectory rollouts
tend to be data-hungry. In prior work, we have demonstrated that topological
tools based on ${\it Morse Graphs}$ (directed acyclic graphs that
combinatorially represent the underlying nonlinear dynamics) offer
data-efficient ${\tt RoA}$ estimation without needing an analytical model. They
struggle, however, with high-dimensional systems as they operate over a
state-space discretization. This paper presents ${\it Mo}$rse Graph-aided
discovery of ${\it R}$egions of ${\it A}$ttraction in a learned ${\it L}$atent
${\it S}$pace (${\tt MORALS}$). The approach combines auto-encoding neural
networks with Morse Graphs. ${\tt MORALS}$ shows promising predictive
capabilities in estimating attractors and their ${\tt RoA}$s for data-driven
controllers operating over high-dimensional systems, including a 67-dim
humanoid robot and a 96-dim 3-fingered manipulator. It first projects the
dynamics of the controlled system into a learned latent space. Then, it
constructs a reduced form of Morse Graphs representing the bistability of the
underlying dynamics, i.e., detecting when the controller results in a desired
versus an undesired behavior. The evaluation on high-dimensional robotic
datasets indicates data efficiency in ${\tt RoA}$ estimation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally to this paper</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-15T00:00:00Z">2024-03-15</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">71</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HumanoidBench: Simulated Humanoid Benchmark for Whole-Body Locomotion
  and Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carmelo Sferrazza, Dun-Ming Huang, Xingyu Lin, Youngwoon Lee, Pieter Abbeel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humanoid robots hold great promise in assisting humans in diverse
environments and tasks, due to their flexibility and adaptability leveraging
human-like morphology. However, research in humanoid robots is often
bottlenecked by the costly and fragile hardware setups. To accelerate
algorithmic research in humanoid robots, we present a high-dimensional,
simulated robot learning benchmark, HumanoidBench, featuring a humanoid robot
equipped with dexterous hands and a variety of challenging whole-body
manipulation and locomotion tasks. Our findings reveal that state-of-the-art
reinforcement learning algorithms struggle with most tasks, whereas a
hierarchical learning baseline achieves superior performance when supported by
robust low-level policies, such as walking or reaching. With HumanoidBench, we
provide the robotics community with a platform to identify the challenges
arising when solving diverse tasks with humanoid robots, facilitating prompt
verification of algorithms and ideas. The open-source code is available at
https://sferrazza.cc/humanoidbench_site.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reconfigurable Robot Identification from Motion Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhang Hu, Yunzhe Wang, Ruibo Liu, Zhou Shen, Hod Lipson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating Large Language Models (VLMs) and Vision-Language Models (VLMs)
with robotic systems enables robots to process and understand complex natural
language instructions and visual information. However, a fundamental challenge
remains: for robots to fully capitalize on these advancements, they must have a
deep understanding of their physical embodiment. The gap between AI models
cognitive capabilities and the understanding of physical embodiment leads to
the following question: Can a robot autonomously understand and adapt to its
physical form and functionalities through interaction with its environment?
This question underscores the transition towards developing self-modeling
robots without reliance on external sensory or pre-programmed knowledge about
their structure. Here, we propose a meta self modeling that can deduce robot
morphology through proprioception (the internal sense of position and
movement). Our study introduces a 12 DoF reconfigurable legged robot,
accompanied by a diverse dataset of 200k unique configurations, to
systematically investigate the relationship between robotic motion and robot
morphology. Utilizing a deep neural network model comprising a robot signature
encoder and a configuration decoder, we demonstrate the capability of our
system to accurately predict robot configurations from proprioceptive signals.
This research contributes to the field of robotic self-modeling, aiming to
enhance understanding of their physical embodiment and adaptability in real
world scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lifelong LERF: Local 3D Semantic Inventory Monitoring Using FogROS2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Rashid, Chung Min Kim, Justin Kerr, Letian Fu, Kush Hari, Ayah Ahmad, Kaiyuan Chen, Huang Huang, Marcus Gualtieri, Michael Wang, Christian Juette, Nan Tian, Liu Ren, Ken Goldberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inventory monitoring in homes, factories, and retail stores relies on
maintaining data despite objects being swapped, added, removed, or moved. We
introduce Lifelong LERF, a method that allows a mobile robot with minimal
compute to jointly optimize a dense language and geometric representation of
its surroundings. Lifelong LERF maintains this representation over time by
detecting semantic changes and selectively updating these regions of the
environment, avoiding the need to exhaustively remap. Human users can query
inventory by providing natural language queries and receiving a 3D heatmap of
potential object locations. To manage the computational load, we use Fog-ROS2,
a cloud robotics platform, to offload resource-intensive tasks. Lifelong LERF
obtains poses from a monocular RGBD SLAM backend, and uses these poses to
progressively optimize a Language Embedded Radiance Field (LERF) for semantic
monitoring. Experiments with 3-5 objects arranged on a tabletop and a Turtlebot
with a RealSense camera suggest that Lifelong LERF can persistently adapt to
changes in objects with up to 91% accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>See project webpage at:
  https://sites.google.com/berkeley.edu/lifelonglerf/home</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stimulate the Potential of Robots via Competition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10487v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10487v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangyao Huang, Di Guo, Xinyu Zhang, Xiangyang Ji, Huaping Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is common for us to feel pressure in a competition environment, which
arises from the desire to obtain success comparing with other individuals or
opponents. Although we might get anxious under the pressure, it could also be a
drive for us to stimulate our potentials to the best in order to keep up with
others. Inspired by this, we propose a competitive learning framework which is
able to help individual robot to acquire knowledge from the competition, fully
stimulating its dynamics potential in the race. Specifically, the competition
information among competitors is introduced as the additional auxiliary signal
to learn advantaged actions. We further build a Multiagent-Race environment,
and extensive experiments are conducted, demonstrating that robots trained in
competitive environments outperform ones that are trained with SoTA algorithms
in single robot environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online Concurrent Multi-Robot Coverage Path Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ratijit Mitra, Indranil Saha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, centralized receding horizon online multi-robot coverage path
planning algorithms have shown remarkable scalability in thoroughly exploring
large, complex, unknown workspaces with many robots. In a horizon, the path
planning and the path execution interleave, meaning when the path planning
occurs for robots with no paths, the robots with outstanding paths do not
execute, and subsequently, when the robots with new or outstanding paths
execute to reach respective goals, path planning does not occur for those
robots yet to get new paths, leading to wastage of both the robotic and the
computation resources. As a remedy, we propose a centralized algorithm that is
not horizon-based. It plans paths at any time for a subset of robots with no
paths, i.e., who have reached their previously assigned goals, while the rest
execute their outstanding paths, thereby enabling concurrent planning and
execution. We formally prove that the proposed algorithm ensures complete
coverage of an unknown workspace and analyze its time complexity. To
demonstrate scalability, we evaluate our algorithm to cover eight large $2$D
grid benchmark workspaces with up to 512 aerial and ground robots,
respectively. A comparison with a state-of-the-art horizon-based algorithm
shows its superiority in completing the coverage with up to 1.6x speedup. For
validation, we perform ROS + Gazebo simulations in six 2D grid benchmark
workspaces with 10 quadcopters and TurtleBots, respectively. We also
successfully conducted one outdoor experiment with three quadcopters and one
indoor with two TurtleBots.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Partially Observable Task and Motion Planning with Uncertainty and Risk
  Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10454v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10454v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aidan Curtis, George Matheos, Nishad Gothoskar, Vikash Mansinghka, Joshua Tenenbaum, Tomás Lozano-Pérez, Leslie Pack Kaelbling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrated task and motion planning (TAMP) has proven to be a valuable
approach to generalizable long-horizon robotic manipulation and navigation
problems. However, the typical TAMP problem formulation assumes full
observability and deterministic action effects. These assumptions limit the
ability of the planner to gather information and make decisions that are
risk-aware. We propose a strategy for TAMP with Uncertainty and Risk Awareness
(TAMPURA) that is capable of efficiently solving long-horizon planning problems
with initial-state and action outcome uncertainty, including problems that
require information gathering and avoiding undesirable and irreversible
outcomes. Our planner reasons under uncertainty at both the abstract task level
and continuous controller level. Given a set of closed-loop goal-conditioned
controllers operating in the primitive action space and a description of their
preconditions and potential capabilities, we learn a high-level abstraction
that can be solved efficiently and then refined to continuous actions for
execution. We demonstrate our approach on several robotics problems where
uncertainty is a crucial factor and show that reasoning under uncertainty in
these problems outperforms previously proposed determinized planning, direct
search, and reinforcement learning strategies. Lastly, we demonstrate our
planner on two real-world robotics problems using recent advancements in
probabilistic perception.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ H-MaP: An Iterative and Hybrid Sequential Manipulation Planner 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10436v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10436v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Berk Cicek, Cankut Bora Tuncer, Busenaz Kerimgil, Ozgur S. Oguz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces the Hybrid Sequential Manipulation Planner (H-MaP), a
novel approach that iteratively does motion planning using contact points and
waypoints for complex sequential manipulation tasks in robotics. Combining
optimization-based methods for generalizability and sampling-based methods for
robustness, H-MaP enhances manipulation planning through active contact mode
switches and enables interactions with auxiliary objects and tools. This
framework, validated by a series of diverse physical manipulation tasks and
real-robot experiments, offers a scalable and adaptable solution for complex
real-world applications in robotic manipulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeuFlow: Real-time, High-accuracy Optical Flow Estimation on Robots
  Using Edge Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyong Zhang, Huaizu Jiang, Hanumant Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time high-accuracy optical flow estimation is a crucial component in
various applications, including localization and mapping in robotics, object
tracking, and activity recognition in computer vision. While recent
learning-based optical flow methods have achieved high accuracy, they often
come with heavy computation costs. In this paper, we propose a highly efficient
optical flow architecture, called NeuFlow, that addresses both high accuracy
and computational cost concerns. The architecture follows a global-to-local
scheme. Given the features of the input images extracted at different spatial
resolutions, global matching is employed to estimate an initial optical flow on
the 1/16 resolution, capturing large displacement, which is then refined on the
1/8 resolution with lightweight CNN layers for better accuracy. We evaluate our
approach on Jetson Orin Nano and RTX 2080 to demonstrate efficiency
improvements across different computing platforms. We achieve a notable 10x-80x
speedup compared to several state-of-the-art methods, while maintaining
comparable accuracy. Our approach achieves around 30 FPS on edge computing
platforms, which represents a significant breakthrough in deploying complex
computer vision tasks such as SLAM on small robots like drones. The full
training and evaluation code is available at
https://github.com/neufieldrobotics/NeuFlow.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SculptDiff: Learning Robotic Clay Sculpting from Humans with Goal
  Conditioned Diffusion Policy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10401v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10401v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alison Bartsch, Arvind Car, Charlotte Avra, Amir Barati Farimani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manipulating deformable objects remains a challenge within robotics due to
the difficulties of state estimation, long-horizon planning, and predicting how
the object will deform given an interaction. These challenges are the most
pronounced with 3D deformable objects. We propose SculptDiff, a
goal-conditioned diffusion-based imitation learning framework that works with
point cloud state observations to directly learn clay sculpting policies for a
variety of target shapes. To the best of our knowledge this is the first
real-world method that successfully learns manipulation policies for 3D
deformable objects. For sculpting videos and access to our dataset and hardware
CAD models, see the project website:
https://sites.google.com/andrew.cmu.edu/imitation-sculpting/home
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collaborative Aquatic Positioning system Utilising Multi-beam Sonar and
  Depth Sensors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueliang Cheng, Barry Lennox, Keir Groves
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate positioning of underwater robots in confined environments is crucial
for inspection and mapping tasks and is also a prerequisite for autonomous
operations. Presently, there are no positioning systems available that are
suited for real-world use in confined underwater environments, unconstrained by
environmental lighting and water turbidity levels and have sufficient accuracy
for reliable and repeatable navigation. This shortage presents a significant
barrier to enhancing the capabilities of ROVs in such scenarios. This paper
introduces an innovative positioning system for ROVs operating in confined,
cluttered underwater settings, achieved through the collaboration of an
omnidirectional surface vehicle and an ROV. A formulation is proposed and
evaluated in the simulation against ground truth. The experimental results from
the simulation form a proof of principle of the proposed system and also
demonstrate its deployability. Unlike many previous approaches, the system does
not rely on fixed infrastructure or tracking of features in the environment and
can cover large enclosed areas without additional equipment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Thermal-NeRF: Neural Radiance Fields from an Infrared Camera 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10340v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10340v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianxiang Ye, Qi Wu, Junyuan Deng, Guoqing Liu, Liu Liu, Songpengcheng Xia, Liang Pang, Wenxian Yu, Ling Pei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Neural Radiance Fields (NeRFs) have demonstrated significant
potential in encoding highly-detailed 3D geometry and environmental appearance,
positioning themselves as a promising alternative to traditional explicit
representation for 3D scene reconstruction. However, the predominant reliance
on RGB imaging presupposes ideal lighting conditions: a premise frequently
unmet in robotic applications plagued by poor lighting or visual obstructions.
This limitation overlooks the capabilities of infrared (IR) cameras, which
excel in low-light detection and present a robust alternative under such
adverse scenarios. To tackle these issues, we introduce Thermal-NeRF, the first
method that estimates a volumetric scene representation in the form of a NeRF
solely from IR imaging. By leveraging a thermal mapping and structural thermal
constraint derived from the thermal characteristics of IR imaging, our method
showcasing unparalleled proficiency in recovering NeRFs in visually degraded
scenes where RGB-based methods fall short. We conduct extensive experiments to
demonstrate that Thermal-NeRF can achieve superior quality compared to existing
methods. Furthermore, we contribute a dataset for IR-based NeRF applications,
paving the way for future research in IR NeRF reconstruction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revolutionizing Packaging: A Robotic Bagging Pipeline with
  Constraint-aware Structure-of-Interest Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaming Qi, Peng Zhou, Pai Zheng, Hongmin Wu, Chenguang Yang, David Navarro-Alarcon, Jia Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bagging operations, common in packaging and assisted living applications, are
challenging due to a bag's complex deformable properties. To address this, we
develop a robotic system for automated bagging tasks using an adaptive
structure-of-interest (SOI) manipulation approach. Our method relies on
real-time visual feedback to dynamically adjust manipulation without requiring
prior knowledge of bag materials or dynamics. We present a robust pipeline
featuring state estimation for SOIs using Gaussian Mixture Models (GMM), SOI
generation via optimization-based bagging techniques, SOI motion planning with
Constrained Bidirectional Rapidly-exploring Random Trees (CBiRRT), and dual-arm
manipulation coordinated by Model Predictive Control (MPC). Experiments
demonstrate the system's ability to achieve precise, stable bagging of various
objects using adaptive coordination of the manipulators. The proposed framework
advances the capability of dual-arm robots to perform more sophisticated
automation of common tasks involving interactions with deformable objects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Investigation of the Factors Influencing Evolutionary Dynamics in the
  Joint Evolution of Robot Body and Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10303v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10303v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Léni K. Le Goff, Edgar Buchanan, Emma Hart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In evolutionary robotics, jointly optimising the design and the controller of
robots is a challenging task due to the huge complexity of the solution space
formed by the possible combinations of body and controller. We focus on the
evolution of robots that can be physically created rather than just simulated,
in a rich morphological space that includes a voxel-based chassis, wheels, legs
and sensors. On the one hand, this space offers a high degree of liberty in the
range of robots that can be produced, while on the other hand introduces a
complexity rarely dealt with in previous works relating to matching controllers
to designs and in evolving closed-loop control. This is usually addressed by
augmenting evolution with a learning algorithm to refine controllers. Although
several frameworks exist, few have studied the role of the \textit{evolutionary
dynamics} of the intertwined `evolution+learning' processes in realising
high-performing robots. We conduct an in-depth study of the factors that
influence these dynamics, specifically: synchronous vs asynchronous evolution;
the mechanism for replacing parents with offspring, and rewarding goal-based
fitness vs novelty via selection. Results show that asynchronicity combined
with goal-based selection and a `replace worst' strategy results in the highest
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Offline Goal-Conditioned Reinforcement Learning for Shape Control of
  Deformable Linear Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10290v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10290v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rita Laezza, Mohammadreza Shetab-Bushehri, Gabriel Arslan Waltersson, Erol Özgür, Youcef Mezouar, Yiannis Karayiannidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deformable objects present several challenges to the field of robotic
manipulation. One of the tasks that best encapsulates the difficulties arising
due to non-rigid behavior is shape control, which requires driving an object to
a desired shape. While shape-servoing methods have been shown successful in
contexts with approximately linear behavior, they can fail in tasks with more
complex dynamics. We investigate an alternative approach, using offline RL to
solve a planar shape control problem of a Deformable Linear Object (DLO). To
evaluate the effect of material properties, two DLOs are tested namely a soft
rope and an elastic cord. We frame this task as a goal-conditioned offline RL
problem, and aim to learn to generalize to unseen goal shapes. Data collection
and augmentation procedures are proposed to limit the amount of experimental
data which needs to be collected with the real robot. We evaluate the amount of
augmentation needed to achieve the best results, and test the effect of
regularization through behavior cloning on the TD3+BC algorithm. Finally, we
show that the proposed approach is able to outperform a shape-servoing baseline
in a curvature inversion experiment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EasyCalib: Simple and Low-Cost In-Situ Calibration for Force
  Reconstruction with Vision-Based Tactile Sensors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingxuan Li, Lunwei Zhang, Yen Hang Zhou, Tiemin Li, Yao Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For elastomer-based tactile sensors, represented by visuotactile sensors,
routine calibration of mechanical parameters (Young's modulus and Poisson's
ratio) has been shown to be important for force reconstruction. However, the
reliance on existing in-situ calibration methods for accurate force
measurements limits their cost-effective and flexible applications. This
article proposes a new in-situ calibration scheme that relies only on comparing
contact deformation. Based on the detailed derivations of the normal contact
and torsional contact theories, we designed a simple and low-cost calibration
device, EasyCalib, and validated its effectiveness through extensive finite
element analysis. We also explored the accuracy of EasyCalib in the practical
application and demonstrated that accurate contact distributed force
reconstruction can be realized based on the mechanical parameters obtained.
EasyCalib balances low hardware cost, ease of operation, and low dependence on
technical expertise and is expected to provide the necessary accuracy
guarantees for wide applications of visuotactile sensors in the wild.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ultra-Wideband Positioning System Based on ESP32 and DWM3000 Modules 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10194v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10194v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Krebs, Tom Herter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, an Ultra-Wideband (UWB) positioning system is introduced, that
leverages six identical custom-designed boards, each featuring an ESP32
microcontroller and a DWM3000 module from Quorvo. The system is capable of
achieving localization with an accuracy of up to 10 cm, by utilizing
Two-Way-Ranging (TWR) measurements between one designated tag and five anchor
devices. The gathered distance measurements are subsequently processed by an
Extended Kalman Filter (EKF) running locally on the tag board, enabling it to
determine its own position, relying on fixed, a priori known positions of the
anchor boards. This paper presents a comprehensive overview of the systems
architecture, the key components, and the capabilities it offers for indoor
positioning and tracking applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grasp Anything: Combining Teacher-Augmented Policy Gradient Learning
  with Instance Segmentation to Grasp Arbitrary Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malte Mosbach, Sven Behnke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interactive grasping from clutter, akin to human dexterity, is one of the
longest-standing problems in robot learning. Challenges stem from the
intricacies of visual perception, the demand for precise motor skills, and the
complex interplay between the two. In this work, we present Teacher-Augmented
Policy Gradient (TAPG), a novel two-stage learning framework that synergizes
reinforcement learning and policy distillation. After training a teacher policy
to master the motor control based on object pose information, TAPG facilitates
guided, yet adaptive, learning of a sensorimotor policy, based on object
segmentation. We zero-shot transfer from simulation to a real robot by using
Segment Anything Model for promptable object segmentation. Our trained policies
adeptly grasp a wide variety of objects from cluttered scenarios in simulation
and the real world based on human-understandable prompts. Furthermore, we show
robust zero-shot transfer to novel objects. Videos of our experiments are
available at \url{https://maltemosbach.github.io/grasp_anything}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RCooper: A Real-world Large-scale <span class="highlight-title">Dataset</span> for Roadside Cooperative
  Perception <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiyang Hao, Siqi Fan, Yingru Dai, Zhenlin Zhang, Chenxi Li, Yuntian Wang, Haibao Yu, Wenxian Yang, Jirui Yuan, Zaiqing Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The value of roadside perception, which could extend the boundaries of
autonomous driving and traffic management, has gradually become more prominent
and acknowledged in recent years. However, existing roadside perception
approaches only focus on the single-infrastructure sensor system, which cannot
realize a comprehensive understanding of a traffic area because of the limited
sensing range and blind spots. Orienting high-quality roadside perception, we
need Roadside Cooperative Perception (RCooper) to achieve practical
area-coverage roadside perception for restricted traffic areas. Rcooper has its
own domain-specific challenges, but further exploration is hindered due to the
lack of datasets. We hence release the first real-world, large-scale RCooper
dataset to bloom the research on practical roadside cooperative perception,
including detection and tracking. The manually annotated dataset comprises 50k
images and 30k point clouds, including two representative traffic scenes (i.e.,
intersection and corridor). The constructed benchmarks prove the effectiveness
of roadside cooperation perception and demonstrate the direction of further
research. Codes and dataset can be accessed at:
https://github.com/AIR-THU/DAIR-RCooper.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2024. 10 pages with 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparative Analysis of Programming by Demonstration Methods:
  Kinesthetic Teaching vs Human Demonstration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bruno Maric, Filip Zoric, Frano Petric, Matko Orsag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Programming by demonstration (PbD) is a simple and efficient way to program
robots without explicit robot programming. PbD enables unskilled operators to
easily demonstrate and guide different robots to execute task. In this paper we
present comparison of demonstration methods with comprehensive user study. Each
participant had to demonstrate drawing simple pattern with human demonstration
using virtual marker and kinesthetic teaching with robot manipulator. To
evaluate differences between demonstration methods, we conducted user study
with 24 participants which filled out NASA raw task load index (rTLX) and
system usability scale (SUS). We also evaluated similarity of the executed
trajectories to measure difference between demonstrated and ideal trajectory.
We concluded study with finding that human demonstration using a virtual marker
is on average 8 times faster, superior in terms of quality and imposes 2 times
less overall workload than kinesthetic teaching.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Bioinspired Neuromorphic Vision-based Tactile Sensor for Fast
  Tactile Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omar Faris, Mohammad I. Awad, Murana A. Awad, Yahya Zweiri, Kinda Khalaf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tactile sensing represents a crucial technique that can enhance the
performance of robotic manipulators in various tasks. This work presents a
novel bioinspired neuromorphic vision-based tactile sensor that uses an
event-based camera to quickly capture and convey information about the
interactions between robotic manipulators and their environment. The camera in
the sensor observes the deformation of a flexible skin manufactured from a
cheap and accessible 3D printed material, whereas a 3D printed rigid casing
houses the components of the sensor together. The sensor is tested in a
grasping stage classification task involving several objects using a
data-driven learning-based approach. The results show that the proposed
approach enables the sensor to detect pressing and slip incidents within a
speed of 2 ms. The fast tactile perception properties of the proposed sensor
makes it an ideal candidate for safe grasping of different objects in
industries that involve high-speed pick-and-place operations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 10 figures, journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Visual-Language Maps Capture Latent Semantics? <span class="chip">IROS-2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matti Pekkanen, Tsvetomila Mihaylova, Francesco Verdoja, Ville Kyrki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual-language models (VLMs) have recently been introduced in robotic
mapping by using the latent representations, i.e., embeddings, of the VLMs to
represent the natural language semantics in the map. The main benefit is moving
beyond a small set of human-created labels toward open-vocabulary scene
understanding. While there is anecdotal evidence that maps built this way
support downstream tasks, such as navigation, rigorous analysis of the quality
of the maps using these embeddings is lacking. We investigate two critical
properties of map quality: queryability and consistency. The evaluation of
queryability addresses the ability to retrieve information from the embeddings.
We investigate two aspects of consistency: intra-map consistency and inter-map
consistency. Intra-map consistency captures the ability of the embeddings to
represent abstract semantic classes, and inter-map consistency captures the
generalization properties of the representation. In this paper, we propose a
way to analyze the quality of maps created using VLMs, which forms an
open-source benchmark to be used when proposing new open-vocabulary map
representations. We demonstrate the benchmark by evaluating the maps created by
two state-of-the-art methods, VLMaps and OpenScene, using two encoders, LSeg
and OpenSeg, using real-world data from the Matterport3D data set. We find that
OpenScene outperforms VLMaps with both encoders, and LSeg outperforms OpenSeg
with both methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Sumitted to IEEE-IROS-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Autonomous Monitoring of Pharmaceutical R&D Laboratories with 6 Axis Arm
  Equipped Quadruped Robot and Generative AI: A Preliminary Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10108v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10108v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunichi Hato, Nozomi Ogawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a proof-of-concept study that examines the utilization of
generative AI and mobile robotics for autonomous laboratory monitoring in the
pharmaceutical R&D laboratory. The study investigates the potential advantages
of anomaly detection and automated reporting by multi-modal model and Vision
Foundation Model (VFM), which have the potential to enhance compliance and
safety in laboratory environments. Additionally, the paper discusses the
current limitations of the generative AI approach and proposes future
directions for its application in lab monitoring.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Belief Aided Navigation using Bayesian Reinforcement Learning for
  Avoiding Humans in Blind Spots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10105v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10105v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyeob Kim, Daewon Kwak, Hyunwoo Rim, Donghan Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research on mobile robot navigation has focused on socially aware
navigation in crowded environments. However, existing methods do not adequately
account for human robot interactions and demand accurate location information
from omnidirectional sensors, rendering them unsuitable for practical
applications. In response to this need, this study introduces a novel
algorithm, BNBRL+, predicated on the partially observable Markov decision
process framework to assess risks in unobservable areas and formulate movement
strategies under uncertainty. BNBRL+ consolidates belief algorithms with
Bayesian neural networks to probabilistically infer beliefs based on the
positional data of humans. It further integrates the dynamics between the
robot, humans, and inferred beliefs to determine the navigation paths and
embeds social norms within the reward function, thereby facilitating socially
aware navigation. Through experiments in various risk laden scenarios, this
study validates the effectiveness of BNBRL+ in navigating crowded environments
with blind spots. The model's ability to navigate effectively in spaces with
limited visibility and avoid obstacles dynamically can significantly improve
the safety and reliability of autonomous vehicles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agile and Safe Trajectory Planning for Quadruped Navigation with Motion
  Anisotropy Awareness <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10101v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10101v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Zhang, Shaohang Xu, Peiyuan Cai, Lijun Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quadruped robots demonstrate robust and agile movements in various terrains;
however, their navigation autonomy is still insufficient. One of the challenges
is that the motion capabilities of the quadruped robot are anisotropic along
different directions, which significantly affects the safety of quadruped robot
navigation. This paper proposes a navigation framework that takes into account
the motion anisotropy of quadruped robots including kinodynamic trajectory
generation, nonlinear trajectory optimization, and nonlinear model predictive
control. In simulation and real robot tests, we demonstrate that our
motion-anisotropy-aware navigation framework could: (1) generate more efficient
trajectories and realize more agile quadruped navigation; (2) significantly
improve the navigation safety in challenging scenarios. The implementation is
realized as an open-source package at
https://github.com/ZWT006/agile_navigation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, submitted to 2024 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HeR-DRL:Heterogeneous Relational Deep Reinforcement Learning for
  Decentralized Multi-Robot Crowd Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10083v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10083v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Zhou, Songhao Piao, Wenzheng Chi, Liguo Chen, Wei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crowd navigation has received significant research attention in recent years,
especially DRL-based methods. While single-robot crowd scenarios have dominated
research, they offer limited applicability to real-world complexities. The
heterogeneity of interaction among multiple agent categories, like in
decentralized multi-robot pedestrian scenarios, are frequently disregarded.
This "interaction blind spot" hinders generalizability and restricts progress
towards robust navigation algorithms. In this paper, we propose a heterogeneous
relational deep reinforcement learning(HeR-DRL), based on customised
heterogeneous GNN, in order to improve navigation strategies in decentralized
multi-robot crowd navigation. Firstly, we devised a method for constructing
robot-crowd heterogenous relation graph that effectively simulates the
heterogeneous pair-wise interaction relationships. We proposed a new
heterogeneous graph neural network for transferring and aggregating the
heterogeneous state information. Finally, we incorporate the encoded
information into deep reinforcement learning to explore the optimal policy.
HeR-DRL are rigorously evaluated through comparing it to state-of-the-art
algorithms in both single-robot and multi-robot circle crowssing scenario. The
experimental results demonstrate that HeR-DRL surpasses the state-of-the-art
approaches in overall performance, particularly excelling in safety and comfort
metrics. This underscores the significance of interaction heterogeneity for
crowd navigation. The source code will be publicly released in
https://github.com/Zhouxy-Debugging-Den/HeR-DRL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GeoPro-VO: Dynamic Obstacle Avoidance with Geometric Projector Based on
  Velocity Obstacle 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10043v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10043v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihao Huang, Xuemin Chi, Jun Zeng, Zhitao Liu, Hongye Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimization-based approaches are widely employed to generate optimal robot
motions while considering various constraints, such as robot dynamics,
collision avoidance, and physical limitations. It is crucial to efficiently
solve the optimization problems in practice, yet achieving rapid computations
remains a great challenge for optimization-based approaches with nonlinear
constraints. In this paper, we propose a geometric projector for dynamic
obstacle avoidance based on velocity obstacle (GeoPro-VO) by leveraging the
projection feature of the velocity cone set represented by VO. Furthermore,
with the proposed GeoPro-VO and the augmented Lagrangian spectral projected
gradient descent (ALSPG) algorithm, we transform an initial mixed integer
nonlinear programming problem (MINLP) in the form of constrained model
predictive control (MPC) into a sub-optimization problem and solve it
efficiently. Numerical simulations are conducted to validate the fast computing
speed of our approach and its capability for reliable dynamic obstacle
avoidance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Embedding Dynamic Personas in Interactive Robots: Masquerading
  Animated Social Kinematics (MASK) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeongeun Park, Taemoon Jeong, Hyeonseong Kim, Taehyun Byun, Seungyoon Shin, Keunjun Choi, Jaewoon Kwon, Taeyoon Lee, Matthew Pan, Sungjoon Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the design and development of an innovative interactive
robotic system to enhance audience engagement using character-like personas.
Built upon the foundations of persona-driven dialog agents, this work extends
the agent application to the physical realm, employing robots to provide a more
immersive and interactive experience. The proposed system, named the
Masquerading Animated Social Kinematics (MASK), leverages an anthropomorphic
robot which interacts with guests using non-verbal interactions, including
facial expressions and gestures. A behavior generation system based upon a
finite-state machine structure effectively conditions robotic behavior to
convey distinct personas. The MASK framework integrates a perception engine, a
behavior selection engine, and a comprehensive action library to enable
real-time, dynamic interactions with minimal human intervention in behavior
design. Throughout the user subject studies, we examined whether the users
could recognize the intended character in film-character-based persona
conditions. We conclude by discussing the role of personas in interactive
agents and the factors to consider for creating an engaging user experience.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-World Computational Aberration Correction via Quantized
  Domain-Mixing Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10012v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10012v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Jiang, Zhonghua Yi, Shaohua Gao, Yao Gao, Xiaolong Qian, Hao Shi, Lei Sun, Zhijie Xu, Kailun Yang, Kaiwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relying on paired synthetic data, existing learning-based Computational
Aberration Correction (CAC) methods are confronted with the intricate and
multifaceted synthetic-to-real domain gap, which leads to suboptimal
performance in real-world applications. In this paper, in contrast to improving
the simulation pipeline, we deliver a novel insight into real-world CAC from
the perspective of Unsupervised Domain Adaptation (UDA). By incorporating
readily accessible unpaired real-world data into training, we formalize the
Domain Adaptive CAC (DACAC) task, and then introduce a comprehensive Real-world
aberrated images (Realab) dataset to benchmark it. The setup task presents a
formidable challenge due to the intricacy of understanding the target
aberration domain. To this intent, we propose a novel Quntized Domain-Mixing
Representation (QDMR) framework as a potent solution to the issue. QDMR adapts
the CAC model to the target domain from three key aspects: (1) reconstructing
aberrated images of both domains by a VQGAN to learn a Domain-Mixing Codebook
(DMC) which characterizes the degradation-aware priors; (2) modulating the deep
features in CAC model with DMC to transfer the target domain knowledge; and (3)
leveraging the trained VQGAN to generate pseudo target aberrated images from
the source ones for convincing target domain supervision. Extensive experiments
on both synthetic and real-world benchmarks reveal that the models with QDMR
consistently surpass the competitive methods in mitigating the
synthetic-to-real gap, which produces visually pleasant real-world CAC results
with fewer artifacts. Codes and datasets will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Codes and datasets will be made publicly available at
  https://github.com/zju-jiangqi/QDMR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language to Map: Topological map generation from natural language path
  instructions <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10008v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10008v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hideki Deguchi, Kazuki Shibata, Shun Taguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, a method for generating a map from path information described
using natural language (textual path) is proposed. In recent years, robotics
research mainly focus on vision-and-language navigation (VLN), a navigation
task based on images and textual paths. Although VLN is expected to facilitate
user instructions to robots, its current implementation requires users to
explain the details of the path for each navigation session, which results in
high explanation costs for users. To solve this problem, we proposed a method
that creates a map as a topological map from a textual path and automatically
creates a new path using this map. We believe that large language models (LLMs)
can be used to understand textual path. Therefore, we propose and evaluate two
methods, one for storing implicit maps in LLMs, and the other for generating
explicit maps using LLMs. The implicit map is in the LLM's memory. It is
created using prompts. In the explicit map, a topological map composed of nodes
and edges is constructed and the actions at each node are stored. This makes it
possible to estimate the path and actions at waypoints on an undescribed path,
if enough information is available. Experimental results on path instructions
generated in a real environment demonstrate that generating explicit maps
achieves significantly higher accuracy than storing implicit maps in the LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures. Accepted to IEEE International Conference on
  Robotics and Automation (ICRA) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLOSURE: Fast Quantification of Pose Uncertainty Sets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihuai Gao, Yukai Tang, Han Qi, Heng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate uncertainty quantification of 6D pose estimation from keypoint
measurements. Assuming unknown-but-bounded measurement noises, a pose
uncertainty set (PURSE) is a subset of SE(3) that contains all possible 6D
poses compatible with the measurements. Despite being simple to formulate and
its ability to embed uncertainty, the PURSE is difficult to manipulate and
interpret due to the many abstract nonconvex polynomial constraints. An
appealing simplification of PURSE is to find its minimum enclosing geodesic
ball (MEGB), i.e., a point pose estimation with minimum worst-case error bound.
We contribute (i) a dynamical system perspective, and (ii) a fast algorithm to
inner approximate the MEGB. Particularly, we show the PURSE corresponds to the
feasible set of a constrained dynamical system, and this perspective allows us
to design an algorithm to densely sample the boundary of the PURSE through
strategic random walks. We then use the miniball algorithm to compute the MEGB
of PURSE samples, leading to an inner approximation. Our algorithm is named
CLOSURE (enClosing baLl frOm purSe boUndaRy samplEs) and it enables computing a
certificate of approximation tightness by calculating the relative size ratio
between the inner approximation and the outer approximation. Running on a
single RTX 3090 GPU, CLOSURE achieves the relative ratio of 92.8% on the LM-O
object pose estimation dataset and 91.4% on the 3DMatch point cloud
registration dataset with the average runtime less than 0.2 second. Obtaining
comparable worst-case error bound but 398x and 833x faster than the outer
approximation GRCC, CLOSURE enables uncertainty quantification of 6D pose
estimation to be implemented in real-time robot perception applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interactive Distance Field Mapping and Planning to Enable Human-Robot
  Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Usama Ali, Lan Wu, Adrian Mueller, Fouad Sukkar, Tobias Kaupp, Teresa Vidal-Calleja
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-robot collaborative applications require scene representations that are
kept up-to-date and facilitate safe motions in dynamic scenes. In this letter,
we present an interactive distance field mapping and planning (IDMP) framework
that handles dynamic objects and collision avoidance through an efficient
representation. We define \textit{interactive} mapping and planning as the
process of creating and updating the representation of the scene online while
simultaneously planning and adapting the robot's actions based on that
representation. Given depth sensor data, our framework builds a continuous
field that allows to query the distance and gradient to the closest obstacle at
any required position in 3D space. The key aspect of this work is an efficient
Gaussian Process field that performs incremental updates and implicitly handles
dynamic objects with a simple and elegant formulation based on a temporary
latent model. In terms of mapping, IDMP is able to fuse point cloud data from
single and multiple sensors, query the free space at any spatial resolution,
and deal with moving objects without semantics. In terms of planning, IDMP
allows seamless integration with gradient-based motion planners facilitating
fast re-planning for collision-free navigation. The framework is evaluated on
both real and synthetic datasets. A comparison with similar state-of-the-art
frameworks shows superior performance when handling dynamic objects and
comparable or better performance in the accuracy of the computed distance and
gradient field. Finally, we show how the framework can be used for fast motion
planning in the presence of moving objects. An accompanying video, code, and
datasets are made publicly available https://uts-ri.github.io/IDMP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Skeleton-Based Human Action Recognition with Noisy Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Xu, Kunyu Peng, Di Wen, Ruiping Liu, Junwei Zheng, Yufan Chen, Jiaming Zhang, Alina Roitberg, Kailun Yang, Rainer Stiefelhagen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding human actions from body poses is critical for assistive robots
sharing space with humans in order to make informed and safe decisions about
the next interaction. However, precise temporal localization and annotation of
activity sequences is time-consuming and the resulting labels are often noisy.
If not effectively addressed, label noise negatively affects the model's
training, resulting in lower recognition quality. Despite its importance,
addressing label noise for skeleton-based action recognition has been
overlooked so far. In this study, we bridge this gap by implementing a
framework that augments well-established skeleton-based human action
recognition methods with label-denoising strategies from various research areas
to serve as the initial benchmark. Observations reveal that these baselines
yield only marginal performance when dealing with sparse skeleton data.
Consequently, we introduce a novel methodology, NoiseEraSAR, which integrates
global sample selection, co-teaching, and Cross-Modal Mixture-of-Experts
(CM-MOE) strategies, aimed at mitigating the adverse impacts of label noise.
Our proposed approach demonstrates better performance on the established
benchmark, setting new state-of-the-art standards. The source code for this
study will be made accessible at https://github.com/xuyizdby/NoiseEraSAR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The source code will be made accessible at
  https://github.com/xuyizdby/NoiseEraSAR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Object Goal Navigation Through LLM-enhanced Object Affinities
  Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09971v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09971v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengying Lin, Yaran Chen, Dongbin Zhao, Zhaoran Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In object goal navigation, agents navigate towards objects identified by
category labels using visual and spatial information. Previously, solely
network-based methods typically rely on historical data for object affinities
estimation, lacking adaptability to new environments and unseen targets.
Simultaneously, employing Large Language Models (LLMs) for navigation as either
planners or agents, though offering a broad knowledge base, is cost-inefficient
and lacks targeted historical experience. Addressing these challenges, we
present the LLM-enhanced Object Affinities Transfer (LOAT) framework,
integrating LLM-derived object semantics with network-based approaches to
leverage experiential object affinities, thus improving adaptability in
unfamiliar settings. LOAT employs a dual-module strategy: a generalized
affinities module for accessing LLMs' vast knowledge and an experiential
affinities module for applying learned object semantic relationships,
complemented by a dynamic fusion module harmonizing these information sources
based on temporal context. The resulting scores activate semantic maps before
feeding into downstream policies, enhancing navigation systems with
context-aware inputs. Our evaluations in AI2-THOR and Habitat simulators
demonstrate improvements in both navigation success rates and efficiency,
validating the LOAT's efficacy in integrating LLM insights for improved object
goal navigation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Design and Control Co-Optimization for Automated Design Iteration of
  Dexterous Anthropomorphic Soft Robotic Hands 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pragna Mannam, Xingyu Liu, Ding Zhao, Jean Oh, Nancy Pollard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We automate soft robotic hand design iteration by co-optimizing design and
control policy for dexterous manipulation skills in simulation. Our design
iteration pipeline combines genetic algorithms and policy transfer to learn
control policies for nearly 400 hand designs, testing grasp quality under
external force disturbances. We validate the optimized designs in the real
world through teleoperation of pickup and reorient manipulation tasks. Our real
world evaluation, from over 900 teleoperated tasks, shows that the trend in
design performance in simulation resembles that of the real world. Furthermore,
we show that optimized hand designs from our approach outperform existing soft
robot hands from prior work in the real world. The results highlight the
usefulness of simulation in guiding parameter choices for anthropomorphic soft
robotic hand systems, and the effectiveness of our automated design iteration
approach, despite the sim-to-real gap.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stackelberg Meta-Learning Based Shared Control for Assistive Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhan Zhao, Quanyan Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Shared control allows the human driver to collaborate with an assistive
driving system while retaining the ability to make decisions and take control
if necessary. However, human-vehicle teaming and planning are challenging due
to environmental uncertainties, the human's bounded rationality, and the
variability in human behaviors. An effective collaboration plan needs to learn
and adapt to these uncertainties. To this end, we develop a Stackelberg
meta-learning algorithm to create automated learning-based planning for shared
control. The Stackelberg games are used to capture the leader-follower
structure in the asymmetric interactions between the human driver and the
assistive driving system. The meta-learning algorithm generates a common
behavioral model, which is capable of fast adaptation using a small amount of
driving data to assist optimal decision-making. We use a case study of an
obstacle avoidance driving scenario to corroborate that the adapted human
behavioral model can successfully assist the human driver in reaching the
target destination. Besides, it saves driving time compared with a driver-only
scheme and is also robust to drivers' bounded rationality and errors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incentive-Compatible and Distributed Allocation for Robotic Service
  Provision Through Contract Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhan Zhao, Quanyan Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot allocation plays an essential role in facilitating robotic service
provision across various domains. Yet the increasing number of users and the
uncertainties regarding the users' true service requirements have posed
challenges for the service provider in effectively allocating service robots to
users to meet their needs. In this work, we first propose a contract-based
approach to enable incentive-compatible service selection so that the service
provider can effectively reduce the user's service uncertainties for precise
service provision. Then, we develop a distributed allocation algorithm that
incorporates robot dynamics and collision avoidance to allocate service robots
and address scalability concerns associated with increasing numbers of service
robots and users. We conduct simulations in eight scenarios to validate our
approach. Comparative analysis against the robust allocation paradigm and two
alternative uncertainty reduction strategies demonstrates that our approach
achieves better allocation efficiency and accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mind the Error! Detection and Localization of Instruction Errors in
  Vision-and-Language Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10700v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10700v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Taioli, Stefano Rosa, Alberto Castellini, Lorenzo Natale, Alessio Del Bue, Alessandro Farinelli, Marco Cristani, Yiming Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-and-Language Navigation in Continuous Environments (VLN-CE) is one of
the most intuitive yet challenging embodied AI tasks. Agents are tasked to
navigate towards a target goal by executing a set of low-level actions,
following a series of natural language instructions. All VLN-CE methods in the
literature assume that language instructions are exact. However, in practice,
instructions given by humans can contain errors when describing a spatial
environment due to inaccurate memory or confusion. Current VLN-CE benchmarks do
not address this scenario, making the state-of-the-art methods in VLN-CE
fragile in the presence of erroneous instructions from human users. For the
first time, we propose a novel benchmark dataset that introduces various types
of instruction errors considering potential human causes. This benchmark
provides valuable insight into the robustness of VLN systems in continuous
environments. We observe a noticeable performance drop (up to -25%) in Success
Rate when evaluating the state-of-the-art VLN-CE methods on our benchmark.
Moreover, we formally define the task of Instruction Error Detection and
Localization, and establish an evaluation protocol on top of our benchmark
dataset. We also propose an effective method, based on a cross-modal
transformer architecture, that achieves the best performance in error detection
and localization, compared to baselines. Surprisingly, our proposed method has
revealed errors in the validation set of the two commonly used datasets for
VLN-CE, i.e., R2R-CE and RxR-CE, demonstrating the utility of our technique in
other tasks. Code and dataset will be made available upon acceptance at
https://intelligolabs.github.io/R2RIE-CE
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3 figures, 8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Latent Object Characteristics Recognition with Visual to Haptic-Audio
  Cross-modal Transfer Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Namiko Saito, Joao Moura, Hiroki Uchida, Sethu Vijayakumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recognising the characteristics of objects while a robot handles them is
crucial for adjusting motions that ensure stable and efficient interactions
with containers. Ahead of realising stable and efficient robot motions for
handling/transferring the containers, this work aims to recognise the latent
unobservable object characteristics. While vision is commonly used for object
recognition by robots, it is ineffective for detecting hidden objects. However,
recognising objects indirectly using other sensors is a challenging task. To
address this challenge, we propose a cross-modal transfer learning approach
from vision to haptic-audio. We initially train the model with vision, directly
observing the target object. Subsequently, we transfer the latent space learned
from vision to a second module, trained only with haptic-audio and motor data.
This transfer learning framework facilitates the representation of object
characteristics using indirect sensor data, thereby improving recognition
accuracy. For evaluating the recognition accuracy of our proposed learning
framework we selected shape, position, and orientation as the object
characteristics. Finally, we demonstrate online recognition of both trained and
untrained objects using the humanoid robot Nextage Open.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spiking Neural Networks for Fast-Moving Object Detection on Neuromorphic
  Hardware Devices Using an Event-Based Camera 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Ziegler, Karl Vetter, Thomas Gossard, Jonas Tebbe, Andreas Zell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Table tennis is a fast-paced and exhilarating sport that demands agility,
precision, and fast reflexes. In recent years, robotic table tennis has become
a popular research challenge for robot perception algorithms. Fast and accurate
ball detection is crucial for enabling a robotic arm to rally the ball back
successfully. Previous approaches have employed conventional frame-based
cameras with Convolutional Neural Networks (CNNs) or traditional computer
vision methods. In this paper, we propose a novel solution that combines an
event-based camera with Spiking Neural Networks (SNNs) for ball detection. We
use multiple state-of-the-art SNN frameworks and develop a SNN architecture for
each of them, complying with their corresponding constraints. Additionally, we
implement the SNN solution across multiple neuromorphic edge devices,
conducting comparisons of their accuracies and run-times. This furnishes
robotics researchers with a benchmark illustrating the capabilities achievable
with each SNN framework and a corresponding neuromorphic edge device. Next to
this comparison of SNN solutions for robots, we also show that an SNN on a
neuromorphic edge device is able to run in real-time in a closed loop robotic
system, a table tennis robot in our use case.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Riemannian Flow Matching Policy for Robot Motion Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Braun, Noémie Jaquier, Leonel Rozo, Tamim Asfour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Riemannian Flow Matching Policies (RFMP), a novel model for
learning and synthesizing robot visuomotor policies. RFMP leverages the
efficient training and inference capabilities of flow matching methods. By
design, RFMP inherits the strengths of flow matching: the ability to encode
high-dimensional multimodal distributions, commonly encountered in robotic
tasks, and a very simple and fast inference process. We demonstrate the
applicability of RFMP to both state-based and vision-conditioned robot motion
policies. Notably, as the robot state resides on a Riemannian manifold, RFMP
inherently incorporates geometric awareness, which is crucial for realistic
robotic tasks. To evaluate RFMP, we conduct two proof-of-concept experiments,
comparing its performance against Diffusion Policies. Although both approaches
successfully learn the considered tasks, our results show that RFMP provides
smoother action trajectories with significantly lower inference times.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Virtual Elastic Tether: a New Approach for Multi-agent Navigation in
  Confined Aquatic Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kanzhong Yao, Xueliang Cheng, Keir Groves, Barry Lennox, Ognjen Marjanovic, Simon Watson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Underwater navigation is a challenging area in the field of mobile robotics
due to inherent constraints in self-localisation and communication in
underwater environments. Some of these challenges can be mitigated by using
collaborative multi-agent teams. However, when applied underwater, the
robustness of traditional multi-agent collaborative control approaches is
highly limited due to the unavailability of reliable measurements. In this
paper, the concept of a Virtual Elastic Tether (VET) is introduced in the
context of incomplete state measurements, which represents an innovative
approach to underwater navigation in confined spaces. The concept of VET is
formulated and validated using the Cooperative Aquatic Vehicle Exploration
System (CAVES), which is a sim-to-real multi-agent aquatic robotic platform.
Within this framework, a vision-based Autonomous Underwater Vehicle-Autonomous
Surface Vehicle leader-follower formulation is developed. Experiments were
conducted in both simulation and on a physical platform, benchmarked against a
traditional Image-Based Visual Servoing approach. Results indicate that the
formation of the baseline approach fails under discrete disturbances, when
induced distances between the robots exceeds 0.6 m in simulation and 0.3 m in
the real world. In contrast, the VET-enhanced system recovers to
pre-perturbation distances within 5 seconds. Furthermore, results illustrate
the successful navigation of VET-enhanced CAVES in a confined water pond where
the baseline approach fails to perform adequately.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ExploRLLM: Guiding Exploration in Reinforcement Learning with Large
  Language Models <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09583v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09583v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runyu Ma, Jelle Luijkx, Zlatan Ajanovic, Jens Kober
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In image-based robot manipulation tasks with large observation and action
spaces, reinforcement learning struggles with low sample efficiency, slow
training speed, and uncertain convergence. As an alternative, large pre-trained
foundation models have shown promise in robotic manipulation, particularly in
zero-shot and few-shot applications. However, using these models directly is
unreliable due to limited reasoning capabilities and challenges in
understanding physical and spatial contexts. This paper introduces ExploRLLM, a
novel approach that leverages the inductive bias of foundation models (e.g.
Large Language Models) to guide exploration in reinforcement learning. We also
exploit these foundation models to reformulate the action and observation
spaces to enhance the training efficiency in reinforcement learning. Our
experiments demonstrate that guided exploration enables much quicker
convergence than training without it. Additionally, we validate that ExploRLLM
outperforms vanilla foundation model baselines and that the policy trained in
simulation can be applied in real-world settings without additional training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages,8 figures, conference IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OccFiner: Offboard Occupancy Refinement with Hybrid Propagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08504v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08504v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Shi, Song Wang, Jiaming Zhang, Xiaoting Yin, Zhongdao Wang, Zhijian Zhao, Guangming Wang, Jianke Zhu, Kailun Yang, Kaiwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-based occupancy prediction, also known as 3D Semantic Scene Completion
(SSC), presents a significant challenge in computer vision. Previous methods,
confined to onboard processing, struggle with simultaneous geometric and
semantic estimation, continuity across varying viewpoints, and single-view
occlusion. Our paper introduces OccFiner, a novel offboard framework designed
to enhance the accuracy of vision-based occupancy predictions. OccFiner
operates in two hybrid phases: 1) a multi-to-multi local propagation network
that implicitly aligns and processes multiple local frames for correcting
onboard model errors and consistently enhancing occupancy accuracy across all
distances. 2) the region-centric global propagation, focuses on refining labels
using explicit multi-view geometry and integrating sensor bias, especially to
increase the accuracy of distant occupied voxels. Extensive experiments
demonstrate that OccFiner improves both geometric and semantic accuracy across
various types of coarse occupancy, setting a new state-of-the-art performance
on the SemanticKITTI dataset. Notably, OccFiner elevates vision-based SSC
models to a level even surpassing that of LiDAR-based onboard SSC models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lowering Detection in Sport Climbing Based on Orientation of the Sensor
  Enhanced Quickdraw 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10164v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10164v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sadaf Moaveninejad, Andrea Janes, Camillo Porcaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tracking climbers' activity to improve services and make the best use of
their infrastructure is a concern for climbing gyms. Each climbing session must
be analyzed from beginning till lowering of the climber. Therefore, spotting
the climbers descending is crucial since it indicates when the ascent has come
to an end. This problem must be addressed while preserving privacy and
convenience of the climbers and the costs of the gyms. To this aim, a hardware
prototype is developed to collect data using accelerometer sensors attached to
a piece of climbing equipment mounted on the wall, called quickdraw, that
connects the climbing rope to the bolt anchors. The corresponding sensors are
configured to be energy-efficient, hence become practical in terms of expenses
and time consumption for replacement when using in large quantity in a climbing
gym. This paper describes hardware specifications, studies data measured by the
sensors in ultra-low power mode, detect sensors' orientation patterns during
lowering different routes, and develop an supervised approach to identify
lowering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2211.02680</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-driven Methods Applied to Soft Robot Modeling and Control: A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.12137v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.12137v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixi Chen, Federico Renda, Alexia Le Gall, Lorenzo Mocellin, Matteo Bernabei, Théo Dangel, Gastone Ciuti, Matteo Cianchetti, Cesare Stefanini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Soft robots show compliance and have infinite degrees of freedom. Thanks to
these properties, such robots can be leveraged for surgery, rehabilitation,
biomimetics, unstructured environment exploring, and industrial grippers. In
this case, they attract scholars from a variety of areas. However, nonlinearity
and hysteresis effects also bring a burden to robot modeling. Moreover,
following their flexibility and adaptation, soft robot control is more
challenging than rigid robot control. In order to model and control soft
robots, a large number of data-driven methods are utilized in pairs or
separately. This review first briefly introduces two foundations for
data-driven approaches, which are physical models and the Jacobian matrix, then
summarizes three kinds of data-driven approaches, which are statistical method,
neural network, and reinforcement learning. This review compares the modeling
and controller features, e.g., model dynamics, data requirement, and target
task, within and among these categories. Finally, we summarize the features of
each method. A discussion about the advantages and limitations of the existing
modeling and control approaches is presented, and we forecast the future of
data-driven approaches in soft robots. A website
(https://sites.google.com/view/23zcb) is built for this review and will be
updated frequently.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 6 figures, 7tables, accepted by IEEE Transactions on
  Automation Science and Engineering on 11 March, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ nvblox: GPU-Accelerated Incremental Signed Distance Field Mapping <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.00626v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.00626v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Millane, Helen Oleynikova, Emilie Wirbel, Remo Steiner, Vikram Ramasamy, David Tingdahl, Roland Siegwart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense, volumetric maps are essential to enable robot navigation and
interaction with the environment. To achieve low latency, dense maps are
typically computed onboard the robot, often on computationally constrained
hardware. Previous works leave a gap between CPU-based systems for robotic
mapping which, due to computation constraints, limit map resolution or scale,
and GPU-based reconstruction systems which omit features that are critical to
robotic path planning, such as computation of the Euclidean Signed Distance
Field (ESDF). We introduce a library, nvblox, that aims to fill this gap, by
GPU-accelerating robotic volumetric mapping. Nvblox delivers a significant
performance improvement over the state of the art, achieving up to a 177x
speed-up in surface reconstruction, and up to a 31x improvement in distance
field computation, and is available open-source.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Control Barrier Functions in Dynamic UAVs for Kinematic Obstacle
  Avoidance: A Collision Cone Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.15871v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.15871v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manan Tayal, Rajpal Singh, Jishnu Keshavan, Shishir Kolathaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unmanned aerial vehicles (UAVs), specifically quadrotors, have revolutionized
various industries with their maneuverability and versatility, but their safe
operation in dynamic environments heavily relies on effective collision
avoidance techniques. This paper introduces a novel technique for safely
navigating a quadrotor along a desired route while avoiding kinematic
obstacles. We propose a new constraint formulation that employs control barrier
functions (CBFs) and collision cones to ensure that the relative velocity
between the quadrotor and the obstacle always avoids a cone of vectors that may
lead to a collision. By showing that the proposed constraint is a valid CBF for
quadrotors, we are able to leverage its real-time implementation via Quadratic
Programs (QPs), called the CBF-QPs. Validation includes PyBullet simulations
and hardware experiments on Crazyflie 2.1, demonstrating effectiveness in
static and moving obstacle scenarios. Comparative analysis with literature,
especially higher order CBF-QPs, highlights the proposed approach's less
conservative nature. Simulation and Hardware videos are available here:
https://tayalmanan28.github.io/C3BF-UAV/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at American Control Conference(ACC) 2024. 6 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AG-CVG: Coverage Planning with a Mobile Recharging UGV and an
  Energy-Constrained UAV <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07621v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07621v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nare Karapetyan, Ahmad Bilal Asghar, Amisha Bhaskar, Guangyao Shi, Dinesh Manocha, Pratap Tokekar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present an approach for coverage path planning for a team
of an energy-constrained Unmanned Aerial Vehicle (UAV) and an Unmanned Ground
Vehicle (UGV). Both the UAV and the UGV have predefined areas that they have to
cover. The goal is to perform complete coverage by both robots while minimizing
the coverage time. The UGV can also serve as a mobile recharging station. The
UAV and UGV need to occasionally rendezvous for recharging. We propose a
heuristic method to address this NP-Hard planning problem. Our approach
involves initially determining coverage paths without factoring in energy
constraints. Subsequently, we cluster segments of these paths and employ graph
matching to assign UAV clusters to UGV clusters for efficient recharging
management. We perform numerical analysis on real-world coverage applications
and show that compared with a greedy approach our method reduces rendezvous
overhead on average by 11.33%. We demonstrate proof-of-concept with a team of a
VOXL m500 drone and a Clearpath Jackal ground vehicle, providing a complete
system from the offline algorithm to the field execution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2024 Proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Human Movement Forecasting with Loose Clothing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09237v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09237v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianchen Shen, Irene Di Giulio, Matthew Howard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human motion prediction and trajectory forecasting are essential in human
motion analysis. Nowadays, sensors can be seamlessly integrated into clothing
using cutting-edge electronic textile (e-textile) technology, allowing
long-term recording of human movements outside the laboratory. Motivated by the
recent findings that clothing-attached sensors can achieve higher activity
recognition accuracy than body-attached sensors. This work investigates the
performance of human motion prediction using clothing-attached sensors compared
with body-attached sensors. It reports experiments in which statistical models
learnt from the movement of loose clothing are used to predict motion patterns
of the body of robotically simulated and real human behaviours.
Counterintuitively, the results show that fabric-attached sensors can have
better motion prediction performance than rigid-attached sensors. Specifically,
The fabric-attached sensor can improve the accuracy up to 40% and requires up
to 80% less duration of the past trajectory to achieve high prediction accuracy
(i.e., 95%) compared to the rigid-attached sensor.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DyST: Towards Dynamic Neural Scene Representations on Real-World Videos <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06020v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06020v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Seitzer, Sjoerd van Steenkiste, Thomas Kipf, Klaus Greff, Mehdi S. M. Sajjadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual understanding of the world goes beyond the semantics and flat
structure of individual images. In this work, we aim to capture both the 3D
structure and dynamics of real-world scenes from monocular real-world videos.
Our Dynamic Scene Transformer (DyST) model leverages recent work in neural
scene representation to learn a latent decomposition of monocular real-world
videos into scene content, per-view scene dynamics, and camera pose. This
separation is achieved through a novel co-training scheme on monocular videos
and our new synthetic dataset DySO. DyST learns tangible latent representations
for dynamic scenes that enable view generation with separate control over the
camera and the content of the scene.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 spotlight. Project website: https://dyst-paper.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RELEAD: Resilient Localization with Enhanced LiDAR Odometry in Adverse
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18934v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18934v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqiang Chen, Hongbo Chen, Yuhua Qi, Shipeng Zhong, Dapeng Feng, Wu Jin, Weisong Wen, Ming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR-based localization is valuable for applications like mining surveys and
underground facility maintenance. However, existing methods can struggle when
dealing with uninformative geometric structures in challenging scenarios. This
paper presents RELEAD, a LiDAR-centric solution designed to address
scan-matching degradation. Our method enables degeneracy-free point cloud
registration by solving constrained ESIKF updates in the front end and
incorporates multisensor constraints, even when dealing with outlier
measurements, through graph optimization based on Graduated Non-Convexity
(GNC). Additionally, we propose a robust Incremental Fixed Lag Smoother (rIFL)
for efficient GNC-based optimization. RELEAD has undergone extensive evaluation
in degenerate scenarios and has outperformed existing state-of-the-art
LiDAR-Inertial odometry and LiDAR-Visual-Inertial odometry methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distilling Knowledge for Short-to-Long Term Trajectory Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.08553v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.08553v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sourav Das, Guglielmo Camporese, Shaokang Cheng, Lamberto Ballan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-term trajectory forecasting is an important and challenging problem in
the fields of computer vision, machine learning, and robotics. One fundamental
difficulty stands in the evolution of the trajectory that becomes more and more
uncertain and unpredictable as the time horizon grows, subsequently increasing
the complexity of the problem. To overcome this issue, in this paper, we
propose Di-Long, a new method that employs the distillation of a short-term
trajectory model forecaster that guides a student network for long-term
trajectory prediction during the training process. Given a total sequence
length that comprehends the allowed observation for the student network and the
complementary target sequence, we let the student and the teacher solve two
different related tasks defined over the same full trajectory: the student
observes a short sequence and predicts a long trajectory, whereas the teacher
observes a longer sequence and predicts the remaining short target trajectory.
The teacher's task is less uncertain, and we use its accurate predictions to
guide the student through our knowledge distillation framework, reducing
long-term future uncertainty. Our experiments show that our proposed Di-Long
method is effective for long-term forecasting and achieves state-of-the-art
performance on the Intersection Drone Dataset (inD) and the Stanford Drone
Dataset (SDD).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RACE-SM: Reinforcement Learning Based Autonomous Control for Social
  On-Ramp Merging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03359v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03359v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jordan Poots
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous parallel-style on-ramp merging in human controlled traffic
continues to be an existing issue for autonomous vehicle control. Existing
non-learning based solutions for vehicle control rely on rules and optimization
primarily. These methods have been seen to present significant challenges.
Recent advancements in Deep Reinforcement Learning have shown promise and have
received significant academic interest however the available learning based
approaches show inadequate attention to other highway vehicles and often rely
on inaccurate road traffic assumptions. In addition, the parallel-style case is
rarely considered. A novel learning based model for acceleration and lane
change decision making that explicitly considers the utility to both the ego
vehicle and its surrounding vehicles which may be cooperative or uncooperative
to produce behaviour that is socially acceptable is proposed. The novel reward
function makes use of Social Value Orientation to weight the vehicle's level of
social cooperation and is divided into ego vehicle and surrounding vehicle
utility which are weighted according to the model's designated Social Value
Orientation. A two-lane highway with an on-ramp divided into a taper-style and
parallel-style section is considered. Simulation results indicated the
importance of considering surrounding vehicles in reward function design and
show that the proposed model matches or surpasses those in literature in terms
of collisions while also introducing socially courteous behaviour avoiding near
misses and anti-social behaviour through direct consideration of the effect of
merging on surrounding vehicles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated explanation of TTC, page 7</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FC-Planner: A Skeleton-guided Planning Framework for Fast Aerial
  Coverage of Complex 3D Scenes <span class="chip">ICRA2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13882v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13882v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Feng, Haojia Li, Mingjie Zhang, Xinyi Chen, Boyu Zhou, Shaojie Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D coverage path planning for UAVs is a crucial problem in diverse practical
applications. However, existing methods have shown unsatisfactory system
simplicity, computation efficiency, and path quality in large and complex
scenes. To address these challenges, we propose FC-Planner, a skeleton-guided
planning framework that can achieve fast aerial coverage of complex 3D scenes
without pre-processing. We decompose the scene into several simple subspaces by
a skeleton-based space decomposition (SSD). Additionally, the skeleton guides
us to effortlessly determine free space. We utilize the skeleton to efficiently
generate a minimal set of specialized and informative viewpoints for complete
coverage. Based on SSD, a hierarchical planner effectively divides the large
planning problem into independent sub-problems, enabling parallel planning for
each subspace. The carefully designed global and local planning strategies are
then incorporated to guarantee both high quality and efficiency in path
generation. We conduct extensive benchmark and real-world tests, where
FC-Planner computes over 10 times faster compared to state-of-the-art methods
with shorter path and more complete coverage. The source code will be made
publicly available to benefit the community. Project page:
https://hkust-aerial-robotics.github.io/FC-Planner.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICRA2024. Code:
  https://github.com/HKUST-Aerial-Robotics/FC-Planner. Video:
  https://www.bilibili.com/video/BV1h84y1D7u5/?spm_id_from=333.999.0.0&vd_source=0af61c122e5e37c944053b57e313025a.
  Project page: https://hkust-aerial-robotics.github.io/FC-Planner</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CompdVision: Combining Near-Field 3D Visual and Tactile Sensing Using a
  Compact Compound-Eye Imaging System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07146v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07146v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lifan Luo, Boyang Zhang, Zhijie Peng, Yik Kin Cheung, Guanlan Zhang, Zhigang Li, Michael Yu Wang, Hongyu Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As automation technologies advance, the need for compact and multi-modal
sensors in robotic applications is growing. To address this demand, we
introduce CompdVision, a novel sensor that employs a compound-eye imaging
system to combine near-field 3D visual and tactile sensing within a compact
form factor. CompdVision utilizes two types of vision units to address diverse
sensing needs, eliminating the need for complex modality conversion. Stereo
units with far-focus lenses can see through the transparent elastomer for depth
estimation beyond the contact surface. Simultaneously, tactile units with
near-focus lenses track the movement of markers embedded in the elastomer to
obtain contact deformation. Experimental results validate the sensor's superior
performance in 3D visual and tactile sensing, proving its capability for
reliable external object depth estimation and precise measurement of tangential
and normal contact forces. The dual modalities and compact design make the
sensor a versatile tool for robotic manipulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Radar Inertial Odometry for 3D State Estimation using mmWave
  Imaging Radar <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08608v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08608v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jui-Te Huang, Ruoyang Xu, Akshay Hinduja, Michael Kaess
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State estimation is a crucial component for the successful implementation of
robotic systems, relying on sensors such as cameras, LiDAR, and IMUs. However,
in real-world scenarios, the performance of these sensors is degraded by
challenging environments, e.g. adverse weather conditions and low-light
scenarios. The emerging 4D imaging radar technology is capable of providing
robust perception in adverse conditions. Despite its potential, challenges
remain for indoor settings where noisy radar data does not present clear
geometric features. Moreover, disparities in radar data resolution and field of
view (FOV) can lead to inaccurate measurements. While prior research has
explored radar-inertial odometry based on Doppler velocity information,
challenges remain for the estimation of 3D motion because of the discrepancy in
the FOV and resolution of the radar sensor. In this paper, we address Doppler
velocity measurement uncertainties. We present a method to optimize body frame
velocity while managing Doppler velocity uncertainty. Based on our
observations, we propose a dual imaging radar configuration to mitigate the
challenge of discrepancy in radar data. To attain high-precision 3D state
estimation, we introduce a strategy that seamlessly integrates radar data with
a consumer-grade IMU sensor using fixed-lag smoothing optimization. Finally, we
evaluate our approach using real-world 3D motion data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exact Consistency Tests for Gaussian Mixture Filters using Normalized
  Deviation Squared Statistics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17420v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17420v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nisar Ahmed, Luke Burks, Kailah Cabral, Alyssa Bekai Rose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of evaluating dynamic consistency in discrete time
probabilistic filters that approximate stochastic system state densities with
Gaussian mixtures. Dynamic consistency means that the estimated probability
distributions correctly describe the actual uncertainties. As such, the problem
of consistency testing naturally arises in applications with regards to
estimator tuning and validation. However, due to the general complexity of the
density functions involved, straightforward approaches for consistency testing
of mixture-based estimators have remained challenging to define and implement.
This paper derives a new exact result for Gaussian mixture consistency testing
within the framework of normalized deviation squared (NDS) statistics. It is
shown that NDS test statistics for generic multivariate Gaussian mixture models
exactly follow mixtures of generalized chi-square distributions, for which
efficient computational tools are available. The accuracy and utility of the
resulting consistency tests are numerically demonstrated on static and dynamic
mixture estimation examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures; final manuscript to be published 2024 American
  Control Conference (ACC 2024), corrected small typos and updated Fig. 1 for
  clarity</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Designing Anthropomorphic Soft Hands through Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04784v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04784v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pragna Mannam, Kenneth Shaw, Dominik Bauer, Jean Oh, Deepak Pathak, Nancy Pollard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling and simulating soft robot hands can aid in design iteration for
complex and high degree-of-freedom (DoF) morphologies. This can be further
supplemented by iterating on the design based on its performance in real world
manipulation tasks. However, iterating in the real world requires an approach
that allows us to test new designs quickly at low costs. In this paper, we
leverage rapid prototyping of the hand using 3D-printing, and utilize
teleoperation to evaluate the hand in real world manipulation tasks. Using this
method, we design a 3D-printed 16-DoF dexterous anthropomorphic soft hand
(DASH) and iteratively improve its design over five iterations. Rapid
prototyping techniques such as 3D-printing allow us to directly evaluate the
fabricated hand without modeling it in simulation. We show that the design
improves over five design iterations through evaluating the hand's performance
in 30 real-world teleoperated manipulation tasks. Testing over 900
demonstrations shows that our final version of DASH can solve 19 of the 30
tasks compared to Allegro, a popular rigid hand in the market, which can only
solve 7 tasks. We open-source our CAD models as well as the teleoperated
dataset for further study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Momentum-Aware Trajectory Optimisation using Full-Centroidal Dynamics
  and Implicit Inverse Kinematics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06074v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06074v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aristotelis Papatheodorou, Wolfgang Merkt, Alexander L. Mitchell, Ioannis Havoutis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current state-of-the-art gradient-based optimisation frameworks are able
to produce impressive dynamic manoeuvres such as linear and rotational jumps.
However, these methods, which optimise over the full rigid-body dynamics of the
robot, often require precise foothold locations apriori, while real-time
performance is not guaranteed without elaborate regularisation and tuning of
the cost function. In contrast, we investigate the advantages of a task-space
optimisation framework, with special focus on acrobatic motions. Our proposed
formulation exploits the system's high-order nonlinearities, such as the
nonholonomy of the angular momentum, in order to produce feasible,
high-acceleration manoeuvres. By leveraging the full-centroidal dynamics of the
quadruped ANYmal C and directly optimising its footholds and contact forces,
the framework is capable of producing efficient motion plans with low
computational overhead. Finally, we deploy our proposed framework on the ANYmal
C platform, and demonstrate its true capabilities through real-world
experiments, with the successful execution of high-acceleration motions, such
as linear and rotational jumps. Extensive analysis of these shows that the
robot's dynamics can be exploited to surpass its hardware limitations of having
a high mass and low-torque limits.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fourier Transporter: Bi-Equivariant Robotic Manipulation in 3D <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.12046v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.12046v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haojie Huang, Owen Howell, Dian Wang, Xupeng Zhu, Robin Walters, Robert Platt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many complex robotic manipulation tasks can be decomposed as a sequence of
pick and place actions. Training a robotic agent to learn this sequence over
many different starting conditions typically requires many iterations or
demonstrations, especially in 3D environments. In this work, we propose Fourier
Transporter (FourTran) which leverages the two-fold SE(d)xSE(d) symmetry in the
pick-place problem to achieve much higher sample efficiency. FourTran is an
open-loop behavior cloning method trained using expert demonstrations to
predict pick-place actions on new environments. FourTran is constrained to
incorporate symmetries of the pick and place actions independently. Our method
utilizes a fiber space Fourier transformation that allows for memory-efficient
construction. We test our proposed network on the RLbench benchmark and achieve
state-of-the-art results across various tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ V-STRONG: Visual <span class="highlight-title">Self-Supervised</span> Traversability Learning for Off-road
  Navigation <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16016v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16016v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanghun Jung, JoonHo Lee, Xiangyun Meng, Byron Boots, Alexander Lambert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable estimation of terrain traversability is critical for the successful
deployment of autonomous systems in wild, outdoor environments. Given the lack
of large-scale annotated datasets for off-road navigation, strictly-supervised
learning approaches remain limited in their generalization ability. To this
end, we introduce a novel, image-based self-supervised learning method for
traversability prediction, leveraging a state-of-the-art vision foundation
model for improved out-of-distribution performance. Our method employs
contrastive representation learning using both human driving data and
instance-based segmentation masks during training. We show that this simple,
yet effective, technique drastically outperforms recent methods in predicting
traversability for both on- and off-trail driving scenarios. We compare our
method with recent baselines on both a common benchmark as well as our own
datasets, covering a diverse range of outdoor environments and varied terrain
types. We also demonstrate the compatibility of resulting costmap predictions
with a model-predictive controller. Finally, we evaluate our approach on zero-
and few-shot tasks, demonstrating unprecedented performance for generalization
to new environments. Videos and additional material can be found here:
https://sites.google.com/view/visual-traversability-learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2024; 8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FEDORA: Flying Event <span class="highlight-title">Dataset</span> fOr Reactive behAvior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14392v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14392v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amogh Joshi, Adarsh Kosta, Wachirawit Ponghiran, Manish Nagaraj, Kaushik Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability of resource-constrained biological systems such as fruitflies to
perform complex and high-speed maneuvers in cluttered environments has been one
of the prime sources of inspiration for developing vision-based autonomous
systems. To emulate this capability, the perception pipeline of such systems
must integrate information cues from tasks including optical flow and depth
estimation, object detection and tracking, and segmentation, among others.
However, the conventional approach of employing slow, synchronous inputs from
standard frame-based cameras constrains these perception capabilities,
particularly during high-speed maneuvers. Recently, event-based sensors have
emerged as low latency and low energy alternatives to standard frame-based
cameras for capturing high-speed motion, effectively speeding up perception and
hence navigation. For coherence, all the perception tasks must be trained on
the same input data. However, present-day datasets are curated mainly for a
single or a handful of tasks and are limited in the rate of the provided ground
truths. To address these limitations, we present Flying Event Dataset fOr
Reactive behAviour (FEDORA) - a fully synthetic dataset for perception tasks,
with raw data from frame-based cameras, event-based cameras, and Inertial
Measurement Units (IMU), along with ground truths for depth, pose, and optical
flow at a rate much higher than existing datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DAVIS-Ag: A Synthetic Plant <span class="highlight-title">Dataset</span> for Prototyping Domain-Inspired
  Active Vision in Agricultural Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05764v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05764v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taeyeong Choi, Dario Guevara, Zifei Cheng, Grisha Bandodkar, Chonghan Wang, Brian N. Bailey, Mason Earles, Xin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In agricultural environments, viewpoint planning can be a critical
functionality for a robot with visual sensors to obtain informative
observations of objects of interest (e.g., fruits) from complex structures of
plant with random occlusions. Although recent studies on active vision have
shown some potential for agricultural tasks, each model has been designed and
validated on a unique environment that would not easily be replicated for
benchmarking novel methods being developed later. In this paper, we introduce a
dataset, so-called DAVIS-Ag, for promoting more extensive research on
Domain-inspired Active VISion in Agriculture. To be specific, we leveraged our
open-source "AgML" framework and 3D plant simulator of "Helios" to produce 502K
RGB images from 30K densely sampled spatial locations in 632 synthetic
orchards. Moreover, plant environments of strawberries, tomatoes, and grapes
are considered at two different scales (i.e., Single-Plant and Multi-Plant).
Useful labels are also provided for each image, including (1) bounding boxes
and (2) instance segmentation masks for all identifiable fruits, and also (3)
pointers to other images of the viewpoints that are reachable by an execution
of action so as to simulate active viewpoint selections at each time step.
Using DAVIS-Ag, we visualize motivating examples where fruit detection rates
can dramatically change depending on the pose of the camera view primarily due
to occlusions by other components, such as leaves. Furthermore, we present
several baseline models with experiment results for benchmarking in the task of
target visibility maximization. Transferability to real strawberry environments
is also investigated to demonstrate the feasibility of using the dataset for
prototyping real-world solutions. For future research, our dataset is made
publicly available online: https://github.com/ctyeong/DAVIS-Ag.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures, 5 tables. Submitted to CASE2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fish-inspired tracking of underwater turbulent plumes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06091v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06091v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Gunnarson, John O. Dabiri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous ocean-exploring vehicles have begun to take advantage of onboard
sensor measurements of water properties such as salinity and temperature to
locate oceanic features in real time. Such targeted sampling strategies enable
more rapid study of ocean environments by actively steering towards areas of
high scientific value. Inspired by the ability of aquatic animals to navigate
via flow sensing, this work investigates hydrodynamic cues for accomplishing
targeted sampling using a palm-sized robotic swimmer. As proof-of-concept
analogy for tracking hydrothermal vent plumes in the ocean, the robot is tasked
with locating the center of turbulent jet flows in a 13,000-liter water tank
using data from onboard pressure sensors. To learn a navigation strategy, we
first implemented Reinforcement Learning (RL) on a simulated version of the
robot navigating in proximity to turbulent jets. After training, the RL
algorithm discovered an effective strategy for locating the jets by following
transverse velocity gradients sensed by pressure sensors located on opposite
sides of the robot. When implemented on the physical robot, this gradient
following strategy enabled the robot to successfully locate the turbulent
plumes at more than twice the rate of random searching. Additionally, we found
that navigation performance improved as the distance between the pressure
sensors increased, which can inform the design of distributed flow sensors in
ocean robots. Our results demonstrate the effectiveness and limits of
flow-based navigation for autonomously locating hydrodynamic features of
interest.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pointing the Way: Refining Radar-Lidar Localization Using Learned ICP
  Weights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08731v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08731v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniil Lisus, Johann Laconte, Keenan Burnett, Timothy D. Barfoot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel deep-learning-based approach to improve
localizing radar measurements against lidar maps. Although the state of the art
for localization is matching lidar data to lidar maps, radar has been
considered as a promising alternative. This is largely due to radar being more
resilient against adverse weather such as precipitation and heavy fog. To make
use of existing high-quality lidar maps, while maintaining performance in
adverse weather, it is of interest to match radar data to lidar maps. However,
owing in part to the unique artefacts present in radar measurements,
radar-lidar localization has struggled to achieve comparable performance to
lidar-lidar systems, preventing it from being viable for autonomous driving.
This work builds on an ICP-based radar-lidar localization system by including a
learned preprocessing step that weights radar points based on high-level scan
information. Combining a proven analytical approach with a learned weight
reduces localization errors in radar-lidar ICP results run on real-world
autonomous driving data by up to 54.94% in translation and 68.39% in rotation,
while maintaining interpretability and robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages (6 content, 2 references). 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Risk-Aware Non-Myopic Motion Planner for Large-Scale Robotic Swarm Using
  CVaR Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16690v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16690v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuru Yang, Yunze Hu, Han Gao, Kang Ding, Zhaoyang Li, Pingping Zhu, Ying Sun, Chang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Swarm robotics has garnered significant attention due to its ability to
accomplish elaborate and synchronized tasks. Existing methodologies for motion
planning of swarm robotic systems mainly encounter difficulties in scalability
and safety guarantee. To address these limitations, we propose a Risk-aware
swarm mOtion planner using conditional ValuE at Risk (ROVER) that
systematically navigates large-scale swarms through cluttered environments
while ensuring safety. ROVER formulates a finite-time model predictive control
(FTMPC) problem predicated upon the macroscopic state of the robot swarm
represented by a Gaussian Mixture Model (GMM) and integrates conditional
value-at-risk (CVaR) to ensure collision avoidance. The key component of ROVER
is imposing a CVaR constraint on the distribution of the Signed Distance
Function between the swarm GMM and obstacles in the FTMPC to enforce collision
avoidance. Utilizing the analytical expression of CVaR of a GMM derived in this
work, we develop a computationally efficient solution to solve the non-linear
constrained FTMPC through sequential linear programming. Simulations and
comparisons with representative benchmark approaches demonstrate the
effectiveness of ROVER in flexibility, scalability, and risk mitigation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Egocentric Visual Self-Modeling for Autonomous Robot Dynamics Prediction
  and Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.03386v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.03386v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhang Hu, Boyuan Chen, Hod Lipson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability of robots to model their own dynamics is key to autonomous
planning and learning, as well as for autonomous damage detection and recovery.
Traditionally, dynamic models are pre-programmed or learned from external
observations. Here, we demonstrate for the first time how a task-agnostic
dynamic self-model can be learned using only a single first-person-view camera
in a self-supervised manner, without any prior knowledge of robot morphology,
kinematics, or task. Through experiments on a 12-DoF robot, we demonstrate the
capabilities of the model in basic locomotion tasks using visual input.
Notably, the robot can autonomously detect anomalies, such as damaged
components, and adapt its behavior, showcasing resilience in dynamic
environments. Furthermore, the model's generalizability was validated across
robots with different configurations, emphasizing its potential as a universal
tool for diverse robotic systems. The egocentric visual self-model proposed in
our work paves the way for more autonomous, adaptable, and resilient robotic
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SOS-Match: Segmentation for Open-Set Robust Correspondence Search and
  Robot Localization in Unstructured Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.04791v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.04791v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Annika Thomas, Jouko Kinnari, Parker Lusk, Kota Kondo, Jonathan P. How
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present SOS-Match, a novel framework for detecting and matching objects in
unstructured environments. Our system consists of 1) a front-end mapping
pipeline using a zero-shot segmentation model to extract object masks from
images and track them across frames and 2) a frame alignment pipeline that uses
the geometric consistency of object relationships to efficiently localize
across a variety of conditions. We evaluate SOS-Match on the Batvik seasonal
dataset which includes drone flights collected over a coastal plot of southern
Finland during different seasons and lighting conditions. Results show that our
approach is more robust to changes in lighting and appearance than classical
image feature-based approaches or global descriptor methods, and it provides
more viewpoint invariance than learning-based feature detection and description
approaches. SOS-Match localizes within a reference map up to 46x faster than
other feature-based approaches and has a map size less than 0.5% the size of
the most compact other maps. SOS-Match is a promising new approach for landmark
detection and correspondence search in unstructured environments that is robust
to changes in lighting and appearance and is more computationally efficient
than other approaches, suggesting that the geometric arrangement of segments is
a valuable localization cue in unstructured environments. We release our
datasets at https://acl.mit.edu/SOS-Match/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Targeted Parallelization of Conflict-Based Search for Multi-Robot Path
  Planning <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11768v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11768v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teng Guo, Jingjin Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Robot Path Planning (MRPP) on graphs, equivalently known as Multi-Agent
Path Finding (MAPF), is a well-established NP-hard problem with critically
important applications. As serial computation in (near)-optimally solving MRPP
approaches the computation efficiency limit, parallelization offers a promising
route to push the limit further, especially in handling hard or large MRPP
instances. In this study, we initiated a \emph{targeted} parallelization effort
to boost the performance of conflict-based search for MRPP. Specifically, when
instances are relatively small but robots are densely packed with strong
interactions, we apply a decentralized parallel algorithm that concurrently
explores multiple branches that leads to markedly enhanced solution discovery.
On the other hand, when instances are large with sparse robot-robot
interactions, we prioritize node expansion and conflict resolution. Our
innovative multi-threaded approach to parallelizing bounded-suboptimal conflict
search-based algorithms demonstrates significant improvements over baseline
serial methods in success rate or runtime. Our contribution further pushes the
understanding of MRPP and charts a promising path for elevating solution
quality and computational efficiency through parallel algorithmic strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knolling Bot: Learning Robotic Object Arrangement from Tidy
  Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04566v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04566v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhang Hu, Zhizhuo Zhang, Xinyue Zhu, Ruibo Liu, Philippe Wyder, Hod Lipson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Addressing the challenge of organizing scattered items in domestic spaces is
complicated by the diversity and subjective nature of tidiness. Just as the
complexity of human language allows for multiple expressions of the same idea,
household tidiness preferences and organizational patterns vary widely, so
presetting object locations would limit the adaptability to new objects and
environments. Inspired by advancements in natural language processing (NLP),
this paper introduces a self-supervised learning framework that allows robots
to understand and replicate the concept of tidiness from demonstrations of
well-organized layouts, akin to using conversational datasets to train Large
Language Models(LLM). We leverage a transformer neural network to predict the
placement of subsequent objects. We demonstrate a ``knolling'' system with a
robotic arm and an RGB camera to organize items of varying sizes and quantities
on a table. Our method not only trains a generalizable concept of tidiness,
enabling the model to provide diverse solutions and adapt to different numbers
of objects, but it can also incorporate human preferences to generate
customized tidy tables without explicit target positions for each object.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Design and Flight Demonstration of a Quadrotor for Urban Mapping and
  Target Tracking Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13195v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13195v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Collin Hague, Nick Kakavitsas, Jincheng Zhang, Chris Beam, Andrew Willis, Artur Wolek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes the hardware design and flight demonstration of a small
quadrotor with imaging sensors for urban mapping, hazard avoidance, and target
tracking research. The vehicle is equipped with five cameras, including two
pairs of fisheye stereo cameras that enable a nearly omnidirectional view and a
two-axis gimbaled camera. An onboard NVIDIA Jetson Orin Nano computer running
the Robot Operating System software is used for data collection. An autonomous
tracking behavior was implemented to coordinate the motion of the quadrotor and
gimbaled camera to track a moving GPS coordinate. The data collection system
was demonstrated through a flight test that tracked a moving GPS-tagged vehicle
through a series of roads and parking lots. A map of the environment was
reconstructed from the collected images using the Direct Sparse Odometry (DSO)
algorithm. The performance of the quadrotor was also characterized by acoustic
noise, communication range, battery voltage in hover, and maximum speed tests.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 10 figures, To be presented at IEEE SoutheastCon 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-14T00:00:00Z">2024-03-14</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">52</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GaussianGrasper: 3D Language Gaussian Splatting for Open-vocabulary
  Robotic Grasping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhang Zheng, Xiangyu Chen, Yupeng Zheng, Songen Gu, Runyi Yang, Bu Jin, Pengfei Li, Chengliang Zhong, Zengmao Wang, Lina Liu, Chao Yang, Dawei Wang, Zhen Chen, Xiaoxiao Long, Meiqing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Constructing a 3D scene capable of accommodating open-ended language queries,
is a pivotal pursuit, particularly within the domain of robotics. Such
technology facilitates robots in executing object manipulations based on human
language directives. To tackle this challenge, some research efforts have been
dedicated to the development of language-embedded implicit fields. However,
implicit fields (e.g. NeRF) encounter limitations due to the necessity of
processing a large number of input views for reconstruction, coupled with their
inherent inefficiencies in inference. Thus, we present the GaussianGrasper,
which utilizes 3D Gaussian Splatting to explicitly represent the scene as a
collection of Gaussian primitives. Our approach takes a limited set of RGB-D
views and employs a tile-based splatting technique to create a feature field.
In particular, we propose an Efficient Feature Distillation (EFD) module that
employs contrastive learning to efficiently and accurately distill language
embeddings derived from foundational models. With the reconstructed geometry of
the Gaussian field, our method enables the pre-trained grasping model to
generate collision-free grasp pose candidates. Furthermore, we propose a
normal-guided grasp module to select the best grasp pose. Through comprehensive
real-world experiments, we demonstrate that GaussianGrasper enables robots to
accurately query and grasp objects with language instructions, providing a new
solution for language-guided manipulation tasks. Data and codes can be
available at https://github.com/MrSecant/GaussianGrasper.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D-VLA: A 3D Vision-Language-Action Generative World Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Zhen, Xiaowen Qiu, Peihao Chen, Jincheng Yang, Xin Yan, Yilun Du, Yining Hong, Chuang Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent vision-language-action (VLA) models rely on 2D inputs, lacking
integration with the broader realm of the 3D physical world. Furthermore, they
perform action prediction by learning a direct mapping from perception to
action, neglecting the vast dynamics of the world and the relations between
actions and dynamics. In contrast, human beings are endowed with world models
that depict imagination about future scenarios to plan actions accordingly. To
this end, we propose 3D-VLA by introducing a new family of embodied foundation
models that seamlessly link 3D perception, reasoning, and action through a
generative world model. Specifically, 3D-VLA is built on top of a 3D-based
large language model (LLM), and a set of interaction tokens is introduced to
engage with the embodied environment. Furthermore, to inject generation
abilities into the model, we train a series of embodied diffusion models and
align them into the LLM for predicting the goal images and point clouds. To
train our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by
extracting vast 3D-related information from existing robotics datasets. Our
experiments on held-in datasets demonstrate that 3D-VLA significantly improves
the reasoning, multimodal generation, and planning capabilities in embodied
environments, showcasing its potential in real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://vis-www.cs.umass.edu/3dvla/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Autonomous Drone Flight in the Forest with Visual-Inertial SLAM
  and Dense Submaps Built without LiDAR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastián Barbas Laina, Simon Boche, Sotiris Papatheodorou, Dimos Tzoumanikas, Simon Schaefer, Hanzhi Chen, Stefan Leutenegger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Forestry constitutes a key element for a sustainable future, while it is
supremely challenging to introduce digital processes to improve efficiency. The
main limitation is the difficulty of obtaining accurate maps at high temporal
and spatial resolution as a basis for informed forestry decision-making, due to
the vast area forests extend over and the sheer number of trees. To address
this challenge, we present an autonomous Micro Aerial Vehicle (MAV) system
which purely relies on cost-effective and light-weight passive visual and
inertial sensors to perform under-canopy autonomous navigation. We leverage
visual-inertial simultaneous localization and mapping (VI-SLAM) for accurate
MAV state estimates and couple it with a volumetric occupancy submapping system
to achieve a scalable mapping framework which can be directly used for path
planning. As opposed to a monolithic map, submaps inherently deal with
inevitable drift and corrections from VI-SLAM, since they move with pose
estimates as they are updated. To ensure the safety of the MAV during
navigation, we also propose a novel reference trajectory anchoring scheme that
moves and deforms the reference trajectory the MAV is tracking upon state
updates from the VI-SLAM system in a consistent way, even upon large changes in
state estimates due to loop-closures. We thoroughly validate our system in both
real and simulated forest environments with high tree densities in excess of
400 trees per hectare and at speeds up to 3 m/s - while not encountering a
single collision or system failure. To the best of our knowledge this is the
first system which achieves this level of performance in such unstructured
environment using low-cost passive visual sensors and fully on-board
computation including VI-SLAM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ExploRLLM: Guiding Exploration in Reinforcement Learning with Large
  Language Models <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runyu Ma, Jelle Luijkx, Zlatan Ajanovic, Jens Kober
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In image-based robot manipulation tasks with large observation and action
spaces, reinforcement learning struggles with low sample efficiency, slow
training speed, and uncertain convergence. As an alternative, large pre-trained
foundation models have shown promise in robotic manipulation, particularly in
zero-shot and few-shot applications. However, using these models directly is
unreliable due to limited reasoning capabilities and challenges in
understanding physical and spatial contexts. This paper introduces ExploRLLM, a
novel approach that leverages the inductive bias of foundation models (e.g.
Large Language Models) to guide exploration in reinforcement learning. We also
exploit these foundation models to reformulate the action and observation
spaces to enhance the training efficiency in reinforcement learning. Our
experiments demonstrate that guided exploration enables much quicker
convergence than training without it. Additionally, we validate that ExploRLLM
outperforms vanilla foundation model baselines and that the policy trained in
simulation can be applied in real-world settings without additional training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages,8 figures, conference IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are you a robot? Detecting Autonomous Vehicles from Behavior Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabio Maresca, Filippo Grazioli, Antonio Albanese, Vincenzo Sciancalepore, Gianpiero Negri, Xavier Costa-Perez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The tremendous hype around autonomous driving is eagerly calling for emerging
and novel technologies to support advanced mobility use cases. As car
manufactures keep developing SAE level 3+ systems to improve the safety and
comfort of passengers, traffic authorities need to establish new procedures to
manage the transition from human-driven to fully-autonomous vehicles while
providing a feedback-loop mechanism to fine-tune envisioned autonomous systems.
Thus, a way to automatically profile autonomous vehicles and differentiate
those from human-driven ones is a must. In this paper, we present a
fully-fledged framework that monitors active vehicles using camera images and
state information in order to determine whether vehicles are autonomous,
without requiring any active notification from the vehicles themselves.
Essentially, it builds on the cooperation among vehicles, which share their
data acquired on the road feeding a machine learning model to identify
autonomous cars. We extensively tested our solution and created the NexusStreet
dataset, by means of the CARLA simulator, employing an autonomous driving
control agent and a steering wheel maneuvered by licensed drivers. Experiments
show it is possible to discriminate the two behaviors by analyzing video clips
with an accuracy of 80%, which improves up to 93% when the target state
information is available. Lastly, we deliberately degraded the state to observe
how the framework performs under non-ideal data collection conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Trust in Autonomous Agents: An Architecture for Accountability
  and Explainability through Blockchain and Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09567v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09567v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laura Fernández-Becerra, Miguel Ángel González-Santamarta, Ángel Manuel Guerrero-Higueras, Francisco Javier Rodríguez-Lera, Vicente Matellán Olivera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The deployment of autonomous agents in environments involving human
interaction has increasingly raised security concerns. Consequently,
understanding the circumstances behind an event becomes critical, requiring the
development of capabilities to justify their behaviors to non-expert users.
Such explanations are essential in enhancing trustworthiness and safety, acting
as a preventive measure against failures, errors, and misunderstandings.
Additionally, they contribute to improving communication, bridging the gap
between the agent and the user, thereby improving the effectiveness of their
interactions. This work presents an accountability and explainability
architecture implemented for ROS-based mobile robots. The proposed solution
consists of two main components. Firstly, a black box-like element to provide
accountability, featuring anti-tampering properties achieved through blockchain
technology. Secondly, a component in charge of generating natural language
explanations by harnessing the capabilities of Large Language Models (LLMs)
over the data contained within the previously mentioned black box. The study
evaluates the performance of our solution in three different scenarios, each
involving autonomous agent navigation functionalities. This evaluation includes
a thorough examination of accountability and explainability metrics,
demonstrating the effectiveness of our approach in using accountable data from
robot actions to obtain coherent, accurate and understandable explanations,
even when facing challenges inherent in the use of autonomous agents in
real-world scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PaperBot: Learning to Design Real-World Tools Using Paper 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruoshi Liu, Junbang Liang, Sruthi Sudhakar, Huy Ha, Cheng Chi, Shuran Song, Carl Vondrick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Paper is a cheap, recyclable, and clean material that is often used to make
practical tools. Traditional tool design either relies on simulation or
physical analysis, which is often inaccurate and time-consuming. In this paper,
we propose PaperBot, an approach that directly learns to design and use a tool
in the real world using paper without human intervention. We demonstrated the
effectiveness and efficiency of PaperBot on two tool design tasks: 1. learning
to fold and throw paper airplanes for maximum travel distance 2. learning to
cut paper into grippers that exert maximum gripping force. We present a
self-supervised learning framework that learns to perform a sequence of
folding, cutting, and dynamic manipulation actions in order to optimize the
design and use of a tool. We deploy our system to a real-world two-arm robotic
system to solve challenging design tasks that involve aerodynamics (paper
airplane) and friction (paper gripper) that are impossible to simulate
accurately.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Website: https://paperbot.cs.columbia.edu/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is Data All That Matters? The Role of Control Frequency for
  Learning-Based Sampled-Data Control of Uncertain Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ralf Römer, Lukas Brunke, Siqi Zhou, Angela P. Schoellig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning models or control policies from data has become a powerful tool to
improve the performance of uncertain systems. While a strong focus has been
placed on increasing the amount and quality of data to improve performance,
data can never fully eliminate uncertainty, making feedback necessary to ensure
stability and performance. We show that the control frequency at which the
input is recalculated is a crucial design parameter, yet it has hardly been
considered before. We address this gap by combining probabilistic model
learning and sampled-data control. We use Gaussian processes (GPs) to learn a
continuous-time model and compute a corresponding discrete-time controller. The
result is an uncertain sampled-data control system, for which we derive robust
stability conditions. We formulate semidefinite programs to compute the minimum
control frequency required for stability and to optimize performance. As a
result, our approach enables us to study the effect of both control frequency
and data on stability and closed-loop performance. We show in numerical
simulations of a quadrotor that performance can be improved by increasing
either the amount of data or the control frequency, and that we can trade off
one for the other. For example, by increasing the control frequency by 33%, we
can reduce the number of data points by half while still achieving similar
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 2024 American Control Conference (ACC), 7 pages, 4
  figures, code is available at https://github.com/ralfroemer99/lb_sd</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VIRUS-NeRF -- Vision, InfraRed and UltraSonic based Neural Radiance
  Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolaj Schmid, Cornelius von Einem, Cesar Cadena, Roland Siegwart, Lorenz Hruby, Florian Tschopp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous mobile robots are an increasingly integral part of modern factory
and warehouse operations. Obstacle detection, avoidance and path planning are
critical safety-relevant tasks, which are often solved using expensive LiDAR
sensors and depth cameras. We propose to use cost-effective low-resolution
ranging sensors, such as ultrasonic and infrared time-of-flight sensors by
developing VIRUS-NeRF - Vision, InfraRed, and UltraSonic based Neural Radiance
Fields. Building upon Instant Neural Graphics Primitives with a Multiresolution
Hash Encoding (Instant-NGP), VIRUS-NeRF incorporates depth measurements from
ultrasonic and infrared sensors and utilizes them to update the occupancy grid
used for ray marching. Experimental evaluation in 2D demonstrates that
VIRUS-NeRF achieves comparable mapping performance to LiDAR point clouds
regarding coverage. Notably, in small environments, its accuracy aligns with
that of LiDAR measurements, while in larger ones, it is bounded by the utilized
ultrasonic sensors. An in-depth ablation study reveals that adding ultrasonic
and infrared sensors is highly effective when dealing with sparse data and low
view variation. Further, the proposed occupancy grid of VIRUS-NeRF improves the
mapping capabilities and increases the training speed by 46% compared to
Instant-NGP. Overall, VIRUS-NeRF presents a promising approach for
cost-effective local mapping in mobile robotics, with potential applications in
safety and navigation tasks. The code can be found at
https://github.com/ethz-asl/virus nerf.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Development of control algorithms for mobile robotics focused on their
  potential use for FPGA-based robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09459v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09459v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrés-David Suárez-Gómez, Andres A. Hernandez Ortega
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the development and optimization of control
algorithms for mobile robotics, with a keen focus on their implementation in
Field-Programmable Gate Arrays (FPGAs). It delves into both classical control
approaches such as PID and modern techniques including deep learning,
addressing their application in sectors ranging from industrial automation to
medical care. The study highlights the practical challenges and advancements in
embedding these algorithms into FPGAs, which offer significant benefits for
mobile robotics due to their high-speed processing and parallel computation
capabilities. Through an analysis of various control strategies, the paper
showcases the improvements in robot performance, particularly in navigation and
obstacle avoidance. It emphasizes the critical role of FPGAs in enhancing the
efficiency and adaptability of control algorithms in dynamic environments.
Additionally, the research discusses the difficulties in benchmarking and
evaluating the performance of these algorithms in real-world applications,
suggesting a need for standardized evaluation criteria. The contribution of
this work lies in its comprehensive examination of control algorithms'
potential in FPGA-based mobile robotics, offering insights into future research
directions for improving robotic autonomy and operational efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OpenGraph: Open-Vocabulary Hierarchical 3D Graph Representation in
  Large-Scale Outdoor Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinan Deng, Jiahui Wang, Jingyu Zhao, Xinyu Tian, Guangyan Chen, Yi Yang, Yufeng Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Environment maps endowed with sophisticated semantics are pivotal for
facilitating seamless interaction between robots and humans, enabling them to
effectively carry out various tasks. Open-vocabulary maps, powered by
Visual-Language models (VLMs), possess inherent advantages, including
multimodal retrieval and open-set classes. However, existing open-vocabulary
maps are constrained to closed indoor scenarios and VLM features, thereby
diminishing their usability and inference capabilities. Moreover, the absence
of topological relationships further complicates the accurate querying of
specific instances. In this work, we propose OpenGraph, a representation of
open-vocabulary hierarchical graph structure designed for large-scale outdoor
environments. OpenGraph initially extracts instances and their captions from
visual images using 2D foundation models, encoding the captions with features
to enhance textual reasoning. Subsequently, 3D incremental panoramic mapping
with feature embedding is achieved by projecting images onto LiDAR point
clouds. Finally, the environment is segmented based on lane graph connectivity
to construct a hierarchical graph. Validation results from real public dataset
SemanticKITTI demonstrate that, even without fine-tuning the models, OpenGraph
exhibits the ability to generalize to novel semantic classes and achieve the
highest segmentation and query accuracy. The source code of OpenGraph is
publicly available at https://github.com/BIT-DYN/OpenGraph.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences
  using Attention-based Temporal Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arul Selvam Periyasamy, Sven Behnke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cluttered bin-picking environments are challenging for pose estimation
models. Despite the impressive progress enabled by deep learning, single-view
RGB pose estimation models perform poorly in cluttered dynamic environments.
Imbuing the rich temporal information contained in the video of scenes has the
potential to enhance models ability to deal with the adverse effects of
occlusion and the dynamic nature of the environments. Moreover, joint object
detection and pose estimation models are better suited to leverage the
co-dependent nature of the tasks for improving the accuracy of both tasks. To
this end, we propose attention-based temporal fusion for multi-object 6D pose
estimation that accumulates information across multiple frames of a video
sequence. Our MOTPose method takes a sequence of images as input and performs
joint object detection and pose estimation for all objects in one forward pass.
It learns to aggregate both object embeddings and object parameters over
multiple time steps using cross-attention-based fusion modules. We evaluate our
method on the physically-realistic cluttered bin-picking dataset SynPick and
the YCB-Video dataset and demonstrate improved pose estimation accuracy as well
as better object detection accuracy
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enabling Waypoint Generation for Collaborative Robots using LLMs and
  Mixed Reality <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09308v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09308v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cathy Mengying Fang, Krzysztof Zieliński, Pattie Maes, Joe Paradiso, Bruce Blumberg, Mikkel Baun Kjærgaard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Programming a robotic is a complex task, as it demands the user to have a
good command of specific programming languages and awareness of the robot's
physical constraints. We propose a framework that simplifies robot deployment
by allowing direct communication using natural language. It uses large language
models (LLM) for prompt processing, workspace understanding, and waypoint
generation. It also employs Augmented Reality (AR) to provide visual feedback
of the planned outcome. We showcase the effectiveness of our framework with a
simple pick-and-place task, which we implement on a real robot. Moreover, we
present an early concept of expressive robot behavior and skill generation that
can be used to communicate with the user and learn new skills (e.g., object
grasping).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to VLMNM 2024 - Workshop, ICRA 2024. This work has been
  submitted to the IEEE for possible publication. Copyright may be transferred
  without notice, after which this version may no longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pushing in the Dark: A Reactive Pushing Strategy for Mobile Robots Using
  Tactile Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Idil Ozdamar, Doganay Sirintuna, Robin Arbaud, Arash Ajoudani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For mobile robots, navigating cluttered or dynamic environments often
necessitates non-prehensile manipulation, particularly when faced with objects
that are too large, irregular, or fragile to grasp. The unpredictable behavior
and varying physical properties of these objects significantly complicate
manipulation tasks. To address this challenge, this manuscript proposes a novel
Reactive Pushing Strategy. This strategy allows a mobile robot to dynamically
adjust its base movements in real-time to achieve successful pushing maneuvers
towards a target location. Notably, our strategy adapts the robot motion based
on changes in contact location obtained through the tactile sensor covering the
base, avoiding dependence on object-related assumptions and its modeled
behavior. The effectiveness of the Reactive Pushing Strategy was initially
evaluated in the simulation environment, where it significantly outperformed
the compared baseline approaches. Following this, we validated the proposed
strategy through real-world experiments, demonstrating the robot capability to
push objects to the target points located in the entire vicinity of the robot.
In both simulation and real-world experiments, the object-specific properties
(shape, mass, friction, inertia) were altered along with the changes in target
locations to assess the robustness of the proposed method comprehensively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, submitted to IEEE Robotics and Automation
  Letters, for associated video, see https://youtu.be/IuGxlNe246M</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ THÖR-MAGNI: A Large-scale Indoor Motion Capture Recording of Human
  Movement and Robot Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Schreiter, Tiago Rodrigues de Almeida, Yufei Zhu, Eduardo Gutierrez Maestro, Lucas Morillo-Mendez, Andrey Rudenko, Luigi Palmieri, Tomasz P. Kucner, Martin Magnusson, Achim J. Lilienthal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new large dataset of indoor human and robot navigation and
interaction, called TH\"OR-MAGNI, that is designed to facilitate research on
social navigation: e.g., modelling and predicting human motion, analyzing
goal-oriented interactions between humans and robots, and investigating visual
attention in a social interaction context. TH\"OR-MAGNI was created to fill a
gap in available datasets for human motion analysis and HRI. This gap is
characterized by a lack of comprehensive inclusion of exogenous factors and
essential target agent cues, which hinders the development of robust models
capable of capturing the relationship between contextual cues and human
behavior in different scenarios. Unlike existing datasets, TH\"OR-MAGNI
includes a broader set of contextual features and offers multiple scenario
variations to facilitate factor isolation. The dataset includes many social
human-human and human-robot interaction scenarios, rich context annotations,
and multi-modal data, such as walking trajectories, gaze tracking data, and
lidar and camera streams recorded from a mobile robot. We also provide a set of
tools for visualization and processing of the recorded data. TH\"OR-MAGNI is,
to the best of our knowledge, unique in the amount and diversity of sensor data
collected in a contextualized and socially dynamic environment, capturing
natural human-robot interactions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to The International Journal of Robotics Research (IJRR) on
  28 of February 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BEHAVIOR-1K: A Human-Centered, Embodied AI Benchmark with 1,000 Everyday
  Activities and Realistic Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09227v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09227v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Martín-Martín, Chen Wang, Gabrael Levine, Wensi Ai, Benjamin Martinez, Hang Yin, Michael Lingelbach, Minjune Hwang, Ayano Hiranaka, Sujay Garlanka, Arman Aydin, Sharon Lee, Jiankai Sun, Mona Anvari, Manasi Sharma, Dhruva Bansal, Samuel Hunter, Kyu-Young Kim, Alan Lou, Caleb R Matthews, Ivan Villa-Renteria, Jerry Huayang Tang, Claire Tang, Fei Xia, Yunzhu Li, Silvio Savarese, Hyowon Gweon, C. Karen Liu, Jiajun Wu, Li Fei-Fei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present BEHAVIOR-1K, a comprehensive simulation benchmark for
human-centered robotics. BEHAVIOR-1K includes two components, guided and
motivated by the results of an extensive survey on "what do you want robots to
do for you?". The first is the definition of 1,000 everyday activities,
grounded in 50 scenes (houses, gardens, restaurants, offices, etc.) with more
than 9,000 objects annotated with rich physical and semantic properties. The
second is OMNIGIBSON, a novel simulation environment that supports these
activities via realistic physics simulation and rendering of rigid bodies,
deformable bodies, and liquids. Our experiments indicate that the activities in
BEHAVIOR-1K are long-horizon and dependent on complex manipulation skills, both
of which remain a challenge for even state-of-the-art robot learning solutions.
To calibrate the simulation-to-reality gap of BEHAVIOR-1K, we provide an
initial study on transferring solutions learned with a mobile manipulator in a
simulated apartment to its real-world counterpart. We hope that BEHAVIOR-1K's
human-grounded nature, diversity, and realism make it valuable for embodied AI
and robot learning research. Project website: https://behavior.stanford.edu.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A preliminary version was published at 6th Conference on Robot
  Learning (CoRL 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synchronisation-Oriented Design Approach for Adaptive Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Namhoon Cho, Seokwon Lee, Hyo-Sang Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents a synchronisation-oriented perspective towards adaptive
control which views model-referenced adaptation as synchronisation between
actual and virtual dynamic systems. In the context of adaptation, model
reference adaptive control methods make the state response of the actual plant
follow a reference model. In the context of synchronisation, consensus methods
involving diffusive coupling induce a collective behaviour across multiple
agents. We draw from the understanding about the two time-scale nature of
synchronisation motivated by the study of blended dynamics. The
synchronisation-oriented approach consists in the design of a coupling input to
achieve desired closed-loop error dynamics followed by the input allocation
process to shape the collective behaviour. We suggest that synchronisation can
be a reasonable design principle allowing a more holistic and systematic
approach to the design of adaptive control systems for improved transient
characteristics. Most notably, the proposed approach enables not only
constructive derivation but also substantial generalisation of the previously
developed closed-loop reference model adaptive control method. Practical
significance of the proposed generalisation lies at the capability to improve
the transient response characteristics and mitigate the unwanted peaking
phenomenon at the same time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 8 figures, extended version for a manuscript submitted to
  Automatica</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cellular-enabled Collaborative Robots Planning and Operations for
  Search-and-Rescue Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arnau Romero, Carmen Delgado, Lanfranco Zanzi, Raúl Suárez, Xavier Costa-Pérez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mission-critical operations, particularly in the context of Search-and-Rescue
(SAR) and emergency response situations, demand optimal performance and
efficiency from every component involved to maximize the success probability of
such operations. In these settings, cellular-enabled collaborative robotic
systems have emerged as invaluable assets, assisting first responders in
several tasks, ranging from victim localization to hazardous area exploration.
However, a critical limitation in the deployment of cellular-enabled
collaborative robots in SAR missions is their energy budget, primarily supplied
by batteries, which directly impacts their task execution and mobility. This
paper tackles this problem, and proposes a search-and-rescue framework for
cellular-enabled collaborative robots use cases that, taking as input the area
size to be explored, the robots fleet size, their energy profile, exploration
rate required and target response time, finds the minimum number of robots able
to meet the SAR mission goals and the path they should follow to explore the
area. Our results, i) show that first responders can rely on a SAR
cellular-enabled robotics framework when planning mission-critical operations
to take informed decisions with limited resources, and, ii) illustrate the
number of robots versus explored area and response time trade-off depending on
the type of robot: wheeled vs quadruped.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Lexicographic Optimization for Prioritized Robot Control and
  Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Pfeiffer, Abderrahmane Kheddar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present several tools for efficient sequential hierarchical
least-squares programming (S-HLSP) for lexicographical optimization tailored to
robot control and planning. As its main step, S-HLSP relies on approximations
of the original non-linear hierarchical least-squares programming (NL-HLSP) to
a hierarchical least-squares programming (HLSP) by the hierarchical Newton's
method or the hierarchical Gauss-Newton algorithm. We present a threshold
adaptation strategy for appropriate switches between the two. This ensures
optimality of infeasible constraints, promotes numerical stability when solving
the HLSP's and enhances optimality of lower priority levels by avoiding
regularized local minima. We introduce the solver $\mathcal{N}$ADM$_2$, an
alternating direction method of multipliers for HLSP based on nullspace
projections of active constraints. The required basis of nullspace of the
active constraints is provided by a computationally efficient turnback
algorithm for system dynamics discretized by the Euler method. It is based on
an upper bound on the bandwidth of linearly independent column subsets within
the linearized constraint matrices. Importantly, an expensive initial
rank-revealing matrix factorization is unnecessary. We show how the high
sparsity of the basis in the fully-actuated case can be preserved in the
under-actuated case. $\mathcal{N}$ADM$_2$ consistently shows faster
computations times than competing off-the-shelf solvers on NL-HLSP composed of
test-functions and whole-body trajectory optimization for fully-actuated and
under-actuated robotic systems. We demonstrate how the inherently lower
accuracy solutions of the alternating direction method of multipliers can be
used to warm-start the non-linear solver for efficient computation of high
accuracy solutions to non-linear hierarchical least-squares programs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VDNA-PR: Using General <span class="highlight-title">Dataset</span> Representations for Robust Sequential
  Visual Place Recognition <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09025v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09025v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Ramtoula, Daniele De Martini, Matthew Gadd, Paul Newman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper adapts a general dataset representation technique to produce
robust Visual Place Recognition (VPR) descriptors, crucial to enable real-world
mobile robot localisation. Two parallel lines of work on VPR have shown, on one
side, that general-purpose off-the-shelf feature representations can provide
robustness to domain shifts, and, on the other, that fused information from
sequences of images improves performance. In our recent work on measuring
domain gaps between image datasets, we proposed a Visual Distribution of Neuron
Activations (VDNA) representation to represent datasets of images. This
representation can naturally handle image sequences and provides a general and
granular feature representation derived from a general-purpose model. Moreover,
our representation is based on tracking neuron activation values over the list
of images to represent and is not limited to a particular neural network layer,
therefore having access to high- and low-level concepts. This work shows how
VDNAs can be used for VPR by learning a very lightweight and simple encoder to
generate task-specific descriptors. Our experiments show that our
representation can allow for better robustness than current solutions to
serious domain shifts away from the training data distribution, such as to
indoor environments and aerial imagery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Right Place, Right Time! Towards ObjectNav for Non-Stationary Goals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishnu Sashank Dorbala, Bhrij Patel, Amrit Singh Bedi, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel approach to tackle the ObjectNav task for non-stationary
and potentially occluded targets in an indoor environment. We refer to this
task Portable ObjectNav (or P-ObjectNav), and in this work, present its
formulation, feasibility, and a navigation benchmark using a novel
memory-enhanced LLM-based policy. In contrast to ObjNav where target object
locations are fixed for each episode, P-ObjectNav tackles the challenging case
where the target objects move during the episode. This adds a layer of
time-sensitivity to navigation, and is particularly relevant in scenarios where
the agent needs to find portable targets (e.g. misplaced wallets) in
human-centric environments. The agent needs to estimate not just the correct
location of the target, but also the time at which the target is at that
location for visual grounding -- raising the question about the feasibility of
the task. We address this concern by inferring results on two cases for object
placement: one where the objects placed follow a routine or a path, and the
other where they are placed at random. We dynamize Matterport3D for these
experiments, and modify PPO and LLM-based navigation policies for evaluation.
Using PPO, we observe that agent performance in the random case stagnates,
while the agent in the routine-following environment continues to improve,
allowing us to infer that P-ObjectNav is solvable in environments with
routine-following object placement. Using memory-enhancement on an LLM-based
policy, we set a benchmark for P-ObjectNav. Our memory-enhanced agent
significantly outperforms their non-memory-based counterparts across object
placement scenarios by 71.76% and 74.68% on average when measured by Success
Rate (SR) and Success Rate weighted by Path Length (SRPL), showing the
influence of memory on improving P-ObjectNav performance. Our code and dataset
will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DTG : Diffusion-based Trajectory Generation for Mapless Global
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09900v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09900v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Liang, Amirreza Payandeh, Daeun Song, Xuesu Xiao, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel end-to-end diffusion-based trajectory generation method,
DTG, for mapless global navigation in challenging outdoor scenarios with
occlusions and unstructured off-road features like grass, buildings, bushes,
etc. Given a distant goal, our approach computes a trajectory that satisfies
the following goals: (1) minimize the travel distance to the goal; (2) maximize
the traversability by choosing paths that do not lie in undesirable areas.
Specifically, we present a novel Conditional RNN(CRNN) for diffusion models to
efficiently generate trajectories. Furthermore, we propose an adaptive training
method that ensures that the diffusion model generates more traversable
trajectories. We evaluate our methods in various outdoor scenes and compare the
performance with other global navigation algorithms on a Husky robot. In
practice, we observe at least a 15% improvement in traveling distance and
around a 7% improvement in traversability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Inertial Odometry using Focal Plane Binary Features (BIT-VIO) <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09882v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09882v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Lisondra, Junseo Kim, Riku Murai, Kourosh Zareinia, Sajad Saeedi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Focal-Plane Sensor-Processor Arrays (FPSP)s are an emerging technology that
can execute vision algorithms directly on the image sensor. Unlike conventional
cameras, FPSPs perform computation on the image plane -- at individual pixels
-- enabling high frame rate image processing while consuming low power, making
them ideal for mobile robotics. FPSPs, such as the SCAMP-5, use parallel
processing and are based on the Single Instruction Multiple Data (SIMD)
paradigm. In this paper, we present BIT-VIO, the first Visual Inertial Odometry
(VIO) which utilises SCAMP-5.BIT-VIO is a loosely-coupled iterated Extended
Kalman Filter (iEKF) which fuses together the visual odometry running fast at
300 FPS with predictions from 400 Hz IMU measurements to provide accurate and
smooth trajectories.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for Presentation Yokohama, Japan for IEEE 2024 ICRA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Touch-GS: Visual-Tactile Supervised 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09875v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09875v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aiden Swann, Matthew Strong, Won Kyung Do, Gadiel Sznaier Camps, Mac Schwager, Monroe Kennedy III
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose a novel method to supervise 3D Gaussian Splatting
(3DGS) scenes using optical tactile sensors. Optical tactile sensors have
become widespread in their use in robotics for manipulation and object
representation; however, raw optical tactile sensor data is unsuitable to
directly supervise a 3DGS scene. Our representation leverages a Gaussian
Process Implicit Surface to implicitly represent the object, combining many
touches into a unified representation with uncertainty. We merge this model
with a monocular depth estimation network, which is aligned in a two stage
process, coarsely aligning with a depth camera and then finely adjusting to
match our touch data. For every training image, our method produces a
corresponding fused depth and uncertainty map. Utilizing this additional
information, we propose a new loss function, variance weighted depth supervised
loss, for training the 3DGS scene model. We leverage the DenseTact optical
tactile sensor and RealSense RGB-D camera to show that combining touch and
vision in this manner leads to quantitatively and qualitatively better results
than vision or touch alone in a few-view scene syntheses on opaque as well as
on reflective and transparent objects. Please see our project page at
http://armlabstanford.github.io/touch-gs
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safety-Critical Control for Autonomous Systems: Control Barrier
  Functions via Reduced-Order Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09865v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09865v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max H. Cohen, Tamas G. Molnar, Aaron D. Ames
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern autonomous systems, such as flying, legged, and wheeled robots, are
generally characterized by high-dimensional nonlinear dynamics, which presents
challenges for model-based safety-critical control design. Motivated by the
success of reduced-order models in robotics, this paper presents a tutorial on
constructive safety-critical control via reduced-order models and control
barrier functions (CBFs). To this end, we provide a unified formulation of
techniques in the literature that share a common foundation of constructing
CBFs for complex systems from CBFs for much simpler systems. Such ideas are
illustrated through formal results, simple numerical examples, and case studies
of real-world systems to which these techniques have been experimentally
applied.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Annual Reviews in Control</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constrained Passive Interaction Control: Leveraging Passivity and Safety
  for Robot Manipulators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiquan Zhang, Tianyu Li, Nadia Figueroa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Passivity is necessary for robots to fluidly collaborate and interact with
humans physically. Nevertheless, due to the unconstrained nature of
passivity-based impedance control laws, the robot is vulnerable to infeasible
and unsafe configurations upon physical perturbations. In this paper, we
propose a novel control architecture that allows a torque-controlled robot to
guarantee safety constraints such as kinematic limits, self-collisions,
external collisions and singularities and is passive only when feasible. This
is achieved by constraining a dynamical system based impedance control law with
a relaxed hierarchical control barrier function quadratic program subject to
multiple concurrent, possibly contradicting, constraints. Joint space
constraints are formulated from efficient data-driven self- and external C^2
collision boundary functions. We theoretically prove constraint satisfaction
and show that the robot is passive when feasible. Our approach is validated in
simulation and real robot experiments on a 7DoF Franka Research 3 manipulator.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MultiGripperGrasp: A <span class="highlight-title">Dataset</span> for Robotic Grasping from Parallel Jaw
  Grippers to Dexterous Hands 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luis Felipe Casas Murrilo, Ninad Khargonkar, Balakrishnan Prabhakaran, Yu Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a large-scale dataset named MultiGripperGrasp for robotic
grasping. Our dataset contains 30.4M grasps from 11 grippers for 345 objects.
These grippers range from two-finger grippers to five-finger grippers,
including a human hand. All grasps in the dataset are verified in Isaac Sim to
classify them as successful and unsuccessful grasps. Additionally, the object
fall-off time for each grasp is recorded as a grasp quality measurement.
Furthermore, the grippers in our dataset are aligned according to the
orientation and position of their palms, allowing us to transfer grasps from
one gripper to another. The grasp transfer significantly increases the number
of successful grasps for each gripper in the dataset. Our dataset is useful to
study generalized grasp planning and grasp transfer across different grippers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Comprehensive Multimodal Perception: Introducing the
  Touch-Language-Vision <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09813v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09813v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ning Cheng, You Li, Jing Gao, Bin Fang, Jinan Xu, Wenjuan Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tactility provides crucial support and enhancement for the perception and
interaction capabilities of both humans and robots. Nevertheless, the
multimodal research related to touch primarily focuses on visual and tactile
modalities, with limited exploration in the domain of language. Beyond
vocabulary, sentence-level descriptions contain richer semantics. Based on
this, we construct a touch-language-vision dataset named TLV
(Touch-Language-Vision) by human-machine cascade collaboration, featuring
sentence-level descriptions for multimode alignment. The new dataset is used to
fine-tune our proposed lightweight training framework, TLV-Link (Linking Touch,
Language, and Vision through Alignment), achieving effective semantic alignment
with minimal parameter adjustments (1%). Project Page:
https://xiaoen0.github.io/touch.page/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BOP Challenge 2023 on Detection, Segmentation and Pose Estimation of
  Seen and Unseen Rigid Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomas Hodan, Martin Sundermeyer, Yann Labbe, Van Nguyen Nguyen, Gu Wang, Eric Brachmann, Bertram Drost, Vincent Lepetit, Carsten Rother, Jiri Matas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the evaluation methodology, datasets and results of the BOP
Challenge 2023, the fifth in a series of public competitions organized to
capture the state of the art in model-based 6D object pose estimation from an
RGB/RGB-D image and related tasks. Besides the three tasks from 2022
(model-based 2D detection, 2D segmentation, and 6D localization of objects seen
during training), the 2023 challenge introduced new variants of these tasks
focused on objects unseen during training. In the new tasks, methods were
required to learn new objects during a short onboarding stage (max 5 minutes, 1
GPU) from provided 3D object models. The best 2023 method for 6D localization
of unseen objects (GenFlow) notably reached the accuracy of the best 2020
method for seen objects (CosyPose), although being noticeably slower. The best
2023 method for seen objects (GPose) achieved a moderate accuracy improvement
but a significant 43% run-time improvement compared to the best 2022
counterpart (GDRNPP). Since 2017, the accuracy of 6D localization of seen
objects has improved by more than 50% (from 56.9 to 85.6 AR_C). The online
evaluation system stays open and is available at: http://bop.felk.cvut.cz/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2302.13075</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Socially Integrated Navigation: A Social Acting Robot with Deep
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09793v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09793v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Flögel, Lars Fischer, Thomas Rudolf, Tobias Schürmann, Sören Hohmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mobile robots are being used on a large scale in various crowded situations
and become part of our society. The socially acceptable navigation behavior of
a mobile robot with individual human consideration is an essential requirement
for scalable applications and human acceptance. Deep Reinforcement Learning
(DRL) approaches are recently used to learn a robot's navigation policy and to
model the complex interactions between robots and humans. We propose to divide
existing DRL-based navigation approaches based on the robot's exhibited social
behavior and distinguish between social collision avoidance with a lack of
social behavior and socially aware approaches with explicit predefined social
behavior. In addition, we propose a novel socially integrated navigation
approach where the robot's social behavior is adaptive and emerges from the
interaction with humans. The formulation of our approach is derived from a
sociological definition, which states that social acting is oriented toward the
acting of others. The DRL policy is trained in an environment where other
agents interact socially integrated and reward the robot's behavior
individually. The simulation results indicate that the proposed socially
integrated navigation approach outperforms a socially aware approach in terms
of distance traveled, time to completion, and negative impact on all agents
within the environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language-Grounded Dynamic Scene Graphs for Interactive Object Search
  with Mobile Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08605v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08605v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Honerkamp, Martin Büchner, Fabien Despinoy, Tim Welschehold, Abhinav Valada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To fully leverage the capabilities of mobile manipulation robots, it is
imperative that they are able to autonomously execute long-horizon tasks in
large unexplored environments. While large language models (LLMs) have shown
emergent reasoning skills on arbitrary tasks, existing work primarily
concentrates on explored environments, typically focusing on either navigation
or manipulation tasks in isolation. In this work, we propose MoMa-LLM, a novel
approach that grounds language models within structured representations derived
from open-vocabulary scene graphs, dynamically updated as the environment is
explored. We tightly interleave these representations with an object-centric
action space. The resulting approach is zero-shot, open-vocabulary, and readily
extendable to a spectrum of mobile manipulation and household robotic tasks. We
demonstrate the effectiveness of MoMa-LLM in a novel semantic interactive
search task in large realistic indoor environments. In extensive experiments in
both simulation and the real world, we show substantially improved search
efficiency compared to conventional baselines and state-of-the-art approaches,
as well as its applicability to more abstract tasks. We make the code publicly
available at http://moma-llm.cs.uni-freiburg.de.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: http://moma-llm.cs.uni-freiburg.de</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ V-PRISM: Probabilistic Mapping of Unknown Tabletop Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08106v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08106v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Herbert Wright, Weiming Zhi, Matthew Johnson-Roberson, Tucker Hermans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to construct concise scene representations from sensor input is
central to the field of robotics. This paper addresses the problem of robustly
creating a 3D representation of a tabletop scene from a segmented RGB-D image.
These representations are then critical for a range of downstream manipulation
tasks. Many previous attempts to tackle this problem do not capture accurate
uncertainty, which is required to subsequently produce safe motion plans. In
this paper, we cast the representation of 3D tabletop scenes as a multi-class
classification problem. To tackle this, we introduce V-PRISM, a framework and
method for robustly creating probabilistic 3D segmentation maps of tabletop
scenes. Our maps contain both occupancy estimates, segmentation information,
and principled uncertainty measures. We evaluate the robustness of our method
in (1) procedurally generated scenes using open-source object datasets, and (2)
real-world tabletop data collected from a depth camera. Our experiments show
that our approach outperforms alternative continuous reconstruction approaches
that do not explicitly reason about objects in a multi-class formulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RGBGrasp: Image-based Object Grasping by Capturing Multiple Views during
  Robot Arm Movement with Neural Radiance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16592v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16592v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Liu, Kejian Shi, Kaichen Zhou, Haoxiao Wang, Jiyao Zhang, Hao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic research encounters a significant hurdle when it comes to the
intricate task of grasping objects that come in various shapes, materials, and
textures. Unlike many prior investigations that heavily leaned on specialized
point-cloud cameras or abundant RGB visual data to gather 3D insights for
object-grasping missions, this paper introduces a pioneering approach called
RGBGrasp. This method depends on a limited set of RGB views to perceive the 3D
surroundings containing transparent and specular objects and achieve accurate
grasping. Our method utilizes pre-trained depth prediction models to establish
geometry constraints, enabling precise 3D structure estimation, even under
limited view conditions. Finally, we integrate hash encoding and a proposal
sampler strategy to significantly accelerate the 3D reconstruction process.
These innovations significantly enhance the adaptability and effectiveness of
our algorithm in real-world scenarios. Through comprehensive experimental
validations, we demonstrate that RGBGrasp achieves remarkable success across a
wide spectrum of object-grasping scenarios, establishing it as a promising
solution for real-world robotic manipulation tasks. The demonstrations of our
method can be found on: https://sites.google.com/view/rgbgrasp
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spatiotemporal Predictive <span class="highlight-title">Pre-train</span>ing for Robotic Motor Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05304v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05304v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiange Yang, Bei Liu, Jianlong Fu, Bocheng Pan, Gangshan Wu, Limin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic motor control necessitates the ability to predict the dynamics of
environments and interaction objects. However, advanced self-supervised
pre-trained visual representations (PVRs) in robotic motor control, leveraging
large-scale egocentric videos, often focus solely on learning the static
content features of sampled image frames. This neglects the crucial temporal
motion clues in human video data, which implicitly contain key knowledge about
sequential interacting and manipulating with the environments and objects. In
this paper, we present a simple yet effective robotic motor control visual
pre-training framework that jointly performs spatiotemporal predictive learning
utilizing large-scale video data, termed as STP. Our STP samples paired frames
from video clips. It adheres to two key designs in a multi-task learning
manner. First, we perform spatial prediction on the masked current frame for
learning content features. Second, we utilize the future frame with an
extremely high masking ratio as a condition, based on the masked current frame,
to conduct temporal prediction of future frame for capturing motion features.
These efficient designs ensure that our representation focusing on motion
information while capturing spatial details. We carry out the largest-scale
evaluation of PVRs for robotic motor control to date, which encompasses 21
tasks within a real-world Franka robot arm and 5 simulated environments.
Extensive experiments demonstrate the effectiveness of STP as well as unleash
its generality and data efficiency by further post-pre-training and hybrid
pre-training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 6 figures, 11 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Drive<span class="highlight-title">GPT</span>4: Interpretable End-to-end Autonomous Driving via Large
  Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01412v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01412v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenhua Xu, Yujia Zhang, Enze Xie, Zhen Zhao, Yong Guo, Kwan-Yee. K. Wong, Zhenguo Li, Hengshuang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have emerged as a prominent area of
interest within the research community, given their proficiency in handling and
reasoning with non-textual data, including images and videos. This study seeks
to extend the application of MLLMs to the realm of autonomous driving by
introducing DriveGPT4, a novel interpretable end-to-end autonomous driving
system based on LLMs. Capable of processing multi-frame video inputs and
textual queries, DriveGPT4 facilitates the interpretation of vehicle actions,
offers pertinent reasoning, and effectively addresses a diverse range of
questions posed by users. Furthermore, DriveGPT4 predicts low-level vehicle
control signals in an end-to-end fashion. These advanced capabilities are
achieved through the utilization of a bespoke visual instruction tuning
dataset, specifically tailored for autonomous driving applications, in
conjunction with a mix-finetuning training strategy. DriveGPT4 represents the
pioneering effort to leverage LLMs for the development of an interpretable
end-to-end autonomous driving solution. Evaluations conducted on the BDD-X
dataset showcase the superior qualitative and quantitative performance of
DriveGPT4. Additionally, the fine-tuning of domain-specific data enables
DriveGPT4 to yield close or even improved results in terms of autonomous
driving grounding when contrasted with GPT4-V. The code and dataset will be
publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The project page is available at
  https://tonyxuqaq.github.io/projects/DriveGPT4/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TTA-Nav: Test-time Adaptive Reconstruction for Point-Goal Navigation
  under Visual Corruptions <span class="chip">IROS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01977v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01977v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maytus Piriyajitakonkij, Mingfei Sun, Mengmi Zhang, Wei Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot navigation under visual corruption presents a formidable challenge. To
address this, we propose a Test-time Adaptation (TTA) method, named as TTA-Nav,
for point-goal navigation under visual corruptions. Our "plug-and-play" method
incorporates a top-down decoder to a pre-trained navigation model. Firstly, the
pre-trained navigation model gets a corrupted image and extracts features.
Secondly, the top-down decoder produces the reconstruction given the high-level
features extracted by the pre-trained model. Then, it feeds the reconstruction
of a corrupted image back to the pre-trained model. Finally, the pre-trained
model does forward pass again to output action. Despite being trained solely on
clean images, the top-down decoder can reconstruct cleaner images from
corrupted ones without the need for gradient-based adaptation. The pre-trained
navigation model with our top-down decoder significantly enhances navigation
performance across almost all visual corruptions in our benchmarks. Our method
improves the success rate of point-goal navigation from the state-of-the-art
result of 46% to 94% on the most severe corruption. This suggests its potential
for broader application in robotic visual navigation. Project page:
https://sites.google.com/view/tta-nav
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cafe-Mpc: A Cascaded-Fidelity Model Predictive Control Framework with
  Tuning-Free Whole-Body Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03995v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03995v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He Li, Patrick M. Wensing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces an optimization-based locomotion control framework for
on-the-fly synthesis of complex dynamic maneuvers. At the core of the proposed
framework is a cascaded-fidelity model predictive controller (Cafe-Mpc).
Cafe-Mpc strategically relaxes the planning problem along the prediction
horizon (i.e., with descending model fidelity, increasingly coarse time steps,
and relaxed constraints) for computational and performance gains. This problem
is numerically solved with an efficient customized multiple-shooting iLQR
(MS-iLQR) solver that is tailored for hybrid systems. The action-value function
from Cafe-Mpc is then used as the basis for a new value-function-based
whole-body control (VWBC) technique that avoids additional tuning for the WBC.
In this respect, the proposed framework unifies whole-body MPC and more
conventional whole-body quadratic programming (QP), which have been treated as
separate components in previous works. We study the effects of the cascaded
relaxations in Cafe-Mpc on the tracking performance and required computation
time. We also show that the Cafe-Mpc, if configured appropriately, advances the
performance of whole-body MPC without necessarily increasing computational
cost. Further, we show the superior performance of the proposed VWBC over the
Riccati feedback controller in terms of constraint handling. The proposed
framework enables accomplishing for the first time gymnastic-style running
barrel rolls on the MIT Mini Cheetah. Video: https://youtu.be/YiNqrgj9mb8.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE Transactions on Robotics. 20 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Propeller Damage Estimation and Adaptation to Fault Tolerant
  Control: Enhancing Quadrotor Resilience 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13091v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13091v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeffrey Mao, Jennifer Yeom, Suraj Nair, Giuseppe Loianno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aerial robots are required to remain operational even in the event of system
disturbances, damages, or failures to ensure resilient and robust task
completion and safety. One common failure case is propeller damage, which
presents a significant challenge in both quantification and compensation. We
propose a novel adaptive control scheme capable of detecting and compensating
for multi-rotor propeller damages, ensuring safe and robust flight
performances. Our control scheme includes an L1 adaptive controller for damage
inference and compensation of single or dual propellers, with the capability to
seamlessly transition to a fault-tolerant solution in case the damage becomes
severe. We experimentally identify the conditions under which the L1 adaptive
solution remains preferable over a fault-tolerant alternative. Experimental
results validate the proposed approach, demonstrating its effectiveness in
running the adaptive strategy in real time on a quadrotor even in case of
damage to multiple propellers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 Pages, 8 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-Shot Object Goal Visual Navigation With Class-Independent
  Relationship Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09883v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09883v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinting Li, Shiguang Zhang, Yue LU, Kerry Dang, Lingyan Ran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the zero-shot object goal visual navigation problem.
In the object goal visual navigation task, the agent needs to locate navigation
targets from its egocentric visual input. "Zero-shot" means that the target the
agent needs to find is not trained during the training phase. To address the
issue of coupling navigation ability with target features during training, we
propose the Class-Independent Relationship Network (CIRN). This method combines
target detection information with the relative semantic similarity between the
target and the navigation target, and constructs a brand new state
representation based on similarity ranking, this state representation does not
include target feature or environment feature, effectively decoupling the
agent's navigation ability from target features. And a Graph Convolutional
Network (GCN) is employed to learn the relationships between different objects
based on their similarities. During testing, our approach demonstrates strong
generalization capabilities, including zero-shot navigation tasks with
different targets and environments. Through extensive experiments in the
AI2-THOR virtual environment, our method outperforms the current
state-of-the-art approaches in the zero-shot object goal visual navigation
task. Furthermore, we conducted experiments in more challenging cross-target
and cross-scene settings, which further validate the robustness and
generalization ability of our method. Our code is available at:
https://github.com/SmartAndCleverRobot/ICRA-CIRN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Collision-Free Robot Navigation in Crowded Environments using Learning
  based Convex Model Predictive Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01450v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01450v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuanglei Wen, Mingze Dong, Xiai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Navigating robots safely and efficiently in crowded and complex environments
remains a significant challenge. However, due to the dynamic and intricate
nature of these settings, planning efficient and collision-free paths for
robots to track is particularly difficult. In this paper, we uniquely bridge
the robot's perception, decision-making and control processes by utilizing the
convex obstacle-free region computed from 2D LiDAR data. The overall pipeline
is threefold: (1) We proposes a robot navigation framework that utilizes deep
reinforcement learning (DRL), conceptualizing the observation as the convex
obstacle-free region, a departure from general reliance on raw sensor inputs.
(2) We design the action space, derived from the intersection of the robot's
kinematic limits and the convex region, to enable efficient sampling of
inherently collision-free reference points. These actions assists in guiding
the robot to move towards the goal and interact with other obstacles during
navigation. (3) We employ model predictive control (MPC) to track the
trajectory formed by the reference points while satisfying constraints imposed
by the convex obstacle-free region and the robot's kinodynamic limits. The
effectiveness of proposed improvements has been validated through two sets of
ablation studies and a comparative experiment against the Timed Elastic Band
(TEB), demonstrating improved navigation performance in crowded and complex
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DCPT: Darkness Clue-<span class="highlight-title">Prompt</span>ed Tracking in Nighttime UAVs <span class="chip">ICRA2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10491v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10491v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawen Zhu, Huayi Tang, Zhi-Qi Cheng, Jun-Yan He, Bin Luo, Shihao Qiu, Shengming Li, Huchuan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing nighttime unmanned aerial vehicle (UAV) trackers follow an
"Enhance-then-Track" architecture - first using a light enhancer to brighten
the nighttime video, then employing a daytime tracker to locate the object.
This separate enhancement and tracking fails to build an end-to-end trainable
vision system. To address this, we propose a novel architecture called Darkness
Clue-Prompted Tracking (DCPT) that achieves robust UAV tracking at night by
efficiently learning to generate darkness clue prompts. Without a separate
enhancer, DCPT directly encodes anti-dark capabilities into prompts using a
darkness clue prompter (DCP). Specifically, DCP iteratively learns emphasizing
and undermining projections for darkness clues. It then injects these learned
visual prompts into a daytime tracker with fixed parameters across transformer
layers. Moreover, a gated feature aggregation mechanism enables adaptive fusion
between prompts and between prompts and the base model. Extensive experiments
show state-of-the-art performance for DCPT on multiple dark scenario
benchmarks. The unified end-to-end learning of enhancement and tracking in DCPT
enables a more trainable system. The darkness clue prompting efficiently
injects anti-dark knowledge without extra modules. Code is available at
https://github.com/bearyi26/DCPT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICRA2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Local Path Planning among Pushable Objects based on Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02407v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02407v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linghong Yao, Valerio Modugno, Andromachi Maria Delfaki, Yuanchang Liu, Danail Stoyanov, Dimitrios Kanoulas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a method to deal with the problem of robot local
path planning among pushable objects -- an open problem in robotics. In
particular, we achieve that by training multiple agents simultaneously in a
physics-based simulation environment, utilizing an Advantage Actor-Critic
algorithm coupled with a deep neural network. The developed online policy
enables these agents to push obstacles in ways that are not limited to axial
alignments, adapt to unforeseen changes in obstacle dynamics instantaneously,
and effectively tackle local path planning in confined areas. We tested the
method in various simulated environments to prove the adaptation effectiveness
to various unseen scenarios in unfamiliar settings. Moreover, we have
successfully applied this policy on an actual quadruped robot, confirming its
capability to handle the unpredictability and noise associated with real-world
sensors and the inherent uncertainties present in unexplored object pushing
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-robot Motion Planning based on Nets-within-Nets Modeling and
  Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.08772v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.08772v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sofia Hustiu, Eva Robillard, Joaquin Ezpeleta, Cristian Mahulea, Marius Kloetzer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on designing motion plans for a heterogeneous team of
robots that has to cooperate in fulfilling a global mission. The robots move in
an environment containing some regions of interest, and the specification for
the whole team can include avoidances, visits, or sequencing when entering
these regions of interest. The specification is expressed in terms of a Petri
net corresponding to an automaton, while each robot is also modeled by a state
machine Petri net. With respect to existing solutions for related problems, the
current work brings the following contributions. First, we propose a novel
model, denoted {High-Level robot team Petri Net (HLPN) system, for
incorporating the specification and the robot models into the Nets-within-Nets
paradigm. A guard function, named Global Enabling Function (gef), is designed
to synchronize the firing of transitions such that the robot motions do not
violate the specification. Then, the solution is found by simulating the HPLN
system in a specific software tool that accommodates Nets-within-Nets. An
illustrative example based on a Linear Temporal Logic (LTL) mission is
described throughout the paper, complementing the proposed rationale of the
framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>[Note for readers] This paper has been extended from a previous
  submission to 62nd IEEE Conference on Decision and Control, Dec. 13-15, 2023.
  This work has been submitted to the IEEE for possible publication. Copyright
  may be transferred without notice, after which this version may no longer be
  accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Grasp Multiple Objects with One Hand 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15599v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15599v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyang Li, Bo Liu, Yiran Geng, Puhao Li, Yaodong Yang, Yixin Zhu, Tengyu Liu, Siyuan Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The intricate kinematics of the human hand enable simultaneous grasping and
manipulation of multiple objects, essential for tasks such as object transfer
and in-hand manipulation. Despite its significance, the domain of robotic
multi-object grasping is relatively unexplored and presents notable challenges
in kinematics, dynamics, and object configurations. This paper introduces
MultiGrasp, a novel two-stage approach for multi-object grasping using a
dexterous multi-fingered robotic hand on a tabletop. The process consists of
(i) generating pre-grasp proposals and (ii) executing the grasp and lifting the
objects. Our experimental focus is primarily on dual-object grasping, achieving
a success rate of 44.13%, highlighting adaptability to new object
configurations and tolerance for imprecise grasps. Additionally, the framework
demonstrates the potential for grasping more than two objects at the cost of
inference speed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safe and Generalized end-to-end Autonomous Driving System with
  Reinforcement Learning and Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11792v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11792v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zuojin Tang, Xiaoyu Chen, YongQiang Li, Jianyu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An intelligent driving system should be capable of dynamically formulating
appropriate driving strategies based on the current environment and vehicle
status, while ensuring the security and reliability of the system. However,
existing methods based on reinforcement learning and imitation learning suffer
from low safety, poor generalization, and inefficient sampling. Additionally,
they cannot accurately predict future driving trajectories, and the accurate
prediction of future driving trajectories is a precondition for making optimal
decisions. To solve these problems, in this paper, we introduce a Safe and
Generalized end-to-end Autonomous Driving System (SGADS) for complex and
various scenarios. Our SGADS incorporates variational inference with
normalizing flows, enabling the intelligent vehicle to accurately predict
future driving trajectories. Moreover, we propose the formulation of robust
safety constraints. Furthermore, we combine reinforcement learning with
demonstrations to augment search process of the agent. The experimental results
demonstrate that our SGADS can significantly improve safety performance,
exhibit strong generalization, and enhance the training efficiency of
intelligent vehicles in complex urban scenarios compared to existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Predictive Learning: Motion Learning Concept inspired by Cognitive
  Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.14714v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.14714v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kanata Suzuki, Hiroshi Ito, Tatsuro Yamada, Kei Kase, Tetsuya Ogata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bridging the gap between motion models and reality is crucial by using
limited data to deploy robots in the real world. Deep learning is expected to
be generalized to diverse situations while reducing feature design costs
through end-to-end learning for environmental recognition and motion
generation. However, data collection for model training is costly, and time and
human resources are essential for robot trial-and-error with physical contact.
We propose "Deep Predictive Learning," a motion learning concept that predicts
the robot's sensorimotor dynamics, assuming imperfections in the prediction
model. The predictive coding theory inspires this concept to solve the above
problems. It is based on the fundamental strategy of predicting the near-future
sensorimotor states of robots and online minimization of the prediction error
between the real world and the model. Based on the acquired sensor information,
the robot can adjust its behavior in real time, thereby tolerating the
difference between the learning experience and reality. Additionally, the robot
was expected to perform a wide range of tasks by combining the motion dynamics
embedded in the model. This paper describes the proposed concept, its
implementation, and examples of its applications in real robots. The code and
documents are available at: https://ogata-lab.github.io/eipl-docs
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Policy: Visuomotor Policy Learning via Action Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04137v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04137v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, Shuran Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Diffusion Policy, a new way of generating robot
behavior by representing a robot's visuomotor policy as a conditional denoising
diffusion process. We benchmark Diffusion Policy across 12 different tasks from
4 different robot manipulation benchmarks and find that it consistently
outperforms existing state-of-the-art robot learning methods with an average
improvement of 46.9%. Diffusion Policy learns the gradient of the
action-distribution score function and iteratively optimizes with respect to
this gradient field during inference via a series of stochastic Langevin
dynamics steps. We find that the diffusion formulation yields powerful
advantages when used for robot policies, including gracefully handling
multimodal action distributions, being suitable for high-dimensional action
spaces, and exhibiting impressive training stability. To fully unlock the
potential of diffusion models for visuomotor policy learning on physical
robots, this paper presents a set of key technical contributions including the
incorporation of receding horizon control, visual conditioning, and the
time-series diffusion transformer. We hope this work will help motivate a new
generation of policy learning techniques that are able to leverage the powerful
generative modeling capabilities of diffusion models. Code, data, and training
details is publicly available diffusion-policy.cs.columbia.edu
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>An extended journal version of the original RSS2023 paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bipedal Robot Running: Human-like Actuation Timing Using Fast and Slow
  Adaptations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.00910v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.00910v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuke Sakurai, Tomoya Kamimura, Yuki Sakamoto, Shohei Nishii, Kodai Sato, Yuta Fujiwara, Akihito Sano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We have been developing human-sized biped robots based on passive dynamic
mechanisms. In human locomotion, the muscles activate at the same rate relative
to the gait cycle during running. To achieve adaptive running for robots, such
characteristics should be reproduced to yield the desired effect, In this
study, we designed a central pattern generator (CPG) involving fast and slow
adaptation to achieve human-like running using a simple spring-mass model and
our developed bipedal robot, which is equipped with actuators that imitate the
human musculoskeletal system. Our results demonstrate that the CPG-based
controller with fast and slow adaptations, and a adjustable actuator control
timing can reproduce human-like running. The results suggest that the CPG
contributes to the adjustment of the muscle activation timing in human running.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 13 figures, accepted to Advanced Robotics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Universal In-Place Reconfiguration Algorithm for Sliding Cube-Shaped
  Robots in a Quadratic Number of Moves 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/0802.3414v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/0802.3414v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zachary Abel, Hugo A. Akitaya, Scott Duke Kominers, Matias Korman, Frederick Stock
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the modular robot reconfiguration problem, we are given $n$ cube-shaped
modules (or robots) as well as two configurations, i.e., placements of the $n$
modules so that their union is face-connected. The goal is to find a sequence
of moves that reconfigures the modules from one configuration to the other
using "sliding moves," in which a module slides over the face or edge of a
neighboring module, maintaining connectivity of the configuration at all times.
  For many years it has been known that certain module configurations in this
model require at least $\Omega(n^2)$ moves to reconfigure between them. In this
paper, we introduce the first universal reconfiguration algorithm -- i.e., we
show that any $n$-module configuration can reconfigure itself into any
specified $n$-module configuration using just sliding moves. Our algorithm
achieves reconfiguration in $O(n^2)$ moves, making it asymptotically tight. We
also present a variation that reconfigures in-place, it ensures that throughout
the reconfiguration process, all modules, except for one, will be contained in
the union of the bounding boxes of the start and end configuration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributed Coverage Control of Constrained Constant-Speed Unicycle
  Multi-Agent Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.05723v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.05723v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingchen Liu, Zengjie Zhang, Nhan Khanh Le, Jiahu Qin, Fangzhou Liu, Sandra Hirche
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel distributed coverage controller for a multi-agent
system with constant-speed unicycle robots (CSUR). The work is motivated by the
limitation of the conventional method that does not ensure the satisfaction of
hard state- and input-dependent constraints and leads to feasibility issues for
multi-CSUR systems. In this paper, we solve these problems by designing a novel
coverage cost function and a saturated gradient-search-based control law.
Invariant set theory and Lyapunov-based techniques are used to prove the
state-dependent confinement and the convergence of the system state to the
optimal coverage configuration, respectively. The controller is implemented in
a distributed manner based on a novel communication standard among the agents.
A series of simulation case studies are conducted to validate the effectiveness
of the proposed coverage controller in different initial conditions and with
control parameters. A comparison study in simulation reveals the advantage of
the proposed method in terms of avoiding infeasibility. The experiment study
verifies the applicability of the method to real robots with uncertainties. The
development procedure of the method from theoretical analysis to experimental
validation provides a novel framework for multi-agent system coordinate control
with complex agent dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Detect Slip through Tactile Estimation of the Contact Force
  Field and its Entropy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.00935v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.00935v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohai Hu, Aparajit Venkatesh, Guiliang Zheng, Xu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detection of slip during object grasping and manipulation plays a vital role
in object handling. Existing solutions primarily rely on visual information to
devise a strategy for grasping. However, for robotic systems to attain a level
of proficiency comparable to humans, especially in consistently handling and
manipulating unfamiliar objects, integrating artificial tactile sensing is
increasingly essential. We introduce a novel physics-informed, data-driven
approach to detect slip continuously in real time. We employ the GelSight Mini,
an optical tactile sensor, attached to custom-designed grippers to gather
tactile data. Our work leverages the inhomogeneity of tactile sensor readings
during slip events to develop distinctive features and formulates slip
detection as a classification problem. To evaluate our approach, we test
multiple data-driven models on 10 common objects under different loading
conditions, textures, and materials. Our results show that the best
classification algorithm achieves a high average accuracy of 95.61%. We further
illustrate the practical application of our research in dynamic robotic
manipulation tasks, where our real-time slip detection and prevention algorithm
is implemented.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, to be submitted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ C3D: Cascade Control with Change Point Detection and Deep Koopman
  Learning for Autonomous Surface Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05972v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05972v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianwen Li, Hyunsang Park, Wenjian Hao, Lei Xin, Jalil Chavez-Galaviz, Ajinkya Chaudhary, Meredith Bloss, Kyle Pattison, Christopher Vo, Devesh Upadhyay, Shreyas Sundaram, Shaoshuai Mou, Nina Mahmoudian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we discuss the development and deployment of a robust
autonomous system capable of performing various tasks in the maritime domain
under unknown dynamic conditions. We investigate a data-driven approach based
on modular design for ease of transfer of autonomy across different maritime
surface vessel platforms. The data-driven approach alleviates issues related to
a priori identification of system models that may become deficient under
evolving system behaviors or shifting, unanticipated, environmental influences.
Our proposed learning-based platform comprises a deep Koopman system model and
a change point detector that provides guidance on domain shifts prompting
relearning under severe exogenous and endogenous perturbations. Motion control
of the autonomous system is achieved via an optimal controller design. The
Koopman linearized model naturally lends itself to a linear-quadratic regulator
(LQR) control design. We propose the C3D control architecture Cascade Control
with Change Point Detection and Deep Koopman Learning. The framework is
verified in station keeping task on an ASV in both simulation and real
experiments. The approach achieved at least 13.9 percent improvement in mean
distance error in all test cases compared to the methods that do not consider
system changes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-13T00:00:00Z">2024-03-13</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">62</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FastMAC: Stochastic Spectral Sampling of Correspondence Graph <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08770v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08770v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Zhang, Hao Zhao, Hongyang Li, Siheng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D correspondence, i.e., a pair of 3D points, is a fundamental concept in
computer vision. A set of 3D correspondences, when equipped with compatibility
edges, forms a correspondence graph. This graph is a critical component in
several state-of-the-art 3D point cloud registration approaches, e.g., the one
based on maximal cliques (MAC). However, its properties have not been well
understood. So we present the first study that introduces graph signal
processing into the domain of correspondence graph. We exploit the generalized
degree signal on correspondence graph and pursue sampling strategies that
preserve high-frequency components of this signal. To address time-consuming
singular value decomposition in deterministic sampling, we resort to a
stochastic approximate sampling strategy. As such, the core of our method is
the stochastic spectral sampling of correspondence graph. As an application, we
build a complete 3D registration algorithm termed as FastMAC, that reaches
real-time speed while leading to little to none performance drop. Through
extensive experiments, we validate that FastMAC works for both indoor and
outdoor benchmarks. For example, FastMAC can accelerate MAC by 80 times while
maintaining high registration success rate on KITTI. Codes are publicly
available at https://github.com/Forrest-110/FastMAC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024, Code: https://github.com/Forrest-110/FastMAC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time 3D semantic occupancy prediction for autonomous vehicles using
  memory-efficient sparse convolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Sze, Lars Kunze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In autonomous vehicles, understanding the surrounding 3D environment of the
ego vehicle in real-time is essential. A compact way to represent scenes while
encoding geometric distances and semantic object information is via 3D semantic
occupancy maps. State of the art 3D mapping methods leverage transformers with
cross-attention mechanisms to elevate 2D vision-centric camera features into
the 3D domain. However, these methods encounter significant challenges in
real-time applications due to their high computational demands during
inference. This limitation is particularly problematic in autonomous vehicles,
where GPU resources must be shared with other tasks such as localization and
planning. In this paper, we introduce an approach that extracts features from
front-view 2D camera images and LiDAR scans, then employs a sparse convolution
network (Minkowski Engine), for 3D semantic occupancy prediction. Given that
outdoor scenes in autonomous driving scenarios are inherently sparse, the
utilization of sparse convolution is particularly apt. By jointly solving the
problems of 3D scene completion of sparse scenes and 3D semantic segmentation,
we provide a more efficient learning framework suitable for real-time
applications in autonomous vehicles. We also demonstrate competitive accuracy
on the nuScenes dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DIFFTACTILE: A Physics-based Differentiable Tactile Simulator for
  Contact-rich Robotic Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08716v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08716v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zilin Si, Gu Zhang, Qingwei Ben, Branden Romero, Zhou Xian, Chao Liu, Chuang Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce DIFFTACTILE, a physics-based differentiable tactile simulation
system designed to enhance robotic manipulation with dense and physically
accurate tactile feedback. In contrast to prior tactile simulators which
primarily focus on manipulating rigid bodies and often rely on simplified
approximations to model stress and deformations of materials in contact,
DIFFTACTILE emphasizes physics-based contact modeling with high fidelity,
supporting simulations of diverse contact modes and interactions with objects
possessing a wide range of material properties. Our system incorporates several
key components, including a Finite Element Method (FEM)-based soft body model
for simulating the sensing elastomer, a multi-material simulator for modeling
diverse object types (such as elastic, elastoplastic, cables) under
manipulation, a penalty-based contact model for handling contact dynamics. The
differentiable nature of our system facilitates gradient-based optimization for
both 1) refining physical properties in simulation using real-world data, hence
narrowing the sim-to-real gap and 2) efficient learning of tactile-assisted
grasping and contact-rich manipulation skills. Additionally, we introduce a
method to infer the optical response of our tactile sensor to contact using an
efficient pixel-based neural module. We anticipate that DIFFTACTILE will serve
as a useful platform for studying contact-rich manipulations, leveraging the
benefits of dense tactile feedback and differentiable physics. Code and
supplementary materials are available at the project website
https://difftactile.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single file motion of robot swarms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08683v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08683v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laciel Alonso-Llanes, Angel Garcimartín, Iker Zuriguel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present experimental results on the single file motion of a group of
robots interacting with each other through position sensors. We successfully
replicate the fundamental diagram typical of these systems, with a transition
from free flow to congested traffic as the density of the system increases. In
the latter scenario we also observe the characteristic stop-and-go waves. The
unique advantages of this novel system, such as experimental stability and
repeatability, allow for extended experimental runs, facilitating a
comprehensive statistical analysis of the global dynamics. Above a certain
density, we observe a divergence of the average jam duration and the average
number of robots involved in it. This discovery enables us to precisely
identify another transition: from congested intermittent flow (for intermediate
densities) to a totally congested scenario for high densities. Beyond this
finding, the present work demonstrates the suitability of robot swarms to model
complex behaviors in many particle systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures plus supplemental material</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive morphing of wing and tail for stable, resilient, and
  energy-efficient flight of avian-informed drones 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon L. Jeger, Valentin Wüest, Charbel Toumieh, Dario Floreano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Avian-informed drones feature morphing wing and tail surfaces, enhancing
agility and adaptability in flight. Despite their large potential, realising
their full capabilities remains challenging due to the lack of generalized
control strategies accommodating their large degrees of freedom and
cross-coupling effects between their control surfaces. Here we propose a new
body-rate controller for avian-informed drones that uses all available
actuators to control the motion of the drone. The method exhibits robustness
against physical perturbations, turbulent airflow, and even loss of certain
actuators mid-flight. Furthermore, wing and tail morphing is leveraged to
enhance energy efficiency at 8m/s, 10m/s and 12m/s using in-flight Bayesian
optimization. The resulting morphing configurations yield significant gains
across all three speeds of up to 11.5% compared to non-morphing configurations
and display a strong resemblance to avian flight at different speeds. This
research lays the groundwork for the development of autonomous avian-informed
drones that operate under diverse wind conditions, emphasizing the role of
morphing in improving energy efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analytical Forward Dynamics Modeling of Linearly Actuated Heavy-Duty
  Parallel-Serial Manipulators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paz Alvaro, Jouni Mattila
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a new geometric and recursive algorithm for analytically
computing the forward dynamics of heavy-duty parallel-serial mechanisms. Our
solution relies on expressing the dynamics of a class of linearly-actuated
parallel mechanism to a lower dimensional dual Lie algebra to find an
analytical solution for the inverse dynamics problem. Thus, by applying the
articulated-body inertias method, we successfully provide analytic expressions
for the total wrench in the linear-actuator reference frame, the linear
acceleration of the actuator, and the total wrench exerted in the base
reference frame of the closed loop. This new formulation allows to backwardly
project and assemble inertia matrices and wrench bias of multiple closed-loops
mechanisms. The final algorithm holds an O(n) algorithmic complexity, where $n$
is the number of degrees of freedom (DoF). We provide accuracy results to
demonstrate its efficiency with 1-DoF closed-loop mechanism and 4-DoF
manipulator composed by serial and parallel mechanisms. Additionally, we
release a URDF multi-DoF code for this recursive algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint submitted to Mechanism and Machine Theory</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OccFiner: Offboard Occupancy Refinement with Hybrid Propagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Shi, Song Wang, Jiaming Zhang, Xiaoting Yin, Zhongdao Wang, Zhijian Zhao, Guangming Wang, Jianke Zhu, Kailun Yang, Kaiwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-based occupancy prediction, also known as 3D Semantic Scene Completion
(SSC), presents a significant challenge in computer vision. Previous methods,
confined to onboard processing, struggle with simultaneous geometric and
semantic estimation, continuity across varying viewpoints, and single-view
occlusion. Our paper introduces OccFiner, a novel offboard framework designed
to enhance the accuracy of vision-based occupancy predictions. OccFiner
operates in two hybrid phases: 1) a multi-to-multi local propagation network
that implicitly aligns and processes multiple local frames for correcting
onboard model errors and consistently enhancing occupancy accuracy across all
distances. 2) the region-centric global propagation, focuses on refining labels
using explicit multi-view geometry and integrating sensor bias, especially to
increase the accuracy of distant occupied voxels. Extensive experiments
demonstrate that OccFiner improves both geometric and semantic accuracy across
various types of coarse occupancy, setting a new state-of-the-art performance
on the SemanticKITTI dataset. Notably, OccFiner elevates vision-based SSC
models to a level even surpassing that of LiDAR-based onboard SSC models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compliant Hierarchical Control for Arbitrary Equality and Inequality
  Tasks with Strict and Soft Priorities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gianluca Garofalo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When a robotic system is redundant with respect to a given task, the
remaining degrees of freedom can be used to satisfy additional objectives. With
current robotic systems having more and more degrees of freedom, this can lead
to an entire hierarchy of tasks that need to be solved according to given
priorities. In this paper, the first compliant control strategy is presented
that allows to consider an arbitrary number of equality and inequality tasks,
while still preserving the natural inertia of the robot. The approach is
therefore a generalization of a passivity-based controller to the case of an
arbitrary number of equality and inequality tasks. The key idea of the method
is to use a Weighted Hierarchical Quadratic Problem to extract the set of
active tasks and use the latter to perform a coordinate transformation that
inertially decouples the tasks. Thereby unifying the line of research focusing
on optimization-based and passivity-based multi-task controllers. The method is
validated in simulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Dense and Accurate Radar Perception Via Efficient Cross-Modal
  Diffusion Model <span class="chip">IROS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruibin Zhang, Donglai Xue, Yuhan Wang, Ruixu Geng, Fei Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Millimeter wave (mmWave) radars have attracted significant attention from
both academia and industry due to their capability to operate in extreme
weather conditions. However, they face challenges in terms of sparsity and
noise interference, which hinder their application in the field of micro aerial
vehicle (MAV) autonomous navigation. To this end, this paper proposes a novel
approach to dense and accurate mmWave radar point cloud construction via
cross-modal learning. Specifically, we introduce diffusion models, which
possess state-of-the-art performance in generative modeling, to predict
LiDAR-like point clouds from paired raw radar data. We also incorporate the
most recent diffusion model inference accelerating techniques to ensure that
the proposed method can be implemented on MAVs with limited computing
resources.We validate the proposed method through extensive benchmark
comparisons and real-world experiments, demonstrating its superior performance
and generalization ability. Code and pretrained models will be available at
https://github.com/ZJU-FAST-Lab/Radar-Diffusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, submitted to IROS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IAMCV Multi-Scenario Vehicle Interaction <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08455v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08455v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Novel Certad, Enrico del Re, Helena Korndörfer, Gregory Schröder, Walter Morales-Alvarez, Sebastian Tschernuth, Delgermaa Gankhuyag, Luigi del Re, Cristina Olaverri-Monreal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The acquisition and analysis of high-quality sensor data constitute an
essential requirement in shaping the development of fully autonomous driving
systems. This process is indispensable for enhancing road safety and ensuring
the effectiveness of the technological advancements in the automotive industry.
This study introduces the Interaction of Autonomous and Manually-Controlled
Vehicles (IAMCV) dataset, a novel and extensive dataset focused on
inter-vehicle interactions. The dataset, enriched with a sophisticated array of
sensors such as Light Detection and Ranging, cameras, Inertial Measurement
Unit/Global Positioning System, and vehicle bus data acquisition, provides a
comprehensive representation of real-world driving scenarios that include
roundabouts, intersections, country roads, and highways, recorded across
diverse locations in Germany. Furthermore, the study shows the versatility of
the IAMCV dataset through several proof-of-concept use cases. Firstly, an
unsupervised trajectory clustering algorithm illustrates the dataset's
capability in categorizing vehicle movements without the need for labeled
training data. Secondly, we compare an online camera calibration method with
the Robot Operating System-based standard, using images captured in the
dataset. Finally, a preliminary test employing the YOLOv8 object-detection
model is conducted, augmented by reflections on the transferability of object
detection across various LIDAR resolutions. These use cases underscore the
practical utility of the collected dataset, emphasizing its potential to
advance research and innovation in the area of intelligent vehicles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Actor-Critic Physics-informed Neural Lyapunov Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarui Wang, Mahyar Fazlyab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing control policies for stabilization tasks with provable guarantees
is a long-standing problem in nonlinear control. A crucial performance metric
is the size of the resulting region of attraction, which essentially serves as
a robustness "margin" of the closed-loop system against uncertainties. In this
paper, we propose a new method to train a stabilizing neural network controller
along with its corresponding Lyapunov certificate, aiming to maximize the
resulting region of attraction while respecting the actuation constraints.
Crucial to our approach is the use of Zubov's Partial Differential Equation
(PDE), which precisely characterizes the true region of attraction of a given
control policy. Our framework follows an actor-critic pattern where we
alternate between improving the control policy (actor) and learning a Zubov
function (critic). Finally, we compute the largest certifiable region of
attraction by invoking an SMT solver after the training procedure. Our
numerical experiments on several design problems show consistent and
significant improvements in the size of the resulting region of attraction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GRF-based Predictive Flocking Control with Dynamic Pattern Formation <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08434v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08434v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenghao Yu, Dengyu Zhang, Qingrui Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is promising but challenging to design flocking control for a robot swarm
to autonomously follow changing patterns or shapes in a optimal distributed
manner. The optimal flocking control with dynamic pattern formation is,
therefore, investigated in this paper. A predictive flocking control algorithm
is proposed based on a Gibbs random field (GRF), where bio-inspired potential
energies are used to charaterize ``robot-robot'' and ``robot-environment''
interactions. Specialized performance-related energies, e.g., motion
smoothness, are introduced in the proposed design to improve the flocking
behaviors. The optimal control is obtained by maximizing a posterior
distribution of a GRF. A region-based shape control is accomplished for pattern
formation in light of a mean shift technique. The proposed algorithm is
evaluated via the comparison with two state-of-the-art flocking control methods
in an environment with obstacles. Both numerical simulations and real-world
experiments are conducted to demonstrate the efficiency of the proposed design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ APACE: Agile and Perception-Aware Trajectory Generation for Quadrotor
  Flights <span class="chip">ICRA2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Chen, Yichen Zhang, Boyu Zhou, Shaojie Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Various perception-aware planning approaches have attempted to enhance the
state estimation accuracy during maneuvers, while the feature matchability
among frames, a crucial factor influencing estimation accuracy, has often been
overlooked. In this paper, we present APACE, an Agile and Perception-Aware
trajeCtory gEneration framework for quadrotors aggressive flight, that takes
into account feature matchability during trajectory planning. We seek to
generate a perception-aware trajectory that reduces the error of visual-based
estimator while satisfying the constraints on smoothness, safety, agility and
the quadrotor dynamics. The perception objective is achieved by maximizing the
number of covisible features while ensuring small enough parallax angles.
Additionally, we propose a differentiable and accurate visibility model that
allows decomposition of the trajectory planning problem for efficient
optimization resolution. Through validations conducted in both a photorealistic
simulator and real-world experiments, we demonstrate that the trajectories
generated by our method significantly improve state estimation accuracy, with
root mean square error (RMSE) reduced by up to an order of magnitude. The
source code will be released to benefit the community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICRA2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Image-based Pose Regressor Models for Underwater Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luyuan Peng, Hari Vishnu, Mandar Chitre, Yuen Min Too, Bharath Kalyan, Rajat Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the performance of image-based pose regressor models in
underwater environments for relocalization. Leveraging PoseNet and PoseLSTM, we
regress a 6-degree-of-freedom pose from single RGB images with high accuracy.
Additionally, we explore data augmentation with stereo camera images to improve
model accuracy. Experimental results demonstrate that the models achieve high
accuracy in both simulated and clear waters, promising effective real-world
underwater navigation and inspection applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at AUV Symposium 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NaturalVLM: Leveraging Fine-grained Natural Language for
  Affordance-Guided Visual Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08355v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08355v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ran Xu, Yan Shen, Xiaoqi Li, Ruihai Wu, Hao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enabling home-assistant robots to perceive and manipulate a diverse range of
3D objects based on human language instructions is a pivotal challenge. Prior
research has predominantly focused on simplistic and task-oriented
instructions, i.e., "Slide the top drawer open". However, many real-world tasks
demand intricate multi-step reasoning, and without human instructions, these
will become extremely difficult for robot manipulation. To address these
challenges, we introduce a comprehensive benchmark, NrVLM, comprising 15
distinct manipulation tasks, containing over 4500 episodes meticulously
annotated with fine-grained language instructions. We split the long-term task
process into several steps, with each step having a natural language
instruction. Moreover, we propose a novel learning framework that completes the
manipulation task step-by-step according to the fine-grained instructions.
Specifically, we first identify the instruction to execute, taking into account
visual observations and the end-effector's current state. Subsequently, our
approach facilitates explicit learning through action-prompts and
perception-prompts to promote manipulation-aware cross-modality alignment.
Leveraging both visual observations and linguistic guidance, our model outputs
a sequence of actionable predictions for manipulation, including contact points
and end-effector poses. We evaluate our method and baselines using the proposed
benchmark NrVLM. The experimental results demonstrate the effectiveness of our
approach. For additional details, please refer to
https://sites.google.com/view/naturalvlm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MorphoGear: An UAV with Multi-Limb Morphogenetic Gear for Rough-Terrain
  Locomotion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08340v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08340v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikhail Martynov, Zhanibek Darush, Aleksey Fedoseev, Dzmitry Tsetserukou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robots able to run, fly, and grasp have a high potential to solve a wide
scope of tasks and navigate in complex environments. Several mechatronic
designs of such robots with adaptive morphologies are emerging. However, the
task of landing on an uneven surface, traversing rough terrain, and
manipulating objects still presents high challenges.
  This paper introduces the design of a novel rotor UAV MorphoGear with
morphogenetic gear and includes a description of the robot's mechanics,
electronics, and control architecture, as well as walking behavior and an
analysis of experimental results. MorphoGear is able to fly, walk on surfaces
with several gaits, and grasp objects with four compatible robotic limbs.
Robotic limbs with three degrees of freedom (DoFs) are used by this UAV as
pedipulators when walking or flying and as manipulators when performing actions
in the environment. We performed a locomotion analysis of the landing gear of
the robot. Three types of robot gaits have been developed.
  The experimental results revealed low crosstrack error of the most accurate
gait (mean of 1.9 cm and max of 5.5 cm) and the ability of the drone to move
with a 210 mm step length. Another type of robot gait also showed low
crosstrack error (mean of 2.3 cm and max of 6.9 cm). The proposed MorphoGear
system can potentially achieve a high scope of tasks in environmental
surveying, delivery, and high-altitude operations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in: 2023 IEEE/ASME International Conference on Advanced
  Intelligent Mechatronics (AIM)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanxing Lu, Shiyi Zhang, Ziwei Wang, Changliu Liu, Jiwen Lu, Yansong Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Performing language-conditioned robotic manipulation tasks in unstructured
environments is highly demanded for general intelligent robots. Conventional
robotic manipulation methods usually learn semantic representation of the
observation for action prediction, which ignores the scene-level spatiotemporal
dynamics for human goal completion. In this paper, we propose a dynamic
Gaussian Splatting method named ManiGaussian for multi-task robotic
manipulation, which mines scene dynamics via future scene reconstruction.
Specifically, we first formulate the dynamic Gaussian Splatting framework that
infers the semantics propagation in the Gaussian embedding space, where the
semantic representation is leveraged to predict the optimal robot action. Then,
we build a Gaussian world model to parameterize the distribution in our dynamic
Gaussian Splatting framework, which provides informative supervision in the
interactive environment via future scene reconstruction. We evaluate our
ManiGaussian on 10 RLBench tasks with 166 variations, and the results
demonstrate our framework can outperform the state-of-the-art methods by 13.1\%
in average success rate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online Multi-Contact Feedback Model Predictive Control for Interactive
  Robotic Tasks <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seo Wook Han, Maged Iskandar, Jinoh Lee, Min Jun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a model predictive control (MPC) that accomplishes
interactive robotic tasks, in which multiple contacts may occur at unknown
locations. To address such scenarios, we made an explicit contact feedback loop
in the MPC framework. An algorithm called Multi-Contact Particle Filter with
Exploration Particle (MCP-EP) is employed to establish real-time feedback of
multi-contact information. Then the interaction locations and forces are
accommodated in the MPC framework via a spring contact model. Moreover, we
achieved real-time control for a 7 degrees of freedom robot without any
simplifying assumptions by employing a Differential-Dynamic-Programming
algorithm. We achieved 6.8kHz, 1.9kHz, and 1.8kHz update rates of the MPC for
0, 1, and 2 contacts, respectively. This allows the robot to handle unexpected
contacts in real time. Real-world experiments show the effectiveness of the
proposed method in various scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for publication at the IEEE
  International Conference on Robotics and Automation (ICRA), Yokohama, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoPa: General Robotic Manipulation through Spatial Constraints of Parts
  with Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxu Huang, Fanqi Lin, Yingdong Hu, Shengjie Wang, Yang Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models pre-trained on web-scale data are shown to encapsulate
extensive world knowledge beneficial for robotic manipulation in the form of
task planning. However, the actual physical implementation of these plans often
relies on task-specific learning methods, which require significant data
collection and struggle with generalizability. In this work, we introduce
Robotic Manipulation through Spatial Constraints of Parts (CoPa), a novel
framework that leverages the common sense knowledge embedded within foundation
models to generate a sequence of 6-DoF end-effector poses for open-world
robotic manipulation. Specifically, we decompose the manipulation process into
two phases: task-oriented grasping and task-aware motion planning. In the
task-oriented grasping phase, we employ foundation vision-language models
(VLMs) to select the object's grasping part through a novel coarse-to-fine
grounding mechanism. During the task-aware motion planning phase, VLMs are
utilized again to identify the spatial geometry constraints of task-relevant
object parts, which are then used to derive post-grasp poses. We also
demonstrate how CoPa can be seamlessly integrated with existing robotic
planning algorithms to accomplish complex, long-horizon tasks. Our
comprehensive real-world experiments show that CoPa possesses a fine-grained
physical understanding of scenes, capable of handling open-set instructions and
objects with minimal prompt engineering and without additional training.
Project page: https://copa-2024.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continuous Object State Recognition for Cooking Robots Using <span class="highlight-title">Pre-Train</span>ed
  Vision-Language Models and Black-box Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kento Kawaharazuka, Naoaki Kanazawa, Yoshiki Obinata, Kei Okada, Masayuki Inaba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The state recognition of the environment and objects by robots is generally
based on the judgement of the current state as a classification problem. On the
other hand, state changes of food in cooking happen continuously and need to be
captured not only at a certain time point but also continuously over time. In
addition, the state changes of food are complex and cannot be easily described
by manual programming. Therefore, we propose a method to recognize the
continuous state changes of food for cooking robots through the spoken language
using pre-trained large-scale vision-language models. By using models that can
compute the similarity between images and texts continuously over time, we can
capture the state changes of food while cooking. We also show that by adjusting
the weighting of each text prompt based on fitting the similarity changes to a
sigmoid function and then performing black-box optimization, more accurate and
robust continuous state recognition can be achieved. We demonstrate the
effectiveness and limitations of this method by performing the recognition of
water boiling, butter melting, egg cooking, and onion stir-frying.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at IEEE Robotics and Automation Letters (RA-L), website -
  https://haraduka.github.io/continuous-state-recognition/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Feature Learning-based Bio-inspired Neural Network for Real-time
  Collision-free Rescue of Multi-Robot Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junfei Li, Simon X. Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural disasters and urban accidents drive the demand for rescue robots to
provide safer, faster, and more efficient rescue trajectories. In this paper, a
feature learning-based bio-inspired neural network (FLBBINN) is proposed to
quickly generate a heuristic rescue path in complex and dynamic environments,
as traditional approaches usually cannot provide a satisfactory solution to
real-time responses to sudden environmental changes. The neurodynamic model is
incorporated into the feature learning method that can use environmental
information to improve path planning strategies. Task assignment and
collision-free rescue trajectory are generated through robot poses and the
dynamic landscape of neural activity. A dual-channel scale filter, a neural
activity channel, and a secondary distance fusion are employed to extract and
filter feature neurons. After completion of the feature learning process, a
neurodynamics-based feature matrix is established to quickly generate the new
heuristic rescue paths with parameter-driven topological adaptability. The
proposed FLBBINN aims to reduce the computational complexity of the neural
network-based approach and enable the feature learning method to achieve
real-time responses to environmental changes. Several simulations and
experiments have been conducted to evaluate the performance of the proposed
FLBBINN. The results show that the proposed FLBBINN would significantly improve
the speed, efficiency, and optimality for rescue operations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted to publish in IEEE Transactions on Industrial
  Electronics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Object Permanence Filter for Robust Tracking with Interactive Robots <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08231v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08231v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaoting Peng, Margaret X. Wang, Julie A. Shah, Nadia Figueroa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object permanence, which refers to the concept that objects continue to exist
even when they are no longer perceivable through the senses, is a crucial
aspect of human cognitive development. In this work, we seek to incorporate
this understanding into interactive robots by proposing a set of assumptions
and rules to represent object permanence in multi-object, multi-agent
interactive scenarios. We integrate these rules into the particle filter,
resulting in the Object Permanence Filter (OPF). For multi-object scenarios, we
propose an ensemble of K interconnected OPFs, where each filter predicts
plausible object tracks that are resilient to missing, noisy, and kinematically
or dynamically infeasible measurements, thus bringing perceptional robustness.
Through several interactive scenarios, we demonstrate that the proposed OPF
approach provides robust tracking in human-robot interactive tasks agnostic to
measurement type, even in the presence of prolonged and complete occlusion.
Webpage: https://opfilter.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2024 IEEE International Conference on Robotics and Automation (ICRA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Empowering Robotics with Large Language Models: osmAG Map Comprehension
  with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fujing Xie, Sören Schwertfeger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Large Language Models (LLMs) have demonstrated great potential in
robotic applications by providing essential general knowledge for situations
that can not be pre-programmed beforehand. Generally speaking, mobile robots
need to understand maps to execute tasks such as localization or navigation. In
this letter, we address the problem of enabling LLMs to comprehend Area Graph,
a text-based map representation, in order to enhance their applicability in the
field of mobile robotics. Area Graph is a hierarchical, topometric semantic map
representation utilizing polygons to demark areas such as rooms, corridors or
buildings. In contrast to commonly used map representations, such as occupancy
grid maps or point clouds, osmAG (Area Graph in OpensStreetMap format) is
stored in a XML textual format naturally readable by LLMs. Furthermore,
conventional robotic algorithms such as localization and path planning are
compatible with osmAG, facilitating this map representation comprehensible by
LLMs, traditional robotic algorithms and humans. Our experiments show that with
a proper map representation, LLMs possess the capability to understand maps and
answer queries based on that understanding. Following simple fine-tuning of
LLaMA2 models, it surpassed ChatGPT-3.5 in tasks involving topology and
hierarchy understanding. Our dataset, dataset generation code, fine-tuned LoRA
adapters can be accessed at
https://github.com/xiefujing/LLM-osmAG-Comprehension.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpaceOctopus: An Octopus-inspired Motion Planning Framework for
  Multi-arm Space Robot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenbo Zhao, Shengjie Wang, Yixuan Fan, Yang Gao, Tao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Space robots have played a critical role in autonomous maintenance and space
junk removal. Multi-arm space robots can efficiently complete the target
capture and base reorientation tasks due to their flexibility and the
collaborative capabilities between the arms. However, the complex coupling
properties arising from both the multiple arms and the free-floating base
present challenges to the motion planning problems of multi-arm space robots.
We observe that the octopus elegantly achieves similar goals when grabbing prey
and escaping from danger. Inspired by the distributed control of octopuses'
limbs, we develop a multi-level decentralized motion planning framework to
manage the movement of different arms of space robots. This motion planning
framework integrates naturally with the multi-agent reinforcement learning
(MARL) paradigm. The results indicate that our method outperforms the previous
method (centralized training). Leveraging the flexibility of the decentralized
framework, we reassemble policies trained for different tasks, enabling the
space robot to complete trajectory planning tasks while adjusting the base
attitude without further learning. Furthermore, our experiments confirm the
superior robustness of our method in the face of external disturbances,
changing base masses, and even the failure of one arm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LIX: Implicitly Infusing Spatial Geometric Prior Knowledge into Visual
  Semantic Segmentation for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08215v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08215v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sicen Guo, Zhiyuan Wu, Qijun Chen, Ioannis Pitas, Rui Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the impressive performance achieved by data-fusion networks with
duplex encoders for visual semantic segmentation, they become ineffective when
spatial geometric data are not available. Implicitly infusing the spatial
geometric prior knowledge acquired by a duplex-encoder teacher model into a
single-encoder student model is a practical, albeit less explored research
avenue. This paper delves into this topic and resorts to knowledge distillation
approaches to address this problem. We introduce the Learning to Infuse "X"
(LIX) framework, with novel contributions in both logit distillation and
feature distillation aspects. We present a mathematical proof that underscores
the limitation of using a single fixed weight in decoupled knowledge
distillation and introduce a logit-wise dynamic weight controller as a solution
to this issue. Furthermore, we develop an adaptively-recalibrated feature
distillation algorithm, including two technical novelties: feature
recalibration via kernel regression and in-depth feature consistency
quantification via centered kernel alignment. Extensive experiments conducted
with intermediate-fusion and late-fusion networks across various public
datasets provide both quantitative and qualitative evaluations, demonstrating
the superior performance of our LIX framework when compared to other
state-of-the-art approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synchronized Dual-arm Rearrangement via Cooperative mTSP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08191v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08191v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Li, Shishun Zhang, Sisi Dai, Hui Huang, Ruizhen Hu, Xiaohong Chen, Kai Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synchronized dual-arm rearrangement is widely studied as a common scenario in
industrial applications. It often faces scalability challenges due to the
computational complexity of robotic arm rearrangement and the high-dimensional
nature of dual-arm planning. To address these challenges, we formulated the
problem as cooperative mTSP, a variant of mTSP where agents share cooperative
costs, and utilized reinforcement learning for its solution. Our approach
involved representing rearrangement tasks using a task state graph that
captured spatial relationships and a cooperative cost matrix that provided
details about action costs. Taking these representations as observations, we
designed an attention-based network to effectively combine them and provide
rational task scheduling. Furthermore, a cost predictor is also introduced to
directly evaluate actions during both training and planning, significantly
expediting the planning process. Our experimental results demonstrate that our
approach outperforms existing methods in terms of both performance and planning
efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Perceive With Confidence: Statistical Safety Assurances for Navigation
  with Learning-Based Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08185v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08185v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anushri Dixit, Zhiting Mei, Meghan Booker, Mariko Storey-Matsutani, Allen Z. Ren, Anirudha Majumdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapid advances in perception have enabled large pre-trained models to be used
out of the box for processing high-dimensional, noisy, and partial observations
of the world into rich geometric representations (e.g., occupancy predictions).
However, safe integration of these models onto robots remains challenging due
to a lack of reliable performance in unfamiliar environments. In this work, we
present a framework for rigorously quantifying the uncertainty of pre-trained
perception models for occupancy prediction in order to provide end-to-end
statistical safety assurances for navigation. We build on techniques from
conformal prediction for producing a calibrated perception system that lightly
processes the outputs of a pre-trained model while ensuring generalization to
novel environments and robustness to distribution shifts in states when
perceptual outputs are used in conjunction with a planner. The calibrated
system can be used in combination with any safe planner to provide an
end-to-end statistical assurance on safety in a new environment with a
user-specified threshold $1-\epsilon$. We evaluate the resulting approach -
which we refer to as Perceive with Confidence (PwC) - with experiments in
simulation and on hardware where a quadruped robot navigates through indoor
environments containing objects unseen during training or calibration. These
experiments validate the safety assurances provided by PwC and demonstrate
significant improvements in empirical safety rates compared to baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Videos and code can be found at
  https://perceive-with-confidence.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Barrier-Certified Polynomial Dynamical Systems for Obstacle
  Avoidance with Robots <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Schonger, Hugo T. M. Kussaba, Lingyun Chen, Luis Figueredo, Abdalla Swikir, Aude Billard, Sami Haddadin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Established techniques that enable robots to learn from demonstrations are
based on learning a stable dynamical system (DS). To increase the robots'
resilience to perturbations during tasks that involve static obstacle
avoidance, we propose incorporating barrier certificates into an optimization
problem to learn a stable and barrier-certified DS. Such optimization problem
can be very complex or extremely conservative when the traditional linear
parameter-varying formulation is used. Thus, different from previous approaches
in the literature, we propose to use polynomial representations for DSs, which
yields an optimization problem that can be tackled by sum-of-squares
techniques. Finally, our approach can handle obstacle shapes that fall outside
the scope of assumptions typically found in the literature concerning obstacle
avoidance within the DS learning framework. Supplementary material can be found
at the project webpage: https://martinschonger.github.io/abc-ds
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures, accepted to the 2024 IEEE International
  Conference on Robotics and Automation (ICRA 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Direct Algorithm for Multi-Gyroscope Infield Calibration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianheng Wang, Stergios I. Roumeliotis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the problem of estimating the rotational
extrinsics, as well as the scale factors of two gyroscopes rigidly mounted on
the same device. In particular, we formulate the problem as a least-squares
minimization and introduce a direct algorithm that computes the estimated
quantities without any iterations, hence avoiding local minima and improving
efficiency. Furthermore, we show that the rotational extrinsics are observable
while the scale factors can be determined up to global scale for general
configurations of the gyroscopes. To this end, we also study special placements
of the gyroscopes where a pair, or all, of their axes are parallel and analyze
their impact on the scale factors' observability. Lastly, we evaluate our
algorithm in simulations and real-world experiments to assess its performance
as a function of key motion and sensor characteristics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Effective Underwater Glider Path Planning in Dynamic 3D Environments
  Using Multi-Point Potential Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanzhi Yang, Nina Mahmoudian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Underwater gliders (UGs) have emerged as highly effective unmanned vehicles
for ocean exploration. However, their operation in dynamic and complex
underwater environments necessitates robust path-planning strategies. Previous
studies have primarily focused on global energy or time-efficient path planning
in explored environments, overlooking challenges posed by unpredictable flow
conditions and unknown obstacles in varying and dynamic areas like fjords and
near-harbor waters. This paper introduces and improves a real-time path
planning method, Multi-Point Potential Field (MPPF), tailored for UGs operating
in 3D space as they are constrained by buoyancy propulsion and internal
actuation. The proposed MPPF method addresses obstacles, flow fields, and local
minima, enhancing the efficiency and robustness of UG path planning. A low-cost
prototype, the Research Oriented Underwater Glider for Hands-on Investigative
Engineering (ROUGHIE), is utilized for validation. Through case studies and
simulations, the efficacy of the enhanced MPPF method is demonstrated,
highlighting its potential for real-world applications in underwater
exploration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 12 figures, submitted for CAMS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Fidelity Reinforcement Learning for Time-Optimal Quadrotor
  Re-planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gilhyun Ryou, Geoffrey Wang, Sertac Karaman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-speed online trajectory planning for UAVs poses a significant challenge
due to the need for precise modeling of complex dynamics while also being
constrained by computational limitations. This paper presents a multi-fidelity
reinforcement learning method (MFRL) that aims to effectively create a
realistic dynamics model and simultaneously train a planning policy that can be
readily deployed in real-time applications. The proposed method involves the
co-training of a planning policy and a reward estimator; the latter predicts
the performance of the policy's output and is trained efficiently through
multi-fidelity Bayesian optimization. This optimization approach models the
correlation between different fidelity levels, thereby constructing a
high-fidelity model based on a low-fidelity foundation, which enables the
accurate development of the reward model with limited high-fidelity
experiments. The framework is further extended to include real-world flight
experiments in reinforcement learning training, allowing the reward model to
precisely reflect real-world constraints and broadening the policy's
applicability to real-world scenarios. We present rigorous evaluations by
training and testing the planning policy in both simulated and real-world
environments. The resulting trained policy not only generates faster and more
reliable trajectories compared to the baseline snap minimization method, but it
also achieves trajectory updates in 2 ms on average, while the baseline method
takes several minutes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Feasibility of EEG-based Motor Intention Detection for Real-Time
  Robot Assistive Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08149v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08149v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ho Jin Choi, Satyajeet Das, Shaoting Peng, Ruzena Bajcsy, Nadia Figueroa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the feasibility of employing EEG-based intention
detection for real-time robot assistive control. We focus on predicting and
distinguishing motor intentions of left/right arm movements by presenting: i)
an offline data collection and training pipeline, used to train a classifier
for left/right motion intention prediction, and ii) an online real-time
prediction pipeline leveraging the trained classifier and integrated with an
assistive robot. Central to our approach is a rich feature representation
composed of the tangent space projection of time-windowed sample covariance
matrices from EEG filtered signals and derivatives; allowing for a simple SVM
classifier to achieve unprecedented accuracy and real-time performance. In
pre-recorded real-time settings (160 Hz), a peak accuracy of 86.88% is
achieved, surpassing prior works. In robot-in-the-loop settings, our system
successfully detects intended motion solely from EEG data with 70% accuracy,
triggering a robot to execute an assistive task. We provide a comprehensive
evaluation of the proposed classifier.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prosody for Intuitive Robotic Interface Design: It's Not What You Said,
  It's How You Said It 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elaheh Sanoubari, Atil Iscen, Leila Takayama, Stefano Saliceti, Corbin Cunningham, Ken Caluwaerts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate the use of 'prosody' (the musical elements of
speech) as a communicative signal for intuitive human-robot interaction
interfaces. Our approach, rooted in Research through Design (RtD), examines the
application of prosody in directing a quadruped robot navigation. We involved
ten team members in an experiment to command a robot through an obstacle course
using natural interaction. A human operator, serving as the robot's sensory and
processing proxy, translated human communication into a basic set of navigation
commands, effectively simulating an intuitive interface. During our analysis of
interaction videos, when lexical and visual cues proved insufficient for
accurate command interpretation, we turned to non-verbal auditory cues.
Qualitative evidence suggests that participants intuitively relied on prosody
to control robot navigation. We highlight specific distinct prosodic constructs
that emerged from this preliminary exploration and discuss their pragmatic
functions. This work contributes a discussion on the broader potential of
prosody as a multifunctional communicative signal for designing future
intuitive robotic interfaces, enabling lifelong learning and personalization in
human-robot interaction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was accepted at the Lifelong Learning and Personalization
  in Long-Term Human-Robot Interaction (LEAP-HRI) workshop at ACM/IEEE
  International Conference on Human Robot Interaction (HRI) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CART: Caltech Aerial RGB-Thermal <span class="highlight-title">Dataset</span> in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Connor Lee, Matthew Anderson, Nikhil Raganathan, Xingxing Zuo, Kevin Do, Georgia Gkioxari, Soon-Jo Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the first publicly available RGB-thermal dataset designed for
aerial robotics operating in natural environments. Our dataset captures a
variety of terrains across the continental United States, including rivers,
lakes, coastlines, deserts, and forests, and consists of synchronized RGB,
long-wave thermal, global positioning, and inertial data. Furthermore, we
provide semantic segmentation annotations for 10 classes commonly encountered
in natural settings in order to facilitate the development of perception
algorithms robust to adverse weather and nighttime conditions. Using this
dataset, we propose new and challenging benchmarks for thermal and RGB-thermal
semantic segmentation, RGB-to-thermal image translation, and visual-inertial
odometry. We present extensive results using state-of-the-art methods and
highlight the challenges posed by temporal and geographical domain shifts in
our data. Dataset and accompanying code will be provided at
https://github.com/aerorobotics/caltech-aerial-rgbt-dataset
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safe Road-Crossing by Autonomous Wheelchairs: a Novel <span class="highlight-title">Dataset</span> and its
  Experimental Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08984v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08984v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlo Grigioni, Franca Corradini, Alessandro Antonucci, Jérôme Guzzi, Francesco Flammini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safe road-crossing by self-driving vehicles is a crucial problem to address
in smart-cities. In this paper, we introduce a multi-sensor fusion approach to
support road-crossing decisions in a system composed by an autonomous
wheelchair and a flying drone featuring a robust sensory system made of diverse
and redundant components. To that aim, we designed an analytical danger
function based on explainable physical conditions evaluated by single sensors,
including those using machine learning and artificial vision. As a
proof-of-concept, we provide an experimental evaluation in a laboratory
environment, showing the advantages of using multiple sensors, which can
improve decision accuracy and effectively support safety assessment. We made
the dataset available to the scientific community for further experimentation.
The work has been developed in the context of an European project named
REXASI-PRO, which aims to develop trustworthy artificial intelligence for
social navigation of people with reduced mobility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collision-Free Platooning of Mobile Robots through a Set-Theoretic
  Predictive Control Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08942v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08942v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suryaprakash Rajkumar, Cristian Tiriolo, Walter Lucia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a control solution to achieve collision-free platooning
control of input-constrained mobile robots. The platooning policy is based on a
leader-follower approach where the leader tracks a reference trajectory while
followers track the leader's pose with an inter-agent delay. First, the leader
and the follower kinematic models are feedback linearized and the platoon's
error dynamics and input constraints characterized. Then, a set-theoretic model
predictive control strategy is proposed to address the platooning trajectory
tracking control problem. An ad-hoc collision avoidance policy is also proposed
to guarantee collision avoidance amongst the agents. Finally, the effectiveness
of the proposed control architecture is validated through experiments performed
on a formation of Khepera IV differential drive robots
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper submitted for publication in the 2024 American Control
  Conference (ACC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Joint Demonstrations: Personalized Expert Guidance for Efficient
  Multi-Agent Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peihong Yu, Manav Mishra, Alec Koppel, Carl Busart, Priya Narayan, Dinesh Manocha, Amrit Bedi, Pratap Tokekar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Agent Reinforcement Learning (MARL) algorithms face the challenge of
efficient exploration due to the exponential increase in the size of the joint
state-action space. While demonstration-guided learning has proven beneficial
in single-agent settings, its direct applicability to MARL is hindered by the
practical difficulty of obtaining joint expert demonstrations. In this work, we
introduce a novel concept of personalized expert demonstrations, tailored for
each individual agent or, more broadly, each individual type of agent within a
heterogeneous team. These demonstrations solely pertain to single-agent
behaviors and how each agent can achieve personal goals without encompassing
any cooperative elements, thus naively imitating them will not achieve
cooperation due to potential conflicts. To this end, we propose an approach
that selectively utilizes personalized expert demonstrations as guidance and
allows agents to learn to cooperate, namely personalized expert-guided MARL
(PegMARL). This algorithm utilizes two discriminators: the first provides
incentives based on the alignment of policy behavior with demonstrations, and
the second regulates incentives based on whether the behavior leads to the
desired objective. We evaluate PegMARL using personalized demonstrations in
both discrete and continuous environments. The results demonstrate that PegMARL
learns near-optimal policies even when provided with suboptimal demonstrations,
and outperforms state-of-the-art MARL algorithms in solving coordinated tasks.
We also showcase PegMARL's capability to leverage joint demonstrations in the
StarCraft scenario and converge effectively even with demonstrations from
non-co-trained policies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neuromorphic force-control in an industrial task: validating energy and
  latency benefits <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08928v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08928v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Camilo Amaya, Evan Eames, Gintautas Palinauskas, Alexander Perzylo, Yulia Sandamirskaya, Axel von Arnim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As robots become smarter and more ubiquitous, optimizing the power
consumption of intelligent compute becomes imperative towards ensuring the
sustainability of technological advancements. Neuromorphic computing hardware
makes use of biologically inspired neural architectures to achieve energy and
latency improvements compared to conventional von Neumann computing
architecture. Applying these benefits to robots has been demonstrated in
several works in the field of neurorobotics, typically on relatively simple
control tasks. Here, we introduce an example of neuromorphic computing applied
to the real-world industrial task of object insertion. We trained a spiking
neural network (SNN) to perform force-torque feedback control using a
reinforcement learning approach in simulation. We then ported the SNN to the
Intel neuromorphic research chip Loihi interfaced with a KUKA robotic arm. At
inference time we show latency competitive with current CPU/GPU architectures,
two orders of magnitude less energy usage in comparison to traditional
low-energy edge-hardware. We offer this example as a proof of concept
implementation of a neuromoprhic controller in real-world robotic setting,
highlighting the benefits of neuromorphic hardware for the development of
intelligent controllers for robots.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rollover Prevention for Mobile Robots with Control Barrier Functions:
  Differentiator-Based Adaptation and Projection-to-State Safety 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ersin Das, Aaron D. Ames, Joel W. Burdick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper develops rollover prevention guarantees for mobile robots using
control barrier function (CBF) theory, and demonstrates these formal results
experimentally. To this end, we consider a safety measure based on the zero
moment point to provide conditions on the control input through the lens of
CBFs. However, these conditions depend on time-varying and noisy parameters. To
address this, we present a differentiator-based safety-critical controller that
estimates these parameters and pairs Input-to-State Stable (ISS) differentiator
dynamics with CBFs to achieve rigorous guarantees of safety. Additionally, to
ensure safety in the presence of disturbance, we utilize a time-varying
extension of Projection-to-State Safety (PSSf). The effectiveness of the
proposed method is demonstrated through experiments on a tracked robot with a
rollover potential on steep slopes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SLCF-Net: Sequential LiDAR-Camera Fusion for Semantic Scene Completion
  using a 3D Recurrent U-Net <span class="chip">ICRA2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08885v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08885v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Helin Cao, Sven Behnke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SLCF-Net, a novel approach for the Semantic Scene Completion
(SSC) task that sequentially fuses LiDAR and camera data. It jointly estimates
missing geometry and semantics in a scene from sequences of RGB images and
sparse LiDAR measurements. The images are semantically segmented by a
pre-trained 2D U-Net and a dense depth prior is estimated from a
depth-conditioned pipeline fueled by Depth Anything. To associate the 2D image
features with the 3D scene volume, we introduce Gaussian-decay Depth-prior
Projection (GDP). This module projects the 2D features into the 3D volume along
the line of sight with a Gaussian-decay function, centered around the depth
prior. Volumetric semantics is computed by a 3D U-Net. We propagate the hidden
3D U-Net state using the sensor motion and design a novel loss to ensure
temporal consistency. We evaluate our approach on the SemanticKITTI dataset and
compare it with leading SSC approaches. The SLCF-Net excels in all SSC metrics
and shows great temporal consistency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2024 IEEE International Conference on Robotics and Automation
  (ICRA2024), Yokohama, Japan, May 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Continual Learning For Interactive Instruction Following Agents <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07548v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07548v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Byeonghwi Kim, Minhyuk Seo, Jonghyun Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In learning an embodied agent executing daily tasks via language directives,
the literature largely assumes that the agent learns all training data at the
beginning. We argue that such a learning scenario is less realistic since a
robotic agent is supposed to learn the world continuously as it explores and
perceives it. To take a step towards a more realistic embodied agent learning
scenario, we propose two continual learning setups for embodied agents;
learning new behaviors (Behavior Incremental Learning, Behavior-IL) and new
environments (Environment Incremental Learning, Environment-IL) For the tasks,
previous 'data prior' based continual learning methods maintain logits for the
past tasks. However, the stored information is often insufficiently learned
information and requires task boundary information, which might not always be
available. Here, we propose to update them based on confidence scores without
task boundary information during training (i.e., task-free) in a moving average
fashion, named Confidence-Aware Moving Average (CAMA). In the proposed
Behavior-IL and Environment-IL setups, our simple CAMA outperforms prior state
of the art in our empirical validations by noticeable margins. The project page
including codes is https://github.com/snumprlab/cl-alfred.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 (Project page:
  https://bhkim94.github.io/projects/CL-ALFRED)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mind Meets Robots: A <span class="highlight-title">Review</span> of EEG-Based Brain-Robot Interaction Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06186v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06186v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchong Zhang, Nona Rajabi, Farzaneh Taleb, Andrii Matviienko, Yong Ma, Mårten Björkman, Danica Kragic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain-robot interaction (BRI) empowers individuals to control
(semi-)automated machines through their brain activity, either passively or
actively. In the past decade, BRI systems have achieved remarkable success,
predominantly harnessing electroencephalogram (EEG) signals as the central
component. This paper offers an up-to-date and exhaustive examination of 87
curated studies published during the last five years (2018-2023), focusing on
identifying the research landscape of EEG-based BRI systems. This review aims
to consolidate and underscore methodologies, interaction modes, application
contexts, system evaluation, existing challenges, and potential avenues for
future investigations in this domain. Based on our analysis, we present a BRI
system model with three entities: Brain, Robot, and Interaction, depicting the
internal relationships of a BRI system. We especially investigate the essence
and principles on interaction modes between human brains and robots, a domain
that has not yet been identified anywhere. We then discuss these entities with
different dimensions encompassed. Within this model, we scrutinize and classify
current research, reveal insights, specify challenges, and provide
recommendations for future research trajectories in this field. Meanwhile, we
envision our findings offer a design space for future human-robot interaction
(HRI) research, informing the creation of efficient BRI frameworks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Synthesizing Robust Walking Gaits via Discrete-Time Barrier Functions
  with Application to Multi-Contact Exoskeleton Locomotion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06169v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06169v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maegan Tucker, Kejun Li, Aaron D. Ames
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Successfully achieving bipedal locomotion remains challenging due to
real-world factors such as model uncertainty, random disturbances, and
imperfect state estimation. In this work, we propose a novel metric for
locomotive robustness -- the estimated size of the hybrid forward invariant set
associated with the step-to-step dynamics. Here, the forward invariant set can
be loosely interpreted as the region of attraction for the discrete-time
dynamics. We illustrate the use of this metric towards synthesizing nominal
walking gaits using a simulation-in-the-loop learning approach. Further, we
leverage discrete-time barrier functions and a sampling-based approach to
approximate sets that are maximally forward invariant. Lastly, we
experimentally demonstrate that this approach results in successful locomotion
for both flat-foot walking and multi-contact walking on the Atalante lower-body
exoskeleton.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SkillDiffuser: Interpretable Hierarchical Planning via Skill
  Abstractions in Diffusion-Based Task Execution <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11598v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11598v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhixuan Liang, Yao Mu, Hengbo Ma, Masayoshi Tomizuka, Mingyu Ding, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated strong potential for robotic trajectory
planning. However, generating coherent trajectories from high-level
instructions remains challenging, especially for long-range composition tasks
requiring multiple sequential skills. We propose SkillDiffuser, an end-to-end
hierarchical planning framework integrating interpretable skill learning with
conditional diffusion planning to address this problem. At the higher level,
the skill abstraction module learns discrete, human-understandable skill
representations from visual observations and language instructions. These
learned skill embeddings are then used to condition the diffusion model to
generate customized latent trajectories aligned with the skills. This allows
generating diverse state trajectories that adhere to the learnable skills. By
integrating skill learning with conditional trajectory generation,
SkillDiffuser produces coherent behavior following abstract instructions across
diverse tasks. Experiments on multi-task robotic manipulation benchmarks like
Meta-World and LOReL demonstrate state-of-the-art performance and
human-interpretable skill representations from SkillDiffuser. More
visualization results and information could be found on our website.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024. Camera ready version. Project page:
  https://skilldiffuser.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GIRA: Gaussian Mixture Models for Inference and Robot Autonomy <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.00071v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.00071v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kshitij Goel, Wennie Tabib
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the open-source framework, GIRA, which implements
fundamental robotics algorithms for reconstruction, pose estimation, and
occupancy modeling using compact generative models. Compactness enables
perception in the large by ensuring that the perceptual models can be
communicated through low-bandwidth channels during large-scale mobile robot
deployments. The generative property enables perception in the small by
providing high-resolution reconstruction capability. These properties address
perception needs for diverse robotic applications, including multi-robot
exploration and dexterous manipulation. State-of-the-art perception systems
construct perceptual models via multiple disparate pipelines that reuse the
same underlying sensor data, which leads to increased computation, redundancy,
and complexity. GIRA bridges this gap by providing a unified perceptual
modeling framework using Gaussian mixture models (GMMs) as well as a novel
systems contribution, which consists of GPU-accelerated functions to learn GMMs
10-100x faster compared to existing CPU implementations. Because few GMM-based
frameworks are open-sourced, this work seeks to accelerate innovation and
broaden adoption of these techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2024 IEEE International Conference on Robotics and Automation (ICRA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mixed Reality Environment and High-Dimensional Continuification Control
  for Swarm Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01573v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01573v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gian Carlo Maffettone, Lorenzo Liguori, Eduardo Palermo, Mario di Bernardo, Maurizio Porfiri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many new methodologies for the control of large-scale multi-agent systems are
based on macroscopic representations of the emerging systemdynamics, in the
form of continuum approximations of large ensembles. These techniques, that are
typically developed in the limit case of an infinite number of agents, are
usually validated only through numerical simulations. In this paper, we
introduce a mixed reality set-up for testing swarm robotics techniques,
focusing on the macroscopic collective motion of robotic swarms. This hybrid
apparatus combines both real differential drive robots and virtual agents to
create a heterogeneous swarm of tunable size. We also extend
continuification-based control methods for swarms to higher dimensions, and
assess experimentally their validity in the new platform. Our study
demonstrates the effectiveness of the platform for conducting large-scale swarm
robotics experiments, and it contributes new theoretical insights into control
algorithms exploiting continuification approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lowering Detection in Sport Climbing Based on Orientation of the Sensor
  Enhanced Quickdraw 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10164v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10164v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sadaf Moaveninejad, Andrea Janes, Camillo Porcaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tracking climbers' activity to improve services and make the best use of
their infrastructure is a concern for climbing gyms. Each climbing session must
be analyzed from beginning till lowering of the climber. Therefore, spotting
the climbers descending is crucial since it indicates when the ascent has come
to an end. This problem must be addressed while preserving privacy and
convenience of the climbers and the costs of the gyms. To this aim, a hardware
prototype is developed to collect data using accelerometer sensors attached to
a piece of climbing equipment mounted on the wall, called quickdraw, that
connects the climbing rope to the bolt anchors. The corresponding sensors are
configured to be energy-efficient, hence become practical in terms of expenses
and time consumption for replacement when using in large quantity in a climbing
gym. This paper describes hardware specifications, studies data measured by the
sensors in ultra-low power mode, detect sensors' orientation patterns during
lowering different routes, and develop an supervised approach to identify
lowering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2211.02680</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Implicit Swept Volume Models for Fast Collision Detection <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15281v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15281v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Joho, Jonas Schwinn, Kirill Safronov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collision detection is one of the most time-consuming operations during
motion planning. Thus, there is an increasing interest in exploring machine
learning techniques to speed up collision detection and sampling-based motion
planning. A recent line of research focuses on utilizing neural signed distance
functions of either the robot geometry or the swept volume of the robot motion.
Building on this, we present a novel neural implicit swept volume model to
continuously represent arbitrary motions parameterized by their start and goal
configurations. This allows to quickly compute signed distances for any point
in the task space to the robot motion. Further, we present an algorithm
combining the speed of the deep learning-based signed distance computations
with the strong accuracy guarantees of geometric collision checkers. We
validate our approach in simulated and real-world robotic experiments, and
demonstrate that it is able to speed up a commercial bin picking application.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published at ICRA 2024. Dominik Joho and Jonas Schwinn have
  equal contribution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03246v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03246v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingrui Li, Shuhong Liu, Heng Zhou, Guohao Zhu, Na Cheng, Tianchen Deng, Hongyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present SGS-SLAM, the first semantic visual SLAM system based on Gaussian
Splatting. It incorporates appearance, geometry, and semantic features through
multi-channel optimization, addressing the oversmoothing limitations of neural
implicit SLAM systems in high-quality rendering, scene understanding, and
object-level geometry. We introduce a unique semantic feature loss that
effectively compensates for the shortcomings of traditional depth and color
losses in object optimization. Through a semantic-guided keyframe selection
strategy, we prevent erroneous reconstructions caused by cumulative errors.
Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-art
performance in camera pose estimation, map reconstruction, precise semantic
segmentation, and object-level geometric accuracy, while ensuring real-time
rendering capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ASAP-MPC: An Asynchronous Update Scheme for Online Motion Planning with
  Nonlinear Model Predictive Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.06263v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.06263v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dries Dirckx, Mathias Bos, Bastiaan Vandewal, Lander Vanroye, Wilm Decré, Jan Swevers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a Nonlinear Model Predictive Control (NMPC) scheme
targeted at motion planning for mechatronic motion systems, such as drones and
mobile platforms. NMPC-based motion planning typically requires low computation
times to be able to provide control inputs at the required rate for system
stability, disturbance rejection, and overall performance. Although there exist
various ways in literature to reduce the solution times in NMPC, such times may
not be low enough to allow real-time implementations. This paper presents
ASAP-MPC, an approach to handle varying, sometimes restrictively large,
solution times with an asynchronous update scheme, always allowing for full
convergence and real-time execution. The NMPC algorithm is combined with a
linear state feedback controller tracking the optimised trajectories for
improved robustness against possible disturbances and plant-model mismatch.
ASAP-MPC seamlessly merges trajectories, resulting from subsequent NMPC
solutions, providing a smooth and continuous overall trajectory for the motion
system. This frameworks applicability to embedded applications is shown on two
different experiment setups where a state-of-the-art method fails: a quadcopter
flying through a cluttered environment in hardware-in-the-loop simulation and a
scale model truck-trailer manoeuvring in a structured lab environment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PROGrasp: Pragmatic Human-Robot Communication for Object Grasping <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.07759v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.07759v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gi-Cheon Kang, Junghyun Kim, Jaein Kim, Byoung-Tak Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interactive Object Grasping (IOG) is the task of identifying and grasping the
desired object via human-robot natural language interaction. Current IOG
systems assume that a human user initially specifies the target object's
category (e.g., bottle). Inspired by pragmatics, where humans often convey
their intentions by relying on context to achieve goals, we introduce a new IOG
task, Pragmatic-IOG, and the corresponding dataset, Intention-oriented
Multi-modal Dialogue (IM-Dial). In our proposed task scenario, an
intention-oriented utterance (e.g., "I am thirsty") is initially given to the
robot. The robot should then identify the target object by interacting with a
human user. Based on the task setup, we propose a new robotic system that can
interpret the user's intention and pick up the target object, Pragmatic Object
Grasping (PROGrasp). PROGrasp performs Pragmatic-IOG by incorporating modules
for visual grounding, question asking, object grasping, and most importantly,
answer interpretation for pragmatic inference. Experimental results show that
PROGrasp is effective in offline (i.e., target object discovery) and online
(i.e., IOG with a physical robot arm) settings. Code and data are available at
https://github.com/gicheonkang/prograsp.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Compliant Robotic Leg Based on Fibre Jamming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.01758v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.01758v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lois Liow, James Brett, Josh Pinskier, Lauren Hanson, Louis Tidswell, Navinda Kottege, David Howard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans possess a remarkable ability to react to unpredictable perturbations
through immediate mechanical responses, which harness the visco-elastic
properties of muscles to maintain balance. Inspired by this behaviour, we
propose a novel design of a robotic leg utilising fibre jammed structures as
passive compliant mechanisms to achieve variable joint stiffness and damping.
We developed multi-material fibre jammed tendons with tunable mechanical
properties, which can be 3D printed in one-go without need for assembly.
Through extensive numerical simulations and experimentation, we demonstrate the
usefulness of these tendons for shock absorbance and maintaining joint
stability. We investigate how they could be used effectively in a multi-joint
robotic leg by evaluating the relative contribution of each tendon to the
overall stiffness of the leg. Further, we showcase the potential of these
jammed structures for legged locomotion, highlighting how morphological
properties of the tendons can be used to enhance stability in robotic legs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 15 figures, IEEE Transactions on Robotics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Motion Memory: Leveraging Past Experiences to Accelerate Future Motion
  Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06198v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06198v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dibyendu Das, Yuanjie Lu, Erion Plaku, Xuesu Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When facing a new motion-planning problem, most motion planners solve it from
scratch, e.g., via sampling and exploration or starting optimization from a
straight-line path. However, most motion planners have to experience a variety
of planning problems throughout their lifetimes, which are yet to be leveraged
for future planning. In this paper, we present a simple but efficient method
called Motion Memory, which allows different motion planners to accelerate
future planning using past experiences. Treating existing motion planners as
either a closed or open box, we present a variety of ways that Motion Memory
can contribute to reduce the planning time when facing a new planning problem.
We provide extensive experiment results with three different motion planners on
three classes of planning problems with over 30,000 problem instances and show
that planning speed can be significantly reduced by up to 89% with the proposed
Motion Memory technique and with increasing past planning experiences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interactive Navigation in Environments with Traversable Obstacles Using
  Large Language and Vision-Language Models <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08873v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08873v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Zhang, Anran Lin, Chun Wai Wong, Xiangyu Chu, Qi Dou, K. W. Samuel Au
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes an interactive navigation framework by using large
language and vision-language models, allowing robots to navigate in
environments with traversable obstacles. We utilize the large language model
(GPT-3.5) and the open-set Vision-language Model (Grounding DINO) to create an
action-aware costmap to perform effective path planning without fine-tuning.
With the large models, we can achieve an end-to-end system from textual
instructions like "Can you pass through the curtains to deliver medicines to
me?", to bounding boxes (e.g., curtains) with action-aware attributes. They can
be used to segment LiDAR point clouds into two parts: traversable and
untraversable parts, and then an action-aware costmap is constructed for
generating a feasible path. The pre-trained large models have great
generalization ability and do not require additional annotated data for
training, allowing fast deployment in the interactive navigation tasks. We
choose to use multiple traversable objects such as curtains and grasses for
verification by instructing the robot to traverse them. Besides, traversing
curtains in a medical scenario was tested. All experimental results
demonstrated the proposed framework's effectiveness and adaptability to diverse
environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2024 IEEE International Conference on Robotics and
  Automation (ICRA), 7 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Level Compositional Reasoning for Interactive Instruction
  Following <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.09387v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.09387v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suvaansh Bhambri, Byeonghwi Kim, Jonghyun Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic agents performing domestic chores by natural language directives are
required to master the complex job of navigating environment and interacting
with objects in the environments. The tasks given to the agents are often
composite thus are challenging as completing them require to reason about
multiple subtasks, e.g., bring a cup of coffee. To address the challenge, we
propose to divide and conquer it by breaking the task into multiple subgoals
and attend to them individually for better navigation and interaction. We call
it Multi-level Compositional Reasoning Agent (MCR-Agent). Specifically, we
learn a three-level action policy. At the highest level, we infer a sequence of
human-interpretable subgoals to be executed based on language instructions by a
high-level policy composition controller. At the middle level, we
discriminatively control the agent's navigation by a master policy by
alternating between a navigation policy and various independent interaction
policies. Finally, at the lowest level, we infer manipulation actions with the
corresponding object masks using the appropriate interaction policy. Our
approach not only generates human interpretable subgoals but also achieves
2.03% absolute gain to comparable state of the arts in the efficiency metric
(PLWSR in unseen set) without using rule-based planning or a semantic spatial
memory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2023 (Oral) (Project page:
  https://bhkim94.github.io/projects/MCR-Agent)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Context-Aware Planning and Environment-Aware Memory for Instruction
  Following Embodied Agents <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.07241v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.07241v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Byeonghwi Kim, Jinyeon Kim, Yuyeong Kim, Cheolhong Min, Jonghyun Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accomplishing household tasks requires to plan step-by-step actions
considering the consequences of previous actions. However, the state-of-the-art
embodied agents often make mistakes in navigating the environment and
interacting with proper objects due to imperfect learning by imitating experts
or algorithmic planners without such knowledge. To improve both visual
navigation and object interaction, we propose to consider the consequence of
taken actions by CAPEAM (Context-Aware Planning and Environment-Aware Memory)
that incorporates semantic context (e.g., appropriate objects to interact with)
in a sequence of actions, and the changed spatial arrangement and states of
interacted objects (e.g., location that the object has been moved to) in
inferring the subsequent actions. We empirically show that the agent with the
proposed CAPEAM achieves state-of-the-art performance in various metrics using
a challenging interactive instruction following benchmark in both seen and
unseen environments by large margins (up to +10.70% in unseen env.).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2023 (Project page: https://bhkim94.github.io/projects/CAPEAM)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generating and Explaining Corner Cases Using Learnt Probabilistic Lane
  Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.13658v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.13658v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enrik Maci, Rhys Howard, Lars Kunze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Validating the safety of Autonomous Vehicles (AVs) operating in open-ended,
dynamic environments is challenging as vehicles will eventually encounter
safety-critical situations for which there is not representative training data.
By increasing the coverage of different road and traffic conditions and by
including corner cases in simulation-based scenario testing, the safety of AVs
can be improved. However, the creation of corner case scenarios including
multiple agents is non-trivial. Our approach allows engineers to generate
novel, realistic corner cases based on historic traffic data and to explain why
situations were safety-critical. In this paper, we introduce Probabilistic Lane
Graphs (PLGs) to describe a finite set of lane positions and directions in
which vehicles might travel. The structure of PLGs is learnt directly from
spatio-temporal traffic data. The graph model represents the actions of the
drivers in response to a given state in the form of a probabilistic policy. We
use reinforcement learning techniques to modify this policy and to generate
realistic and explainable corner case scenarios which can be used for assessing
the safety of AVs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 Pages, 3 Figures, 1 Table, Published in the Proceedings of the 26th
  IEEE International Conference on Intelligent Transport Systems (2023), Final
  submission version with added IEEE copyright notice</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Primal-Dual iLQR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00748v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00748v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        João Sousa-Pinto, Dominique Orban
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new algorithm for solving unconstrained discrete-time optimal
control problems. Our method follows a direct multiple shooting approach, and
consists of applying the SQP method together with an $\ell_2$ augmented
Lagrangian primal-dual merit function. We use the LQR algorithm to efficiently
solve the primal component of the Newton-KKT system, and use a dual LQR
backward pass to solve its dual component. We also present a new parallel
algorithm for solving the dual component of the Newton-KKT system in
$O(\log(N))$ parallel time, where $N$ is the number of stages. Combining it
with (S\"{a}rkk\"{a} and Garc\'{i}a-Fern\'{a}ndez, 2023), we are able to solve
the full Newton-KKT system in $O(\log(N))$ parallel time. The remaining parts
of our method have constant parallel time complexity per iteration. Therefore,
this paper provides, for the first time, a practical, highly parallelizable
(for example, with a GPU) method for solving nonlinear discrete-time optimal
control problems. As our algorithm is a specialization of NPSQP (Gill et al.
1992), it inherits its generic properties, including global convergence, fast
local convergence, and the lack of need for second order corrections or
dimension expansions, improving on existing direct multiple shooting approaches
such as acados (Verschueren et al. 2022), ALTRO (Howell et al. 2019), GNMS
(Giftthaler et al. 2018), FATROP (Vanroye et al. 2023), and FDDP (Mastalli et
al. 2020).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 1 figure, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OccFusion: A Straightforward and Effective Multi-Sensor Fusion Framework
  for 3D Occupancy Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01644v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01644v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenxing Ming, Julie Stephany Berrio, Mao Shan, Stewart Worrall
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces OccFusion, a straightforward and efficient sensor
fusion framework for predicting 3D occupancy. A comprehensive understanding of
3D scenes is crucial in autonomous driving, and recent models for 3D semantic
occupancy prediction have successfully addressed the challenge of describing
real-world objects with varied shapes and classes. However, existing methods
for 3D occupancy prediction heavily rely on surround-view camera images, making
them susceptible to changes in lighting and weather conditions. By integrating
features from additional sensors, such as lidar and surround view radars, our
framework enhances the accuracy and robustness of occupancy prediction,
resulting in top-tier performance on the nuScenes benchmark. Furthermore,
extensive experiments conducted on the nuScenes dataset, including challenging
night and rainy scenarios, confirm the superior performance of our sensor
fusion strategy across various perception ranges. The code for this framework
will be made available at https://github.com/DanielMing123/OCCFusion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RTS-GT: Robotic Total Stations Ground Truthing <span class="highlight-title">dataset</span> <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11935v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11935v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxime Vaidis, Mohsen Hassanzadeh Shahraji, Effie Daum, William Dubois, Philippe Giguère, François Pomerleau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerous datasets and benchmarks exist to assess and compare Simultaneous
Localization and Mapping (SLAM) algorithms. Nevertheless, their precision must
follow the rate at which SLAM algorithms improved in recent years. Moreover,
current datasets fall short of comprehensive data-collection protocol for
reproducibility and the evaluation of the precision or accuracy of the recorded
trajectories. With this objective in mind, we proposed the Robotic Total
Stations Ground Truthing dataset (RTS-GT) dataset to support localization
research with the generation of six-Degrees Of Freedom (DOF) ground truth
trajectories. This novel dataset includes six-DOF ground truth trajectories
generated using a system of three Robotic Total Stations (RTSs) tracking moving
robotic platforms. Furthermore, we compare the performance of the RTS-based
system to a Global Navigation Satellite System (GNSS)-based setup. The dataset
comprises around sixty experiments conducted in various conditions over a
period of 17 months, and encompasses over 49 kilometers of trajectories, making
it the most extensive dataset of RTS-based measurements to date. Additionally,
we provide the precision of all poses for each experiment, a feature not found
in the current state-of-the-art datasets. Our results demonstrate that RTSs
provide measurements that are 22 times more stable than GNSS in various
environmental settings, making them a valuable resource for SLAM benchmark
development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages; Accepted to ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Collision-Resilient Passive Deformable Quadrotors for Exploration,
  Mapping and Swift Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17217v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17217v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karishma Patnaik, Aravind Adhith Pandian Saravanakumaran, Wenlong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article, we introduce XPLORER, a passive deformable quadrotor
optimized for performing contact-rich tasks by utilizing collision-induced
deformation. We present a novel external force estimation technique, and
advanced planning and control algorithms that exploit the compliant nature of
XPLORER's chassis. These algorithms enable three distinct flight behaviors:
static-wrench application, where XPLORER can exert desired forces and torque on
surfaces for precise manipulation; disturbance rejection, wherein the quadrotor
actively mitigates external forces and yaw disturbances to maintain its
intended trajectory; and yielding to disturbance, enabling XPLORER to
dynamically adapt its position and orientation to evade undesired forces,
ensuring stable flight amidst unpredictable environmental factors. Leveraging
these behaviors, we develop innovative mission strategies including
tactile-traversal, tactile-turning, and collide-to-brake for contact-based
exploration of unknown areas, contact-based mapping and swift navigation.
Through experimental validation, we demonstrate the effectiveness of these
strategies in enabling efficient exploration and rapid navigation in unknown
environments, leveraging collisions as a means for feedback and control. This
study contributes to the growing field of aerial robotics by showcasing the
potential of passive deformable quadrotors for versatile and robust interaction
tasks in real-world scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Constrained Bimanual Planning with Analytic Inverse Kinematics <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08770v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08770v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Cohn, Seiji Shaw, Max Simchowitz, Russ Tedrake
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order for a bimanual robot to manipulate an object that is held by both
hands, it must construct motion plans such that the transformation between its
end effectors remains fixed. This amounts to complicated nonlinear equality
constraints in the configuration space, which are difficult for trajectory
optimizers. In addition, the set of feasible configurations becomes a measure
zero set, which presents a challenge to sampling-based motion planners. We
leverage an analytic solution to the inverse kinematics problem to parametrize
the configuration space, resulting in a lower-dimensional representation where
the set of valid configurations has positive measure. We describe how to use
this parametrization with existing motion planning algorithms, including
sampling-based approaches, trajectory optimizers, and techniques that plan
through convex inner-approximations of collision-free space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICRA 2024. 8 pages, 4 figures. Interactive results
  available at https://cohnt.github.io/Bimanual-Web/index.html</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-03-28T00:48:48.313776462Z">
            2024-03-28 00:48:48 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
