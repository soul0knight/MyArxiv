{"2024-03-13T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2403.07548v2","updated":"2024-03-13T02:31:47Z","published":"2024-03-12T11:33:48Z","title":"Online Continual Learning For Interactive Instruction Following Agents","summary":"  In learning an embodied agent executing daily tasks via language directives,\nthe literature largely assumes that the agent learns all training data at the\nbeginning. We argue that such a learning scenario is less realistic since a\nrobotic agent is supposed to learn the world continuously as it explores and\nperceives it. To take a step towards a more realistic embodied agent learning\nscenario, we propose two continual learning setups for embodied agents;\nlearning new behaviors (Behavior Incremental Learning, Behavior-IL) and new\nenvironments (Environment Incremental Learning, Environment-IL) For the tasks,\nprevious 'data prior' based continual learning methods maintain logits for the\npast tasks. However, the stored information is often insufficiently learned\ninformation and requires task boundary information, which might not always be\navailable. Here, we propose to update them based on confidence scores without\ntask boundary information during training (i.e., task-free) in a moving average\nfashion, named Confidence-Aware Moving Average (CAMA). In the proposed\nBehavior-IL and Environment-IL setups, our simple CAMA outperforms prior state\nof the art in our empirical validations by noticeable margins. The project page\nincluding codes is https://github.com/snumprlab/cl-alfred.\n","authors":["Byeonghwi Kim","Minhyuk Seo","Jonghyun Choi"],"pdf_url":"https://arxiv.org/pdf/2403.07548v2.pdf","comment":"ICLR 2024 (Project page:\n  https://bhkim94.github.io/projects/CL-ALFRED)"},{"id":"http://arxiv.org/abs/2403.06186v2","updated":"2024-03-13T09:54:29Z","published":"2024-03-10T12:06:45Z","title":"Mind Meets Robots: A Review of EEG-Based Brain-Robot Interaction Systems","summary":"  Brain-robot interaction (BRI) empowers individuals to control\n(semi-)automated machines through their brain activity, either passively or\nactively. In the past decade, BRI systems have achieved remarkable success,\npredominantly harnessing electroencephalogram (EEG) signals as the central\ncomponent. This paper offers an up-to-date and exhaustive examination of 87\ncurated studies published during the last five years (2018-2023), focusing on\nidentifying the research landscape of EEG-based BRI systems. This review aims\nto consolidate and underscore methodologies, interaction modes, application\ncontexts, system evaluation, existing challenges, and potential avenues for\nfuture investigations in this domain. Based on our analysis, we present a BRI\nsystem model with three entities: Brain, Robot, and Interaction, depicting the\ninternal relationships of a BRI system. We especially investigate the essence\nand principles on interaction modes between human brains and robots, a domain\nthat has not yet been identified anywhere. We then discuss these entities with\ndifferent dimensions encompassed. Within this model, we scrutinize and classify\ncurrent research, reveal insights, specify challenges, and provide\nrecommendations for future research trajectories in this field. Meanwhile, we\nenvision our findings offer a design space for future human-robot interaction\n(HRI) research, informing the creation of efficient BRI frameworks.\n","authors":["Yuchong Zhang","Nona Rajabi","Farzaneh Taleb","Andrii Matviienko","Yong Ma","Mårten Björkman","Danica Kragic"],"pdf_url":"https://arxiv.org/pdf/2403.06186v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08770v1","updated":"2024-03-13T17:59:56Z","published":"2024-03-13T17:59:56Z","title":"FastMAC: Stochastic Spectral Sampling of Correspondence Graph","summary":"  3D correspondence, i.e., a pair of 3D points, is a fundamental concept in\ncomputer vision. A set of 3D correspondences, when equipped with compatibility\nedges, forms a correspondence graph. This graph is a critical component in\nseveral state-of-the-art 3D point cloud registration approaches, e.g., the one\nbased on maximal cliques (MAC). However, its properties have not been well\nunderstood. So we present the first study that introduces graph signal\nprocessing into the domain of correspondence graph. We exploit the generalized\ndegree signal on correspondence graph and pursue sampling strategies that\npreserve high-frequency components of this signal. To address time-consuming\nsingular value decomposition in deterministic sampling, we resort to a\nstochastic approximate sampling strategy. As such, the core of our method is\nthe stochastic spectral sampling of correspondence graph. As an application, we\nbuild a complete 3D registration algorithm termed as FastMAC, that reaches\nreal-time speed while leading to little to none performance drop. Through\nextensive experiments, we validate that FastMAC works for both indoor and\noutdoor benchmarks. For example, FastMAC can accelerate MAC by 80 times while\nmaintaining high registration success rate on KITTI. Codes are publicly\navailable at https://github.com/Forrest-110/FastMAC.\n","authors":["Yifei Zhang","Hao Zhao","Hongyang Li","Siheng Chen"],"pdf_url":"https://arxiv.org/pdf/2403.08770v1.pdf","comment":"CVPR 2024, Code: https://github.com/Forrest-110/FastMAC"},{"id":"http://arxiv.org/abs/2403.08748v1","updated":"2024-03-13T17:50:59Z","published":"2024-03-13T17:50:59Z","title":"Real-time 3D semantic occupancy prediction for autonomous vehicles using\n  memory-efficient sparse convolution","summary":"  In autonomous vehicles, understanding the surrounding 3D environment of the\nego vehicle in real-time is essential. A compact way to represent scenes while\nencoding geometric distances and semantic object information is via 3D semantic\noccupancy maps. State of the art 3D mapping methods leverage transformers with\ncross-attention mechanisms to elevate 2D vision-centric camera features into\nthe 3D domain. However, these methods encounter significant challenges in\nreal-time applications due to their high computational demands during\ninference. This limitation is particularly problematic in autonomous vehicles,\nwhere GPU resources must be shared with other tasks such as localization and\nplanning. In this paper, we introduce an approach that extracts features from\nfront-view 2D camera images and LiDAR scans, then employs a sparse convolution\nnetwork (Minkowski Engine), for 3D semantic occupancy prediction. Given that\noutdoor scenes in autonomous driving scenarios are inherently sparse, the\nutilization of sparse convolution is particularly apt. By jointly solving the\nproblems of 3D scene completion of sparse scenes and 3D semantic segmentation,\nwe provide a more efficient learning framework suitable for real-time\napplications in autonomous vehicles. We also demonstrate competitive accuracy\non the nuScenes dataset.\n","authors":["Samuel Sze","Lars Kunze"],"pdf_url":"https://arxiv.org/pdf/2403.08748v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.08716v1","updated":"2024-03-13T17:18:19Z","published":"2024-03-13T17:18:19Z","title":"DIFFTACTILE: A Physics-based Differentiable Tactile Simulator for\n  Contact-rich Robotic Manipulation","summary":"  We introduce DIFFTACTILE, a physics-based differentiable tactile simulation\nsystem designed to enhance robotic manipulation with dense and physically\naccurate tactile feedback. In contrast to prior tactile simulators which\nprimarily focus on manipulating rigid bodies and often rely on simplified\napproximations to model stress and deformations of materials in contact,\nDIFFTACTILE emphasizes physics-based contact modeling with high fidelity,\nsupporting simulations of diverse contact modes and interactions with objects\npossessing a wide range of material properties. Our system incorporates several\nkey components, including a Finite Element Method (FEM)-based soft body model\nfor simulating the sensing elastomer, a multi-material simulator for modeling\ndiverse object types (such as elastic, elastoplastic, cables) under\nmanipulation, a penalty-based contact model for handling contact dynamics. The\ndifferentiable nature of our system facilitates gradient-based optimization for\nboth 1) refining physical properties in simulation using real-world data, hence\nnarrowing the sim-to-real gap and 2) efficient learning of tactile-assisted\ngrasping and contact-rich manipulation skills. Additionally, we introduce a\nmethod to infer the optical response of our tactile sensor to contact using an\nefficient pixel-based neural module. We anticipate that DIFFTACTILE will serve\nas a useful platform for studying contact-rich manipulations, leveraging the\nbenefits of dense tactile feedback and differentiable physics. Code and\nsupplementary materials are available at the project website\nhttps://difftactile.github.io/.\n","authors":["Zilin Si","Gu Zhang","Qingwei Ben","Branden Romero","Zhou Xian","Chao Liu","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2403.08716v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06169v2","updated":"2024-03-13T16:56:37Z","published":"2023-10-09T21:47:15Z","title":"Synthesizing Robust Walking Gaits via Discrete-Time Barrier Functions\n  with Application to Multi-Contact Exoskeleton Locomotion","summary":"  Successfully achieving bipedal locomotion remains challenging due to\nreal-world factors such as model uncertainty, random disturbances, and\nimperfect state estimation. In this work, we propose a novel metric for\nlocomotive robustness -- the estimated size of the hybrid forward invariant set\nassociated with the step-to-step dynamics. Here, the forward invariant set can\nbe loosely interpreted as the region of attraction for the discrete-time\ndynamics. We illustrate the use of this metric towards synthesizing nominal\nwalking gaits using a simulation-in-the-loop learning approach. Further, we\nleverage discrete-time barrier functions and a sampling-based approach to\napproximate sets that are maximally forward invariant. Lastly, we\nexperimentally demonstrate that this approach results in successful locomotion\nfor both flat-foot walking and multi-contact walking on the Atalante lower-body\nexoskeleton.\n","authors":["Maegan Tucker","Kejun Li","Aaron D. Ames"],"pdf_url":"https://arxiv.org/pdf/2310.06169v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08683v1","updated":"2024-03-13T16:39:32Z","published":"2024-03-13T16:39:32Z","title":"Single file motion of robot swarms","summary":"  We present experimental results on the single file motion of a group of\nrobots interacting with each other through position sensors. We successfully\nreplicate the fundamental diagram typical of these systems, with a transition\nfrom free flow to congested traffic as the density of the system increases. In\nthe latter scenario we also observe the characteristic stop-and-go waves. The\nunique advantages of this novel system, such as experimental stability and\nrepeatability, allow for extended experimental runs, facilitating a\ncomprehensive statistical analysis of the global dynamics. Above a certain\ndensity, we observe a divergence of the average jam duration and the average\nnumber of robots involved in it. This discovery enables us to precisely\nidentify another transition: from congested intermittent flow (for intermediate\ndensities) to a totally congested scenario for high densities. Beyond this\nfinding, the present work demonstrates the suitability of robot swarms to model\ncomplex behaviors in many particle systems.\n","authors":["Laciel Alonso-Llanes","Angel Garcimartín","Iker Zuriguel"],"pdf_url":"https://arxiv.org/pdf/2403.08683v1.pdf","comment":"5 pages, 4 figures plus supplemental material"},{"id":"http://arxiv.org/abs/2312.11598v2","updated":"2024-03-13T16:29:50Z","published":"2023-12-18T18:16:52Z","title":"SkillDiffuser: Interpretable Hierarchical Planning via Skill\n  Abstractions in Diffusion-Based Task Execution","summary":"  Diffusion models have demonstrated strong potential for robotic trajectory\nplanning. However, generating coherent trajectories from high-level\ninstructions remains challenging, especially for long-range composition tasks\nrequiring multiple sequential skills. We propose SkillDiffuser, an end-to-end\nhierarchical planning framework integrating interpretable skill learning with\nconditional diffusion planning to address this problem. At the higher level,\nthe skill abstraction module learns discrete, human-understandable skill\nrepresentations from visual observations and language instructions. These\nlearned skill embeddings are then used to condition the diffusion model to\ngenerate customized latent trajectories aligned with the skills. This allows\ngenerating diverse state trajectories that adhere to the learnable skills. By\nintegrating skill learning with conditional trajectory generation,\nSkillDiffuser produces coherent behavior following abstract instructions across\ndiverse tasks. Experiments on multi-task robotic manipulation benchmarks like\nMeta-World and LOReL demonstrate state-of-the-art performance and\nhuman-interpretable skill representations from SkillDiffuser. More\nvisualization results and information could be found on our website.\n","authors":["Zhixuan Liang","Yao Mu","Hengbo Ma","Masayoshi Tomizuka","Mingyu Ding","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2312.11598v2.pdf","comment":"Accepted by CVPR 2024. Camera ready version. Project page:\n  https://skilldiffuser.github.io/"},{"id":"http://arxiv.org/abs/2403.08598v1","updated":"2024-03-13T15:03:50Z","published":"2024-03-13T15:03:50Z","title":"Adaptive morphing of wing and tail for stable, resilient, and\n  energy-efficient flight of avian-informed drones","summary":"  Avian-informed drones feature morphing wing and tail surfaces, enhancing\nagility and adaptability in flight. Despite their large potential, realising\ntheir full capabilities remains challenging due to the lack of generalized\ncontrol strategies accommodating their large degrees of freedom and\ncross-coupling effects between their control surfaces. Here we propose a new\nbody-rate controller for avian-informed drones that uses all available\nactuators to control the motion of the drone. The method exhibits robustness\nagainst physical perturbations, turbulent airflow, and even loss of certain\nactuators mid-flight. Furthermore, wing and tail morphing is leveraged to\nenhance energy efficiency at 8m/s, 10m/s and 12m/s using in-flight Bayesian\noptimization. The resulting morphing configurations yield significant gains\nacross all three speeds of up to 11.5% compared to non-morphing configurations\nand display a strong resemblance to avian flight at different speeds. This\nresearch lays the groundwork for the development of autonomous avian-informed\ndrones that operate under diverse wind conditions, emphasizing the role of\nmorphing in improving energy efficiency.\n","authors":["Simon L. Jeger","Valentin Wüest","Charbel Toumieh","Dario Floreano"],"pdf_url":"https://arxiv.org/pdf/2403.08598v1.pdf","comment":"22 pages, 9 figures"},{"id":"http://arxiv.org/abs/2307.00071v3","updated":"2024-03-13T14:03:11Z","published":"2023-06-30T18:21:48Z","title":"GIRA: Gaussian Mixture Models for Inference and Robot Autonomy","summary":"  This paper introduces the open-source framework, GIRA, which implements\nfundamental robotics algorithms for reconstruction, pose estimation, and\noccupancy modeling using compact generative models. Compactness enables\nperception in the large by ensuring that the perceptual models can be\ncommunicated through low-bandwidth channels during large-scale mobile robot\ndeployments. The generative property enables perception in the small by\nproviding high-resolution reconstruction capability. These properties address\nperception needs for diverse robotic applications, including multi-robot\nexploration and dexterous manipulation. State-of-the-art perception systems\nconstruct perceptual models via multiple disparate pipelines that reuse the\nsame underlying sensor data, which leads to increased computation, redundancy,\nand complexity. GIRA bridges this gap by providing a unified perceptual\nmodeling framework using Gaussian mixture models (GMMs) as well as a novel\nsystems contribution, which consists of GPU-accelerated functions to learn GMMs\n10-100x faster compared to existing CPU implementations. Because few GMM-based\nframeworks are open-sourced, this work seeks to accelerate innovation and\nbroaden adoption of these techniques.\n","authors":["Kshitij Goel","Wennie Tabib"],"pdf_url":"https://arxiv.org/pdf/2307.00071v3.pdf","comment":"2024 IEEE International Conference on Robotics and Automation (ICRA)"},{"id":"http://arxiv.org/abs/2310.01573v3","updated":"2024-03-13T13:36:15Z","published":"2023-10-02T19:05:24Z","title":"Mixed Reality Environment and High-Dimensional Continuification Control\n  for Swarm Robotics","summary":"  Many new methodologies for the control of large-scale multi-agent systems are\nbased on macroscopic representations of the emerging systemdynamics, in the\nform of continuum approximations of large ensembles. These techniques, that are\ntypically developed in the limit case of an infinite number of agents, are\nusually validated only through numerical simulations. In this paper, we\nintroduce a mixed reality set-up for testing swarm robotics techniques,\nfocusing on the macroscopic collective motion of robotic swarms. This hybrid\napparatus combines both real differential drive robots and virtual agents to\ncreate a heterogeneous swarm of tunable size. We also extend\ncontinuification-based control methods for swarms to higher dimensions, and\nassess experimentally their validity in the new platform. Our study\ndemonstrates the effectiveness of the platform for conducting large-scale swarm\nrobotics experiments, and it contributes new theoretical insights into control\nalgorithms exploiting continuification approaches.\n","authors":["Gian Carlo Maffettone","Lorenzo Liguori","Eduardo Palermo","Mario di Bernardo","Maurizio Porfiri"],"pdf_url":"https://arxiv.org/pdf/2310.01573v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08524v1","updated":"2024-03-13T13:32:32Z","published":"2024-03-13T13:32:32Z","title":"Analytical Forward Dynamics Modeling of Linearly Actuated Heavy-Duty\n  Parallel-Serial Manipulators","summary":"  This paper presents a new geometric and recursive algorithm for analytically\ncomputing the forward dynamics of heavy-duty parallel-serial mechanisms. Our\nsolution relies on expressing the dynamics of a class of linearly-actuated\nparallel mechanism to a lower dimensional dual Lie algebra to find an\nanalytical solution for the inverse dynamics problem. Thus, by applying the\narticulated-body inertias method, we successfully provide analytic expressions\nfor the total wrench in the linear-actuator reference frame, the linear\nacceleration of the actuator, and the total wrench exerted in the base\nreference frame of the closed loop. This new formulation allows to backwardly\nproject and assemble inertia matrices and wrench bias of multiple closed-loops\nmechanisms. The final algorithm holds an O(n) algorithmic complexity, where $n$\nis the number of degrees of freedom (DoF). We provide accuracy results to\ndemonstrate its efficiency with 1-DoF closed-loop mechanism and 4-DoF\nmanipulator composed by serial and parallel mechanisms. Additionally, we\nrelease a URDF multi-DoF code for this recursive algorithm.\n","authors":["Paz Alvaro","Jouni Mattila"],"pdf_url":"https://arxiv.org/pdf/2403.08524v1.pdf","comment":"Preprint submitted to Mechanism and Machine Theory"},{"id":"http://arxiv.org/abs/2403.08504v1","updated":"2024-03-13T13:12:42Z","published":"2024-03-13T13:12:42Z","title":"OccFiner: Offboard Occupancy Refinement with Hybrid Propagation","summary":"  Vision-based occupancy prediction, also known as 3D Semantic Scene Completion\n(SSC), presents a significant challenge in computer vision. Previous methods,\nconfined to onboard processing, struggle with simultaneous geometric and\nsemantic estimation, continuity across varying viewpoints, and single-view\nocclusion. Our paper introduces OccFiner, a novel offboard framework designed\nto enhance the accuracy of vision-based occupancy predictions. OccFiner\noperates in two hybrid phases: 1) a multi-to-multi local propagation network\nthat implicitly aligns and processes multiple local frames for correcting\nonboard model errors and consistently enhancing occupancy accuracy across all\ndistances. 2) the region-centric global propagation, focuses on refining labels\nusing explicit multi-view geometry and integrating sensor bias, especially to\nincrease the accuracy of distant occupied voxels. Extensive experiments\ndemonstrate that OccFiner improves both geometric and semantic accuracy across\nvarious types of coarse occupancy, setting a new state-of-the-art performance\non the SemanticKITTI dataset. Notably, OccFiner elevates vision-based SSC\nmodels to a level even surpassing that of LiDAR-based onboard SSC models.\n","authors":["Hao Shi","Song Wang","Jiaming Zhang","Xiaoting Yin","Zhongdao Wang","Zhijian Zhao","Guangming Wang","Jianke Zhu","Kailun Yang","Kaiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2403.08504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08491v1","updated":"2024-03-13T12:54:09Z","published":"2024-03-13T12:54:09Z","title":"Compliant Hierarchical Control for Arbitrary Equality and Inequality\n  Tasks with Strict and Soft Priorities","summary":"  When a robotic system is redundant with respect to a given task, the\nremaining degrees of freedom can be used to satisfy additional objectives. With\ncurrent robotic systems having more and more degrees of freedom, this can lead\nto an entire hierarchy of tasks that need to be solved according to given\npriorities. In this paper, the first compliant control strategy is presented\nthat allows to consider an arbitrary number of equality and inequality tasks,\nwhile still preserving the natural inertia of the robot. The approach is\ntherefore a generalization of a passivity-based controller to the case of an\narbitrary number of equality and inequality tasks. The key idea of the method\nis to use a Weighted Hierarchical Quadratic Problem to extract the set of\nactive tasks and use the latter to perform a coordinate transformation that\ninertially decouples the tasks. Thereby unifying the line of research focusing\non optimization-based and passivity-based multi-task controllers. The method is\nvalidated in simulation.\n","authors":["Gianluca Garofalo"],"pdf_url":"https://arxiv.org/pdf/2403.08491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08460v1","updated":"2024-03-13T12:20:20Z","published":"2024-03-13T12:20:20Z","title":"Towards Dense and Accurate Radar Perception Via Efficient Cross-Modal\n  Diffusion Model","summary":"  Millimeter wave (mmWave) radars have attracted significant attention from\nboth academia and industry due to their capability to operate in extreme\nweather conditions. However, they face challenges in terms of sparsity and\nnoise interference, which hinder their application in the field of micro aerial\nvehicle (MAV) autonomous navigation. To this end, this paper proposes a novel\napproach to dense and accurate mmWave radar point cloud construction via\ncross-modal learning. Specifically, we introduce diffusion models, which\npossess state-of-the-art performance in generative modeling, to predict\nLiDAR-like point clouds from paired raw radar data. We also incorporate the\nmost recent diffusion model inference accelerating techniques to ensure that\nthe proposed method can be implemented on MAVs with limited computing\nresources.We validate the proposed method through extensive benchmark\ncomparisons and real-world experiments, demonstrating its superior performance\nand generalization ability. Code and pretrained models will be available at\nhttps://github.com/ZJU-FAST-Lab/Radar-Diffusion.\n","authors":["Ruibin Zhang","Donglai Xue","Yuhan Wang","Ruixu Geng","Fei Gao"],"pdf_url":"https://arxiv.org/pdf/2403.08460v1.pdf","comment":"8 pages, 6 figures, submitted to IROS2024"},{"id":"http://arxiv.org/abs/2403.08455v1","updated":"2024-03-13T12:09:44Z","published":"2024-03-13T12:09:44Z","title":"IAMCV Multi-Scenario Vehicle Interaction Dataset","summary":"  The acquisition and analysis of high-quality sensor data constitute an\nessential requirement in shaping the development of fully autonomous driving\nsystems. This process is indispensable for enhancing road safety and ensuring\nthe effectiveness of the technological advancements in the automotive industry.\nThis study introduces the Interaction of Autonomous and Manually-Controlled\nVehicles (IAMCV) dataset, a novel and extensive dataset focused on\ninter-vehicle interactions. The dataset, enriched with a sophisticated array of\nsensors such as Light Detection and Ranging, cameras, Inertial Measurement\nUnit/Global Positioning System, and vehicle bus data acquisition, provides a\ncomprehensive representation of real-world driving scenarios that include\nroundabouts, intersections, country roads, and highways, recorded across\ndiverse locations in Germany. Furthermore, the study shows the versatility of\nthe IAMCV dataset through several proof-of-concept use cases. Firstly, an\nunsupervised trajectory clustering algorithm illustrates the dataset's\ncapability in categorizing vehicle movements without the need for labeled\ntraining data. Secondly, we compare an online camera calibration method with\nthe Robot Operating System-based standard, using images captured in the\ndataset. Finally, a preliminary test employing the YOLOv8 object-detection\nmodel is conducted, augmented by reflections on the transferability of object\ndetection across various LIDAR resolutions. These use cases underscore the\npractical utility of the collected dataset, emphasizing its potential to\nadvance research and innovation in the area of intelligent vehicles.\n","authors":["Novel Certad","Enrico del Re","Helena Korndörfer","Gregory Schröder","Walter Morales-Alvarez","Sebastian Tschernuth","Delgermaa Gankhuyag","Luigi del Re","Cristina Olaverri-Monreal"],"pdf_url":"https://arxiv.org/pdf/2403.08455v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08448v1","updated":"2024-03-13T12:03:27Z","published":"2024-03-13T12:03:27Z","title":"Actor-Critic Physics-informed Neural Lyapunov Control","summary":"  Designing control policies for stabilization tasks with provable guarantees\nis a long-standing problem in nonlinear control. A crucial performance metric\nis the size of the resulting region of attraction, which essentially serves as\na robustness \"margin\" of the closed-loop system against uncertainties. In this\npaper, we propose a new method to train a stabilizing neural network controller\nalong with its corresponding Lyapunov certificate, aiming to maximize the\nresulting region of attraction while respecting the actuation constraints.\nCrucial to our approach is the use of Zubov's Partial Differential Equation\n(PDE), which precisely characterizes the true region of attraction of a given\ncontrol policy. Our framework follows an actor-critic pattern where we\nalternate between improving the control policy (actor) and learning a Zubov\nfunction (critic). Finally, we compute the largest certifiable region of\nattraction by invoking an SMT solver after the training procedure. Our\nnumerical experiments on several design problems show consistent and\nsignificant improvements in the size of the resulting region of attraction.\n","authors":["Jiarui Wang","Mahyar Fazlyab"],"pdf_url":"https://arxiv.org/pdf/2403.08448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.10164v2","updated":"2024-03-13T11:43:22Z","published":"2023-01-17T15:06:33Z","title":"Lowering Detection in Sport Climbing Based on Orientation of the Sensor\n  Enhanced Quickdraw","summary":"  Tracking climbers' activity to improve services and make the best use of\ntheir infrastructure is a concern for climbing gyms. Each climbing session must\nbe analyzed from beginning till lowering of the climber. Therefore, spotting\nthe climbers descending is crucial since it indicates when the ascent has come\nto an end. This problem must be addressed while preserving privacy and\nconvenience of the climbers and the costs of the gyms. To this aim, a hardware\nprototype is developed to collect data using accelerometer sensors attached to\na piece of climbing equipment mounted on the wall, called quickdraw, that\nconnects the climbing rope to the bolt anchors. The corresponding sensors are\nconfigured to be energy-efficient, hence become practical in terms of expenses\nand time consumption for replacement when using in large quantity in a climbing\ngym. This paper describes hardware specifications, studies data measured by the\nsensors in ultra-low power mode, detect sensors' orientation patterns during\nlowering different routes, and develop an supervised approach to identify\nlowering.\n","authors":["Sadaf Moaveninejad","Andrea Janes","Camillo Porcaro"],"pdf_url":"https://arxiv.org/pdf/2301.10164v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2211.02680"},{"id":"http://arxiv.org/abs/2403.08434v1","updated":"2024-03-13T11:35:02Z","published":"2024-03-13T11:35:02Z","title":"GRF-based Predictive Flocking Control with Dynamic Pattern Formation","summary":"  It is promising but challenging to design flocking control for a robot swarm\nto autonomously follow changing patterns or shapes in a optimal distributed\nmanner. The optimal flocking control with dynamic pattern formation is,\ntherefore, investigated in this paper. A predictive flocking control algorithm\nis proposed based on a Gibbs random field (GRF), where bio-inspired potential\nenergies are used to charaterize ``robot-robot'' and ``robot-environment''\ninteractions. Specialized performance-related energies, e.g., motion\nsmoothness, are introduced in the proposed design to improve the flocking\nbehaviors. The optimal control is obtained by maximizing a posterior\ndistribution of a GRF. A region-based shape control is accomplished for pattern\nformation in light of a mean shift technique. The proposed algorithm is\nevaluated via the comparison with two state-of-the-art flocking control methods\nin an environment with obstacles. Both numerical simulations and real-world\nexperiments are conducted to demonstrate the efficiency of the proposed design.\n","authors":["Chenghao Yu","Dengyu Zhang","Qingrui Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08434v1.pdf","comment":"Accepted by ICRA 2024"},{"id":"http://arxiv.org/abs/2403.08365v1","updated":"2024-03-13T09:25:44Z","published":"2024-03-13T09:25:44Z","title":"APACE: Agile and Perception-Aware Trajectory Generation for Quadrotor\n  Flights","summary":"  Various perception-aware planning approaches have attempted to enhance the\nstate estimation accuracy during maneuvers, while the feature matchability\namong frames, a crucial factor influencing estimation accuracy, has often been\noverlooked. In this paper, we present APACE, an Agile and Perception-Aware\ntrajeCtory gEneration framework for quadrotors aggressive flight, that takes\ninto account feature matchability during trajectory planning. We seek to\ngenerate a perception-aware trajectory that reduces the error of visual-based\nestimator while satisfying the constraints on smoothness, safety, agility and\nthe quadrotor dynamics. The perception objective is achieved by maximizing the\nnumber of covisible features while ensuring small enough parallax angles.\nAdditionally, we propose a differentiable and accurate visibility model that\nallows decomposition of the trajectory planning problem for efficient\noptimization resolution. Through validations conducted in both a photorealistic\nsimulator and real-world experiments, we demonstrate that the trajectories\ngenerated by our method significantly improve state estimation accuracy, with\nroot mean square error (RMSE) reduced by up to an order of magnitude. The\nsource code will be released to benefit the community.\n","authors":["Xinyi Chen","Yichen Zhang","Boyu Zhou","Shaojie Shen"],"pdf_url":"https://arxiv.org/pdf/2403.08365v1.pdf","comment":"Accepted by ICRA2024"},{"id":"http://arxiv.org/abs/2403.08360v1","updated":"2024-03-13T09:20:43Z","published":"2024-03-13T09:20:43Z","title":"Improved Image-based Pose Regressor Models for Underwater Environments","summary":"  We investigate the performance of image-based pose regressor models in\nunderwater environments for relocalization. Leveraging PoseNet and PoseLSTM, we\nregress a 6-degree-of-freedom pose from single RGB images with high accuracy.\nAdditionally, we explore data augmentation with stereo camera images to improve\nmodel accuracy. Experimental results demonstrate that the models achieve high\naccuracy in both simulated and clear waters, promising effective real-world\nunderwater navigation and inspection applications.\n","authors":["Luyuan Peng","Hari Vishnu","Mandar Chitre","Yuen Min Too","Bharath Kalyan","Rajat Mishra"],"pdf_url":"https://arxiv.org/pdf/2403.08360v1.pdf","comment":"Presented at AUV Symposium 2022"},{"id":"http://arxiv.org/abs/2403.08355v1","updated":"2024-03-13T09:12:16Z","published":"2024-03-13T09:12:16Z","title":"NaturalVLM: Leveraging Fine-grained Natural Language for\n  Affordance-Guided Visual Manipulation","summary":"  Enabling home-assistant robots to perceive and manipulate a diverse range of\n3D objects based on human language instructions is a pivotal challenge. Prior\nresearch has predominantly focused on simplistic and task-oriented\ninstructions, i.e., \"Slide the top drawer open\". However, many real-world tasks\ndemand intricate multi-step reasoning, and without human instructions, these\nwill become extremely difficult for robot manipulation. To address these\nchallenges, we introduce a comprehensive benchmark, NrVLM, comprising 15\ndistinct manipulation tasks, containing over 4500 episodes meticulously\nannotated with fine-grained language instructions. We split the long-term task\nprocess into several steps, with each step having a natural language\ninstruction. Moreover, we propose a novel learning framework that completes the\nmanipulation task step-by-step according to the fine-grained instructions.\nSpecifically, we first identify the instruction to execute, taking into account\nvisual observations and the end-effector's current state. Subsequently, our\napproach facilitates explicit learning through action-prompts and\nperception-prompts to promote manipulation-aware cross-modality alignment.\nLeveraging both visual observations and linguistic guidance, our model outputs\na sequence of actionable predictions for manipulation, including contact points\nand end-effector poses. We evaluate our method and baselines using the proposed\nbenchmark NrVLM. The experimental results demonstrate the effectiveness of our\napproach. For additional details, please refer to\nhttps://sites.google.com/view/naturalvlm.\n","authors":["Ran Xu","Yan Shen","Xiaoqi Li","Ruihai Wu","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2403.08355v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08340v1","updated":"2024-03-13T08:43:06Z","published":"2024-03-13T08:43:06Z","title":"MorphoGear: An UAV with Multi-Limb Morphogenetic Gear for Rough-Terrain\n  Locomotion","summary":"  Robots able to run, fly, and grasp have a high potential to solve a wide\nscope of tasks and navigate in complex environments. Several mechatronic\ndesigns of such robots with adaptive morphologies are emerging. However, the\ntask of landing on an uneven surface, traversing rough terrain, and\nmanipulating objects still presents high challenges.\n  This paper introduces the design of a novel rotor UAV MorphoGear with\nmorphogenetic gear and includes a description of the robot's mechanics,\nelectronics, and control architecture, as well as walking behavior and an\nanalysis of experimental results. MorphoGear is able to fly, walk on surfaces\nwith several gaits, and grasp objects with four compatible robotic limbs.\nRobotic limbs with three degrees of freedom (DoFs) are used by this UAV as\npedipulators when walking or flying and as manipulators when performing actions\nin the environment. We performed a locomotion analysis of the landing gear of\nthe robot. Three types of robot gaits have been developed.\n  The experimental results revealed low crosstrack error of the most accurate\ngait (mean of 1.9 cm and max of 5.5 cm) and the ability of the drone to move\nwith a 210 mm step length. Another type of robot gait also showed low\ncrosstrack error (mean of 2.3 cm and max of 6.9 cm). The proposed MorphoGear\nsystem can potentially achieve a high scope of tasks in environmental\nsurveying, delivery, and high-altitude operations.\n","authors":["Mikhail Martynov","Zhanibek Darush","Aleksey Fedoseev","Dzmitry Tsetserukou"],"pdf_url":"https://arxiv.org/pdf/2403.08340v1.pdf","comment":"Published in: 2023 IEEE/ASME International Conference on Advanced\n  Intelligent Mechatronics (AIM)"},{"id":"http://arxiv.org/abs/2402.15281v3","updated":"2024-03-13T08:34:47Z","published":"2024-02-23T12:06:48Z","title":"Neural Implicit Swept Volume Models for Fast Collision Detection","summary":"  Collision detection is one of the most time-consuming operations during\nmotion planning. Thus, there is an increasing interest in exploring machine\nlearning techniques to speed up collision detection and sampling-based motion\nplanning. A recent line of research focuses on utilizing neural signed distance\nfunctions of either the robot geometry or the swept volume of the robot motion.\nBuilding on this, we present a novel neural implicit swept volume model to\ncontinuously represent arbitrary motions parameterized by their start and goal\nconfigurations. This allows to quickly compute signed distances for any point\nin the task space to the robot motion. Further, we present an algorithm\ncombining the speed of the deep learning-based signed distance computations\nwith the strong accuracy guarantees of geometric collision checkers. We\nvalidate our approach in simulated and real-world robotic experiments, and\ndemonstrate that it is able to speed up a commercial bin picking application.\n","authors":["Dominik Joho","Jonas Schwinn","Kirill Safronov"],"pdf_url":"https://arxiv.org/pdf/2402.15281v3.pdf","comment":"To be published at ICRA 2024. Dominik Joho and Jonas Schwinn have\n  equal contribution"},{"id":"http://arxiv.org/abs/2403.08321v1","updated":"2024-03-13T08:06:41Z","published":"2024-03-13T08:06:41Z","title":"ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic\n  Manipulation","summary":"  Performing language-conditioned robotic manipulation tasks in unstructured\nenvironments is highly demanded for general intelligent robots. Conventional\nrobotic manipulation methods usually learn semantic representation of the\nobservation for action prediction, which ignores the scene-level spatiotemporal\ndynamics for human goal completion. In this paper, we propose a dynamic\nGaussian Splatting method named ManiGaussian for multi-task robotic\nmanipulation, which mines scene dynamics via future scene reconstruction.\nSpecifically, we first formulate the dynamic Gaussian Splatting framework that\ninfers the semantics propagation in the Gaussian embedding space, where the\nsemantic representation is leveraged to predict the optimal robot action. Then,\nwe build a Gaussian world model to parameterize the distribution in our dynamic\nGaussian Splatting framework, which provides informative supervision in the\ninteractive environment via future scene reconstruction. We evaluate our\nManiGaussian on 10 RLBench tasks with 166 variations, and the results\ndemonstrate our framework can outperform the state-of-the-art methods by 13.1\\%\nin average success rate.\n","authors":["Guanxing Lu","Shiyi Zhang","Ziwei Wang","Changliu Liu","Jiwen Lu","Yansong Tang"],"pdf_url":"https://arxiv.org/pdf/2403.08321v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03246v4","updated":"2024-03-13T07:55:38Z","published":"2024-02-05T18:03:53Z","title":"SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM","summary":"  We present SGS-SLAM, the first semantic visual SLAM system based on Gaussian\nSplatting. It incorporates appearance, geometry, and semantic features through\nmulti-channel optimization, addressing the oversmoothing limitations of neural\nimplicit SLAM systems in high-quality rendering, scene understanding, and\nobject-level geometry. We introduce a unique semantic feature loss that\neffectively compensates for the shortcomings of traditional depth and color\nlosses in object optimization. Through a semantic-guided keyframe selection\nstrategy, we prevent erroneous reconstructions caused by cumulative errors.\nExtensive experiments demonstrate that SGS-SLAM delivers state-of-the-art\nperformance in camera pose estimation, map reconstruction, precise semantic\nsegmentation, and object-level geometric accuracy, while ensuring real-time\nrendering capabilities.\n","authors":["Mingrui Li","Shuhong Liu","Heng Zhou","Guohao Zhu","Na Cheng","Tianchen Deng","Hongyu Wang"],"pdf_url":"https://arxiv.org/pdf/2402.03246v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.06263v2","updated":"2024-03-13T07:53:08Z","published":"2024-02-09T09:24:42Z","title":"ASAP-MPC: An Asynchronous Update Scheme for Online Motion Planning with\n  Nonlinear Model Predictive Control","summary":"  This paper presents a Nonlinear Model Predictive Control (NMPC) scheme\ntargeted at motion planning for mechatronic motion systems, such as drones and\nmobile platforms. NMPC-based motion planning typically requires low computation\ntimes to be able to provide control inputs at the required rate for system\nstability, disturbance rejection, and overall performance. Although there exist\nvarious ways in literature to reduce the solution times in NMPC, such times may\nnot be low enough to allow real-time implementations. This paper presents\nASAP-MPC, an approach to handle varying, sometimes restrictively large,\nsolution times with an asynchronous update scheme, always allowing for full\nconvergence and real-time execution. The NMPC algorithm is combined with a\nlinear state feedback controller tracking the optimised trajectories for\nimproved robustness against possible disturbances and plant-model mismatch.\nASAP-MPC seamlessly merges trajectories, resulting from subsequent NMPC\nsolutions, providing a smooth and continuous overall trajectory for the motion\nsystem. This frameworks applicability to embedded applications is shown on two\ndifferent experiment setups where a state-of-the-art method fails: a quadcopter\nflying through a cluttered environment in hardware-in-the-loop simulation and a\nscale model truck-trailer manoeuvring in a structured lab environment.\n","authors":["Dries Dirckx","Mathias Bos","Bastiaan Vandewal","Lander Vanroye","Wilm Decré","Jan Swevers"],"pdf_url":"https://arxiv.org/pdf/2402.06263v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2309.07759v3","updated":"2024-03-13T07:27:10Z","published":"2023-09-14T14:45:47Z","title":"PROGrasp: Pragmatic Human-Robot Communication for Object Grasping","summary":"  Interactive Object Grasping (IOG) is the task of identifying and grasping the\ndesired object via human-robot natural language interaction. Current IOG\nsystems assume that a human user initially specifies the target object's\ncategory (e.g., bottle). Inspired by pragmatics, where humans often convey\ntheir intentions by relying on context to achieve goals, we introduce a new IOG\ntask, Pragmatic-IOG, and the corresponding dataset, Intention-oriented\nMulti-modal Dialogue (IM-Dial). In our proposed task scenario, an\nintention-oriented utterance (e.g., \"I am thirsty\") is initially given to the\nrobot. The robot should then identify the target object by interacting with a\nhuman user. Based on the task setup, we propose a new robotic system that can\ninterpret the user's intention and pick up the target object, Pragmatic Object\nGrasping (PROGrasp). PROGrasp performs Pragmatic-IOG by incorporating modules\nfor visual grounding, question asking, object grasping, and most importantly,\nanswer interpretation for pragmatic inference. Experimental results show that\nPROGrasp is effective in offline (i.e., target object discovery) and online\n(i.e., IOG with a physical robot arm) settings. Code and data are available at\nhttps://github.com/gicheonkang/prograsp.\n","authors":["Gi-Cheon Kang","Junghyun Kim","Jaein Kim","Byoung-Tak Zhang"],"pdf_url":"https://arxiv.org/pdf/2309.07759v3.pdf","comment":"ICRA 2024"},{"id":"http://arxiv.org/abs/2403.08302v1","updated":"2024-03-13T07:21:21Z","published":"2024-03-13T07:21:21Z","title":"Online Multi-Contact Feedback Model Predictive Control for Interactive\n  Robotic Tasks","summary":"  In this paper, we propose a model predictive control (MPC) that accomplishes\ninteractive robotic tasks, in which multiple contacts may occur at unknown\nlocations. To address such scenarios, we made an explicit contact feedback loop\nin the MPC framework. An algorithm called Multi-Contact Particle Filter with\nExploration Particle (MCP-EP) is employed to establish real-time feedback of\nmulti-contact information. Then the interaction locations and forces are\naccommodated in the MPC framework via a spring contact model. Moreover, we\nachieved real-time control for a 7 degrees of freedom robot without any\nsimplifying assumptions by employing a Differential-Dynamic-Programming\nalgorithm. We achieved 6.8kHz, 1.9kHz, and 1.8kHz update rates of the MPC for\n0, 1, and 2 contacts, respectively. This allows the robot to handle unexpected\ncontacts in real time. Real-world experiments show the effectiveness of the\nproposed method in various scenarios.\n","authors":["Seo Wook Han","Maged Iskandar","Jinoh Lee","Min Jun Kim"],"pdf_url":"https://arxiv.org/pdf/2403.08302v1.pdf","comment":"This paper has been accepted for publication at the IEEE\n  International Conference on Robotics and Automation (ICRA), Yokohama, 2024"},{"id":"http://arxiv.org/abs/2403.08248v1","updated":"2024-03-13T05:03:58Z","published":"2024-03-13T05:03:58Z","title":"CoPa: General Robotic Manipulation through Spatial Constraints of Parts\n  with Foundation Models","summary":"  Foundation models pre-trained on web-scale data are shown to encapsulate\nextensive world knowledge beneficial for robotic manipulation in the form of\ntask planning. However, the actual physical implementation of these plans often\nrelies on task-specific learning methods, which require significant data\ncollection and struggle with generalizability. In this work, we introduce\nRobotic Manipulation through Spatial Constraints of Parts (CoPa), a novel\nframework that leverages the common sense knowledge embedded within foundation\nmodels to generate a sequence of 6-DoF end-effector poses for open-world\nrobotic manipulation. Specifically, we decompose the manipulation process into\ntwo phases: task-oriented grasping and task-aware motion planning. In the\ntask-oriented grasping phase, we employ foundation vision-language models\n(VLMs) to select the object's grasping part through a novel coarse-to-fine\ngrounding mechanism. During the task-aware motion planning phase, VLMs are\nutilized again to identify the spatial geometry constraints of task-relevant\nobject parts, which are then used to derive post-grasp poses. We also\ndemonstrate how CoPa can be seamlessly integrated with existing robotic\nplanning algorithms to accomplish complex, long-horizon tasks. Our\ncomprehensive real-world experiments show that CoPa possesses a fine-grained\nphysical understanding of scenes, capable of handling open-set instructions and\nobjects with minimal prompt engineering and without additional training.\nProject page: https://copa-2024.github.io/\n","authors":["Haoxu Huang","Fanqi Lin","Yingdong Hu","Shengjie Wang","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2403.08248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.01758v2","updated":"2024-03-13T04:59:21Z","published":"2023-08-03T13:39:50Z","title":"A Compliant Robotic Leg Based on Fibre Jamming","summary":"  Humans possess a remarkable ability to react to unpredictable perturbations\nthrough immediate mechanical responses, which harness the visco-elastic\nproperties of muscles to maintain balance. Inspired by this behaviour, we\npropose a novel design of a robotic leg utilising fibre jammed structures as\npassive compliant mechanisms to achieve variable joint stiffness and damping.\nWe developed multi-material fibre jammed tendons with tunable mechanical\nproperties, which can be 3D printed in one-go without need for assembly.\nThrough extensive numerical simulations and experimentation, we demonstrate the\nusefulness of these tendons for shock absorbance and maintaining joint\nstability. We investigate how they could be used effectively in a multi-joint\nrobotic leg by evaluating the relative contribution of each tendon to the\noverall stiffness of the leg. Further, we showcase the potential of these\njammed structures for legged locomotion, highlighting how morphological\nproperties of the tendons can be used to enhance stability in robotic legs.\n","authors":["Lois Liow","James Brett","Josh Pinskier","Lauren Hanson","Louis Tidswell","Navinda Kottege","David Howard"],"pdf_url":"https://arxiv.org/pdf/2308.01758v2.pdf","comment":"20 pages, 15 figures, IEEE Transactions on Robotics"},{"id":"http://arxiv.org/abs/2403.08239v1","updated":"2024-03-13T04:45:40Z","published":"2024-03-13T04:45:40Z","title":"Continuous Object State Recognition for Cooking Robots Using Pre-Trained\n  Vision-Language Models and Black-box Optimization","summary":"  The state recognition of the environment and objects by robots is generally\nbased on the judgement of the current state as a classification problem. On the\nother hand, state changes of food in cooking happen continuously and need to be\ncaptured not only at a certain time point but also continuously over time. In\naddition, the state changes of food are complex and cannot be easily described\nby manual programming. Therefore, we propose a method to recognize the\ncontinuous state changes of food for cooking robots through the spoken language\nusing pre-trained large-scale vision-language models. By using models that can\ncompute the similarity between images and texts continuously over time, we can\ncapture the state changes of food while cooking. We also show that by adjusting\nthe weighting of each text prompt based on fitting the similarity changes to a\nsigmoid function and then performing black-box optimization, more accurate and\nrobust continuous state recognition can be achieved. We demonstrate the\neffectiveness and limitations of this method by performing the recognition of\nwater boiling, butter melting, egg cooking, and onion stir-frying.\n","authors":["Kento Kawaharazuka","Naoaki Kanazawa","Yoshiki Obinata","Kei Okada","Masayuki Inaba"],"pdf_url":"https://arxiv.org/pdf/2403.08239v1.pdf","comment":"accepted at IEEE Robotics and Automation Letters (RA-L), website -\n  https://haraduka.github.io/continuous-state-recognition/"},{"id":"http://arxiv.org/abs/2403.08238v1","updated":"2024-03-13T04:43:10Z","published":"2024-03-13T04:43:10Z","title":"A Novel Feature Learning-based Bio-inspired Neural Network for Real-time\n  Collision-free Rescue of Multi-Robot Systems","summary":"  Natural disasters and urban accidents drive the demand for rescue robots to\nprovide safer, faster, and more efficient rescue trajectories. In this paper, a\nfeature learning-based bio-inspired neural network (FLBBINN) is proposed to\nquickly generate a heuristic rescue path in complex and dynamic environments,\nas traditional approaches usually cannot provide a satisfactory solution to\nreal-time responses to sudden environmental changes. The neurodynamic model is\nincorporated into the feature learning method that can use environmental\ninformation to improve path planning strategies. Task assignment and\ncollision-free rescue trajectory are generated through robot poses and the\ndynamic landscape of neural activity. A dual-channel scale filter, a neural\nactivity channel, and a secondary distance fusion are employed to extract and\nfilter feature neurons. After completion of the feature learning process, a\nneurodynamics-based feature matrix is established to quickly generate the new\nheuristic rescue paths with parameter-driven topological adaptability. The\nproposed FLBBINN aims to reduce the computational complexity of the neural\nnetwork-based approach and enable the feature learning method to achieve\nreal-time responses to environmental changes. Several simulations and\nexperiments have been conducted to evaluate the performance of the proposed\nFLBBINN. The results show that the proposed FLBBINN would significantly improve\nthe speed, efficiency, and optimality for rescue operations.\n","authors":["Junfei Li","Simon X. Yang"],"pdf_url":"https://arxiv.org/pdf/2403.08238v1.pdf","comment":"This paper is accepted to publish in IEEE Transactions on Industrial\n  Electronics"},{"id":"http://arxiv.org/abs/2403.08231v1","updated":"2024-03-13T04:15:43Z","published":"2024-03-13T04:15:43Z","title":"Object Permanence Filter for Robust Tracking with Interactive Robots","summary":"  Object permanence, which refers to the concept that objects continue to exist\neven when they are no longer perceivable through the senses, is a crucial\naspect of human cognitive development. In this work, we seek to incorporate\nthis understanding into interactive robots by proposing a set of assumptions\nand rules to represent object permanence in multi-object, multi-agent\ninteractive scenarios. We integrate these rules into the particle filter,\nresulting in the Object Permanence Filter (OPF). For multi-object scenarios, we\npropose an ensemble of K interconnected OPFs, where each filter predicts\nplausible object tracks that are resilient to missing, noisy, and kinematically\nor dynamically infeasible measurements, thus bringing perceptional robustness.\nThrough several interactive scenarios, we demonstrate that the proposed OPF\napproach provides robust tracking in human-robot interactive tasks agnostic to\nmeasurement type, even in the presence of prolonged and complete occlusion.\nWebpage: https://opfilter.github.io/.\n","authors":["Shaoting Peng","Margaret X. Wang","Julie A. Shah","Nadia Figueroa"],"pdf_url":"https://arxiv.org/pdf/2403.08231v1.pdf","comment":"2024 IEEE International Conference on Robotics and Automation (ICRA)"},{"id":"http://arxiv.org/abs/2403.08228v1","updated":"2024-03-13T04:11:41Z","published":"2024-03-13T04:11:41Z","title":"Empowering Robotics with Large Language Models: osmAG Map Comprehension\n  with LLMs","summary":"  Recently, Large Language Models (LLMs) have demonstrated great potential in\nrobotic applications by providing essential general knowledge for situations\nthat can not be pre-programmed beforehand. Generally speaking, mobile robots\nneed to understand maps to execute tasks such as localization or navigation. In\nthis letter, we address the problem of enabling LLMs to comprehend Area Graph,\na text-based map representation, in order to enhance their applicability in the\nfield of mobile robotics. Area Graph is a hierarchical, topometric semantic map\nrepresentation utilizing polygons to demark areas such as rooms, corridors or\nbuildings. In contrast to commonly used map representations, such as occupancy\ngrid maps or point clouds, osmAG (Area Graph in OpensStreetMap format) is\nstored in a XML textual format naturally readable by LLMs. Furthermore,\nconventional robotic algorithms such as localization and path planning are\ncompatible with osmAG, facilitating this map representation comprehensible by\nLLMs, traditional robotic algorithms and humans. Our experiments show that with\na proper map representation, LLMs possess the capability to understand maps and\nanswer queries based on that understanding. Following simple fine-tuning of\nLLaMA2 models, it surpassed ChatGPT-3.5 in tasks involving topology and\nhierarchy understanding. Our dataset, dataset generation code, fine-tuned LoRA\nadapters can be accessed at\nhttps://github.com/xiefujing/LLM-osmAG-Comprehension.\n","authors":["Fujing Xie","Sören Schwertfeger"],"pdf_url":"https://arxiv.org/pdf/2403.08228v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06198v2","updated":"2024-03-13T04:07:57Z","published":"2023-10-09T23:01:32Z","title":"Motion Memory: Leveraging Past Experiences to Accelerate Future Motion\n  Planning","summary":"  When facing a new motion-planning problem, most motion planners solve it from\nscratch, e.g., via sampling and exploration or starting optimization from a\nstraight-line path. However, most motion planners have to experience a variety\nof planning problems throughout their lifetimes, which are yet to be leveraged\nfor future planning. In this paper, we present a simple but efficient method\ncalled Motion Memory, which allows different motion planners to accelerate\nfuture planning using past experiences. Treating existing motion planners as\neither a closed or open box, we present a variety of ways that Motion Memory\ncan contribute to reduce the planning time when facing a new planning problem.\nWe provide extensive experiment results with three different motion planners on\nthree classes of planning problems with over 30,000 problem instances and show\nthat planning speed can be significantly reduced by up to 89% with the proposed\nMotion Memory technique and with increasing past planning experiences.\n","authors":["Dibyendu Das","Yuanjie Lu","Erion Plaku","Xuesu Xiao"],"pdf_url":"https://arxiv.org/pdf/2310.06198v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08219v1","updated":"2024-03-13T03:34:00Z","published":"2024-03-13T03:34:00Z","title":"SpaceOctopus: An Octopus-inspired Motion Planning Framework for\n  Multi-arm Space Robot","summary":"  Space robots have played a critical role in autonomous maintenance and space\njunk removal. Multi-arm space robots can efficiently complete the target\ncapture and base reorientation tasks due to their flexibility and the\ncollaborative capabilities between the arms. However, the complex coupling\nproperties arising from both the multiple arms and the free-floating base\npresent challenges to the motion planning problems of multi-arm space robots.\nWe observe that the octopus elegantly achieves similar goals when grabbing prey\nand escaping from danger. Inspired by the distributed control of octopuses'\nlimbs, we develop a multi-level decentralized motion planning framework to\nmanage the movement of different arms of space robots. This motion planning\nframework integrates naturally with the multi-agent reinforcement learning\n(MARL) paradigm. The results indicate that our method outperforms the previous\nmethod (centralized training). Leveraging the flexibility of the decentralized\nframework, we reassemble policies trained for different tasks, enabling the\nspace robot to complete trajectory planning tasks while adjusting the base\nattitude without further learning. Furthermore, our experiments confirm the\nsuperior robustness of our method in the face of external disturbances,\nchanging base masses, and even the failure of one arm.\n","authors":["Wenbo Zhao","Shengjie Wang","Yixuan Fan","Yang Gao","Tao Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08219v1.pdf","comment":"8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.08215v1","updated":"2024-03-13T03:24:36Z","published":"2024-03-13T03:24:36Z","title":"LIX: Implicitly Infusing Spatial Geometric Prior Knowledge into Visual\n  Semantic Segmentation for Autonomous Driving","summary":"  Despite the impressive performance achieved by data-fusion networks with\nduplex encoders for visual semantic segmentation, they become ineffective when\nspatial geometric data are not available. Implicitly infusing the spatial\ngeometric prior knowledge acquired by a duplex-encoder teacher model into a\nsingle-encoder student model is a practical, albeit less explored research\navenue. This paper delves into this topic and resorts to knowledge distillation\napproaches to address this problem. We introduce the Learning to Infuse \"X\"\n(LIX) framework, with novel contributions in both logit distillation and\nfeature distillation aspects. We present a mathematical proof that underscores\nthe limitation of using a single fixed weight in decoupled knowledge\ndistillation and introduce a logit-wise dynamic weight controller as a solution\nto this issue. Furthermore, we develop an adaptively-recalibrated feature\ndistillation algorithm, including two technical novelties: feature\nrecalibration via kernel regression and in-depth feature consistency\nquantification via centered kernel alignment. Extensive experiments conducted\nwith intermediate-fusion and late-fusion networks across various public\ndatasets provide both quantitative and qualitative evaluations, demonstrating\nthe superior performance of our LIX framework when compared to other\nstate-of-the-art approaches.\n","authors":["Sicen Guo","Zhiyuan Wu","Qijun Chen","Ioannis Pitas","Rui Fan"],"pdf_url":"https://arxiv.org/pdf/2403.08215v1.pdf","comment":"13 pages, 4 figures, 5 tables"},{"id":"http://arxiv.org/abs/2310.08873v3","updated":"2024-03-13T02:53:30Z","published":"2023-10-13T05:59:03Z","title":"Interactive Navigation in Environments with Traversable Obstacles Using\n  Large Language and Vision-Language Models","summary":"  This paper proposes an interactive navigation framework by using large\nlanguage and vision-language models, allowing robots to navigate in\nenvironments with traversable obstacles. We utilize the large language model\n(GPT-3.5) and the open-set Vision-language Model (Grounding DINO) to create an\naction-aware costmap to perform effective path planning without fine-tuning.\nWith the large models, we can achieve an end-to-end system from textual\ninstructions like \"Can you pass through the curtains to deliver medicines to\nme?\", to bounding boxes (e.g., curtains) with action-aware attributes. They can\nbe used to segment LiDAR point clouds into two parts: traversable and\nuntraversable parts, and then an action-aware costmap is constructed for\ngenerating a feasible path. The pre-trained large models have great\ngeneralization ability and do not require additional annotated data for\ntraining, allowing fast deployment in the interactive navigation tasks. We\nchoose to use multiple traversable objects such as curtains and grasses for\nverification by instructing the robot to traverse them. Besides, traversing\ncurtains in a medical scenario was tested. All experimental results\ndemonstrated the proposed framework's effectiveness and adaptability to diverse\nenvironments.\n","authors":["Zhen Zhang","Anran Lin","Chun Wai Wong","Xiangyu Chu","Qi Dou","K. W. Samuel Au"],"pdf_url":"https://arxiv.org/pdf/2310.08873v3.pdf","comment":"Accepted by 2024 IEEE International Conference on Robotics and\n  Automation (ICRA), 7 pages, 8 figures"},{"id":"http://arxiv.org/abs/2308.09387v2","updated":"2024-03-13T02:37:47Z","published":"2023-08-18T08:38:28Z","title":"Multi-Level Compositional Reasoning for Interactive Instruction\n  Following","summary":"  Robotic agents performing domestic chores by natural language directives are\nrequired to master the complex job of navigating environment and interacting\nwith objects in the environments. The tasks given to the agents are often\ncomposite thus are challenging as completing them require to reason about\nmultiple subtasks, e.g., bring a cup of coffee. To address the challenge, we\npropose to divide and conquer it by breaking the task into multiple subgoals\nand attend to them individually for better navigation and interaction. We call\nit Multi-level Compositional Reasoning Agent (MCR-Agent). Specifically, we\nlearn a three-level action policy. At the highest level, we infer a sequence of\nhuman-interpretable subgoals to be executed based on language instructions by a\nhigh-level policy composition controller. At the middle level, we\ndiscriminatively control the agent's navigation by a master policy by\nalternating between a navigation policy and various independent interaction\npolicies. Finally, at the lowest level, we infer manipulation actions with the\ncorresponding object masks using the appropriate interaction policy. Our\napproach not only generates human interpretable subgoals but also achieves\n2.03% absolute gain to comparable state of the arts in the efficiency metric\n(PLWSR in unseen set) without using rule-based planning or a semantic spatial\nmemory.\n","authors":["Suvaansh Bhambri","Byeonghwi Kim","Jonghyun Choi"],"pdf_url":"https://arxiv.org/pdf/2308.09387v2.pdf","comment":"AAAI 2023 (Oral) (Project page:\n  https://bhkim94.github.io/projects/MCR-Agent)"},{"id":"http://arxiv.org/abs/2308.07241v4","updated":"2024-03-13T02:34:31Z","published":"2023-08-14T16:23:21Z","title":"Context-Aware Planning and Environment-Aware Memory for Instruction\n  Following Embodied Agents","summary":"  Accomplishing household tasks requires to plan step-by-step actions\nconsidering the consequences of previous actions. However, the state-of-the-art\nembodied agents often make mistakes in navigating the environment and\ninteracting with proper objects due to imperfect learning by imitating experts\nor algorithmic planners without such knowledge. To improve both visual\nnavigation and object interaction, we propose to consider the consequence of\ntaken actions by CAPEAM (Context-Aware Planning and Environment-Aware Memory)\nthat incorporates semantic context (e.g., appropriate objects to interact with)\nin a sequence of actions, and the changed spatial arrangement and states of\ninteracted objects (e.g., location that the object has been moved to) in\ninferring the subsequent actions. We empirically show that the agent with the\nproposed CAPEAM achieves state-of-the-art performance in various metrics using\na challenging interactive instruction following benchmark in both seen and\nunseen environments by large margins (up to +10.70% in unseen env.).\n","authors":["Byeonghwi Kim","Jinyeon Kim","Yuyeong Kim","Cheolhong Min","Jonghyun Choi"],"pdf_url":"https://arxiv.org/pdf/2308.07241v4.pdf","comment":"ICCV 2023 (Project page: https://bhkim94.github.io/projects/CAPEAM)"},{"id":"http://arxiv.org/abs/2403.08191v1","updated":"2024-03-13T02:26:15Z","published":"2024-03-13T02:26:15Z","title":"Synchronized Dual-arm Rearrangement via Cooperative mTSP","summary":"  Synchronized dual-arm rearrangement is widely studied as a common scenario in\nindustrial applications. It often faces scalability challenges due to the\ncomputational complexity of robotic arm rearrangement and the high-dimensional\nnature of dual-arm planning. To address these challenges, we formulated the\nproblem as cooperative mTSP, a variant of mTSP where agents share cooperative\ncosts, and utilized reinforcement learning for its solution. Our approach\ninvolved representing rearrangement tasks using a task state graph that\ncaptured spatial relationships and a cooperative cost matrix that provided\ndetails about action costs. Taking these representations as observations, we\ndesigned an attention-based network to effectively combine them and provide\nrational task scheduling. Furthermore, a cost predictor is also introduced to\ndirectly evaluate actions during both training and planning, significantly\nexpediting the planning process. Our experimental results demonstrate that our\napproach outperforms existing methods in terms of both performance and planning\nefficiency.\n","authors":["Wenhao Li","Shishun Zhang","Sisi Dai","Hui Huang","Ruizhen Hu","Xiaohong Chen","Kai Xu"],"pdf_url":"https://arxiv.org/pdf/2403.08191v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08185v1","updated":"2024-03-13T02:18:33Z","published":"2024-03-13T02:18:33Z","title":"Perceive With Confidence: Statistical Safety Assurances for Navigation\n  with Learning-Based Perception","summary":"  Rapid advances in perception have enabled large pre-trained models to be used\nout of the box for processing high-dimensional, noisy, and partial observations\nof the world into rich geometric representations (e.g., occupancy predictions).\nHowever, safe integration of these models onto robots remains challenging due\nto a lack of reliable performance in unfamiliar environments. In this work, we\npresent a framework for rigorously quantifying the uncertainty of pre-trained\nperception models for occupancy prediction in order to provide end-to-end\nstatistical safety assurances for navigation. We build on techniques from\nconformal prediction for producing a calibrated perception system that lightly\nprocesses the outputs of a pre-trained model while ensuring generalization to\nnovel environments and robustness to distribution shifts in states when\nperceptual outputs are used in conjunction with a planner. The calibrated\nsystem can be used in combination with any safe planner to provide an\nend-to-end statistical assurance on safety in a new environment with a\nuser-specified threshold $1-\\epsilon$. We evaluate the resulting approach -\nwhich we refer to as Perceive with Confidence (PwC) - with experiments in\nsimulation and on hardware where a quadruped robot navigates through indoor\nenvironments containing objects unseen during training or calibration. These\nexperiments validate the safety assurances provided by PwC and demonstrate\nsignificant improvements in empirical safety rates compared to baselines.\n","authors":["Anushri Dixit","Zhiting Mei","Meghan Booker","Mariko Storey-Matsutani","Allen Z. Ren","Anirudha Majumdar"],"pdf_url":"https://arxiv.org/pdf/2403.08185v1.pdf","comment":"Videos and code can be found at\n  https://perceive-with-confidence.github.io"},{"id":"http://arxiv.org/abs/2308.13658v2","updated":"2024-03-13T02:08:34Z","published":"2023-08-25T20:17:49Z","title":"Generating and Explaining Corner Cases Using Learnt Probabilistic Lane\n  Graphs","summary":"  Validating the safety of Autonomous Vehicles (AVs) operating in open-ended,\ndynamic environments is challenging as vehicles will eventually encounter\nsafety-critical situations for which there is not representative training data.\nBy increasing the coverage of different road and traffic conditions and by\nincluding corner cases in simulation-based scenario testing, the safety of AVs\ncan be improved. However, the creation of corner case scenarios including\nmultiple agents is non-trivial. Our approach allows engineers to generate\nnovel, realistic corner cases based on historic traffic data and to explain why\nsituations were safety-critical. In this paper, we introduce Probabilistic Lane\nGraphs (PLGs) to describe a finite set of lane positions and directions in\nwhich vehicles might travel. The structure of PLGs is learnt directly from\nspatio-temporal traffic data. The graph model represents the actions of the\ndrivers in response to a given state in the form of a probabilistic policy. We\nuse reinforcement learning techniques to modify this policy and to generate\nrealistic and explainable corner case scenarios which can be used for assessing\nthe safety of AVs.\n","authors":["Enrik Maci","Rhys Howard","Lars Kunze"],"pdf_url":"https://arxiv.org/pdf/2308.13658v2.pdf","comment":"8 Pages, 3 Figures, 1 Table, Published in the Proceedings of the 26th\n  IEEE International Conference on Intelligent Transport Systems (2023), Final\n  submission version with added IEEE copyright notice"},{"id":"http://arxiv.org/abs/2403.08178v1","updated":"2024-03-13T02:04:57Z","published":"2024-03-13T02:04:57Z","title":"Learning Barrier-Certified Polynomial Dynamical Systems for Obstacle\n  Avoidance with Robots","summary":"  Established techniques that enable robots to learn from demonstrations are\nbased on learning a stable dynamical system (DS). To increase the robots'\nresilience to perturbations during tasks that involve static obstacle\navoidance, we propose incorporating barrier certificates into an optimization\nproblem to learn a stable and barrier-certified DS. Such optimization problem\ncan be very complex or extremely conservative when the traditional linear\nparameter-varying formulation is used. Thus, different from previous approaches\nin the literature, we propose to use polynomial representations for DSs, which\nyields an optimization problem that can be tackled by sum-of-squares\ntechniques. Finally, our approach can handle obstacle shapes that fall outside\nthe scope of assumptions typically found in the literature concerning obstacle\navoidance within the DS learning framework. Supplementary material can be found\nat the project webpage: https://martinschonger.github.io/abc-ds\n","authors":["Martin Schonger","Hugo T. M. Kussaba","Lingyun Chen","Luis Figueredo","Abdalla Swikir","Aude Billard","Sami Haddadin"],"pdf_url":"https://arxiv.org/pdf/2403.08178v1.pdf","comment":"7 pages, 7 figures, accepted to the 2024 IEEE International\n  Conference on Robotics and Automation (ICRA 2024)"},{"id":"http://arxiv.org/abs/2403.08177v1","updated":"2024-03-13T02:03:53Z","published":"2024-03-13T02:03:53Z","title":"A Direct Algorithm for Multi-Gyroscope Infield Calibration","summary":"  In this paper, we address the problem of estimating the rotational\nextrinsics, as well as the scale factors of two gyroscopes rigidly mounted on\nthe same device. In particular, we formulate the problem as a least-squares\nminimization and introduce a direct algorithm that computes the estimated\nquantities without any iterations, hence avoiding local minima and improving\nefficiency. Furthermore, we show that the rotational extrinsics are observable\nwhile the scale factors can be determined up to global scale for general\nconfigurations of the gyroscopes. To this end, we also study special placements\nof the gyroscopes where a pair, or all, of their axes are parallel and analyze\ntheir impact on the scale factors' observability. Lastly, we evaluate our\nalgorithm in simulations and real-world experiments to assess its performance\nas a function of key motion and sensor characteristics.\n","authors":["Tianheng Wang","Stergios I. Roumeliotis"],"pdf_url":"https://arxiv.org/pdf/2403.08177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00748v3","updated":"2024-03-13T01:33:19Z","published":"2024-03-01T18:48:48Z","title":"Primal-Dual iLQR","summary":"  We introduce a new algorithm for solving unconstrained discrete-time optimal\ncontrol problems. Our method follows a direct multiple shooting approach, and\nconsists of applying the SQP method together with an $\\ell_2$ augmented\nLagrangian primal-dual merit function. We use the LQR algorithm to efficiently\nsolve the primal component of the Newton-KKT system, and use a dual LQR\nbackward pass to solve its dual component. We also present a new parallel\nalgorithm for solving the dual component of the Newton-KKT system in\n$O(\\log(N))$ parallel time, where $N$ is the number of stages. Combining it\nwith (S\\\"{a}rkk\\\"{a} and Garc\\'{i}a-Fern\\'{a}ndez, 2023), we are able to solve\nthe full Newton-KKT system in $O(\\log(N))$ parallel time. The remaining parts\nof our method have constant parallel time complexity per iteration. Therefore,\nthis paper provides, for the first time, a practical, highly parallelizable\n(for example, with a GPU) method for solving nonlinear discrete-time optimal\ncontrol problems. As our algorithm is a specialization of NPSQP (Gill et al.\n1992), it inherits its generic properties, including global convergence, fast\nlocal convergence, and the lack of need for second order corrections or\ndimension expansions, improving on existing direct multiple shooting approaches\nsuch as acados (Verschueren et al. 2022), ALTRO (Howell et al. 2019), GNMS\n(Giftthaler et al. 2018), FATROP (Vanroye et al. 2023), and FDDP (Mastalli et\nal. 2020).\n","authors":["João Sousa-Pinto","Dominique Orban"],"pdf_url":"https://arxiv.org/pdf/2403.00748v3.pdf","comment":"7 pages, 1 figure, 1 table"},{"id":"http://arxiv.org/abs/2403.08163v1","updated":"2024-03-13T01:25:18Z","published":"2024-03-13T01:25:18Z","title":"Effective Underwater Glider Path Planning in Dynamic 3D Environments\n  Using Multi-Point Potential Fields","summary":"  Underwater gliders (UGs) have emerged as highly effective unmanned vehicles\nfor ocean exploration. However, their operation in dynamic and complex\nunderwater environments necessitates robust path-planning strategies. Previous\nstudies have primarily focused on global energy or time-efficient path planning\nin explored environments, overlooking challenges posed by unpredictable flow\nconditions and unknown obstacles in varying and dynamic areas like fjords and\nnear-harbor waters. This paper introduces and improves a real-time path\nplanning method, Multi-Point Potential Field (MPPF), tailored for UGs operating\nin 3D space as they are constrained by buoyancy propulsion and internal\nactuation. The proposed MPPF method addresses obstacles, flow fields, and local\nminima, enhancing the efficiency and robustness of UG path planning. A low-cost\nprototype, the Research Oriented Underwater Glider for Hands-on Investigative\nEngineering (ROUGHIE), is utilized for validation. Through case studies and\nsimulations, the efficacy of the enhanced MPPF method is demonstrated,\nhighlighting its potential for real-world applications in underwater\nexploration.\n","authors":["Hanzhi Yang","Nina Mahmoudian"],"pdf_url":"https://arxiv.org/pdf/2403.08163v1.pdf","comment":"7 pages, 12 figures, submitted for CAMS 2024"},{"id":"http://arxiv.org/abs/2403.01644v2","updated":"2024-03-13T01:23:59Z","published":"2024-03-03T23:46:06Z","title":"OccFusion: A Straightforward and Effective Multi-Sensor Fusion Framework\n  for 3D Occupancy Prediction","summary":"  This paper introduces OccFusion, a straightforward and efficient sensor\nfusion framework for predicting 3D occupancy. A comprehensive understanding of\n3D scenes is crucial in autonomous driving, and recent models for 3D semantic\noccupancy prediction have successfully addressed the challenge of describing\nreal-world objects with varied shapes and classes. However, existing methods\nfor 3D occupancy prediction heavily rely on surround-view camera images, making\nthem susceptible to changes in lighting and weather conditions. By integrating\nfeatures from additional sensors, such as lidar and surround view radars, our\nframework enhances the accuracy and robustness of occupancy prediction,\nresulting in top-tier performance on the nuScenes benchmark. Furthermore,\nextensive experiments conducted on the nuScenes dataset, including challenging\nnight and rainy scenarios, confirm the superior performance of our sensor\nfusion strategy across various perception ranges. The code for this framework\nwill be made available at https://github.com/DanielMing123/OCCFusion.\n","authors":["Zhenxing Ming","Julie Stephany Berrio","Mao Shan","Stewart Worrall"],"pdf_url":"https://arxiv.org/pdf/2403.01644v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11935v2","updated":"2024-03-13T00:44:02Z","published":"2023-09-21T09:47:55Z","title":"RTS-GT: Robotic Total Stations Ground Truthing dataset","summary":"  Numerous datasets and benchmarks exist to assess and compare Simultaneous\nLocalization and Mapping (SLAM) algorithms. Nevertheless, their precision must\nfollow the rate at which SLAM algorithms improved in recent years. Moreover,\ncurrent datasets fall short of comprehensive data-collection protocol for\nreproducibility and the evaluation of the precision or accuracy of the recorded\ntrajectories. With this objective in mind, we proposed the Robotic Total\nStations Ground Truthing dataset (RTS-GT) dataset to support localization\nresearch with the generation of six-Degrees Of Freedom (DOF) ground truth\ntrajectories. This novel dataset includes six-DOF ground truth trajectories\ngenerated using a system of three Robotic Total Stations (RTSs) tracking moving\nrobotic platforms. Furthermore, we compare the performance of the RTS-based\nsystem to a Global Navigation Satellite System (GNSS)-based setup. The dataset\ncomprises around sixty experiments conducted in various conditions over a\nperiod of 17 months, and encompasses over 49 kilometers of trajectories, making\nit the most extensive dataset of RTS-based measurements to date. Additionally,\nwe provide the precision of all poses for each experiment, a feature not found\nin the current state-of-the-art datasets. Our results demonstrate that RTSs\nprovide measurements that are 22 times more stable than GNSS in various\nenvironmental settings, making them a valuable resource for SLAM benchmark\ndevelopment.\n","authors":["Maxime Vaidis","Mohsen Hassanzadeh Shahraji","Effie Daum","William Dubois","Philippe Giguère","François Pomerleau"],"pdf_url":"https://arxiv.org/pdf/2309.11935v2.pdf","comment":"7 pages; Accepted to ICRA 2024"},{"id":"http://arxiv.org/abs/2403.08152v1","updated":"2024-03-13T00:30:09Z","published":"2024-03-13T00:30:09Z","title":"Multi-Fidelity Reinforcement Learning for Time-Optimal Quadrotor\n  Re-planning","summary":"  High-speed online trajectory planning for UAVs poses a significant challenge\ndue to the need for precise modeling of complex dynamics while also being\nconstrained by computational limitations. This paper presents a multi-fidelity\nreinforcement learning method (MFRL) that aims to effectively create a\nrealistic dynamics model and simultaneously train a planning policy that can be\nreadily deployed in real-time applications. The proposed method involves the\nco-training of a planning policy and a reward estimator; the latter predicts\nthe performance of the policy's output and is trained efficiently through\nmulti-fidelity Bayesian optimization. This optimization approach models the\ncorrelation between different fidelity levels, thereby constructing a\nhigh-fidelity model based on a low-fidelity foundation, which enables the\naccurate development of the reward model with limited high-fidelity\nexperiments. The framework is further extended to include real-world flight\nexperiments in reinforcement learning training, allowing the reward model to\nprecisely reflect real-world constraints and broadening the policy's\napplicability to real-world scenarios. We present rigorous evaluations by\ntraining and testing the planning policy in both simulated and real-world\nenvironments. The resulting trained policy not only generates faster and more\nreliable trajectories compared to the baseline snap minimization method, but it\nalso achieves trajectory updates in 2 ms on average, while the baseline method\ntakes several minutes.\n","authors":["Gilhyun Ryou","Geoffrey Wang","Sertac Karaman"],"pdf_url":"https://arxiv.org/pdf/2403.08152v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08149v1","updated":"2024-03-13T00:19:44Z","published":"2024-03-13T00:19:44Z","title":"On the Feasibility of EEG-based Motor Intention Detection for Real-Time\n  Robot Assistive Control","summary":"  This paper explores the feasibility of employing EEG-based intention\ndetection for real-time robot assistive control. We focus on predicting and\ndistinguishing motor intentions of left/right arm movements by presenting: i)\nan offline data collection and training pipeline, used to train a classifier\nfor left/right motion intention prediction, and ii) an online real-time\nprediction pipeline leveraging the trained classifier and integrated with an\nassistive robot. Central to our approach is a rich feature representation\ncomposed of the tangent space projection of time-windowed sample covariance\nmatrices from EEG filtered signals and derivatives; allowing for a simple SVM\nclassifier to achieve unprecedented accuracy and real-time performance. In\npre-recorded real-time settings (160 Hz), a peak accuracy of 86.88% is\nachieved, surpassing prior works. In robot-in-the-loop settings, our system\nsuccessfully detects intended motion solely from EEG data with 70% accuracy,\ntriggering a robot to execute an assistive task. We provide a comprehensive\nevaluation of the proposed classifier.\n","authors":["Ho Jin Choi","Satyajeet Das","Shaoting Peng","Ruzena Bajcsy","Nadia Figueroa"],"pdf_url":"https://arxiv.org/pdf/2403.08149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08144v1","updated":"2024-03-13T00:10:18Z","published":"2024-03-13T00:10:18Z","title":"Prosody for Intuitive Robotic Interface Design: It's Not What You Said,\n  It's How You Said It","summary":"  In this paper, we investigate the use of 'prosody' (the musical elements of\nspeech) as a communicative signal for intuitive human-robot interaction\ninterfaces. Our approach, rooted in Research through Design (RtD), examines the\napplication of prosody in directing a quadruped robot navigation. We involved\nten team members in an experiment to command a robot through an obstacle course\nusing natural interaction. A human operator, serving as the robot's sensory and\nprocessing proxy, translated human communication into a basic set of navigation\ncommands, effectively simulating an intuitive interface. During our analysis of\ninteraction videos, when lexical and visual cues proved insufficient for\naccurate command interpretation, we turned to non-verbal auditory cues.\nQualitative evidence suggests that participants intuitively relied on prosody\nto control robot navigation. We highlight specific distinct prosodic constructs\nthat emerged from this preliminary exploration and discuss their pragmatic\nfunctions. This work contributes a discussion on the broader potential of\nprosody as a multifunctional communicative signal for designing future\nintuitive robotic interfaces, enabling lifelong learning and personalization in\nhuman-robot interaction.\n","authors":["Elaheh Sanoubari","Atil Iscen","Leila Takayama","Stefano Saliceti","Corbin Cunningham","Ken Caluwaerts"],"pdf_url":"https://arxiv.org/pdf/2403.08144v1.pdf","comment":"This paper was accepted at the Lifelong Learning and Personalization\n  in Long-Term Human-Robot Interaction (LEAP-HRI) workshop at ACM/IEEE\n  International Conference on Human Robot Interaction (HRI) 2024"},{"id":"http://arxiv.org/abs/2403.08997v1","updated":"2024-03-13T23:31:04Z","published":"2024-03-13T23:31:04Z","title":"CART: Caltech Aerial RGB-Thermal Dataset in the Wild","summary":"  We present the first publicly available RGB-thermal dataset designed for\naerial robotics operating in natural environments. Our dataset captures a\nvariety of terrains across the continental United States, including rivers,\nlakes, coastlines, deserts, and forests, and consists of synchronized RGB,\nlong-wave thermal, global positioning, and inertial data. Furthermore, we\nprovide semantic segmentation annotations for 10 classes commonly encountered\nin natural settings in order to facilitate the development of perception\nalgorithms robust to adverse weather and nighttime conditions. Using this\ndataset, we propose new and challenging benchmarks for thermal and RGB-thermal\nsemantic segmentation, RGB-to-thermal image translation, and visual-inertial\nodometry. We present extensive results using state-of-the-art methods and\nhighlight the challenges posed by temporal and geographical domain shifts in\nour data. Dataset and accompanying code will be provided at\nhttps://github.com/aerorobotics/caltech-aerial-rgbt-dataset\n","authors":["Connor Lee","Matthew Anderson","Nikhil Raganathan","Xingxing Zuo","Kevin Do","Georgia Gkioxari","Soon-Jo Chung"],"pdf_url":"https://arxiv.org/pdf/2403.08997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.17217v2","updated":"2024-03-13T22:27:27Z","published":"2023-05-26T19:22:31Z","title":"Collision-Resilient Passive Deformable Quadrotors for Exploration,\n  Mapping and Swift Navigation","summary":"  In this article, we introduce XPLORER, a passive deformable quadrotor\noptimized for performing contact-rich tasks by utilizing collision-induced\ndeformation. We present a novel external force estimation technique, and\nadvanced planning and control algorithms that exploit the compliant nature of\nXPLORER's chassis. These algorithms enable three distinct flight behaviors:\nstatic-wrench application, where XPLORER can exert desired forces and torque on\nsurfaces for precise manipulation; disturbance rejection, wherein the quadrotor\nactively mitigates external forces and yaw disturbances to maintain its\nintended trajectory; and yielding to disturbance, enabling XPLORER to\ndynamically adapt its position and orientation to evade undesired forces,\nensuring stable flight amidst unpredictable environmental factors. Leveraging\nthese behaviors, we develop innovative mission strategies including\ntactile-traversal, tactile-turning, and collide-to-brake for contact-based\nexploration of unknown areas, contact-based mapping and swift navigation.\nThrough experimental validation, we demonstrate the effectiveness of these\nstrategies in enabling efficient exploration and rapid navigation in unknown\nenvironments, leveraging collisions as a means for feedback and control. This\nstudy contributes to the growing field of aerial robotics by showcasing the\npotential of passive deformable quadrotors for versatile and robust interaction\ntasks in real-world scenarios.\n","authors":["Karishma Patnaik","Aravind Adhith Pandian Saravanakumaran","Wenlong Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.17217v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08984v1","updated":"2024-03-13T22:19:06Z","published":"2024-03-13T22:19:06Z","title":"Safe Road-Crossing by Autonomous Wheelchairs: a Novel Dataset and its\n  Experimental Evaluation","summary":"  Safe road-crossing by self-driving vehicles is a crucial problem to address\nin smart-cities. In this paper, we introduce a multi-sensor fusion approach to\nsupport road-crossing decisions in a system composed by an autonomous\nwheelchair and a flying drone featuring a robust sensory system made of diverse\nand redundant components. To that aim, we designed an analytical danger\nfunction based on explainable physical conditions evaluated by single sensors,\nincluding those using machine learning and artificial vision. As a\nproof-of-concept, we provide an experimental evaluation in a laboratory\nenvironment, showing the advantages of using multiple sensors, which can\nimprove decision accuracy and effectively support safety assessment. We made\nthe dataset available to the scientific community for further experimentation.\nThe work has been developed in the context of an European project named\nREXASI-PRO, which aims to develop trustworthy artificial intelligence for\nsocial navigation of people with reduced mobility.\n","authors":["Carlo Grigioni","Franca Corradini","Alessandro Antonucci","Jérôme Guzzi","Francesco Flammini"],"pdf_url":"https://arxiv.org/pdf/2403.08984v1.pdf","comment":"14 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.08942v1","updated":"2024-03-13T20:16:46Z","published":"2024-03-13T20:16:46Z","title":"Collision-Free Platooning of Mobile Robots through a Set-Theoretic\n  Predictive Control Approach","summary":"  This paper proposes a control solution to achieve collision-free platooning\ncontrol of input-constrained mobile robots. The platooning policy is based on a\nleader-follower approach where the leader tracks a reference trajectory while\nfollowers track the leader's pose with an inter-agent delay. First, the leader\nand the follower kinematic models are feedback linearized and the platoon's\nerror dynamics and input constraints characterized. Then, a set-theoretic model\npredictive control strategy is proposed to address the platooning trajectory\ntracking control problem. An ad-hoc collision avoidance policy is also proposed\nto guarantee collision avoidance amongst the agents. Finally, the effectiveness\nof the proposed control architecture is validated through experiments performed\non a formation of Khepera IV differential drive robots\n","authors":["Suryaprakash Rajkumar","Cristian Tiriolo","Walter Lucia"],"pdf_url":"https://arxiv.org/pdf/2403.08942v1.pdf","comment":"Paper submitted for publication in the 2024 American Control\n  Conference (ACC)"},{"id":"http://arxiv.org/abs/2403.08936v1","updated":"2024-03-13T20:11:20Z","published":"2024-03-13T20:11:20Z","title":"Beyond Joint Demonstrations: Personalized Expert Guidance for Efficient\n  Multi-Agent Reinforcement Learning","summary":"  Multi-Agent Reinforcement Learning (MARL) algorithms face the challenge of\nefficient exploration due to the exponential increase in the size of the joint\nstate-action space. While demonstration-guided learning has proven beneficial\nin single-agent settings, its direct applicability to MARL is hindered by the\npractical difficulty of obtaining joint expert demonstrations. In this work, we\nintroduce a novel concept of personalized expert demonstrations, tailored for\neach individual agent or, more broadly, each individual type of agent within a\nheterogeneous team. These demonstrations solely pertain to single-agent\nbehaviors and how each agent can achieve personal goals without encompassing\nany cooperative elements, thus naively imitating them will not achieve\ncooperation due to potential conflicts. To this end, we propose an approach\nthat selectively utilizes personalized expert demonstrations as guidance and\nallows agents to learn to cooperate, namely personalized expert-guided MARL\n(PegMARL). This algorithm utilizes two discriminators: the first provides\nincentives based on the alignment of policy behavior with demonstrations, and\nthe second regulates incentives based on whether the behavior leads to the\ndesired objective. We evaluate PegMARL using personalized demonstrations in\nboth discrete and continuous environments. The results demonstrate that PegMARL\nlearns near-optimal policies even when provided with suboptimal demonstrations,\nand outperforms state-of-the-art MARL algorithms in solving coordinated tasks.\nWe also showcase PegMARL's capability to leverage joint demonstrations in the\nStarCraft scenario and converge effectively even with demonstrations from\nnon-co-trained policies.\n","authors":["Peihong Yu","Manav Mishra","Alec Koppel","Carl Busart","Priya Narayan","Dinesh Manocha","Amrit Bedi","Pratap Tokekar"],"pdf_url":"https://arxiv.org/pdf/2403.08936v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08928v1","updated":"2024-03-13T19:36:03Z","published":"2024-03-13T19:36:03Z","title":"Neuromorphic force-control in an industrial task: validating energy and\n  latency benefits","summary":"  As robots become smarter and more ubiquitous, optimizing the power\nconsumption of intelligent compute becomes imperative towards ensuring the\nsustainability of technological advancements. Neuromorphic computing hardware\nmakes use of biologically inspired neural architectures to achieve energy and\nlatency improvements compared to conventional von Neumann computing\narchitecture. Applying these benefits to robots has been demonstrated in\nseveral works in the field of neurorobotics, typically on relatively simple\ncontrol tasks. Here, we introduce an example of neuromorphic computing applied\nto the real-world industrial task of object insertion. We trained a spiking\nneural network (SNN) to perform force-torque feedback control using a\nreinforcement learning approach in simulation. We then ported the SNN to the\nIntel neuromorphic research chip Loihi interfaced with a KUKA robotic arm. At\ninference time we show latency competitive with current CPU/GPU architectures,\ntwo orders of magnitude less energy usage in comparison to traditional\nlow-energy edge-hardware. We offer this example as a proof of concept\nimplementation of a neuromoprhic controller in real-world robotic setting,\nhighlighting the benefits of neuromorphic hardware for the development of\nintelligent controllers for robots.\n","authors":["Camilo Amaya","Evan Eames","Gintautas Palinauskas","Alexander Perzylo","Yulia Sandamirskaya","Axel von Arnim"],"pdf_url":"https://arxiv.org/pdf/2403.08928v1.pdf","comment":"Submitted to IROS 2024"},{"id":"http://arxiv.org/abs/2403.08916v1","updated":"2024-03-13T19:16:10Z","published":"2024-03-13T19:16:10Z","title":"Rollover Prevention for Mobile Robots with Control Barrier Functions:\n  Differentiator-Based Adaptation and Projection-to-State Safety","summary":"  This paper develops rollover prevention guarantees for mobile robots using\ncontrol barrier function (CBF) theory, and demonstrates these formal results\nexperimentally. To this end, we consider a safety measure based on the zero\nmoment point to provide conditions on the control input through the lens of\nCBFs. However, these conditions depend on time-varying and noisy parameters. To\naddress this, we present a differentiator-based safety-critical controller that\nestimates these parameters and pairs Input-to-State Stable (ISS) differentiator\ndynamics with CBFs to achieve rigorous guarantees of safety. Additionally, to\nensure safety in the presence of disturbance, we utilize a time-varying\nextension of Projection-to-State Safety (PSSf). The effectiveness of the\nproposed method is demonstrated through experiments on a tracked robot with a\nrollover potential on steep slopes.\n","authors":["Ersin Das","Aaron D. Ames","Joel W. Burdick"],"pdf_url":"https://arxiv.org/pdf/2403.08916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08885v1","updated":"2024-03-13T18:12:53Z","published":"2024-03-13T18:12:53Z","title":"SLCF-Net: Sequential LiDAR-Camera Fusion for Semantic Scene Completion\n  using a 3D Recurrent U-Net","summary":"  We introduce SLCF-Net, a novel approach for the Semantic Scene Completion\n(SSC) task that sequentially fuses LiDAR and camera data. It jointly estimates\nmissing geometry and semantics in a scene from sequences of RGB images and\nsparse LiDAR measurements. The images are semantically segmented by a\npre-trained 2D U-Net and a dense depth prior is estimated from a\ndepth-conditioned pipeline fueled by Depth Anything. To associate the 2D image\nfeatures with the 3D scene volume, we introduce Gaussian-decay Depth-prior\nProjection (GDP). This module projects the 2D features into the 3D volume along\nthe line of sight with a Gaussian-decay function, centered around the depth\nprior. Volumetric semantics is computed by a 3D U-Net. We propagate the hidden\n3D U-Net state using the sensor motion and design a novel loss to ensure\ntemporal consistency. We evaluate our approach on the SemanticKITTI dataset and\ncompare it with leading SSC approaches. The SLCF-Net excels in all SSC metrics\nand shows great temporal consistency.\n","authors":["Helin Cao","Sven Behnke"],"pdf_url":"https://arxiv.org/pdf/2403.08885v1.pdf","comment":"2024 IEEE International Conference on Robotics and Automation\n  (ICRA2024), Yokohama, Japan, May 2024"},{"id":"http://arxiv.org/abs/2309.08770v2","updated":"2024-03-13T18:02:12Z","published":"2023-09-15T21:24:45Z","title":"Constrained Bimanual Planning with Analytic Inverse Kinematics","summary":"  In order for a bimanual robot to manipulate an object that is held by both\nhands, it must construct motion plans such that the transformation between its\nend effectors remains fixed. This amounts to complicated nonlinear equality\nconstraints in the configuration space, which are difficult for trajectory\noptimizers. In addition, the set of feasible configurations becomes a measure\nzero set, which presents a challenge to sampling-based motion planners. We\nleverage an analytic solution to the inverse kinematics problem to parametrize\nthe configuration space, resulting in a lower-dimensional representation where\nthe set of valid configurations has positive measure. We describe how to use\nthis parametrization with existing motion planning algorithms, including\nsampling-based approaches, trajectory optimizers, and techniques that plan\nthrough convex inner-approximations of collision-free space.\n","authors":["Thomas Cohn","Seiji Shaw","Max Simchowitz","Russ Tedrake"],"pdf_url":"https://arxiv.org/pdf/2309.08770v2.pdf","comment":"Accepted to ICRA 2024. 8 pages, 4 figures. Interactive results\n  available at https://cohnt.github.io/Bimanual-Web/index.html"}]},"2024-03-14T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2403.08605v2","updated":"2024-03-14T09:56:28Z","published":"2024-03-13T15:15:21Z","title":"Language-Grounded Dynamic Scene Graphs for Interactive Object Search\n  with Mobile Manipulation","summary":"  To fully leverage the capabilities of mobile manipulation robots, it is\nimperative that they are able to autonomously execute long-horizon tasks in\nlarge unexplored environments. While large language models (LLMs) have shown\nemergent reasoning skills on arbitrary tasks, existing work primarily\nconcentrates on explored environments, typically focusing on either navigation\nor manipulation tasks in isolation. In this work, we propose MoMa-LLM, a novel\napproach that grounds language models within structured representations derived\nfrom open-vocabulary scene graphs, dynamically updated as the environment is\nexplored. We tightly interleave these representations with an object-centric\naction space. The resulting approach is zero-shot, open-vocabulary, and readily\nextendable to a spectrum of mobile manipulation and household robotic tasks. We\ndemonstrate the effectiveness of MoMa-LLM in a novel semantic interactive\nsearch task in large realistic indoor environments. In extensive experiments in\nboth simulation and the real world, we show substantially improved search\nefficiency compared to conventional baselines and state-of-the-art approaches,\nas well as its applicability to more abstract tasks. We make the code publicly\navailable at http://moma-llm.cs.uni-freiburg.de.\n","authors":["Daniel Honerkamp","Martin Büchner","Fabien Despinoy","Tim Welschehold","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2403.08605v2.pdf","comment":"Project website: http://moma-llm.cs.uni-freiburg.de"},{"id":"http://arxiv.org/abs/2403.08106v2","updated":"2024-03-14T02:07:50Z","published":"2024-03-12T22:26:45Z","title":"V-PRISM: Probabilistic Mapping of Unknown Tabletop Scenes","summary":"  The ability to construct concise scene representations from sensor input is\ncentral to the field of robotics. This paper addresses the problem of robustly\ncreating a 3D representation of a tabletop scene from a segmented RGB-D image.\nThese representations are then critical for a range of downstream manipulation\ntasks. Many previous attempts to tackle this problem do not capture accurate\nuncertainty, which is required to subsequently produce safe motion plans. In\nthis paper, we cast the representation of 3D tabletop scenes as a multi-class\nclassification problem. To tackle this, we introduce V-PRISM, a framework and\nmethod for robustly creating probabilistic 3D segmentation maps of tabletop\nscenes. Our maps contain both occupancy estimates, segmentation information,\nand principled uncertainty measures. We evaluate the robustness of our method\nin (1) procedurally generated scenes using open-source object datasets, and (2)\nreal-world tabletop data collected from a depth camera. Our experiments show\nthat our approach outperforms alternative continuous reconstruction approaches\nthat do not explicitly reason about objects in a multi-class formulation.\n","authors":["Herbert Wright","Weiming Zhi","Matthew Johnson-Roberson","Tucker Hermans"],"pdf_url":"https://arxiv.org/pdf/2403.08106v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09637v1","updated":"2024-03-14T17:59:46Z","published":"2024-03-14T17:59:46Z","title":"GaussianGrasper: 3D Language Gaussian Splatting for Open-vocabulary\n  Robotic Grasping","summary":"  Constructing a 3D scene capable of accommodating open-ended language queries,\nis a pivotal pursuit, particularly within the domain of robotics. Such\ntechnology facilitates robots in executing object manipulations based on human\nlanguage directives. To tackle this challenge, some research efforts have been\ndedicated to the development of language-embedded implicit fields. However,\nimplicit fields (e.g. NeRF) encounter limitations due to the necessity of\nprocessing a large number of input views for reconstruction, coupled with their\ninherent inefficiencies in inference. Thus, we present the GaussianGrasper,\nwhich utilizes 3D Gaussian Splatting to explicitly represent the scene as a\ncollection of Gaussian primitives. Our approach takes a limited set of RGB-D\nviews and employs a tile-based splatting technique to create a feature field.\nIn particular, we propose an Efficient Feature Distillation (EFD) module that\nemploys contrastive learning to efficiently and accurately distill language\nembeddings derived from foundational models. With the reconstructed geometry of\nthe Gaussian field, our method enables the pre-trained grasping model to\ngenerate collision-free grasp pose candidates. Furthermore, we propose a\nnormal-guided grasp module to select the best grasp pose. Through comprehensive\nreal-world experiments, we demonstrate that GaussianGrasper enables robots to\naccurately query and grasp objects with language instructions, providing a new\nsolution for language-guided manipulation tasks. Data and codes can be\navailable at https://github.com/MrSecant/GaussianGrasper.\n","authors":["Yuhang Zheng","Xiangyu Chen","Yupeng Zheng","Songen Gu","Runyi Yang","Bu Jin","Pengfei Li","Chengliang Zhong","Zengmao Wang","Lina Liu","Chao Yang","Dawei Wang","Zhen Chen","Xiaoxiao Long","Meiqing Wang"],"pdf_url":"https://arxiv.org/pdf/2403.09637v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09631v1","updated":"2024-03-14T17:58:41Z","published":"2024-03-14T17:58:41Z","title":"3D-VLA: A 3D Vision-Language-Action Generative World Model","summary":"  Recent vision-language-action (VLA) models rely on 2D inputs, lacking\nintegration with the broader realm of the 3D physical world. Furthermore, they\nperform action prediction by learning a direct mapping from perception to\naction, neglecting the vast dynamics of the world and the relations between\nactions and dynamics. In contrast, human beings are endowed with world models\nthat depict imagination about future scenarios to plan actions accordingly. To\nthis end, we propose 3D-VLA by introducing a new family of embodied foundation\nmodels that seamlessly link 3D perception, reasoning, and action through a\ngenerative world model. Specifically, 3D-VLA is built on top of a 3D-based\nlarge language model (LLM), and a set of interaction tokens is introduced to\nengage with the embodied environment. Furthermore, to inject generation\nabilities into the model, we train a series of embodied diffusion models and\nalign them into the LLM for predicting the goal images and point clouds. To\ntrain our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by\nextracting vast 3D-related information from existing robotics datasets. Our\nexperiments on held-in datasets demonstrate that 3D-VLA significantly improves\nthe reasoning, multimodal generation, and planning capabilities in embodied\nenvironments, showcasing its potential in real-world applications.\n","authors":["Haoyu Zhen","Xiaowen Qiu","Peihao Chen","Jincheng Yang","Xin Yan","Yilun Du","Yining Hong","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2403.09631v1.pdf","comment":"Project page: https://vis-www.cs.umass.edu/3dvla/"},{"id":"http://arxiv.org/abs/2311.16592v2","updated":"2024-03-14T17:48:51Z","published":"2023-11-28T08:17:58Z","title":"RGBGrasp: Image-based Object Grasping by Capturing Multiple Views during\n  Robot Arm Movement with Neural Radiance Fields","summary":"  Robotic research encounters a significant hurdle when it comes to the\nintricate task of grasping objects that come in various shapes, materials, and\ntextures. Unlike many prior investigations that heavily leaned on specialized\npoint-cloud cameras or abundant RGB visual data to gather 3D insights for\nobject-grasping missions, this paper introduces a pioneering approach called\nRGBGrasp. This method depends on a limited set of RGB views to perceive the 3D\nsurroundings containing transparent and specular objects and achieve accurate\ngrasping. Our method utilizes pre-trained depth prediction models to establish\ngeometry constraints, enabling precise 3D structure estimation, even under\nlimited view conditions. Finally, we integrate hash encoding and a proposal\nsampler strategy to significantly accelerate the 3D reconstruction process.\nThese innovations significantly enhance the adaptability and effectiveness of\nour algorithm in real-world scenarios. Through comprehensive experimental\nvalidations, we demonstrate that RGBGrasp achieves remarkable success across a\nwide spectrum of object-grasping scenarios, establishing it as a promising\nsolution for real-world robotic manipulation tasks. The demonstrations of our\nmethod can be found on: https://sites.google.com/view/rgbgrasp\n","authors":["Chang Liu","Kejian Shi","Kaichen Zhou","Haoxiao Wang","Jiyao Zhang","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2311.16592v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09596v1","updated":"2024-03-14T17:36:16Z","published":"2024-03-14T17:36:16Z","title":"Scalable Autonomous Drone Flight in the Forest with Visual-Inertial SLAM\n  and Dense Submaps Built without LiDAR","summary":"  Forestry constitutes a key element for a sustainable future, while it is\nsupremely challenging to introduce digital processes to improve efficiency. The\nmain limitation is the difficulty of obtaining accurate maps at high temporal\nand spatial resolution as a basis for informed forestry decision-making, due to\nthe vast area forests extend over and the sheer number of trees. To address\nthis challenge, we present an autonomous Micro Aerial Vehicle (MAV) system\nwhich purely relies on cost-effective and light-weight passive visual and\ninertial sensors to perform under-canopy autonomous navigation. We leverage\nvisual-inertial simultaneous localization and mapping (VI-SLAM) for accurate\nMAV state estimates and couple it with a volumetric occupancy submapping system\nto achieve a scalable mapping framework which can be directly used for path\nplanning. As opposed to a monolithic map, submaps inherently deal with\ninevitable drift and corrections from VI-SLAM, since they move with pose\nestimates as they are updated. To ensure the safety of the MAV during\nnavigation, we also propose a novel reference trajectory anchoring scheme that\nmoves and deforms the reference trajectory the MAV is tracking upon state\nupdates from the VI-SLAM system in a consistent way, even upon large changes in\nstate estimates due to loop-closures. We thoroughly validate our system in both\nreal and simulated forest environments with high tree densities in excess of\n400 trees per hectare and at speeds up to 3 m/s - while not encountering a\nsingle collision or system failure. To the best of our knowledge this is the\nfirst system which achieves this level of performance in such unstructured\nenvironment using low-cost passive visual sensors and fully on-board\ncomputation including VI-SLAM.\n","authors":["Sebastián Barbas Laina","Simon Boche","Sotiris Papatheodorou","Dimos Tzoumanikas","Simon Schaefer","Hanzhi Chen","Stefan Leutenegger"],"pdf_url":"https://arxiv.org/pdf/2403.09596v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.05304v2","updated":"2024-03-14T17:22:59Z","published":"2024-03-08T13:33:00Z","title":"Spatiotemporal Predictive Pre-training for Robotic Motor Control","summary":"  Robotic motor control necessitates the ability to predict the dynamics of\nenvironments and interaction objects. However, advanced self-supervised\npre-trained visual representations (PVRs) in robotic motor control, leveraging\nlarge-scale egocentric videos, often focus solely on learning the static\ncontent features of sampled image frames. This neglects the crucial temporal\nmotion clues in human video data, which implicitly contain key knowledge about\nsequential interacting and manipulating with the environments and objects. In\nthis paper, we present a simple yet effective robotic motor control visual\npre-training framework that jointly performs spatiotemporal predictive learning\nutilizing large-scale video data, termed as STP. Our STP samples paired frames\nfrom video clips. It adheres to two key designs in a multi-task learning\nmanner. First, we perform spatial prediction on the masked current frame for\nlearning content features. Second, we utilize the future frame with an\nextremely high masking ratio as a condition, based on the masked current frame,\nto conduct temporal prediction of future frame for capturing motion features.\nThese efficient designs ensure that our representation focusing on motion\ninformation while capturing spatial details. We carry out the largest-scale\nevaluation of PVRs for robotic motor control to date, which encompasses 21\ntasks within a real-world Franka robot arm and 5 simulated environments.\nExtensive experiments demonstrate the effectiveness of STP as well as unleash\nits generality and data efficiency by further post-pre-training and hybrid\npre-training.\n","authors":["Jiange Yang","Bei Liu","Jianlong Fu","Bocheng Pan","Gangshan Wu","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.05304v2.pdf","comment":"25 pages, 6 figures, 11 tables"},{"id":"http://arxiv.org/abs/2403.09583v1","updated":"2024-03-14T17:18:15Z","published":"2024-03-14T17:18:15Z","title":"ExploRLLM: Guiding Exploration in Reinforcement Learning with Large\n  Language Models","summary":"  In image-based robot manipulation tasks with large observation and action\nspaces, reinforcement learning struggles with low sample efficiency, slow\ntraining speed, and uncertain convergence. As an alternative, large pre-trained\nfoundation models have shown promise in robotic manipulation, particularly in\nzero-shot and few-shot applications. However, using these models directly is\nunreliable due to limited reasoning capabilities and challenges in\nunderstanding physical and spatial contexts. This paper introduces ExploRLLM, a\nnovel approach that leverages the inductive bias of foundation models (e.g.\nLarge Language Models) to guide exploration in reinforcement learning. We also\nexploit these foundation models to reformulate the action and observation\nspaces to enhance the training efficiency in reinforcement learning. Our\nexperiments demonstrate that guided exploration enables much quicker\nconvergence than training without it. Additionally, we validate that ExploRLLM\noutperforms vanilla foundation model baselines and that the policy trained in\nsimulation can be applied in real-world settings without additional training.\n","authors":["Runyu Ma","Jelle Luijkx","Zlatan Ajanovic","Jens Kober"],"pdf_url":"https://arxiv.org/pdf/2403.09583v1.pdf","comment":"8 pages,8 figures, conference IROS 2024"},{"id":"http://arxiv.org/abs/2310.01412v4","updated":"2024-03-14T17:05:43Z","published":"2023-10-02T17:59:52Z","title":"DriveGPT4: Interpretable End-to-end Autonomous Driving via Large\n  Language Model","summary":"  Multimodal large language models (MLLMs) have emerged as a prominent area of\ninterest within the research community, given their proficiency in handling and\nreasoning with non-textual data, including images and videos. This study seeks\nto extend the application of MLLMs to the realm of autonomous driving by\nintroducing DriveGPT4, a novel interpretable end-to-end autonomous driving\nsystem based on LLMs. Capable of processing multi-frame video inputs and\ntextual queries, DriveGPT4 facilitates the interpretation of vehicle actions,\noffers pertinent reasoning, and effectively addresses a diverse range of\nquestions posed by users. Furthermore, DriveGPT4 predicts low-level vehicle\ncontrol signals in an end-to-end fashion. These advanced capabilities are\nachieved through the utilization of a bespoke visual instruction tuning\ndataset, specifically tailored for autonomous driving applications, in\nconjunction with a mix-finetuning training strategy. DriveGPT4 represents the\npioneering effort to leverage LLMs for the development of an interpretable\nend-to-end autonomous driving solution. Evaluations conducted on the BDD-X\ndataset showcase the superior qualitative and quantitative performance of\nDriveGPT4. Additionally, the fine-tuning of domain-specific data enables\nDriveGPT4 to yield close or even improved results in terms of autonomous\ndriving grounding when contrasted with GPT4-V. The code and dataset will be\npublicly available.\n","authors":["Zhenhua Xu","Yujia Zhang","Enze Xie","Zhen Zhao","Yong Guo","Kwan-Yee. K. Wong","Zhenguo Li","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2310.01412v4.pdf","comment":"The project page is available at\n  https://tonyxuqaq.github.io/projects/DriveGPT4/"},{"id":"http://arxiv.org/abs/2403.09571v1","updated":"2024-03-14T17:00:29Z","published":"2024-03-14T17:00:29Z","title":"Are you a robot? Detecting Autonomous Vehicles from Behavior Analysis","summary":"  The tremendous hype around autonomous driving is eagerly calling for emerging\nand novel technologies to support advanced mobility use cases. As car\nmanufactures keep developing SAE level 3+ systems to improve the safety and\ncomfort of passengers, traffic authorities need to establish new procedures to\nmanage the transition from human-driven to fully-autonomous vehicles while\nproviding a feedback-loop mechanism to fine-tune envisioned autonomous systems.\nThus, a way to automatically profile autonomous vehicles and differentiate\nthose from human-driven ones is a must. In this paper, we present a\nfully-fledged framework that monitors active vehicles using camera images and\nstate information in order to determine whether vehicles are autonomous,\nwithout requiring any active notification from the vehicles themselves.\nEssentially, it builds on the cooperation among vehicles, which share their\ndata acquired on the road feeding a machine learning model to identify\nautonomous cars. We extensively tested our solution and created the NexusStreet\ndataset, by means of the CARLA simulator, employing an autonomous driving\ncontrol agent and a steering wheel maneuvered by licensed drivers. Experiments\nshow it is possible to discriminate the two behaviors by analyzing video clips\nwith an accuracy of 80%, which improves up to 93% when the target state\ninformation is available. Lastly, we deliberately degraded the state to observe\nhow the framework performs under non-ideal data collection conditions.\n","authors":["Fabio Maresca","Filippo Grazioli","Antonio Albanese","Vincenzo Sciancalepore","Gianpiero Negri","Xavier Costa-Perez"],"pdf_url":"https://arxiv.org/pdf/2403.09571v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09567v1","updated":"2024-03-14T16:57:18Z","published":"2024-03-14T16:57:18Z","title":"Enhancing Trust in Autonomous Agents: An Architecture for Accountability\n  and Explainability through Blockchain and Large Language Models","summary":"  The deployment of autonomous agents in environments involving human\ninteraction has increasingly raised security concerns. Consequently,\nunderstanding the circumstances behind an event becomes critical, requiring the\ndevelopment of capabilities to justify their behaviors to non-expert users.\nSuch explanations are essential in enhancing trustworthiness and safety, acting\nas a preventive measure against failures, errors, and misunderstandings.\nAdditionally, they contribute to improving communication, bridging the gap\nbetween the agent and the user, thereby improving the effectiveness of their\ninteractions. This work presents an accountability and explainability\narchitecture implemented for ROS-based mobile robots. The proposed solution\nconsists of two main components. Firstly, a black box-like element to provide\naccountability, featuring anti-tampering properties achieved through blockchain\ntechnology. Secondly, a component in charge of generating natural language\nexplanations by harnessing the capabilities of Large Language Models (LLMs)\nover the data contained within the previously mentioned black box. The study\nevaluates the performance of our solution in three different scenarios, each\ninvolving autonomous agent navigation functionalities. This evaluation includes\na thorough examination of accountability and explainability metrics,\ndemonstrating the effectiveness of our approach in using accountable data from\nrobot actions to obtain coherent, accurate and understandable explanations,\neven when facing challenges inherent in the use of autonomous agents in\nreal-world scenarios.\n","authors":["Laura Fernández-Becerra","Miguel Ángel González-Santamarta","Ángel Manuel Guerrero-Higueras","Francisco Javier Rodríguez-Lera","Vicente Matellán Olivera"],"pdf_url":"https://arxiv.org/pdf/2403.09567v1.pdf","comment":"21 pages, 12 figures"},{"id":"http://arxiv.org/abs/2403.09566v1","updated":"2024-03-14T16:56:56Z","published":"2024-03-14T16:56:56Z","title":"PaperBot: Learning to Design Real-World Tools Using Paper","summary":"  Paper is a cheap, recyclable, and clean material that is often used to make\npractical tools. Traditional tool design either relies on simulation or\nphysical analysis, which is often inaccurate and time-consuming. In this paper,\nwe propose PaperBot, an approach that directly learns to design and use a tool\nin the real world using paper without human intervention. We demonstrated the\neffectiveness and efficiency of PaperBot on two tool design tasks: 1. learning\nto fold and throw paper airplanes for maximum travel distance 2. learning to\ncut paper into grippers that exert maximum gripping force. We present a\nself-supervised learning framework that learns to perform a sequence of\nfolding, cutting, and dynamic manipulation actions in order to optimize the\ndesign and use of a tool. We deploy our system to a real-world two-arm robotic\nsystem to solve challenging design tasks that involve aerodynamics (paper\nairplane) and friction (paper gripper) that are impossible to simulate\naccurately.\n","authors":["Ruoshi Liu","Junbang Liang","Sruthi Sudhakar","Huy Ha","Cheng Chi","Shuran Song","Carl Vondrick"],"pdf_url":"https://arxiv.org/pdf/2403.09566v1.pdf","comment":"Project Website: https://paperbot.cs.columbia.edu/"},{"id":"http://arxiv.org/abs/2403.01977v2","updated":"2024-03-14T16:30:48Z","published":"2024-03-04T12:20:29Z","title":"TTA-Nav: Test-time Adaptive Reconstruction for Point-Goal Navigation\n  under Visual Corruptions","summary":"  Robot navigation under visual corruption presents a formidable challenge. To\naddress this, we propose a Test-time Adaptation (TTA) method, named as TTA-Nav,\nfor point-goal navigation under visual corruptions. Our \"plug-and-play\" method\nincorporates a top-down decoder to a pre-trained navigation model. Firstly, the\npre-trained navigation model gets a corrupted image and extracts features.\nSecondly, the top-down decoder produces the reconstruction given the high-level\nfeatures extracted by the pre-trained model. Then, it feeds the reconstruction\nof a corrupted image back to the pre-trained model. Finally, the pre-trained\nmodel does forward pass again to output action. Despite being trained solely on\nclean images, the top-down decoder can reconstruct cleaner images from\ncorrupted ones without the need for gradient-based adaptation. The pre-trained\nnavigation model with our top-down decoder significantly enhances navigation\nperformance across almost all visual corruptions in our benchmarks. Our method\nimproves the success rate of point-goal navigation from the state-of-the-art\nresult of 46% to 94% on the most severe corruption. This suggests its potential\nfor broader application in robotic visual navigation. Project page:\nhttps://sites.google.com/view/tta-nav\n","authors":["Maytus Piriyajitakonkij","Mingfei Sun","Mengmi Zhang","Wei Pan"],"pdf_url":"https://arxiv.org/pdf/2403.01977v2.pdf","comment":"Submitted to IROS2024"},{"id":"http://arxiv.org/abs/2403.09504v1","updated":"2024-03-14T15:48:07Z","published":"2024-03-14T15:48:07Z","title":"Is Data All That Matters? The Role of Control Frequency for\n  Learning-Based Sampled-Data Control of Uncertain Systems","summary":"  Learning models or control policies from data has become a powerful tool to\nimprove the performance of uncertain systems. While a strong focus has been\nplaced on increasing the amount and quality of data to improve performance,\ndata can never fully eliminate uncertainty, making feedback necessary to ensure\nstability and performance. We show that the control frequency at which the\ninput is recalculated is a crucial design parameter, yet it has hardly been\nconsidered before. We address this gap by combining probabilistic model\nlearning and sampled-data control. We use Gaussian processes (GPs) to learn a\ncontinuous-time model and compute a corresponding discrete-time controller. The\nresult is an uncertain sampled-data control system, for which we derive robust\nstability conditions. We formulate semidefinite programs to compute the minimum\ncontrol frequency required for stability and to optimize performance. As a\nresult, our approach enables us to study the effect of both control frequency\nand data on stability and closed-loop performance. We show in numerical\nsimulations of a quadrotor that performance can be improved by increasing\neither the amount of data or the control frequency, and that we can trade off\none for the other. For example, by increasing the control frequency by 33%, we\ncan reduce the number of data points by half while still achieving similar\nperformance.\n","authors":["Ralf Römer","Lukas Brunke","Siqi Zhou","Angela P. Schoellig"],"pdf_url":"https://arxiv.org/pdf/2403.09504v1.pdf","comment":"Accepted to the 2024 American Control Conference (ACC), 7 pages, 4\n  figures, code is available at https://github.com/ralfroemer99/lb_sd"},{"id":"http://arxiv.org/abs/2403.03995v3","updated":"2024-03-14T15:43:10Z","published":"2024-03-06T19:11:08Z","title":"Cafe-Mpc: A Cascaded-Fidelity Model Predictive Control Framework with\n  Tuning-Free Whole-Body Control","summary":"  This work introduces an optimization-based locomotion control framework for\non-the-fly synthesis of complex dynamic maneuvers. At the core of the proposed\nframework is a cascaded-fidelity model predictive controller (Cafe-Mpc).\nCafe-Mpc strategically relaxes the planning problem along the prediction\nhorizon (i.e., with descending model fidelity, increasingly coarse time steps,\nand relaxed constraints) for computational and performance gains. This problem\nis numerically solved with an efficient customized multiple-shooting iLQR\n(MS-iLQR) solver that is tailored for hybrid systems. The action-value function\nfrom Cafe-Mpc is then used as the basis for a new value-function-based\nwhole-body control (VWBC) technique that avoids additional tuning for the WBC.\nIn this respect, the proposed framework unifies whole-body MPC and more\nconventional whole-body quadratic programming (QP), which have been treated as\nseparate components in previous works. We study the effects of the cascaded\nrelaxations in Cafe-Mpc on the tracking performance and required computation\ntime. We also show that the Cafe-Mpc, if configured appropriately, advances the\nperformance of whole-body MPC without necessarily increasing computational\ncost. Further, we show the superior performance of the proposed VWBC over the\nRiccati feedback controller in terms of constraint handling. The proposed\nframework enables accomplishing for the first time gymnastic-style running\nbarrel rolls on the MIT Mini Cheetah. Video: https://youtu.be/YiNqrgj9mb8.\n","authors":["He Li","Patrick M. Wensing"],"pdf_url":"https://arxiv.org/pdf/2403.03995v3.pdf","comment":"submitted to IEEE Transactions on Robotics. 20 pages, 18 figures"},{"id":"http://arxiv.org/abs/2310.13091v2","updated":"2024-03-14T15:28:09Z","published":"2023-10-19T18:37:13Z","title":"From Propeller Damage Estimation and Adaptation to Fault Tolerant\n  Control: Enhancing Quadrotor Resilience","summary":"  Aerial robots are required to remain operational even in the event of system\ndisturbances, damages, or failures to ensure resilient and robust task\ncompletion and safety. One common failure case is propeller damage, which\npresents a significant challenge in both quantification and compensation. We\npropose a novel adaptive control scheme capable of detecting and compensating\nfor multi-rotor propeller damages, ensuring safe and robust flight\nperformances. Our control scheme includes an L1 adaptive controller for damage\ninference and compensation of single or dual propellers, with the capability to\nseamlessly transition to a fault-tolerant solution in case the damage becomes\nsevere. We experimentally identify the conditions under which the L1 adaptive\nsolution remains preferable over a fault-tolerant alternative. Experimental\nresults validate the proposed approach, demonstrating its effectiveness in\nrunning the adaptive strategy in real time on a quadrotor even in case of\ndamage to multiple propellers.\n","authors":["Jeffrey Mao","Jennifer Yeom","Suraj Nair","Giuseppe Loianno"],"pdf_url":"https://arxiv.org/pdf/2310.13091v2.pdf","comment":"8 Pages, 8 Figures"},{"id":"http://arxiv.org/abs/2403.09477v1","updated":"2024-03-14T15:19:19Z","published":"2024-03-14T15:19:19Z","title":"VIRUS-NeRF -- Vision, InfraRed and UltraSonic based Neural Radiance\n  Fields","summary":"  Autonomous mobile robots are an increasingly integral part of modern factory\nand warehouse operations. Obstacle detection, avoidance and path planning are\ncritical safety-relevant tasks, which are often solved using expensive LiDAR\nsensors and depth cameras. We propose to use cost-effective low-resolution\nranging sensors, such as ultrasonic and infrared time-of-flight sensors by\ndeveloping VIRUS-NeRF - Vision, InfraRed, and UltraSonic based Neural Radiance\nFields. Building upon Instant Neural Graphics Primitives with a Multiresolution\nHash Encoding (Instant-NGP), VIRUS-NeRF incorporates depth measurements from\nultrasonic and infrared sensors and utilizes them to update the occupancy grid\nused for ray marching. Experimental evaluation in 2D demonstrates that\nVIRUS-NeRF achieves comparable mapping performance to LiDAR point clouds\nregarding coverage. Notably, in small environments, its accuracy aligns with\nthat of LiDAR measurements, while in larger ones, it is bounded by the utilized\nultrasonic sensors. An in-depth ablation study reveals that adding ultrasonic\nand infrared sensors is highly effective when dealing with sparse data and low\nview variation. Further, the proposed occupancy grid of VIRUS-NeRF improves the\nmapping capabilities and increases the training speed by 46% compared to\nInstant-NGP. Overall, VIRUS-NeRF presents a promising approach for\ncost-effective local mapping in mobile robotics, with potential applications in\nsafety and navigation tasks. The code can be found at\nhttps://github.com/ethz-asl/virus nerf.\n","authors":["Nicolaj Schmid","Cornelius von Einem","Cesar Cadena","Roland Siegwart","Lorenz Hruby","Florian Tschopp"],"pdf_url":"https://arxiv.org/pdf/2403.09477v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09459v1","updated":"2024-03-14T15:02:24Z","published":"2024-03-14T15:02:24Z","title":"Development of control algorithms for mobile robotics focused on their\n  potential use for FPGA-based robots","summary":"  This paper investigates the development and optimization of control\nalgorithms for mobile robotics, with a keen focus on their implementation in\nField-Programmable Gate Arrays (FPGAs). It delves into both classical control\napproaches such as PID and modern techniques including deep learning,\naddressing their application in sectors ranging from industrial automation to\nmedical care. The study highlights the practical challenges and advancements in\nembedding these algorithms into FPGAs, which offer significant benefits for\nmobile robotics due to their high-speed processing and parallel computation\ncapabilities. Through an analysis of various control strategies, the paper\nshowcases the improvements in robot performance, particularly in navigation and\nobstacle avoidance. It emphasizes the critical role of FPGAs in enhancing the\nefficiency and adaptability of control algorithms in dynamic environments.\nAdditionally, the research discusses the difficulties in benchmarking and\nevaluating the performance of these algorithms in real-world applications,\nsuggesting a need for standardized evaluation criteria. The contribution of\nthis work lies in its comprehensive examination of control algorithms'\npotential in FPGA-based mobile robotics, offering insights into future research\ndirections for improving robotic autonomy and operational efficiency.\n","authors":["Andrés-David Suárez-Gómez","Andres A. Hernandez Ortega"],"pdf_url":"https://arxiv.org/pdf/2403.09459v1.pdf","comment":"10 pages, 1 figure"},{"id":"http://arxiv.org/abs/2310.09883v2","updated":"2024-03-14T14:40:15Z","published":"2023-10-15T16:42:14Z","title":"Zero-Shot Object Goal Visual Navigation With Class-Independent\n  Relationship Network","summary":"  This paper investigates the zero-shot object goal visual navigation problem.\nIn the object goal visual navigation task, the agent needs to locate navigation\ntargets from its egocentric visual input. \"Zero-shot\" means that the target the\nagent needs to find is not trained during the training phase. To address the\nissue of coupling navigation ability with target features during training, we\npropose the Class-Independent Relationship Network (CIRN). This method combines\ntarget detection information with the relative semantic similarity between the\ntarget and the navigation target, and constructs a brand new state\nrepresentation based on similarity ranking, this state representation does not\ninclude target feature or environment feature, effectively decoupling the\nagent's navigation ability from target features. And a Graph Convolutional\nNetwork (GCN) is employed to learn the relationships between different objects\nbased on their similarities. During testing, our approach demonstrates strong\ngeneralization capabilities, including zero-shot navigation tasks with\ndifferent targets and environments. Through extensive experiments in the\nAI2-THOR virtual environment, our method outperforms the current\nstate-of-the-art approaches in the zero-shot object goal visual navigation\ntask. Furthermore, we conducted experiments in more challenging cross-target\nand cross-scene settings, which further validate the robustness and\ngeneralization ability of our method. Our code is available at:\nhttps://github.com/SmartAndCleverRobot/ICRA-CIRN.\n","authors":["Xinting Li","Shiguang Zhang","Yue LU","Kerry Dang","Lingyan Ran"],"pdf_url":"https://arxiv.org/pdf/2310.09883v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09412v1","updated":"2024-03-14T14:03:29Z","published":"2024-03-14T14:03:29Z","title":"OpenGraph: Open-Vocabulary Hierarchical 3D Graph Representation in\n  Large-Scale Outdoor Environments","summary":"  Environment maps endowed with sophisticated semantics are pivotal for\nfacilitating seamless interaction between robots and humans, enabling them to\neffectively carry out various tasks. Open-vocabulary maps, powered by\nVisual-Language models (VLMs), possess inherent advantages, including\nmultimodal retrieval and open-set classes. However, existing open-vocabulary\nmaps are constrained to closed indoor scenarios and VLM features, thereby\ndiminishing their usability and inference capabilities. Moreover, the absence\nof topological relationships further complicates the accurate querying of\nspecific instances. In this work, we propose OpenGraph, a representation of\nopen-vocabulary hierarchical graph structure designed for large-scale outdoor\nenvironments. OpenGraph initially extracts instances and their captions from\nvisual images using 2D foundation models, encoding the captions with features\nto enhance textual reasoning. Subsequently, 3D incremental panoramic mapping\nwith feature embedding is achieved by projecting images onto LiDAR point\nclouds. Finally, the environment is segmented based on lane graph connectivity\nto construct a hierarchical graph. Validation results from real public dataset\nSemanticKITTI demonstrate that, even without fine-tuning the models, OpenGraph\nexhibits the ability to generalize to novel semantic classes and achieve the\nhighest segmentation and query accuracy. The source code of OpenGraph is\npublicly available at https://github.com/BIT-DYN/OpenGraph.\n","authors":["Yinan Deng","Jiahui Wang","Jingyu Zhao","Xinyu Tian","Guangyan Chen","Yi Yang","Yufeng Yue"],"pdf_url":"https://arxiv.org/pdf/2403.09412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01450v2","updated":"2024-03-14T13:54:56Z","published":"2024-03-03T09:08:07Z","title":"Collision-Free Robot Navigation in Crowded Environments using Learning\n  based Convex Model Predictive Control","summary":"  Navigating robots safely and efficiently in crowded and complex environments\nremains a significant challenge. However, due to the dynamic and intricate\nnature of these settings, planning efficient and collision-free paths for\nrobots to track is particularly difficult. In this paper, we uniquely bridge\nthe robot's perception, decision-making and control processes by utilizing the\nconvex obstacle-free region computed from 2D LiDAR data. The overall pipeline\nis threefold: (1) We proposes a robot navigation framework that utilizes deep\nreinforcement learning (DRL), conceptualizing the observation as the convex\nobstacle-free region, a departure from general reliance on raw sensor inputs.\n(2) We design the action space, derived from the intersection of the robot's\nkinematic limits and the convex region, to enable efficient sampling of\ninherently collision-free reference points. These actions assists in guiding\nthe robot to move towards the goal and interact with other obstacles during\nnavigation. (3) We employ model predictive control (MPC) to track the\ntrajectory formed by the reference points while satisfying constraints imposed\nby the convex obstacle-free region and the robot's kinodynamic limits. The\neffectiveness of proposed improvements has been validated through two sets of\nablation studies and a comparative experiment against the Timed Elastic Band\n(TEB), demonstrating improved navigation performance in crowded and complex\nenvironments.\n","authors":["Zhuanglei Wen","Mingze Dong","Xiai Chen"],"pdf_url":"https://arxiv.org/pdf/2403.01450v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10491v4","updated":"2024-03-14T12:06:10Z","published":"2023-09-19T09:59:08Z","title":"DCPT: Darkness Clue-Prompted Tracking in Nighttime UAVs","summary":"  Existing nighttime unmanned aerial vehicle (UAV) trackers follow an\n\"Enhance-then-Track\" architecture - first using a light enhancer to brighten\nthe nighttime video, then employing a daytime tracker to locate the object.\nThis separate enhancement and tracking fails to build an end-to-end trainable\nvision system. To address this, we propose a novel architecture called Darkness\nClue-Prompted Tracking (DCPT) that achieves robust UAV tracking at night by\nefficiently learning to generate darkness clue prompts. Without a separate\nenhancer, DCPT directly encodes anti-dark capabilities into prompts using a\ndarkness clue prompter (DCP). Specifically, DCP iteratively learns emphasizing\nand undermining projections for darkness clues. It then injects these learned\nvisual prompts into a daytime tracker with fixed parameters across transformer\nlayers. Moreover, a gated feature aggregation mechanism enables adaptive fusion\nbetween prompts and between prompts and the base model. Extensive experiments\nshow state-of-the-art performance for DCPT on multiple dark scenario\nbenchmarks. The unified end-to-end learning of enhancement and tracking in DCPT\nenables a more trainable system. The darkness clue prompting efficiently\ninjects anti-dark knowledge without extra modules. Code is available at\nhttps://github.com/bearyi26/DCPT.\n","authors":["Jiawen Zhu","Huayi Tang","Zhi-Qi Cheng","Jun-Yan He","Bin Luo","Shihao Qiu","Shengming Li","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2309.10491v4.pdf","comment":"Accepted by ICRA2024"},{"id":"http://arxiv.org/abs/2403.09309v1","updated":"2024-03-14T11:59:32Z","published":"2024-03-14T11:59:32Z","title":"MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences\n  using Attention-based Temporal Fusion","summary":"  Cluttered bin-picking environments are challenging for pose estimation\nmodels. Despite the impressive progress enabled by deep learning, single-view\nRGB pose estimation models perform poorly in cluttered dynamic environments.\nImbuing the rich temporal information contained in the video of scenes has the\npotential to enhance models ability to deal with the adverse effects of\nocclusion and the dynamic nature of the environments. Moreover, joint object\ndetection and pose estimation models are better suited to leverage the\nco-dependent nature of the tasks for improving the accuracy of both tasks. To\nthis end, we propose attention-based temporal fusion for multi-object 6D pose\nestimation that accumulates information across multiple frames of a video\nsequence. Our MOTPose method takes a sequence of images as input and performs\njoint object detection and pose estimation for all objects in one forward pass.\nIt learns to aggregate both object embeddings and object parameters over\nmultiple time steps using cross-attention-based fusion modules. We evaluate our\nmethod on the physically-realistic cluttered bin-picking dataset SynPick and\nthe YCB-Video dataset and demonstrate improved pose estimation accuracy as well\nas better object detection accuracy\n","authors":["Arul Selvam Periyasamy","Sven Behnke"],"pdf_url":"https://arxiv.org/pdf/2403.09309v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09308v1","updated":"2024-03-14T11:59:07Z","published":"2024-03-14T11:59:07Z","title":"Enabling Waypoint Generation for Collaborative Robots using LLMs and\n  Mixed Reality","summary":"  Programming a robotic is a complex task, as it demands the user to have a\ngood command of specific programming languages and awareness of the robot's\nphysical constraints. We propose a framework that simplifies robot deployment\nby allowing direct communication using natural language. It uses large language\nmodels (LLM) for prompt processing, workspace understanding, and waypoint\ngeneration. It also employs Augmented Reality (AR) to provide visual feedback\nof the planned outcome. We showcase the effectiveness of our framework with a\nsimple pick-and-place task, which we implement on a real robot. Moreover, we\npresent an early concept of expressive robot behavior and skill generation that\ncan be used to communicate with the user and learn new skills (e.g., object\ngrasping).\n","authors":["Cathy Mengying Fang","Krzysztof Zieliński","Pattie Maes","Joe Paradiso","Bruce Blumberg","Mikkel Baun Kjærgaard"],"pdf_url":"https://arxiv.org/pdf/2403.09308v1.pdf","comment":"Submitted to VLMNM 2024 - Workshop, ICRA 2024. This work has been\n  submitted to the IEEE for possible publication. Copyright may be transferred\n  without notice, after which this version may no longer be accessible"},{"id":"http://arxiv.org/abs/2403.09305v1","updated":"2024-03-14T11:56:13Z","published":"2024-03-14T11:56:13Z","title":"Pushing in the Dark: A Reactive Pushing Strategy for Mobile Robots Using\n  Tactile Feedback","summary":"  For mobile robots, navigating cluttered or dynamic environments often\nnecessitates non-prehensile manipulation, particularly when faced with objects\nthat are too large, irregular, or fragile to grasp. The unpredictable behavior\nand varying physical properties of these objects significantly complicate\nmanipulation tasks. To address this challenge, this manuscript proposes a novel\nReactive Pushing Strategy. This strategy allows a mobile robot to dynamically\nadjust its base movements in real-time to achieve successful pushing maneuvers\ntowards a target location. Notably, our strategy adapts the robot motion based\non changes in contact location obtained through the tactile sensor covering the\nbase, avoiding dependence on object-related assumptions and its modeled\nbehavior. The effectiveness of the Reactive Pushing Strategy was initially\nevaluated in the simulation environment, where it significantly outperformed\nthe compared baseline approaches. Following this, we validated the proposed\nstrategy through real-world experiments, demonstrating the robot capability to\npush objects to the target points located in the entire vicinity of the robot.\nIn both simulation and real-world experiments, the object-specific properties\n(shape, mass, friction, inertia) were altered along with the changes in target\nlocations to assess the robustness of the proposed method comprehensively.\n","authors":["Idil Ozdamar","Doganay Sirintuna","Robin Arbaud","Arash Ajoudani"],"pdf_url":"https://arxiv.org/pdf/2403.09305v1.pdf","comment":"8 pages, 7 figures, submitted to IEEE Robotics and Automation\n  Letters, for associated video, see https://youtu.be/IuGxlNe246M"},{"id":"http://arxiv.org/abs/2403.09285v1","updated":"2024-03-14T11:12:16Z","published":"2024-03-14T11:12:16Z","title":"THÖR-MAGNI: A Large-scale Indoor Motion Capture Recording of Human\n  Movement and Robot Interaction","summary":"  We present a new large dataset of indoor human and robot navigation and\ninteraction, called TH\\\"OR-MAGNI, that is designed to facilitate research on\nsocial navigation: e.g., modelling and predicting human motion, analyzing\ngoal-oriented interactions between humans and robots, and investigating visual\nattention in a social interaction context. TH\\\"OR-MAGNI was created to fill a\ngap in available datasets for human motion analysis and HRI. This gap is\ncharacterized by a lack of comprehensive inclusion of exogenous factors and\nessential target agent cues, which hinders the development of robust models\ncapable of capturing the relationship between contextual cues and human\nbehavior in different scenarios. Unlike existing datasets, TH\\\"OR-MAGNI\nincludes a broader set of contextual features and offers multiple scenario\nvariations to facilitate factor isolation. The dataset includes many social\nhuman-human and human-robot interaction scenarios, rich context annotations,\nand multi-modal data, such as walking trajectories, gaze tracking data, and\nlidar and camera streams recorded from a mobile robot. We also provide a set of\ntools for visualization and processing of the recorded data. TH\\\"OR-MAGNI is,\nto the best of our knowledge, unique in the amount and diversity of sensor data\ncollected in a contextualized and socially dynamic environment, capturing\nnatural human-robot interactions.\n","authors":["Tim Schreiter","Tiago Rodrigues de Almeida","Yufei Zhu","Eduardo Gutierrez Maestro","Lucas Morillo-Mendez","Andrey Rudenko","Luigi Palmieri","Tomasz P. Kucner","Martin Magnusson","Achim J. Lilienthal"],"pdf_url":"https://arxiv.org/pdf/2403.09285v1.pdf","comment":"Submitted to The International Journal of Robotics Research (IJRR) on\n  28 of February 2024"},{"id":"http://arxiv.org/abs/2303.02407v3","updated":"2024-03-14T10:48:06Z","published":"2023-03-04T12:56:15Z","title":"Local Path Planning among Pushable Objects based on Reinforcement\n  Learning","summary":"  In this paper, we introduce a method to deal with the problem of robot local\npath planning among pushable objects -- an open problem in robotics. In\nparticular, we achieve that by training multiple agents simultaneously in a\nphysics-based simulation environment, utilizing an Advantage Actor-Critic\nalgorithm coupled with a deep neural network. The developed online policy\nenables these agents to push obstacles in ways that are not limited to axial\nalignments, adapt to unforeseen changes in obstacle dynamics instantaneously,\nand effectively tackle local path planning in confined areas. We tested the\nmethod in various simulated environments to prove the adaptation effectiveness\nto various unseen scenarios in unfamiliar settings. Moreover, we have\nsuccessfully applied this policy on an actual quadruped robot, confirming its\ncapability to handle the unpredictability and noise associated with real-world\nsensors and the inherent uncertainties present in unexplored object pushing\ntasks.\n","authors":["Linghong Yao","Valerio Modugno","Andromachi Maria Delfaki","Yuanchang Liu","Danail Stoyanov","Dimitrios Kanoulas"],"pdf_url":"https://arxiv.org/pdf/2303.02407v3.pdf","comment":"7 pages, 7 figures, 4 tables"},{"id":"http://arxiv.org/abs/2304.08772v3","updated":"2024-03-14T10:05:34Z","published":"2023-04-18T07:06:07Z","title":"Multi-robot Motion Planning based on Nets-within-Nets Modeling and\n  Simulation","summary":"  This paper focuses on designing motion plans for a heterogeneous team of\nrobots that has to cooperate in fulfilling a global mission. The robots move in\nan environment containing some regions of interest, and the specification for\nthe whole team can include avoidances, visits, or sequencing when entering\nthese regions of interest. The specification is expressed in terms of a Petri\nnet corresponding to an automaton, while each robot is also modeled by a state\nmachine Petri net. With respect to existing solutions for related problems, the\ncurrent work brings the following contributions. First, we propose a novel\nmodel, denoted {High-Level robot team Petri Net (HLPN) system, for\nincorporating the specification and the robot models into the Nets-within-Nets\nparadigm. A guard function, named Global Enabling Function (gef), is designed\nto synchronize the firing of transitions such that the robot motions do not\nviolate the specification. Then, the solution is found by simulating the HPLN\nsystem in a specific software tool that accommodates Nets-within-Nets. An\nillustrative example based on a Linear Temporal Logic (LTL) mission is\ndescribed throughout the paper, complementing the proposed rationale of the\nframework.\n","authors":["Sofia Hustiu","Eva Robillard","Joaquin Ezpeleta","Cristian Mahulea","Marius Kloetzer"],"pdf_url":"https://arxiv.org/pdf/2304.08772v3.pdf","comment":"[Note for readers] This paper has been extended from a previous\n  submission to 62nd IEEE Conference on Decision and Control, Dec. 13-15, 2023.\n  This work has been submitted to the IEEE for possible publication. Copyright\n  may be transferred without notice, after which this version may no longer be\n  accessible"},{"id":"http://arxiv.org/abs/2310.15599v2","updated":"2024-03-14T09:53:05Z","published":"2023-10-24T08:01:12Z","title":"Grasp Multiple Objects with One Hand","summary":"  The intricate kinematics of the human hand enable simultaneous grasping and\nmanipulation of multiple objects, essential for tasks such as object transfer\nand in-hand manipulation. Despite its significance, the domain of robotic\nmulti-object grasping is relatively unexplored and presents notable challenges\nin kinematics, dynamics, and object configurations. This paper introduces\nMultiGrasp, a novel two-stage approach for multi-object grasping using a\ndexterous multi-fingered robotic hand on a tabletop. The process consists of\n(i) generating pre-grasp proposals and (ii) executing the grasp and lifting the\nobjects. Our experimental focus is primarily on dual-object grasping, achieving\na success rate of 44.13%, highlighting adaptability to new object\nconfigurations and tolerance for imprecise grasps. Additionally, the framework\ndemonstrates the potential for grasping more than two objects at the cost of\ninference speed.\n","authors":["Yuyang Li","Bo Liu","Yiran Geng","Puhao Li","Yaodong Yang","Yixin Zhu","Tengyu Liu","Siyuan Huang"],"pdf_url":"https://arxiv.org/pdf/2310.15599v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09227v1","updated":"2024-03-14T09:48:36Z","published":"2024-03-14T09:48:36Z","title":"BEHAVIOR-1K: A Human-Centered, Embodied AI Benchmark with 1,000 Everyday\n  Activities and Realistic Simulation","summary":"  We present BEHAVIOR-1K, a comprehensive simulation benchmark for\nhuman-centered robotics. BEHAVIOR-1K includes two components, guided and\nmotivated by the results of an extensive survey on \"what do you want robots to\ndo for you?\". The first is the definition of 1,000 everyday activities,\ngrounded in 50 scenes (houses, gardens, restaurants, offices, etc.) with more\nthan 9,000 objects annotated with rich physical and semantic properties. The\nsecond is OMNIGIBSON, a novel simulation environment that supports these\nactivities via realistic physics simulation and rendering of rigid bodies,\ndeformable bodies, and liquids. Our experiments indicate that the activities in\nBEHAVIOR-1K are long-horizon and dependent on complex manipulation skills, both\nof which remain a challenge for even state-of-the-art robot learning solutions.\nTo calibrate the simulation-to-reality gap of BEHAVIOR-1K, we provide an\ninitial study on transferring solutions learned with a mobile manipulator in a\nsimulated apartment to its real-world counterpart. We hope that BEHAVIOR-1K's\nhuman-grounded nature, diversity, and realism make it valuable for embodied AI\nand robot learning research. Project website: https://behavior.stanford.edu.\n","authors":["Chengshu Li","Ruohan Zhang","Josiah Wong","Cem Gokmen","Sanjana Srivastava","Roberto Martín-Martín","Chen Wang","Gabrael Levine","Wensi Ai","Benjamin Martinez","Hang Yin","Michael Lingelbach","Minjune Hwang","Ayano Hiranaka","Sujay Garlanka","Arman Aydin","Sharon Lee","Jiankai Sun","Mona Anvari","Manasi Sharma","Dhruva Bansal","Samuel Hunter","Kyu-Young Kim","Alan Lou","Caleb R Matthews","Ivan Villa-Renteria","Jerry Huayang Tang","Claire Tang","Fei Xia","Yunzhu Li","Silvio Savarese","Hyowon Gweon","C. Karen Liu","Jiajun Wu","Li Fei-Fei"],"pdf_url":"https://arxiv.org/pdf/2403.09227v1.pdf","comment":"A preliminary version was published at 6th Conference on Robot\n  Learning (CoRL 2022)"},{"id":"http://arxiv.org/abs/2403.09179v1","updated":"2024-03-14T08:47:19Z","published":"2024-03-14T08:47:19Z","title":"Synchronisation-Oriented Design Approach for Adaptive Control","summary":"  This study presents a synchronisation-oriented perspective towards adaptive\ncontrol which views model-referenced adaptation as synchronisation between\nactual and virtual dynamic systems. In the context of adaptation, model\nreference adaptive control methods make the state response of the actual plant\nfollow a reference model. In the context of synchronisation, consensus methods\ninvolving diffusive coupling induce a collective behaviour across multiple\nagents. We draw from the understanding about the two time-scale nature of\nsynchronisation motivated by the study of blended dynamics. The\nsynchronisation-oriented approach consists in the design of a coupling input to\nachieve desired closed-loop error dynamics followed by the input allocation\nprocess to shape the collective behaviour. We suggest that synchronisation can\nbe a reasonable design principle allowing a more holistic and systematic\napproach to the design of adaptive control systems for improved transient\ncharacteristics. Most notably, the proposed approach enables not only\nconstructive derivation but also substantial generalisation of the previously\ndeveloped closed-loop reference model adaptive control method. Practical\nsignificance of the proposed generalisation lies at the capability to improve\nthe transient response characteristics and mitigate the unwanted peaking\nphenomenon at the same time.\n","authors":["Namhoon Cho","Seokwon Lee","Hyo-Sang Shin"],"pdf_url":"https://arxiv.org/pdf/2403.09179v1.pdf","comment":"34 pages, 8 figures, extended version for a manuscript submitted to\n  Automatica"},{"id":"http://arxiv.org/abs/2403.09177v1","updated":"2024-03-14T08:44:15Z","published":"2024-03-14T08:44:15Z","title":"Cellular-enabled Collaborative Robots Planning and Operations for\n  Search-and-Rescue Scenarios","summary":"  Mission-critical operations, particularly in the context of Search-and-Rescue\n(SAR) and emergency response situations, demand optimal performance and\nefficiency from every component involved to maximize the success probability of\nsuch operations. In these settings, cellular-enabled collaborative robotic\nsystems have emerged as invaluable assets, assisting first responders in\nseveral tasks, ranging from victim localization to hazardous area exploration.\nHowever, a critical limitation in the deployment of cellular-enabled\ncollaborative robots in SAR missions is their energy budget, primarily supplied\nby batteries, which directly impacts their task execution and mobility. This\npaper tackles this problem, and proposes a search-and-rescue framework for\ncellular-enabled collaborative robots use cases that, taking as input the area\nsize to be explored, the robots fleet size, their energy profile, exploration\nrate required and target response time, finds the minimum number of robots able\nto meet the SAR mission goals and the path they should follow to explore the\narea. Our results, i) show that first responders can rely on a SAR\ncellular-enabled robotics framework when planning mission-critical operations\nto take informed decisions with limited resources, and, ii) illustrate the\nnumber of robots versus explored area and response time trade-off depending on\nthe type of robot: wheeled vs quadruped.\n","authors":["Arnau Romero","Carmen Delgado","Lanfranco Zanzi","Raúl Suárez","Xavier Costa-Pérez"],"pdf_url":"https://arxiv.org/pdf/2403.09177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09160v1","updated":"2024-03-14T08:16:50Z","published":"2024-03-14T08:16:50Z","title":"Efficient Lexicographic Optimization for Prioritized Robot Control and\n  Planning","summary":"  In this work, we present several tools for efficient sequential hierarchical\nleast-squares programming (S-HLSP) for lexicographical optimization tailored to\nrobot control and planning. As its main step, S-HLSP relies on approximations\nof the original non-linear hierarchical least-squares programming (NL-HLSP) to\na hierarchical least-squares programming (HLSP) by the hierarchical Newton's\nmethod or the hierarchical Gauss-Newton algorithm. We present a threshold\nadaptation strategy for appropriate switches between the two. This ensures\noptimality of infeasible constraints, promotes numerical stability when solving\nthe HLSP's and enhances optimality of lower priority levels by avoiding\nregularized local minima. We introduce the solver $\\mathcal{N}$ADM$_2$, an\nalternating direction method of multipliers for HLSP based on nullspace\nprojections of active constraints. The required basis of nullspace of the\nactive constraints is provided by a computationally efficient turnback\nalgorithm for system dynamics discretized by the Euler method. It is based on\nan upper bound on the bandwidth of linearly independent column subsets within\nthe linearized constraint matrices. Importantly, an expensive initial\nrank-revealing matrix factorization is unnecessary. We show how the high\nsparsity of the basis in the fully-actuated case can be preserved in the\nunder-actuated case. $\\mathcal{N}$ADM$_2$ consistently shows faster\ncomputations times than competing off-the-shelf solvers on NL-HLSP composed of\ntest-functions and whole-body trajectory optimization for fully-actuated and\nunder-actuated robotic systems. We demonstrate how the inherently lower\naccuracy solutions of the alternating direction method of multipliers can be\nused to warm-start the non-linear solver for efficient computation of high\naccuracy solutions to non-linear hierarchical least-squares programs.\n","authors":["Kai Pfeiffer","Abderrahmane Kheddar"],"pdf_url":"https://arxiv.org/pdf/2403.09160v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.11792v5","updated":"2024-03-14T07:47:03Z","published":"2024-01-22T09:44:16Z","title":"Safe and Generalized end-to-end Autonomous Driving System with\n  Reinforcement Learning and Demonstrations","summary":"  An intelligent driving system should be capable of dynamically formulating\nappropriate driving strategies based on the current environment and vehicle\nstatus, while ensuring the security and reliability of the system. However,\nexisting methods based on reinforcement learning and imitation learning suffer\nfrom low safety, poor generalization, and inefficient sampling. Additionally,\nthey cannot accurately predict future driving trajectories, and the accurate\nprediction of future driving trajectories is a precondition for making optimal\ndecisions. To solve these problems, in this paper, we introduce a Safe and\nGeneralized end-to-end Autonomous Driving System (SGADS) for complex and\nvarious scenarios. Our SGADS incorporates variational inference with\nnormalizing flows, enabling the intelligent vehicle to accurately predict\nfuture driving trajectories. Moreover, we propose the formulation of robust\nsafety constraints. Furthermore, we combine reinforcement learning with\ndemonstrations to augment search process of the agent. The experimental results\ndemonstrate that our SGADS can significantly improve safety performance,\nexhibit strong generalization, and enhance the training efficiency of\nintelligent vehicles in complex urban scenarios compared to existing methods.\n","authors":["Zuojin Tang","Xiaoyu Chen","YongQiang Li","Jianyu Chen"],"pdf_url":"https://arxiv.org/pdf/2401.11792v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.14714v2","updated":"2024-03-14T04:46:29Z","published":"2023-06-26T14:04:07Z","title":"Deep Predictive Learning: Motion Learning Concept inspired by Cognitive\n  Robotics","summary":"  Bridging the gap between motion models and reality is crucial by using\nlimited data to deploy robots in the real world. Deep learning is expected to\nbe generalized to diverse situations while reducing feature design costs\nthrough end-to-end learning for environmental recognition and motion\ngeneration. However, data collection for model training is costly, and time and\nhuman resources are essential for robot trial-and-error with physical contact.\nWe propose \"Deep Predictive Learning,\" a motion learning concept that predicts\nthe robot's sensorimotor dynamics, assuming imperfections in the prediction\nmodel. The predictive coding theory inspires this concept to solve the above\nproblems. It is based on the fundamental strategy of predicting the near-future\nsensorimotor states of robots and online minimization of the prediction error\nbetween the real world and the model. Based on the acquired sensor information,\nthe robot can adjust its behavior in real time, thereby tolerating the\ndifference between the learning experience and reality. Additionally, the robot\nwas expected to perform a wide range of tasks by combining the motion dynamics\nembedded in the model. This paper describes the proposed concept, its\nimplementation, and examples of its applications in real robots. The code and\ndocuments are available at: https://ogata-lab.github.io/eipl-docs\n","authors":["Kanata Suzuki","Hiroshi Ito","Tatsuro Yamada","Kei Kase","Tetsuya Ogata"],"pdf_url":"https://arxiv.org/pdf/2306.14714v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04137v5","updated":"2024-03-14T04:36:31Z","published":"2023-03-07T18:50:03Z","title":"Diffusion Policy: Visuomotor Policy Learning via Action Diffusion","summary":"  This paper introduces Diffusion Policy, a new way of generating robot\nbehavior by representing a robot's visuomotor policy as a conditional denoising\ndiffusion process. We benchmark Diffusion Policy across 12 different tasks from\n4 different robot manipulation benchmarks and find that it consistently\noutperforms existing state-of-the-art robot learning methods with an average\nimprovement of 46.9%. Diffusion Policy learns the gradient of the\naction-distribution score function and iteratively optimizes with respect to\nthis gradient field during inference via a series of stochastic Langevin\ndynamics steps. We find that the diffusion formulation yields powerful\nadvantages when used for robot policies, including gracefully handling\nmultimodal action distributions, being suitable for high-dimensional action\nspaces, and exhibiting impressive training stability. To fully unlock the\npotential of diffusion models for visuomotor policy learning on physical\nrobots, this paper presents a set of key technical contributions including the\nincorporation of receding horizon control, visual conditioning, and the\ntime-series diffusion transformer. We hope this work will help motivate a new\ngeneration of policy learning techniques that are able to leverage the powerful\ngenerative modeling capabilities of diffusion models. Code, data, and training\ndetails is publicly available diffusion-policy.cs.columbia.edu\n","authors":["Cheng Chi","Zhenjia Xu","Siyuan Feng","Eric Cousineau","Yilun Du","Benjamin Burchfiel","Russ Tedrake","Shuran Song"],"pdf_url":"https://arxiv.org/pdf/2303.04137v5.pdf","comment":"An extended journal version of the original RSS2023 paper"},{"id":"http://arxiv.org/abs/2303.00910v3","updated":"2024-03-14T03:32:55Z","published":"2023-03-02T02:12:21Z","title":"Bipedal Robot Running: Human-like Actuation Timing Using Fast and Slow\n  Adaptations","summary":"  We have been developing human-sized biped robots based on passive dynamic\nmechanisms. In human locomotion, the muscles activate at the same rate relative\nto the gait cycle during running. To achieve adaptive running for robots, such\ncharacteristics should be reproduced to yield the desired effect, In this\nstudy, we designed a central pattern generator (CPG) involving fast and slow\nadaptation to achieve human-like running using a simple spring-mass model and\nour developed bipedal robot, which is equipped with actuators that imitate the\nhuman musculoskeletal system. Our results demonstrate that the CPG-based\ncontroller with fast and slow adaptations, and a adjustable actuator control\ntiming can reproduce human-like running. The results suggest that the CPG\ncontributes to the adjustment of the muscle activation timing in human running.\n","authors":["Yusuke Sakurai","Tomoya Kamimura","Yuki Sakamoto","Shohei Nishii","Kodai Sato","Yuta Fujiwara","Akihito Sano"],"pdf_url":"https://arxiv.org/pdf/2303.00910v3.pdf","comment":"17 pages, 13 figures, accepted to Advanced Robotics"},{"id":"http://arxiv.org/abs/2403.09025v1","updated":"2024-03-14T01:30:28Z","published":"2024-03-14T01:30:28Z","title":"VDNA-PR: Using General Dataset Representations for Robust Sequential\n  Visual Place Recognition","summary":"  This paper adapts a general dataset representation technique to produce\nrobust Visual Place Recognition (VPR) descriptors, crucial to enable real-world\nmobile robot localisation. Two parallel lines of work on VPR have shown, on one\nside, that general-purpose off-the-shelf feature representations can provide\nrobustness to domain shifts, and, on the other, that fused information from\nsequences of images improves performance. In our recent work on measuring\ndomain gaps between image datasets, we proposed a Visual Distribution of Neuron\nActivations (VDNA) representation to represent datasets of images. This\nrepresentation can naturally handle image sequences and provides a general and\ngranular feature representation derived from a general-purpose model. Moreover,\nour representation is based on tracking neuron activation values over the list\nof images to represent and is not limited to a particular neural network layer,\ntherefore having access to high- and low-level concepts. This work shows how\nVDNAs can be used for VPR by learning a very lightweight and simple encoder to\ngenerate task-specific descriptors. Our experiments show that our\nrepresentation can allow for better robustness than current solutions to\nserious domain shifts away from the training data distribution, such as to\nindoor environments and aerial imagery.\n","authors":["Benjamin Ramtoula","Daniele De Martini","Matthew Gadd","Paul Newman"],"pdf_url":"https://arxiv.org/pdf/2403.09025v1.pdf","comment":"Published at ICRA 2024"},{"id":"http://arxiv.org/abs/0802.3414v4","updated":"2024-03-14T15:57:07Z","published":"2008-02-23T00:54:13Z","title":"A Universal In-Place Reconfiguration Algorithm for Sliding Cube-Shaped\n  Robots in a Quadratic Number of Moves","summary":"  In the modular robot reconfiguration problem, we are given $n$ cube-shaped\nmodules (or robots) as well as two configurations, i.e., placements of the $n$\nmodules so that their union is face-connected. The goal is to find a sequence\nof moves that reconfigures the modules from one configuration to the other\nusing \"sliding moves,\" in which a module slides over the face or edge of a\nneighboring module, maintaining connectivity of the configuration at all times.\n  For many years it has been known that certain module configurations in this\nmodel require at least $\\Omega(n^2)$ moves to reconfigure between them. In this\npaper, we introduce the first universal reconfiguration algorithm -- i.e., we\nshow that any $n$-module configuration can reconfigure itself into any\nspecified $n$-module configuration using just sliding moves. Our algorithm\nachieves reconfiguration in $O(n^2)$ moves, making it asymptotically tight. We\nalso present a variation that reconfigures in-place, it ensures that throughout\nthe reconfiguration process, all modules, except for one, will be contained in\nthe union of the bounding boxes of the start and end configuration.\n","authors":["Zachary Abel","Hugo A. Akitaya","Scott Duke Kominers","Matias Korman","Frederick Stock"],"pdf_url":"https://arxiv.org/pdf/0802.3414v4.pdf","comment":"23 pages, 11 figures"},{"id":"http://arxiv.org/abs/2403.09905v1","updated":"2024-03-14T22:33:22Z","published":"2024-03-14T22:33:22Z","title":"Right Place, Right Time! Towards ObjectNav for Non-Stationary Goals","summary":"  We present a novel approach to tackle the ObjectNav task for non-stationary\nand potentially occluded targets in an indoor environment. We refer to this\ntask Portable ObjectNav (or P-ObjectNav), and in this work, present its\nformulation, feasibility, and a navigation benchmark using a novel\nmemory-enhanced LLM-based policy. In contrast to ObjNav where target object\nlocations are fixed for each episode, P-ObjectNav tackles the challenging case\nwhere the target objects move during the episode. This adds a layer of\ntime-sensitivity to navigation, and is particularly relevant in scenarios where\nthe agent needs to find portable targets (e.g. misplaced wallets) in\nhuman-centric environments. The agent needs to estimate not just the correct\nlocation of the target, but also the time at which the target is at that\nlocation for visual grounding -- raising the question about the feasibility of\nthe task. We address this concern by inferring results on two cases for object\nplacement: one where the objects placed follow a routine or a path, and the\nother where they are placed at random. We dynamize Matterport3D for these\nexperiments, and modify PPO and LLM-based navigation policies for evaluation.\nUsing PPO, we observe that agent performance in the random case stagnates,\nwhile the agent in the routine-following environment continues to improve,\nallowing us to infer that P-ObjectNav is solvable in environments with\nroutine-following object placement. Using memory-enhancement on an LLM-based\npolicy, we set a benchmark for P-ObjectNav. Our memory-enhanced agent\nsignificantly outperforms their non-memory-based counterparts across object\nplacement scenarios by 71.76% and 74.68% on average when measured by Success\nRate (SR) and Success Rate weighted by Path Length (SRPL), showing the\ninfluence of memory on improving P-ObjectNav performance. Our code and dataset\nwill be made publicly available.\n","authors":["Vishnu Sashank Dorbala","Bhrij Patel","Amrit Singh Bedi","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2403.09905v1.pdf","comment":"32"},{"id":"http://arxiv.org/abs/2403.09900v1","updated":"2024-03-14T22:22:22Z","published":"2024-03-14T22:22:22Z","title":"DTG : Diffusion-based Trajectory Generation for Mapless Global\n  Navigation","summary":"  We present a novel end-to-end diffusion-based trajectory generation method,\nDTG, for mapless global navigation in challenging outdoor scenarios with\nocclusions and unstructured off-road features like grass, buildings, bushes,\netc. Given a distant goal, our approach computes a trajectory that satisfies\nthe following goals: (1) minimize the travel distance to the goal; (2) maximize\nthe traversability by choosing paths that do not lie in undesirable areas.\nSpecifically, we present a novel Conditional RNN(CRNN) for diffusion models to\nefficiently generate trajectories. Furthermore, we propose an adaptive training\nmethod that ensures that the diffusion model generates more traversable\ntrajectories. We evaluate our methods in various outdoor scenes and compare the\nperformance with other global navigation algorithms on a Husky robot. In\npractice, we observe at least a 15% improvement in traveling distance and\naround a 7% improvement in traversability.\n","authors":["Jing Liang","Amirreza Payandeh","Daeun Song","Xuesu Xiao","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2403.09900v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2403.09882v1","updated":"2024-03-14T21:30:41Z","published":"2024-03-14T21:30:41Z","title":"Visual Inertial Odometry using Focal Plane Binary Features (BIT-VIO)","summary":"  Focal-Plane Sensor-Processor Arrays (FPSP)s are an emerging technology that\ncan execute vision algorithms directly on the image sensor. Unlike conventional\ncameras, FPSPs perform computation on the image plane -- at individual pixels\n-- enabling high frame rate image processing while consuming low power, making\nthem ideal for mobile robotics. FPSPs, such as the SCAMP-5, use parallel\nprocessing and are based on the Single Instruction Multiple Data (SIMD)\nparadigm. In this paper, we present BIT-VIO, the first Visual Inertial Odometry\n(VIO) which utilises SCAMP-5.BIT-VIO is a loosely-coupled iterated Extended\nKalman Filter (iEKF) which fuses together the visual odometry running fast at\n300 FPS with predictions from 400 Hz IMU measurements to provide accurate and\nsmooth trajectories.\n","authors":["Matthew Lisondra","Junseo Kim","Riku Murai","Kourosh Zareinia","Sajad Saeedi"],"pdf_url":"https://arxiv.org/pdf/2403.09882v1.pdf","comment":"Accepted for Presentation Yokohama, Japan for IEEE 2024 ICRA"},{"id":"http://arxiv.org/abs/2403.09875v1","updated":"2024-03-14T21:09:59Z","published":"2024-03-14T21:09:59Z","title":"Touch-GS: Visual-Tactile Supervised 3D Gaussian Splatting","summary":"  In this work, we propose a novel method to supervise 3D Gaussian Splatting\n(3DGS) scenes using optical tactile sensors. Optical tactile sensors have\nbecome widespread in their use in robotics for manipulation and object\nrepresentation; however, raw optical tactile sensor data is unsuitable to\ndirectly supervise a 3DGS scene. Our representation leverages a Gaussian\nProcess Implicit Surface to implicitly represent the object, combining many\ntouches into a unified representation with uncertainty. We merge this model\nwith a monocular depth estimation network, which is aligned in a two stage\nprocess, coarsely aligning with a depth camera and then finely adjusting to\nmatch our touch data. For every training image, our method produces a\ncorresponding fused depth and uncertainty map. Utilizing this additional\ninformation, we propose a new loss function, variance weighted depth supervised\nloss, for training the 3DGS scene model. We leverage the DenseTact optical\ntactile sensor and RealSense RGB-D camera to show that combining touch and\nvision in this manner leads to quantitatively and qualitatively better results\nthan vision or touch alone in a few-view scene syntheses on opaque as well as\non reflective and transparent objects. Please see our project page at\nhttp://armlabstanford.github.io/touch-gs\n","authors":["Aiden Swann","Matthew Strong","Won Kyung Do","Gadiel Sznaier Camps","Mac Schwager","Monroe Kennedy III"],"pdf_url":"https://arxiv.org/pdf/2403.09875v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09865v1","updated":"2024-03-14T20:55:55Z","published":"2024-03-14T20:55:55Z","title":"Safety-Critical Control for Autonomous Systems: Control Barrier\n  Functions via Reduced-Order Models","summary":"  Modern autonomous systems, such as flying, legged, and wheeled robots, are\ngenerally characterized by high-dimensional nonlinear dynamics, which presents\nchallenges for model-based safety-critical control design. Motivated by the\nsuccess of reduced-order models in robotics, this paper presents a tutorial on\nconstructive safety-critical control via reduced-order models and control\nbarrier functions (CBFs). To this end, we provide a unified formulation of\ntechniques in the literature that share a common foundation of constructing\nCBFs for complex systems from CBFs for much simpler systems. Such ideas are\nillustrated through formal results, simple numerical examples, and case studies\nof real-world systems to which these techniques have been experimentally\napplied.\n","authors":["Max H. Cohen","Tamas G. Molnar","Aaron D. Ames"],"pdf_url":"https://arxiv.org/pdf/2403.09865v1.pdf","comment":"To appear in Annual Reviews in Control"},{"id":"http://arxiv.org/abs/2304.05723v2","updated":"2024-03-14T20:53:52Z","published":"2023-04-12T09:28:51Z","title":"Distributed Coverage Control of Constrained Constant-Speed Unicycle\n  Multi-Agent Systems","summary":"  This paper proposes a novel distributed coverage controller for a multi-agent\nsystem with constant-speed unicycle robots (CSUR). The work is motivated by the\nlimitation of the conventional method that does not ensure the satisfaction of\nhard state- and input-dependent constraints and leads to feasibility issues for\nmulti-CSUR systems. In this paper, we solve these problems by designing a novel\ncoverage cost function and a saturated gradient-search-based control law.\nInvariant set theory and Lyapunov-based techniques are used to prove the\nstate-dependent confinement and the convergence of the system state to the\noptimal coverage configuration, respectively. The controller is implemented in\na distributed manner based on a novel communication standard among the agents.\nA series of simulation case studies are conducted to validate the effectiveness\nof the proposed coverage controller in different initial conditions and with\ncontrol parameters. A comparison study in simulation reveals the advantage of\nthe proposed method in terms of avoiding infeasibility. The experiment study\nverifies the applicability of the method to real robots with uncertainties. The\ndevelopment procedure of the method from theoretical analysis to experimental\nvalidation provides a novel framework for multi-agent system coordinate control\nwith complex agent dynamics.\n","authors":["Qingchen Liu","Zengjie Zhang","Nhan Khanh Le","Jiahu Qin","Fangzhou Liu","Sandra Hirche"],"pdf_url":"https://arxiv.org/pdf/2304.05723v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09853v1","updated":"2024-03-14T20:27:39Z","published":"2024-03-14T20:27:39Z","title":"Constrained Passive Interaction Control: Leveraging Passivity and Safety\n  for Robot Manipulators","summary":"  Passivity is necessary for robots to fluidly collaborate and interact with\nhumans physically. Nevertheless, due to the unconstrained nature of\npassivity-based impedance control laws, the robot is vulnerable to infeasible\nand unsafe configurations upon physical perturbations. In this paper, we\npropose a novel control architecture that allows a torque-controlled robot to\nguarantee safety constraints such as kinematic limits, self-collisions,\nexternal collisions and singularities and is passive only when feasible. This\nis achieved by constraining a dynamical system based impedance control law with\na relaxed hierarchical control barrier function quadratic program subject to\nmultiple concurrent, possibly contradicting, constraints. Joint space\nconstraints are formulated from efficient data-driven self- and external C^2\ncollision boundary functions. We theoretically prove constraint satisfaction\nand show that the robot is passive when feasible. Our approach is validated in\nsimulation and real robot experiments on a 7DoF Franka Research 3 manipulator.\n","authors":["Zhiquan Zhang","Tianyu Li","Nadia Figueroa"],"pdf_url":"https://arxiv.org/pdf/2403.09853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09841v1","updated":"2024-03-14T20:10:58Z","published":"2024-03-14T20:10:58Z","title":"MultiGripperGrasp: A Dataset for Robotic Grasping from Parallel Jaw\n  Grippers to Dexterous Hands","summary":"  We introduce a large-scale dataset named MultiGripperGrasp for robotic\ngrasping. Our dataset contains 30.4M grasps from 11 grippers for 345 objects.\nThese grippers range from two-finger grippers to five-finger grippers,\nincluding a human hand. All grasps in the dataset are verified in Isaac Sim to\nclassify them as successful and unsuccessful grasps. Additionally, the object\nfall-off time for each grasp is recorded as a grasp quality measurement.\nFurthermore, the grippers in our dataset are aligned according to the\norientation and position of their palms, allowing us to transfer grasps from\none gripper to another. The grasp transfer significantly increases the number\nof successful grasps for each gripper in the dataset. Our dataset is useful to\nstudy generalized grasp planning and grasp transfer across different grippers.\n","authors":["Luis Felipe Casas Murrilo","Ninad Khargonkar","Balakrishnan Prabhakaran","Yu Xiang"],"pdf_url":"https://arxiv.org/pdf/2403.09841v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.00935v3","updated":"2024-03-14T20:04:33Z","published":"2023-03-02T03:16:21Z","title":"Learning to Detect Slip through Tactile Estimation of the Contact Force\n  Field and its Entropy","summary":"  Detection of slip during object grasping and manipulation plays a vital role\nin object handling. Existing solutions primarily rely on visual information to\ndevise a strategy for grasping. However, for robotic systems to attain a level\nof proficiency comparable to humans, especially in consistently handling and\nmanipulating unfamiliar objects, integrating artificial tactile sensing is\nincreasingly essential. We introduce a novel physics-informed, data-driven\napproach to detect slip continuously in real time. We employ the GelSight Mini,\nan optical tactile sensor, attached to custom-designed grippers to gather\ntactile data. Our work leverages the inhomogeneity of tactile sensor readings\nduring slip events to develop distinctive features and formulates slip\ndetection as a classification problem. To evaluate our approach, we test\nmultiple data-driven models on 10 common objects under different loading\nconditions, textures, and materials. Our results show that the best\nclassification algorithm achieves a high average accuracy of 95.61%. We further\nillustrate the practical application of our research in dynamic robotic\nmanipulation tasks, where our real-time slip detection and prevention algorithm\nis implemented.\n","authors":["Xiaohai Hu","Aparajit Venkatesh","Guiliang Zheng","Xu Chen"],"pdf_url":"https://arxiv.org/pdf/2303.00935v3.pdf","comment":"8 pages, 7 figures, to be submitted"},{"id":"http://arxiv.org/abs/2403.09813v1","updated":"2024-03-14T19:01:54Z","published":"2024-03-14T19:01:54Z","title":"Towards Comprehensive Multimodal Perception: Introducing the\n  Touch-Language-Vision Dataset","summary":"  Tactility provides crucial support and enhancement for the perception and\ninteraction capabilities of both humans and robots. Nevertheless, the\nmultimodal research related to touch primarily focuses on visual and tactile\nmodalities, with limited exploration in the domain of language. Beyond\nvocabulary, sentence-level descriptions contain richer semantics. Based on\nthis, we construct a touch-language-vision dataset named TLV\n(Touch-Language-Vision) by human-machine cascade collaboration, featuring\nsentence-level descriptions for multimode alignment. The new dataset is used to\nfine-tune our proposed lightweight training framework, TLV-Link (Linking Touch,\nLanguage, and Vision through Alignment), achieving effective semantic alignment\nwith minimal parameter adjustments (1%). Project Page:\nhttps://xiaoen0.github.io/touch.page/.\n","authors":["Ning Cheng","You Li","Jing Gao","Bin Fang","Jinan Xu","Wenjuan Han"],"pdf_url":"https://arxiv.org/pdf/2403.09813v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09799v1","updated":"2024-03-14T18:37:46Z","published":"2024-03-14T18:37:46Z","title":"BOP Challenge 2023 on Detection, Segmentation and Pose Estimation of\n  Seen and Unseen Rigid Objects","summary":"  We present the evaluation methodology, datasets and results of the BOP\nChallenge 2023, the fifth in a series of public competitions organized to\ncapture the state of the art in model-based 6D object pose estimation from an\nRGB/RGB-D image and related tasks. Besides the three tasks from 2022\n(model-based 2D detection, 2D segmentation, and 6D localization of objects seen\nduring training), the 2023 challenge introduced new variants of these tasks\nfocused on objects unseen during training. In the new tasks, methods were\nrequired to learn new objects during a short onboarding stage (max 5 minutes, 1\nGPU) from provided 3D object models. The best 2023 method for 6D localization\nof unseen objects (GenFlow) notably reached the accuracy of the best 2020\nmethod for seen objects (CosyPose), although being noticeably slower. The best\n2023 method for seen objects (GPose) achieved a moderate accuracy improvement\nbut a significant 43% run-time improvement compared to the best 2022\ncounterpart (GDRNPP). Since 2017, the accuracy of 6D localization of seen\nobjects has improved by more than 50% (from 56.9 to 85.6 AR_C). The online\nevaluation system stays open and is available at: http://bop.felk.cvut.cz/.\n","authors":["Tomas Hodan","Martin Sundermeyer","Yann Labbe","Van Nguyen Nguyen","Gu Wang","Eric Brachmann","Bertram Drost","Vincent Lepetit","Carsten Rother","Jiri Matas"],"pdf_url":"https://arxiv.org/pdf/2403.09799v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2302.13075"},{"id":"http://arxiv.org/abs/2403.05972v2","updated":"2024-03-14T18:26:36Z","published":"2024-03-09T17:37:05Z","title":"C3D: Cascade Control with Change Point Detection and Deep Koopman\n  Learning for Autonomous Surface Vehicles","summary":"  In this paper, we discuss the development and deployment of a robust\nautonomous system capable of performing various tasks in the maritime domain\nunder unknown dynamic conditions. We investigate a data-driven approach based\non modular design for ease of transfer of autonomy across different maritime\nsurface vessel platforms. The data-driven approach alleviates issues related to\na priori identification of system models that may become deficient under\nevolving system behaviors or shifting, unanticipated, environmental influences.\nOur proposed learning-based platform comprises a deep Koopman system model and\na change point detector that provides guidance on domain shifts prompting\nrelearning under severe exogenous and endogenous perturbations. Motion control\nof the autonomous system is achieved via an optimal controller design. The\nKoopman linearized model naturally lends itself to a linear-quadratic regulator\n(LQR) control design. We propose the C3D control architecture Cascade Control\nwith Change Point Detection and Deep Koopman Learning. The framework is\nverified in station keeping task on an ASV in both simulation and real\nexperiments. The approach achieved at least 13.9 percent improvement in mean\ndistance error in all test cases compared to the methods that do not consider\nsystem changes.\n","authors":["Jianwen Li","Hyunsang Park","Wenjian Hao","Lei Xin","Jalil Chavez-Galaviz","Ajinkya Chaudhary","Meredith Bloss","Kyle Pattison","Christopher Vo","Devesh Upadhyay","Shreyas Sundaram","Shaoshuai Mou","Nina Mahmoudian"],"pdf_url":"https://arxiv.org/pdf/2403.05972v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2403.09793v1","updated":"2024-03-14T18:25:40Z","published":"2024-03-14T18:25:40Z","title":"Socially Integrated Navigation: A Social Acting Robot with Deep\n  Reinforcement Learning","summary":"  Mobile robots are being used on a large scale in various crowded situations\nand become part of our society. The socially acceptable navigation behavior of\na mobile robot with individual human consideration is an essential requirement\nfor scalable applications and human acceptance. Deep Reinforcement Learning\n(DRL) approaches are recently used to learn a robot's navigation policy and to\nmodel the complex interactions between robots and humans. We propose to divide\nexisting DRL-based navigation approaches based on the robot's exhibited social\nbehavior and distinguish between social collision avoidance with a lack of\nsocial behavior and socially aware approaches with explicit predefined social\nbehavior. In addition, we propose a novel socially integrated navigation\napproach where the robot's social behavior is adaptive and emerges from the\ninteraction with humans. The formulation of our approach is derived from a\nsociological definition, which states that social acting is oriented toward the\nacting of others. The DRL policy is trained in an environment where other\nagents interact socially integrated and reward the robot's behavior\nindividually. The simulation results indicate that the proposed socially\nintegrated navigation approach outperforms a socially aware approach in terms\nof distance traveled, time to completion, and negative impact on all agents\nwithin the environment.\n","authors":["Daniel Flögel","Lars Fischer","Thomas Rudolf","Tobias Schürmann","Sören Hohmann"],"pdf_url":"https://arxiv.org/pdf/2403.09793v1.pdf","comment":null}]},"2024-03-15T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2403.09583v2","updated":"2024-03-15T08:47:48Z","published":"2024-03-14T17:18:15Z","title":"ExploRLLM: Guiding Exploration in Reinforcement Learning with Large\n  Language Models","summary":"  In image-based robot manipulation tasks with large observation and action\nspaces, reinforcement learning struggles with low sample efficiency, slow\ntraining speed, and uncertain convergence. As an alternative, large pre-trained\nfoundation models have shown promise in robotic manipulation, particularly in\nzero-shot and few-shot applications. However, using these models directly is\nunreliable due to limited reasoning capabilities and challenges in\nunderstanding physical and spatial contexts. This paper introduces ExploRLLM, a\nnovel approach that leverages the inductive bias of foundation models (e.g.\nLarge Language Models) to guide exploration in reinforcement learning. We also\nexploit these foundation models to reformulate the action and observation\nspaces to enhance the training efficiency in reinforcement learning. Our\nexperiments demonstrate that guided exploration enables much quicker\nconvergence than training without it. Additionally, we validate that ExploRLLM\noutperforms vanilla foundation model baselines and that the policy trained in\nsimulation can be applied in real-world settings without additional training.\n","authors":["Runyu Ma","Jelle Luijkx","Zlatan Ajanovic","Jens Kober"],"pdf_url":"https://arxiv.org/pdf/2403.09583v2.pdf","comment":"8 pages,8 figures, conference IROS 2024"},{"id":"http://arxiv.org/abs/2403.08504v2","updated":"2024-03-15T06:31:45Z","published":"2024-03-13T13:12:42Z","title":"OccFiner: Offboard Occupancy Refinement with Hybrid Propagation","summary":"  Vision-based occupancy prediction, also known as 3D Semantic Scene Completion\n(SSC), presents a significant challenge in computer vision. Previous methods,\nconfined to onboard processing, struggle with simultaneous geometric and\nsemantic estimation, continuity across varying viewpoints, and single-view\nocclusion. Our paper introduces OccFiner, a novel offboard framework designed\nto enhance the accuracy of vision-based occupancy predictions. OccFiner\noperates in two hybrid phases: 1) a multi-to-multi local propagation network\nthat implicitly aligns and processes multiple local frames for correcting\nonboard model errors and consistently enhancing occupancy accuracy across all\ndistances. 2) the region-centric global propagation, focuses on refining labels\nusing explicit multi-view geometry and integrating sensor bias, especially to\nincrease the accuracy of distant occupied voxels. Extensive experiments\ndemonstrate that OccFiner improves both geometric and semantic accuracy across\nvarious types of coarse occupancy, setting a new state-of-the-art performance\non the SemanticKITTI dataset. Notably, OccFiner elevates vision-based SSC\nmodels to a level even surpassing that of LiDAR-based onboard SSC models.\n","authors":["Hao Shi","Song Wang","Jiaming Zhang","Xiaoting Yin","Zhongdao Wang","Zhijian Zhao","Guangming Wang","Jianke Zhu","Kailun Yang","Kaiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2403.08504v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.10164v3","updated":"2024-03-15T09:18:20Z","published":"2023-01-17T15:06:33Z","title":"Lowering Detection in Sport Climbing Based on Orientation of the Sensor\n  Enhanced Quickdraw","summary":"  Tracking climbers' activity to improve services and make the best use of\ntheir infrastructure is a concern for climbing gyms. Each climbing session must\nbe analyzed from beginning till lowering of the climber. Therefore, spotting\nthe climbers descending is crucial since it indicates when the ascent has come\nto an end. This problem must be addressed while preserving privacy and\nconvenience of the climbers and the costs of the gyms. To this aim, a hardware\nprototype is developed to collect data using accelerometer sensors attached to\na piece of climbing equipment mounted on the wall, called quickdraw, that\nconnects the climbing rope to the bolt anchors. The corresponding sensors are\nconfigured to be energy-efficient, hence become practical in terms of expenses\nand time consumption for replacement when using in large quantity in a climbing\ngym. This paper describes hardware specifications, studies data measured by the\nsensors in ultra-low power mode, detect sensors' orientation patterns during\nlowering different routes, and develop an supervised approach to identify\nlowering.\n","authors":["Sadaf Moaveninejad","Andrea Janes","Camillo Porcaro"],"pdf_url":"https://arxiv.org/pdf/2301.10164v3.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2211.02680"},{"id":"http://arxiv.org/abs/2305.12137v3","updated":"2024-03-15T08:55:17Z","published":"2023-05-20T08:38:37Z","title":"Data-driven Methods Applied to Soft Robot Modeling and Control: A Review","summary":"  Soft robots show compliance and have infinite degrees of freedom. Thanks to\nthese properties, such robots can be leveraged for surgery, rehabilitation,\nbiomimetics, unstructured environment exploring, and industrial grippers. In\nthis case, they attract scholars from a variety of areas. However, nonlinearity\nand hysteresis effects also bring a burden to robot modeling. Moreover,\nfollowing their flexibility and adaptation, soft robot control is more\nchallenging than rigid robot control. In order to model and control soft\nrobots, a large number of data-driven methods are utilized in pairs or\nseparately. This review first briefly introduces two foundations for\ndata-driven approaches, which are physical models and the Jacobian matrix, then\nsummarizes three kinds of data-driven approaches, which are statistical method,\nneural network, and reinforcement learning. This review compares the modeling\nand controller features, e.g., model dynamics, data requirement, and target\ntask, within and among these categories. Finally, we summarize the features of\neach method. A discussion about the advantages and limitations of the existing\nmodeling and control approaches is presented, and we forecast the future of\ndata-driven approaches in soft robots. A website\n(https://sites.google.com/view/23zcb) is built for this review and will be\nupdated frequently.\n","authors":["Zixi Chen","Federico Renda","Alexia Le Gall","Lorenzo Mocellin","Matteo Bernabei","Théo Dangel","Gastone Ciuti","Matteo Cianchetti","Cesare Stefanini"],"pdf_url":"https://arxiv.org/pdf/2305.12137v3.pdf","comment":"16 pages, 6 figures, 7tables, accepted by IEEE Transactions on\n  Automation Science and Engineering on 11 March, 2024"},{"id":"http://arxiv.org/abs/2403.10506v1","updated":"2024-03-15T17:45:44Z","published":"2024-03-15T17:45:44Z","title":"HumanoidBench: Simulated Humanoid Benchmark for Whole-Body Locomotion\n  and Manipulation","summary":"  Humanoid robots hold great promise in assisting humans in diverse\nenvironments and tasks, due to their flexibility and adaptability leveraging\nhuman-like morphology. However, research in humanoid robots is often\nbottlenecked by the costly and fragile hardware setups. To accelerate\nalgorithmic research in humanoid robots, we present a high-dimensional,\nsimulated robot learning benchmark, HumanoidBench, featuring a humanoid robot\nequipped with dexterous hands and a variety of challenging whole-body\nmanipulation and locomotion tasks. Our findings reveal that state-of-the-art\nreinforcement learning algorithms struggle with most tasks, whereas a\nhierarchical learning baseline achieves superior performance when supported by\nrobust low-level policies, such as walking or reaching. With HumanoidBench, we\nprovide the robotics community with a platform to identify the challenges\narising when solving diverse tasks with humanoid robots, facilitating prompt\nverification of algorithms and ideas. The open-source code is available at\nhttps://sferrazza.cc/humanoidbench_site.\n","authors":["Carmelo Sferrazza","Dun-Ming Huang","Xingyu Lin","Youngwoon Lee","Pieter Abbeel"],"pdf_url":"https://arxiv.org/pdf/2403.10506v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10496v1","updated":"2024-03-15T17:31:42Z","published":"2024-03-15T17:31:42Z","title":"Reconfigurable Robot Identification from Motion Data","summary":"  Integrating Large Language Models (VLMs) and Vision-Language Models (VLMs)\nwith robotic systems enables robots to process and understand complex natural\nlanguage instructions and visual information. However, a fundamental challenge\nremains: for robots to fully capitalize on these advancements, they must have a\ndeep understanding of their physical embodiment. The gap between AI models\ncognitive capabilities and the understanding of physical embodiment leads to\nthe following question: Can a robot autonomously understand and adapt to its\nphysical form and functionalities through interaction with its environment?\nThis question underscores the transition towards developing self-modeling\nrobots without reliance on external sensory or pre-programmed knowledge about\ntheir structure. Here, we propose a meta self modeling that can deduce robot\nmorphology through proprioception (the internal sense of position and\nmovement). Our study introduces a 12 DoF reconfigurable legged robot,\naccompanied by a diverse dataset of 200k unique configurations, to\nsystematically investigate the relationship between robotic motion and robot\nmorphology. Utilizing a deep neural network model comprising a robot signature\nencoder and a configuration decoder, we demonstrate the capability of our\nsystem to accurately predict robot configurations from proprioceptive signals.\nThis research contributes to the field of robotic self-modeling, aiming to\nenhance understanding of their physical embodiment and adaptability in real\nworld scenarios.\n","authors":["Yuhang Hu","Yunzhe Wang","Ruibo Liu","Zhou Shen","Hod Lipson"],"pdf_url":"https://arxiv.org/pdf/2403.10496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10494v1","updated":"2024-03-15T17:29:54Z","published":"2024-03-15T17:29:54Z","title":"Lifelong LERF: Local 3D Semantic Inventory Monitoring Using FogROS2","summary":"  Inventory monitoring in homes, factories, and retail stores relies on\nmaintaining data despite objects being swapped, added, removed, or moved. We\nintroduce Lifelong LERF, a method that allows a mobile robot with minimal\ncompute to jointly optimize a dense language and geometric representation of\nits surroundings. Lifelong LERF maintains this representation over time by\ndetecting semantic changes and selectively updating these regions of the\nenvironment, avoiding the need to exhaustively remap. Human users can query\ninventory by providing natural language queries and receiving a 3D heatmap of\npotential object locations. To manage the computational load, we use Fog-ROS2,\na cloud robotics platform, to offload resource-intensive tasks. Lifelong LERF\nobtains poses from a monocular RGBD SLAM backend, and uses these poses to\nprogressively optimize a Language Embedded Radiance Field (LERF) for semantic\nmonitoring. Experiments with 3-5 objects arranged on a tabletop and a Turtlebot\nwith a RealSense camera suggest that Lifelong LERF can persistently adapt to\nchanges in objects with up to 91% accuracy.\n","authors":["Adam Rashid","Chung Min Kim","Justin Kerr","Letian Fu","Kush Hari","Ayah Ahmad","Kaiyuan Chen","Huang Huang","Marcus Gualtieri","Michael Wang","Christian Juette","Nan Tian","Liu Ren","Ken Goldberg"],"pdf_url":"https://arxiv.org/pdf/2403.10494v1.pdf","comment":"See project webpage at:\n  https://sites.google.com/berkeley.edu/lifelonglerf/home"},{"id":"http://arxiv.org/abs/2403.10487v1","updated":"2024-03-15T17:21:39Z","published":"2024-03-15T17:21:39Z","title":"Stimulate the Potential of Robots via Competition","summary":"  It is common for us to feel pressure in a competition environment, which\narises from the desire to obtain success comparing with other individuals or\nopponents. Although we might get anxious under the pressure, it could also be a\ndrive for us to stimulate our potentials to the best in order to keep up with\nothers. Inspired by this, we propose a competitive learning framework which is\nable to help individual robot to acquire knowledge from the competition, fully\nstimulating its dynamics potential in the race. Specifically, the competition\ninformation among competitors is introduced as the additional auxiliary signal\nto learn advantaged actions. We further build a Multiagent-Race environment,\nand extensive experiments are conducted, demonstrating that robots trained in\ncompetitive environments outperform ones that are trained with SoTA algorithms\nin single robot environment.\n","authors":["Kangyao Huang","Di Guo","Xinyu Zhang","Xiangyang Ji","Huaping Liu"],"pdf_url":"https://arxiv.org/pdf/2403.10487v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.00626v2","updated":"2024-03-15T16:57:29Z","published":"2023-11-01T16:23:59Z","title":"nvblox: GPU-Accelerated Incremental Signed Distance Field Mapping","summary":"  Dense, volumetric maps are essential to enable robot navigation and\ninteraction with the environment. To achieve low latency, dense maps are\ntypically computed onboard the robot, often on computationally constrained\nhardware. Previous works leave a gap between CPU-based systems for robotic\nmapping which, due to computation constraints, limit map resolution or scale,\nand GPU-based reconstruction systems which omit features that are critical to\nrobotic path planning, such as computation of the Euclidean Signed Distance\nField (ESDF). We introduce a library, nvblox, that aims to fill this gap, by\nGPU-accelerating robotic volumetric mapping. Nvblox delivers a significant\nperformance improvement over the state of the art, achieving up to a 177x\nspeed-up in surface reconstruction, and up to a 31x improvement in distance\nfield computation, and is available open-source.\n","authors":["Alexander Millane","Helen Oleynikova","Emilie Wirbel","Remo Steiner","Vikram Ramasamy","David Tingdahl","Roland Siegwart"],"pdf_url":"https://arxiv.org/pdf/2311.00626v2.pdf","comment":"Accepted to ICRA 2024"},{"id":"http://arxiv.org/abs/2403.10460v1","updated":"2024-03-15T16:51:30Z","published":"2024-03-15T16:51:30Z","title":"Online Concurrent Multi-Robot Coverage Path Planning","summary":"  Recently, centralized receding horizon online multi-robot coverage path\nplanning algorithms have shown remarkable scalability in thoroughly exploring\nlarge, complex, unknown workspaces with many robots. In a horizon, the path\nplanning and the path execution interleave, meaning when the path planning\noccurs for robots with no paths, the robots with outstanding paths do not\nexecute, and subsequently, when the robots with new or outstanding paths\nexecute to reach respective goals, path planning does not occur for those\nrobots yet to get new paths, leading to wastage of both the robotic and the\ncomputation resources. As a remedy, we propose a centralized algorithm that is\nnot horizon-based. It plans paths at any time for a subset of robots with no\npaths, i.e., who have reached their previously assigned goals, while the rest\nexecute their outstanding paths, thereby enabling concurrent planning and\nexecution. We formally prove that the proposed algorithm ensures complete\ncoverage of an unknown workspace and analyze its time complexity. To\ndemonstrate scalability, we evaluate our algorithm to cover eight large $2$D\ngrid benchmark workspaces with up to 512 aerial and ground robots,\nrespectively. A comparison with a state-of-the-art horizon-based algorithm\nshows its superiority in completing the coverage with up to 1.6x speedup. For\nvalidation, we perform ROS + Gazebo simulations in six 2D grid benchmark\nworkspaces with 10 quadcopters and TurtleBots, respectively. We also\nsuccessfully conducted one outdoor experiment with three quadcopters and one\nindoor with two TurtleBots.\n","authors":["Ratijit Mitra","Indranil Saha"],"pdf_url":"https://arxiv.org/pdf/2403.10460v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10454v1","updated":"2024-03-15T16:42:14Z","published":"2024-03-15T16:42:14Z","title":"Partially Observable Task and Motion Planning with Uncertainty and Risk\n  Awareness","summary":"  Integrated task and motion planning (TAMP) has proven to be a valuable\napproach to generalizable long-horizon robotic manipulation and navigation\nproblems. However, the typical TAMP problem formulation assumes full\nobservability and deterministic action effects. These assumptions limit the\nability of the planner to gather information and make decisions that are\nrisk-aware. We propose a strategy for TAMP with Uncertainty and Risk Awareness\n(TAMPURA) that is capable of efficiently solving long-horizon planning problems\nwith initial-state and action outcome uncertainty, including problems that\nrequire information gathering and avoiding undesirable and irreversible\noutcomes. Our planner reasons under uncertainty at both the abstract task level\nand continuous controller level. Given a set of closed-loop goal-conditioned\ncontrollers operating in the primitive action space and a description of their\npreconditions and potential capabilities, we learn a high-level abstraction\nthat can be solved efficiently and then refined to continuous actions for\nexecution. We demonstrate our approach on several robotics problems where\nuncertainty is a crucial factor and show that reasoning under uncertainty in\nthese problems outperforms previously proposed determinized planning, direct\nsearch, and reinforcement learning strategies. Lastly, we demonstrate our\nplanner on two real-world robotics problems using recent advancements in\nprobabilistic perception.\n","authors":["Aidan Curtis","George Matheos","Nishad Gothoskar","Vikash Mansinghka","Joshua Tenenbaum","Tomás Lozano-Pérez","Leslie Pack Kaelbling"],"pdf_url":"https://arxiv.org/pdf/2403.10454v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10436v1","updated":"2024-03-15T16:20:10Z","published":"2024-03-15T16:20:10Z","title":"H-MaP: An Iterative and Hybrid Sequential Manipulation Planner","summary":"  This study introduces the Hybrid Sequential Manipulation Planner (H-MaP), a\nnovel approach that iteratively does motion planning using contact points and\nwaypoints for complex sequential manipulation tasks in robotics. Combining\noptimization-based methods for generalizability and sampling-based methods for\nrobustness, H-MaP enhances manipulation planning through active contact mode\nswitches and enables interactions with auxiliary objects and tools. This\nframework, validated by a series of diverse physical manipulation tasks and\nreal-robot experiments, offers a scalable and adaptable solution for complex\nreal-world applications in robotic manipulation.\n","authors":["Berk Cicek","Cankut Bora Tuncer","Busenaz Kerimgil","Ozgur S. Oguz"],"pdf_url":"https://arxiv.org/pdf/2403.10436v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15871v2","updated":"2024-03-15T16:01:40Z","published":"2023-03-28T10:26:30Z","title":"Control Barrier Functions in Dynamic UAVs for Kinematic Obstacle\n  Avoidance: A Collision Cone Approach","summary":"  Unmanned aerial vehicles (UAVs), specifically quadrotors, have revolutionized\nvarious industries with their maneuverability and versatility, but their safe\noperation in dynamic environments heavily relies on effective collision\navoidance techniques. This paper introduces a novel technique for safely\nnavigating a quadrotor along a desired route while avoiding kinematic\nobstacles. We propose a new constraint formulation that employs control barrier\nfunctions (CBFs) and collision cones to ensure that the relative velocity\nbetween the quadrotor and the obstacle always avoids a cone of vectors that may\nlead to a collision. By showing that the proposed constraint is a valid CBF for\nquadrotors, we are able to leverage its real-time implementation via Quadratic\nPrograms (QPs), called the CBF-QPs. Validation includes PyBullet simulations\nand hardware experiments on Crazyflie 2.1, demonstrating effectiveness in\nstatic and moving obstacle scenarios. Comparative analysis with literature,\nespecially higher order CBF-QPs, highlights the proposed approach's less\nconservative nature. Simulation and Hardware videos are available here:\nhttps://tayalmanan28.github.io/C3BF-UAV/\n","authors":["Manan Tayal","Rajpal Singh","Jishnu Keshavan","Shishir Kolathaya"],"pdf_url":"https://arxiv.org/pdf/2303.15871v2.pdf","comment":"Accepted at American Control Conference(ACC) 2024. 6 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.10425v1","updated":"2024-03-15T15:58:51Z","published":"2024-03-15T15:58:51Z","title":"NeuFlow: Real-time, High-accuracy Optical Flow Estimation on Robots\n  Using Edge Devices","summary":"  Real-time high-accuracy optical flow estimation is a crucial component in\nvarious applications, including localization and mapping in robotics, object\ntracking, and activity recognition in computer vision. While recent\nlearning-based optical flow methods have achieved high accuracy, they often\ncome with heavy computation costs. In this paper, we propose a highly efficient\noptical flow architecture, called NeuFlow, that addresses both high accuracy\nand computational cost concerns. The architecture follows a global-to-local\nscheme. Given the features of the input images extracted at different spatial\nresolutions, global matching is employed to estimate an initial optical flow on\nthe 1/16 resolution, capturing large displacement, which is then refined on the\n1/8 resolution with lightweight CNN layers for better accuracy. We evaluate our\napproach on Jetson Orin Nano and RTX 2080 to demonstrate efficiency\nimprovements across different computing platforms. We achieve a notable 10x-80x\nspeedup compared to several state-of-the-art methods, while maintaining\ncomparable accuracy. Our approach achieves around 30 FPS on edge computing\nplatforms, which represents a significant breakthrough in deploying complex\ncomputer vision tasks such as SLAM on small robots like drones. The full\ntraining and evaluation code is available at\nhttps://github.com/neufieldrobotics/NeuFlow.\n","authors":["Zhiyong Zhang","Huaizu Jiang","Hanumant Singh"],"pdf_url":"https://arxiv.org/pdf/2403.10425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10401v1","updated":"2024-03-15T15:34:59Z","published":"2024-03-15T15:34:59Z","title":"SculptDiff: Learning Robotic Clay Sculpting from Humans with Goal\n  Conditioned Diffusion Policy","summary":"  Manipulating deformable objects remains a challenge within robotics due to\nthe difficulties of state estimation, long-horizon planning, and predicting how\nthe object will deform given an interaction. These challenges are the most\npronounced with 3D deformable objects. We propose SculptDiff, a\ngoal-conditioned diffusion-based imitation learning framework that works with\npoint cloud state observations to directly learn clay sculpting policies for a\nvariety of target shapes. To the best of our knowledge this is the first\nreal-world method that successfully learns manipulation policies for 3D\ndeformable objects. For sculpting videos and access to our dataset and hardware\nCAD models, see the project website:\nhttps://sites.google.com/andrew.cmu.edu/imitation-sculpting/home\n","authors":["Alison Bartsch","Arvind Car","Charlotte Avra","Amir Barati Farimani"],"pdf_url":"https://arxiv.org/pdf/2403.10401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10397v1","updated":"2024-03-15T15:31:13Z","published":"2024-03-15T15:31:13Z","title":"Collaborative Aquatic Positioning system Utilising Multi-beam Sonar and\n  Depth Sensors","summary":"  Accurate positioning of underwater robots in confined environments is crucial\nfor inspection and mapping tasks and is also a prerequisite for autonomous\noperations. Presently, there are no positioning systems available that are\nsuited for real-world use in confined underwater environments, unconstrained by\nenvironmental lighting and water turbidity levels and have sufficient accuracy\nfor reliable and repeatable navigation. This shortage presents a significant\nbarrier to enhancing the capabilities of ROVs in such scenarios. This paper\nintroduces an innovative positioning system for ROVs operating in confined,\ncluttered underwater settings, achieved through the collaboration of an\nomnidirectional surface vehicle and an ROV. A formulation is proposed and\nevaluated in the simulation against ground truth. The experimental results from\nthe simulation form a proof of principle of the proposed system and also\ndemonstrate its deployability. Unlike many previous approaches, the system does\nnot rely on fixed infrastructure or tracking of features in the environment and\ncan cover large enclosed areas without additional equipment.\n","authors":["Xueliang Cheng","Barry Lennox","Keir Groves"],"pdf_url":"https://arxiv.org/pdf/2403.10397v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2310.07621v2","updated":"2024-03-15T15:03:54Z","published":"2023-10-11T16:04:02Z","title":"AG-CVG: Coverage Planning with a Mobile Recharging UGV and an\n  Energy-Constrained UAV","summary":"  In this paper, we present an approach for coverage path planning for a team\nof an energy-constrained Unmanned Aerial Vehicle (UAV) and an Unmanned Ground\nVehicle (UGV). Both the UAV and the UGV have predefined areas that they have to\ncover. The goal is to perform complete coverage by both robots while minimizing\nthe coverage time. The UGV can also serve as a mobile recharging station. The\nUAV and UGV need to occasionally rendezvous for recharging. We propose a\nheuristic method to address this NP-Hard planning problem. Our approach\ninvolves initially determining coverage paths without factoring in energy\nconstraints. Subsequently, we cluster segments of these paths and employ graph\nmatching to assign UAV clusters to UGV clusters for efficient recharging\nmanagement. We perform numerical analysis on real-world coverage applications\nand show that compared with a greedy approach our method reduces rendezvous\noverhead on average by 11.33%. We demonstrate proof-of-concept with a team of a\nVOXL m500 drone and a Clearpath Jackal ground vehicle, providing a complete\nsystem from the offline algorithm to the field execution.\n","authors":["Nare Karapetyan","Ahmad Bilal Asghar","Amisha Bhaskar","Guangyao Shi","Dinesh Manocha","Pratap Tokekar"],"pdf_url":"https://arxiv.org/pdf/2310.07621v2.pdf","comment":"ICRA 2024 Proceedings"},{"id":"http://arxiv.org/abs/2403.10340v1","updated":"2024-03-15T14:27:15Z","published":"2024-03-15T14:27:15Z","title":"Thermal-NeRF: Neural Radiance Fields from an Infrared Camera","summary":"  In recent years, Neural Radiance Fields (NeRFs) have demonstrated significant\npotential in encoding highly-detailed 3D geometry and environmental appearance,\npositioning themselves as a promising alternative to traditional explicit\nrepresentation for 3D scene reconstruction. However, the predominant reliance\non RGB imaging presupposes ideal lighting conditions: a premise frequently\nunmet in robotic applications plagued by poor lighting or visual obstructions.\nThis limitation overlooks the capabilities of infrared (IR) cameras, which\nexcel in low-light detection and present a robust alternative under such\nadverse scenarios. To tackle these issues, we introduce Thermal-NeRF, the first\nmethod that estimates a volumetric scene representation in the form of a NeRF\nsolely from IR imaging. By leveraging a thermal mapping and structural thermal\nconstraint derived from the thermal characteristics of IR imaging, our method\nshowcasing unparalleled proficiency in recovering NeRFs in visually degraded\nscenes where RGB-based methods fall short. We conduct extensive experiments to\ndemonstrate that Thermal-NeRF can achieve superior quality compared to existing\nmethods. Furthermore, we contribute a dataset for IR-based NeRF applications,\npaving the way for future research in IR NeRF reconstruction.\n","authors":["Tianxiang Ye","Qi Wu","Junyuan Deng","Guoqing Liu","Liu Liu","Songpengcheng Xia","Liang Pang","Wenxian Yu","Ling Pei"],"pdf_url":"https://arxiv.org/pdf/2403.10340v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.09237v2","updated":"2024-03-15T13:56:23Z","published":"2023-09-17T10:56:06Z","title":"Human Movement Forecasting with Loose Clothing","summary":"  Human motion prediction and trajectory forecasting are essential in human\nmotion analysis. Nowadays, sensors can be seamlessly integrated into clothing\nusing cutting-edge electronic textile (e-textile) technology, allowing\nlong-term recording of human movements outside the laboratory. Motivated by the\nrecent findings that clothing-attached sensors can achieve higher activity\nrecognition accuracy than body-attached sensors. This work investigates the\nperformance of human motion prediction using clothing-attached sensors compared\nwith body-attached sensors. It reports experiments in which statistical models\nlearnt from the movement of loose clothing are used to predict motion patterns\nof the body of robotically simulated and real human behaviours.\nCounterintuitively, the results show that fabric-attached sensors can have\nbetter motion prediction performance than rigid-attached sensors. Specifically,\nThe fabric-attached sensor can improve the accuracy up to 40% and requires up\nto 80% less duration of the past trajectory to achieve high prediction accuracy\n(i.e., 95%) compared to the rigid-attached sensor.\n","authors":["Tianchen Shen","Irene Di Giulio","Matthew Howard"],"pdf_url":"https://arxiv.org/pdf/2309.09237v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10309v1","updated":"2024-03-15T13:53:50Z","published":"2024-03-15T13:53:50Z","title":"Revolutionizing Packaging: A Robotic Bagging Pipeline with\n  Constraint-aware Structure-of-Interest Planning","summary":"  Bagging operations, common in packaging and assisted living applications, are\nchallenging due to a bag's complex deformable properties. To address this, we\ndevelop a robotic system for automated bagging tasks using an adaptive\nstructure-of-interest (SOI) manipulation approach. Our method relies on\nreal-time visual feedback to dynamically adjust manipulation without requiring\nprior knowledge of bag materials or dynamics. We present a robust pipeline\nfeaturing state estimation for SOIs using Gaussian Mixture Models (GMM), SOI\ngeneration via optimization-based bagging techniques, SOI motion planning with\nConstrained Bidirectional Rapidly-exploring Random Trees (CBiRRT), and dual-arm\nmanipulation coordinated by Model Predictive Control (MPC). Experiments\ndemonstrate the system's ability to achieve precise, stable bagging of various\nobjects using adaptive coordination of the manipulators. The proposed framework\nadvances the capability of dual-arm robots to perform more sophisticated\nautomation of common tasks involving interactions with deformable objects.\n","authors":["Jiaming Qi","Peng Zhou","Pai Zheng","Hongmin Wu","Chenguang Yang","David Navarro-Alarcon","Jia Pan"],"pdf_url":"https://arxiv.org/pdf/2403.10309v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06020v2","updated":"2024-03-15T13:53:19Z","published":"2023-10-09T18:00:01Z","title":"DyST: Towards Dynamic Neural Scene Representations on Real-World Videos","summary":"  Visual understanding of the world goes beyond the semantics and flat\nstructure of individual images. In this work, we aim to capture both the 3D\nstructure and dynamics of real-world scenes from monocular real-world videos.\nOur Dynamic Scene Transformer (DyST) model leverages recent work in neural\nscene representation to learn a latent decomposition of monocular real-world\nvideos into scene content, per-view scene dynamics, and camera pose. This\nseparation is achieved through a novel co-training scheme on monocular videos\nand our new synthetic dataset DySO. DyST learns tangible latent representations\nfor dynamic scenes that enable view generation with separate control over the\ncamera and the content of the scene.\n","authors":["Maximilian Seitzer","Sjoerd van Steenkiste","Thomas Kipf","Klaus Greff","Mehdi S. M. Sajjadi"],"pdf_url":"https://arxiv.org/pdf/2310.06020v2.pdf","comment":"ICLR 2024 spotlight. Project website: https://dyst-paper.github.io/"},{"id":"http://arxiv.org/abs/2403.10303v1","updated":"2024-03-15T13:45:27Z","published":"2024-03-15T13:45:27Z","title":"An Investigation of the Factors Influencing Evolutionary Dynamics in the\n  Joint Evolution of Robot Body and Control","summary":"  In evolutionary robotics, jointly optimising the design and the controller of\nrobots is a challenging task due to the huge complexity of the solution space\nformed by the possible combinations of body and controller. We focus on the\nevolution of robots that can be physically created rather than just simulated,\nin a rich morphological space that includes a voxel-based chassis, wheels, legs\nand sensors. On the one hand, this space offers a high degree of liberty in the\nrange of robots that can be produced, while on the other hand introduces a\ncomplexity rarely dealt with in previous works relating to matching controllers\nto designs and in evolving closed-loop control. This is usually addressed by\naugmenting evolution with a learning algorithm to refine controllers. Although\nseveral frameworks exist, few have studied the role of the \\textit{evolutionary\ndynamics} of the intertwined `evolution+learning' processes in realising\nhigh-performing robots. We conduct an in-depth study of the factors that\ninfluence these dynamics, specifically: synchronous vs asynchronous evolution;\nthe mechanism for replacing parents with offspring, and rewarding goal-based\nfitness vs novelty via selection. Results show that asynchronicity combined\nwith goal-based selection and a `replace worst' strategy results in the highest\nperformance.\n","authors":["Léni K. Le Goff","Edgar Buchanan","Emma Hart"],"pdf_url":"https://arxiv.org/pdf/2403.10303v1.pdf","comment":"9 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.10290v1","updated":"2024-03-15T13:31:27Z","published":"2024-03-15T13:31:27Z","title":"Offline Goal-Conditioned Reinforcement Learning for Shape Control of\n  Deformable Linear Objects","summary":"  Deformable objects present several challenges to the field of robotic\nmanipulation. One of the tasks that best encapsulates the difficulties arising\ndue to non-rigid behavior is shape control, which requires driving an object to\na desired shape. While shape-servoing methods have been shown successful in\ncontexts with approximately linear behavior, they can fail in tasks with more\ncomplex dynamics. We investigate an alternative approach, using offline RL to\nsolve a planar shape control problem of a Deformable Linear Object (DLO). To\nevaluate the effect of material properties, two DLOs are tested namely a soft\nrope and an elastic cord. We frame this task as a goal-conditioned offline RL\nproblem, and aim to learn to generalize to unseen goal shapes. Data collection\nand augmentation procedures are proposed to limit the amount of experimental\ndata which needs to be collected with the real robot. We evaluate the amount of\naugmentation needed to achieve the best results, and test the effect of\nregularization through behavior cloning on the TD3+BC algorithm. Finally, we\nshow that the proposed approach is able to outperform a shape-servoing baseline\nin a curvature inversion experiment.\n","authors":["Rita Laezza","Mohammadreza Shetab-Bushehri","Gabriel Arslan Waltersson","Erol Özgür","Youcef Mezouar","Yiannis Karayiannidis"],"pdf_url":"https://arxiv.org/pdf/2403.10290v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10256v1","updated":"2024-03-15T12:46:16Z","published":"2024-03-15T12:46:16Z","title":"EasyCalib: Simple and Low-Cost In-Situ Calibration for Force\n  Reconstruction with Vision-Based Tactile Sensors","summary":"  For elastomer-based tactile sensors, represented by visuotactile sensors,\nroutine calibration of mechanical parameters (Young's modulus and Poisson's\nratio) has been shown to be important for force reconstruction. However, the\nreliance on existing in-situ calibration methods for accurate force\nmeasurements limits their cost-effective and flexible applications. This\narticle proposes a new in-situ calibration scheme that relies only on comparing\ncontact deformation. Based on the detailed derivations of the normal contact\nand torsional contact theories, we designed a simple and low-cost calibration\ndevice, EasyCalib, and validated its effectiveness through extensive finite\nelement analysis. We also explored the accuracy of EasyCalib in the practical\napplication and demonstrated that accurate contact distributed force\nreconstruction can be realized based on the mechanical parameters obtained.\nEasyCalib balances low hardware cost, ease of operation, and low dependence on\ntechnical expertise and is expected to provide the necessary accuracy\nguarantees for wide applications of visuotactile sensors in the wild.\n","authors":["Mingxuan Li","Lunwei Zhang","Yen Hang Zhou","Tiemin Li","Yao Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.10256v1.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.10194v1","updated":"2024-03-15T10:57:09Z","published":"2024-03-15T10:57:09Z","title":"Ultra-Wideband Positioning System Based on ESP32 and DWM3000 Modules","summary":"  In this paper, an Ultra-Wideband (UWB) positioning system is introduced, that\nleverages six identical custom-designed boards, each featuring an ESP32\nmicrocontroller and a DWM3000 module from Quorvo. The system is capable of\nachieving localization with an accuracy of up to 10 cm, by utilizing\nTwo-Way-Ranging (TWR) measurements between one designated tag and five anchor\ndevices. The gathered distance measurements are subsequently processed by an\nExtended Kalman Filter (EKF) running locally on the tag board, enabling it to\ndetermine its own position, relying on fixed, a priori known positions of the\nanchor boards. This paper presents a comprehensive overview of the systems\narchitecture, the key components, and the capabilities it offers for indoor\npositioning and tracking applications.\n","authors":["Sebastian Krebs","Tom Herter"],"pdf_url":"https://arxiv.org/pdf/2403.10194v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18934v2","updated":"2024-03-15T10:55:39Z","published":"2024-02-29T08:01:47Z","title":"RELEAD: Resilient Localization with Enhanced LiDAR Odometry in Adverse\n  Environments","summary":"  LiDAR-based localization is valuable for applications like mining surveys and\nunderground facility maintenance. However, existing methods can struggle when\ndealing with uninformative geometric structures in challenging scenarios. This\npaper presents RELEAD, a LiDAR-centric solution designed to address\nscan-matching degradation. Our method enables degeneracy-free point cloud\nregistration by solving constrained ESIKF updates in the front end and\nincorporates multisensor constraints, even when dealing with outlier\nmeasurements, through graph optimization based on Graduated Non-Convexity\n(GNC). Additionally, we propose a robust Incremental Fixed Lag Smoother (rIFL)\nfor efficient GNC-based optimization. RELEAD has undergone extensive evaluation\nin degenerate scenarios and has outperformed existing state-of-the-art\nLiDAR-Inertial odometry and LiDAR-Visual-Inertial odometry methods.\n","authors":["Zhiqiang Chen","Hongbo Chen","Yuhua Qi","Shipeng Zhong","Dapeng Feng","Wu Jin","Weisong Wen","Ming Liu"],"pdf_url":"https://arxiv.org/pdf/2402.18934v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.08553v3","updated":"2024-03-15T10:53:11Z","published":"2023-05-15T11:30:28Z","title":"Distilling Knowledge for Short-to-Long Term Trajectory Prediction","summary":"  Long-term trajectory forecasting is an important and challenging problem in\nthe fields of computer vision, machine learning, and robotics. One fundamental\ndifficulty stands in the evolution of the trajectory that becomes more and more\nuncertain and unpredictable as the time horizon grows, subsequently increasing\nthe complexity of the problem. To overcome this issue, in this paper, we\npropose Di-Long, a new method that employs the distillation of a short-term\ntrajectory model forecaster that guides a student network for long-term\ntrajectory prediction during the training process. Given a total sequence\nlength that comprehends the allowed observation for the student network and the\ncomplementary target sequence, we let the student and the teacher solve two\ndifferent related tasks defined over the same full trajectory: the student\nobserves a short sequence and predicts a long trajectory, whereas the teacher\nobserves a longer sequence and predicts the remaining short target trajectory.\nThe teacher's task is less uncertain, and we use its accurate predictions to\nguide the student through our knowledge distillation framework, reducing\nlong-term future uncertainty. Our experiments show that our proposed Di-Long\nmethod is effective for long-term forecasting and achieves state-of-the-art\nperformance on the Intersection Drone Dataset (inD) and the Stanford Drone\nDataset (SDD).\n","authors":["Sourav Das","Guglielmo Camporese","Shaokang Cheng","Lamberto Ballan"],"pdf_url":"https://arxiv.org/pdf/2305.08553v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10187v1","updated":"2024-03-15T10:48:16Z","published":"2024-03-15T10:48:16Z","title":"Grasp Anything: Combining Teacher-Augmented Policy Gradient Learning\n  with Instance Segmentation to Grasp Arbitrary Objects","summary":"  Interactive grasping from clutter, akin to human dexterity, is one of the\nlongest-standing problems in robot learning. Challenges stem from the\nintricacies of visual perception, the demand for precise motor skills, and the\ncomplex interplay between the two. In this work, we present Teacher-Augmented\nPolicy Gradient (TAPG), a novel two-stage learning framework that synergizes\nreinforcement learning and policy distillation. After training a teacher policy\nto master the motor control based on object pose information, TAPG facilitates\nguided, yet adaptive, learning of a sensorimotor policy, based on object\nsegmentation. We zero-shot transfer from simulation to a real robot by using\nSegment Anything Model for promptable object segmentation. Our trained policies\nadeptly grasp a wide variety of objects from cluttered scenarios in simulation\nand the real world based on human-understandable prompts. Furthermore, we show\nrobust zero-shot transfer to novel objects. Videos of our experiments are\navailable at \\url{https://maltemosbach.github.io/grasp_anything}.\n","authors":["Malte Mosbach","Sven Behnke"],"pdf_url":"https://arxiv.org/pdf/2403.10187v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03359v2","updated":"2024-03-15T10:32:45Z","published":"2024-03-05T23:03:56Z","title":"RACE-SM: Reinforcement Learning Based Autonomous Control for Social\n  On-Ramp Merging","summary":"  Autonomous parallel-style on-ramp merging in human controlled traffic\ncontinues to be an existing issue for autonomous vehicle control. Existing\nnon-learning based solutions for vehicle control rely on rules and optimization\nprimarily. These methods have been seen to present significant challenges.\nRecent advancements in Deep Reinforcement Learning have shown promise and have\nreceived significant academic interest however the available learning based\napproaches show inadequate attention to other highway vehicles and often rely\non inaccurate road traffic assumptions. In addition, the parallel-style case is\nrarely considered. A novel learning based model for acceleration and lane\nchange decision making that explicitly considers the utility to both the ego\nvehicle and its surrounding vehicles which may be cooperative or uncooperative\nto produce behaviour that is socially acceptable is proposed. The novel reward\nfunction makes use of Social Value Orientation to weight the vehicle's level of\nsocial cooperation and is divided into ego vehicle and surrounding vehicle\nutility which are weighted according to the model's designated Social Value\nOrientation. A two-lane highway with an on-ramp divided into a taper-style and\nparallel-style section is considered. Simulation results indicated the\nimportance of considering surrounding vehicles in reward function design and\nshow that the proposed model matches or surpasses those in literature in terms\nof collisions while also introducing socially courteous behaviour avoiding near\nmisses and anti-social behaviour through direct consideration of the effect of\nmerging on surrounding vehicles.\n","authors":["Jordan Poots"],"pdf_url":"https://arxiv.org/pdf/2403.03359v2.pdf","comment":"Updated explanation of TTC, page 7"},{"id":"http://arxiv.org/abs/2403.10145v1","updated":"2024-03-15T09:44:02Z","published":"2024-03-15T09:44:02Z","title":"RCooper: A Real-world Large-scale Dataset for Roadside Cooperative\n  Perception","summary":"  The value of roadside perception, which could extend the boundaries of\nautonomous driving and traffic management, has gradually become more prominent\nand acknowledged in recent years. However, existing roadside perception\napproaches only focus on the single-infrastructure sensor system, which cannot\nrealize a comprehensive understanding of a traffic area because of the limited\nsensing range and blind spots. Orienting high-quality roadside perception, we\nneed Roadside Cooperative Perception (RCooper) to achieve practical\narea-coverage roadside perception for restricted traffic areas. Rcooper has its\nown domain-specific challenges, but further exploration is hindered due to the\nlack of datasets. We hence release the first real-world, large-scale RCooper\ndataset to bloom the research on practical roadside cooperative perception,\nincluding detection and tracking. The manually annotated dataset comprises 50k\nimages and 30k point clouds, including two representative traffic scenes (i.e.,\nintersection and corridor). The constructed benchmarks prove the effectiveness\nof roadside cooperation perception and demonstrate the direction of further\nresearch. Codes and dataset can be accessed at:\nhttps://github.com/AIR-THU/DAIR-RCooper.\n","authors":["Ruiyang Hao","Siqi Fan","Yingru Dai","Zhenlin Zhang","Chenxi Li","Yuntian Wang","Haibao Yu","Wenxian Yang","Jirui Yuan","Zaiqing Nie"],"pdf_url":"https://arxiv.org/pdf/2403.10145v1.pdf","comment":"Accepted by CVPR2024. 10 pages with 6 figures"},{"id":"http://arxiv.org/abs/2403.10140v1","updated":"2024-03-15T09:38:34Z","published":"2024-03-15T09:38:34Z","title":"Comparative Analysis of Programming by Demonstration Methods:\n  Kinesthetic Teaching vs Human Demonstration","summary":"  Programming by demonstration (PbD) is a simple and efficient way to program\nrobots without explicit robot programming. PbD enables unskilled operators to\neasily demonstrate and guide different robots to execute task. In this paper we\npresent comparison of demonstration methods with comprehensive user study. Each\nparticipant had to demonstrate drawing simple pattern with human demonstration\nusing virtual marker and kinesthetic teaching with robot manipulator. To\nevaluate differences between demonstration methods, we conducted user study\nwith 24 participants which filled out NASA raw task load index (rTLX) and\nsystem usability scale (SUS). We also evaluated similarity of the executed\ntrajectories to measure difference between demonstrated and ideal trajectory.\nWe concluded study with finding that human demonstration using a virtual marker\nis on average 8 times faster, superior in terms of quality and imposes 2 times\nless overall workload than kinesthetic teaching.\n","authors":["Bruno Maric","Filip Zoric","Frano Petric","Matko Orsag"],"pdf_url":"https://arxiv.org/pdf/2403.10140v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10120v1","updated":"2024-03-15T09:10:28Z","published":"2024-03-15T09:10:28Z","title":"A Novel Bioinspired Neuromorphic Vision-based Tactile Sensor for Fast\n  Tactile Perception","summary":"  Tactile sensing represents a crucial technique that can enhance the\nperformance of robotic manipulators in various tasks. This work presents a\nnovel bioinspired neuromorphic vision-based tactile sensor that uses an\nevent-based camera to quickly capture and convey information about the\ninteractions between robotic manipulators and their environment. The camera in\nthe sensor observes the deformation of a flexible skin manufactured from a\ncheap and accessible 3D printed material, whereas a 3D printed rigid casing\nhouses the components of the sensor together. The sensor is tested in a\ngrasping stage classification task involving several objects using a\ndata-driven learning-based approach. The results show that the proposed\napproach enables the sensor to detect pressing and slip incidents within a\nspeed of 2 ms. The fast tactile perception properties of the proposed sensor\nmakes it an ideal candidate for safe grasping of different objects in\nindustries that involve high-speed pick-and-place operations.\n","authors":["Omar Faris","Mohammad I. Awad","Murana A. Awad","Yahya Zweiri","Kinda Khalaf"],"pdf_url":"https://arxiv.org/pdf/2403.10120v1.pdf","comment":"9 pages, 10 figures, journal"},{"id":"http://arxiv.org/abs/2403.10117v1","updated":"2024-03-15T09:06:50Z","published":"2024-03-15T09:06:50Z","title":"Do Visual-Language Maps Capture Latent Semantics?","summary":"  Visual-language models (VLMs) have recently been introduced in robotic\nmapping by using the latent representations, i.e., embeddings, of the VLMs to\nrepresent the natural language semantics in the map. The main benefit is moving\nbeyond a small set of human-created labels toward open-vocabulary scene\nunderstanding. While there is anecdotal evidence that maps built this way\nsupport downstream tasks, such as navigation, rigorous analysis of the quality\nof the maps using these embeddings is lacking. We investigate two critical\nproperties of map quality: queryability and consistency. The evaluation of\nqueryability addresses the ability to retrieve information from the embeddings.\nWe investigate two aspects of consistency: intra-map consistency and inter-map\nconsistency. Intra-map consistency captures the ability of the embeddings to\nrepresent abstract semantic classes, and inter-map consistency captures the\ngeneralization properties of the representation. In this paper, we propose a\nway to analyze the quality of maps created using VLMs, which forms an\nopen-source benchmark to be used when proposing new open-vocabulary map\nrepresentations. We demonstrate the benchmark by evaluating the maps created by\ntwo state-of-the-art methods, VLMaps and OpenScene, using two encoders, LSeg\nand OpenSeg, using real-world data from the Matterport3D data set. We find that\nOpenScene outperforms VLMaps with both encoders, and LSeg outperforms OpenSeg\nwith both methods.\n","authors":["Matti Pekkanen","Tsvetomila Mihaylova","Francesco Verdoja","Ville Kyrki"],"pdf_url":"https://arxiv.org/pdf/2403.10117v1.pdf","comment":"Sumitted to IEEE-IROS-2024"},{"id":"http://arxiv.org/abs/2403.10108v1","updated":"2024-03-15T08:51:38Z","published":"2024-03-15T08:51:38Z","title":"Autonomous Monitoring of Pharmaceutical R&D Laboratories with 6 Axis Arm\n  Equipped Quadruped Robot and Generative AI: A Preliminary Study","summary":"  This paper presents a proof-of-concept study that examines the utilization of\ngenerative AI and mobile robotics for autonomous laboratory monitoring in the\npharmaceutical R&D laboratory. The study investigates the potential advantages\nof anomaly detection and automated reporting by multi-modal model and Vision\nFoundation Model (VFM), which have the potential to enhance compliance and\nsafety in laboratory environments. Additionally, the paper discusses the\ncurrent limitations of the generative AI approach and proposes future\ndirections for its application in lab monitoring.\n","authors":["Shunichi Hato","Nozomi Ogawa"],"pdf_url":"https://arxiv.org/pdf/2403.10108v1.pdf","comment":"8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.10105v1","updated":"2024-03-15T08:50:39Z","published":"2024-03-15T08:50:39Z","title":"Belief Aided Navigation using Bayesian Reinforcement Learning for\n  Avoiding Humans in Blind Spots","summary":"  Recent research on mobile robot navigation has focused on socially aware\nnavigation in crowded environments. However, existing methods do not adequately\naccount for human robot interactions and demand accurate location information\nfrom omnidirectional sensors, rendering them unsuitable for practical\napplications. In response to this need, this study introduces a novel\nalgorithm, BNBRL+, predicated on the partially observable Markov decision\nprocess framework to assess risks in unobservable areas and formulate movement\nstrategies under uncertainty. BNBRL+ consolidates belief algorithms with\nBayesian neural networks to probabilistically infer beliefs based on the\npositional data of humans. It further integrates the dynamics between the\nrobot, humans, and inferred beliefs to determine the navigation paths and\nembeds social norms within the reward function, thereby facilitating socially\naware navigation. Through experiments in various risk laden scenarios, this\nstudy validates the effectiveness of BNBRL+ in navigating crowded environments\nwith blind spots. The model's ability to navigate effectively in spaces with\nlimited visibility and avoid obstacles dynamically can significantly improve\nthe safety and reliability of autonomous vehicles.\n","authors":["Jinyeob Kim","Daewon Kwak","Hyunwoo Rim","Donghan Kim"],"pdf_url":"https://arxiv.org/pdf/2403.10105v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.10101v1","updated":"2024-03-15T08:46:49Z","published":"2024-03-15T08:46:49Z","title":"Agile and Safe Trajectory Planning for Quadruped Navigation with Motion\n  Anisotropy Awareness","summary":"  Quadruped robots demonstrate robust and agile movements in various terrains;\nhowever, their navigation autonomy is still insufficient. One of the challenges\nis that the motion capabilities of the quadruped robot are anisotropic along\ndifferent directions, which significantly affects the safety of quadruped robot\nnavigation. This paper proposes a navigation framework that takes into account\nthe motion anisotropy of quadruped robots including kinodynamic trajectory\ngeneration, nonlinear trajectory optimization, and nonlinear model predictive\ncontrol. In simulation and real robot tests, we demonstrate that our\nmotion-anisotropy-aware navigation framework could: (1) generate more efficient\ntrajectories and realize more agile quadruped navigation; (2) significantly\nimprove the navigation safety in challenging scenarios. The implementation is\nrealized as an open-source package at\nhttps://github.com/ZWT006/agile_navigation.\n","authors":["Wentao Zhang","Shaohang Xu","Peiyuan Cai","Lijun Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.10101v1.pdf","comment":"8 pages, 6 figures, submitted to 2024 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS)"},{"id":"http://arxiv.org/abs/2403.10083v1","updated":"2024-03-15T07:54:36Z","published":"2024-03-15T07:54:36Z","title":"HeR-DRL:Heterogeneous Relational Deep Reinforcement Learning for\n  Decentralized Multi-Robot Crowd Navigation","summary":"  Crowd navigation has received significant research attention in recent years,\nespecially DRL-based methods. While single-robot crowd scenarios have dominated\nresearch, they offer limited applicability to real-world complexities. The\nheterogeneity of interaction among multiple agent categories, like in\ndecentralized multi-robot pedestrian scenarios, are frequently disregarded.\nThis \"interaction blind spot\" hinders generalizability and restricts progress\ntowards robust navigation algorithms. In this paper, we propose a heterogeneous\nrelational deep reinforcement learning(HeR-DRL), based on customised\nheterogeneous GNN, in order to improve navigation strategies in decentralized\nmulti-robot crowd navigation. Firstly, we devised a method for constructing\nrobot-crowd heterogenous relation graph that effectively simulates the\nheterogeneous pair-wise interaction relationships. We proposed a new\nheterogeneous graph neural network for transferring and aggregating the\nheterogeneous state information. Finally, we incorporate the encoded\ninformation into deep reinforcement learning to explore the optimal policy.\nHeR-DRL are rigorously evaluated through comparing it to state-of-the-art\nalgorithms in both single-robot and multi-robot circle crowssing scenario. The\nexperimental results demonstrate that HeR-DRL surpasses the state-of-the-art\napproaches in overall performance, particularly excelling in safety and comfort\nmetrics. This underscores the significance of interaction heterogeneity for\ncrowd navigation. The source code will be publicly released in\nhttps://github.com/Zhouxy-Debugging-Den/HeR-DRL.\n","authors":["Xinyu Zhou","Songhao Piao","Wenzheng Chi","Liguo Chen","Wei Li"],"pdf_url":"https://arxiv.org/pdf/2403.10083v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10043v1","updated":"2024-03-15T06:25:22Z","published":"2024-03-15T06:25:22Z","title":"GeoPro-VO: Dynamic Obstacle Avoidance with Geometric Projector Based on\n  Velocity Obstacle","summary":"  Optimization-based approaches are widely employed to generate optimal robot\nmotions while considering various constraints, such as robot dynamics,\ncollision avoidance, and physical limitations. It is crucial to efficiently\nsolve the optimization problems in practice, yet achieving rapid computations\nremains a great challenge for optimization-based approaches with nonlinear\nconstraints. In this paper, we propose a geometric projector for dynamic\nobstacle avoidance based on velocity obstacle (GeoPro-VO) by leveraging the\nprojection feature of the velocity cone set represented by VO. Furthermore,\nwith the proposed GeoPro-VO and the augmented Lagrangian spectral projected\ngradient descent (ALSPG) algorithm, we transform an initial mixed integer\nnonlinear programming problem (MINLP) in the form of constrained model\npredictive control (MPC) into a sub-optimization problem and solve it\nefficiently. Numerical simulations are conducted to validate the fast computing\nspeed of our approach and its capability for reliable dynamic obstacle\navoidance.\n","authors":["Jihao Huang","Xuemin Chi","Jun Zeng","Zhitao Liu","Hongye Su"],"pdf_url":"https://arxiv.org/pdf/2403.10043v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10041v1","updated":"2024-03-15T06:22:32Z","published":"2024-03-15T06:22:32Z","title":"Towards Embedding Dynamic Personas in Interactive Robots: Masquerading\n  Animated Social Kinematics (MASK)","summary":"  This paper presents the design and development of an innovative interactive\nrobotic system to enhance audience engagement using character-like personas.\nBuilt upon the foundations of persona-driven dialog agents, this work extends\nthe agent application to the physical realm, employing robots to provide a more\nimmersive and interactive experience. The proposed system, named the\nMasquerading Animated Social Kinematics (MASK), leverages an anthropomorphic\nrobot which interacts with guests using non-verbal interactions, including\nfacial expressions and gestures. A behavior generation system based upon a\nfinite-state machine structure effectively conditions robotic behavior to\nconvey distinct personas. The MASK framework integrates a perception engine, a\nbehavior selection engine, and a comprehensive action library to enable\nreal-time, dynamic interactions with minimal human intervention in behavior\ndesign. Throughout the user subject studies, we examined whether the users\ncould recognize the intended character in film-character-based persona\nconditions. We conclude by discussing the role of personas in interactive\nagents and the factors to consider for creating an engaging user experience.\n","authors":["Jeongeun Park","Taemoon Jeong","Hyeonseong Kim","Taehyun Byun","Seungyoon Shin","Keunjun Choi","Jaewoon Kwon","Taeyoon Lee","Matthew Pan","Sungjoon Choi"],"pdf_url":"https://arxiv.org/pdf/2403.10041v1.pdf","comment":"4 pages, 3 figures"},{"id":"http://arxiv.org/abs/2309.13882v2","updated":"2024-03-15T06:18:10Z","published":"2023-09-25T05:25:55Z","title":"FC-Planner: A Skeleton-guided Planning Framework for Fast Aerial\n  Coverage of Complex 3D Scenes","summary":"  3D coverage path planning for UAVs is a crucial problem in diverse practical\napplications. However, existing methods have shown unsatisfactory system\nsimplicity, computation efficiency, and path quality in large and complex\nscenes. To address these challenges, we propose FC-Planner, a skeleton-guided\nplanning framework that can achieve fast aerial coverage of complex 3D scenes\nwithout pre-processing. We decompose the scene into several simple subspaces by\na skeleton-based space decomposition (SSD). Additionally, the skeleton guides\nus to effortlessly determine free space. We utilize the skeleton to efficiently\ngenerate a minimal set of specialized and informative viewpoints for complete\ncoverage. Based on SSD, a hierarchical planner effectively divides the large\nplanning problem into independent sub-problems, enabling parallel planning for\neach subspace. The carefully designed global and local planning strategies are\nthen incorporated to guarantee both high quality and efficiency in path\ngeneration. We conduct extensive benchmark and real-world tests, where\nFC-Planner computes over 10 times faster compared to state-of-the-art methods\nwith shorter path and more complete coverage. The source code will be made\npublicly available to benefit the community. Project page:\nhttps://hkust-aerial-robotics.github.io/FC-Planner.\n","authors":["Chen Feng","Haojia Li","Mingjie Zhang","Xinyi Chen","Boyu Zhou","Shaojie Shen"],"pdf_url":"https://arxiv.org/pdf/2309.13882v2.pdf","comment":"Accepted to ICRA2024. Code:\n  https://github.com/HKUST-Aerial-Robotics/FC-Planner. Video:\n  https://www.bilibili.com/video/BV1h84y1D7u5/?spm_id_from=333.999.0.0&vd_source=0af61c122e5e37c944053b57e313025a.\n  Project page: https://hkust-aerial-robotics.github.io/FC-Planner"},{"id":"http://arxiv.org/abs/2312.07146v2","updated":"2024-03-15T04:37:58Z","published":"2023-12-12T10:33:02Z","title":"CompdVision: Combining Near-Field 3D Visual and Tactile Sensing Using a\n  Compact Compound-Eye Imaging System","summary":"  As automation technologies advance, the need for compact and multi-modal\nsensors in robotic applications is growing. To address this demand, we\nintroduce CompdVision, a novel sensor that employs a compound-eye imaging\nsystem to combine near-field 3D visual and tactile sensing within a compact\nform factor. CompdVision utilizes two types of vision units to address diverse\nsensing needs, eliminating the need for complex modality conversion. Stereo\nunits with far-focus lenses can see through the transparent elastomer for depth\nestimation beyond the contact surface. Simultaneously, tactile units with\nnear-focus lenses track the movement of markers embedded in the elastomer to\nobtain contact deformation. Experimental results validate the sensor's superior\nperformance in 3D visual and tactile sensing, proving its capability for\nreliable external object depth estimation and precise measurement of tangential\nand normal contact forces. The dual modalities and compact design make the\nsensor a versatile tool for robotic manipulation.\n","authors":["Lifan Luo","Boyang Zhang","Zhijie Peng","Yik Kin Cheung","Guanlan Zhang","Zhigang Li","Michael Yu Wang","Hongyu Yu"],"pdf_url":"https://arxiv.org/pdf/2312.07146v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10012v1","updated":"2024-03-15T04:35:25Z","published":"2024-03-15T04:35:25Z","title":"Real-World Computational Aberration Correction via Quantized\n  Domain-Mixing Representation","summary":"  Relying on paired synthetic data, existing learning-based Computational\nAberration Correction (CAC) methods are confronted with the intricate and\nmultifaceted synthetic-to-real domain gap, which leads to suboptimal\nperformance in real-world applications. In this paper, in contrast to improving\nthe simulation pipeline, we deliver a novel insight into real-world CAC from\nthe perspective of Unsupervised Domain Adaptation (UDA). By incorporating\nreadily accessible unpaired real-world data into training, we formalize the\nDomain Adaptive CAC (DACAC) task, and then introduce a comprehensive Real-world\naberrated images (Realab) dataset to benchmark it. The setup task presents a\nformidable challenge due to the intricacy of understanding the target\naberration domain. To this intent, we propose a novel Quntized Domain-Mixing\nRepresentation (QDMR) framework as a potent solution to the issue. QDMR adapts\nthe CAC model to the target domain from three key aspects: (1) reconstructing\naberrated images of both domains by a VQGAN to learn a Domain-Mixing Codebook\n(DMC) which characterizes the degradation-aware priors; (2) modulating the deep\nfeatures in CAC model with DMC to transfer the target domain knowledge; and (3)\nleveraging the trained VQGAN to generate pseudo target aberrated images from\nthe source ones for convincing target domain supervision. Extensive experiments\non both synthetic and real-world benchmarks reveal that the models with QDMR\nconsistently surpass the competitive methods in mitigating the\nsynthetic-to-real gap, which produces visually pleasant real-world CAC results\nwith fewer artifacts. Codes and datasets will be made publicly available.\n","authors":["Qi Jiang","Zhonghua Yi","Shaohua Gao","Yao Gao","Xiaolong Qian","Hao Shi","Lei Sun","Zhijie Xu","Kailun Yang","Kaiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2403.10012v1.pdf","comment":"Codes and datasets will be made publicly available at\n  https://github.com/zju-jiangqi/QDMR"},{"id":"http://arxiv.org/abs/2403.10008v1","updated":"2024-03-15T04:22:14Z","published":"2024-03-15T04:22:14Z","title":"Language to Map: Topological map generation from natural language path\n  instructions","summary":"  In this paper, a method for generating a map from path information described\nusing natural language (textual path) is proposed. In recent years, robotics\nresearch mainly focus on vision-and-language navigation (VLN), a navigation\ntask based on images and textual paths. Although VLN is expected to facilitate\nuser instructions to robots, its current implementation requires users to\nexplain the details of the path for each navigation session, which results in\nhigh explanation costs for users. To solve this problem, we proposed a method\nthat creates a map as a topological map from a textual path and automatically\ncreates a new path using this map. We believe that large language models (LLMs)\ncan be used to understand textual path. Therefore, we propose and evaluate two\nmethods, one for storing implicit maps in LLMs, and the other for generating\nexplicit maps using LLMs. The implicit map is in the LLM's memory. It is\ncreated using prompts. In the explicit map, a topological map composed of nodes\nand edges is constructed and the actions at each node are stored. This makes it\npossible to estimate the path and actions at waypoints on an undescribed path,\nif enough information is available. Experimental results on path instructions\ngenerated in a real environment demonstrate that generating explicit maps\nachieves significantly higher accuracy than storing implicit maps in the LLMs.\n","authors":["Hideki Deguchi","Kazuki Shibata","Shun Taguchi"],"pdf_url":"https://arxiv.org/pdf/2403.10008v1.pdf","comment":"7 pages, 7 figures. Accepted to IEEE International Conference on\n  Robotics and Automation (ICRA) 2024"},{"id":"http://arxiv.org/abs/2311.08608v2","updated":"2024-03-15T03:55:28Z","published":"2023-11-15T00:15:07Z","title":"Multi-Radar Inertial Odometry for 3D State Estimation using mmWave\n  Imaging Radar","summary":"  State estimation is a crucial component for the successful implementation of\nrobotic systems, relying on sensors such as cameras, LiDAR, and IMUs. However,\nin real-world scenarios, the performance of these sensors is degraded by\nchallenging environments, e.g. adverse weather conditions and low-light\nscenarios. The emerging 4D imaging radar technology is capable of providing\nrobust perception in adverse conditions. Despite its potential, challenges\nremain for indoor settings where noisy radar data does not present clear\ngeometric features. Moreover, disparities in radar data resolution and field of\nview (FOV) can lead to inaccurate measurements. While prior research has\nexplored radar-inertial odometry based on Doppler velocity information,\nchallenges remain for the estimation of 3D motion because of the discrepancy in\nthe FOV and resolution of the radar sensor. In this paper, we address Doppler\nvelocity measurement uncertainties. We present a method to optimize body frame\nvelocity while managing Doppler velocity uncertainty. Based on our\nobservations, we propose a dual imaging radar configuration to mitigate the\nchallenge of discrepancy in radar data. To attain high-precision 3D state\nestimation, we introduce a strategy that seamlessly integrates radar data with\na consumer-grade IMU sensor using fixed-lag smoothing optimization. Finally, we\nevaluate our approach using real-world 3D motion data.\n","authors":["Jui-Te Huang","Ruoyang Xu","Akshay Hinduja","Michael Kaess"],"pdf_url":"https://arxiv.org/pdf/2311.08608v2.pdf","comment":"Accepted to ICRA 2024"},{"id":"http://arxiv.org/abs/2403.09990v1","updated":"2024-03-15T03:20:10Z","published":"2024-03-15T03:20:10Z","title":"CLOSURE: Fast Quantification of Pose Uncertainty Sets","summary":"  We investigate uncertainty quantification of 6D pose estimation from keypoint\nmeasurements. Assuming unknown-but-bounded measurement noises, a pose\nuncertainty set (PURSE) is a subset of SE(3) that contains all possible 6D\nposes compatible with the measurements. Despite being simple to formulate and\nits ability to embed uncertainty, the PURSE is difficult to manipulate and\ninterpret due to the many abstract nonconvex polynomial constraints. An\nappealing simplification of PURSE is to find its minimum enclosing geodesic\nball (MEGB), i.e., a point pose estimation with minimum worst-case error bound.\nWe contribute (i) a dynamical system perspective, and (ii) a fast algorithm to\ninner approximate the MEGB. Particularly, we show the PURSE corresponds to the\nfeasible set of a constrained dynamical system, and this perspective allows us\nto design an algorithm to densely sample the boundary of the PURSE through\nstrategic random walks. We then use the miniball algorithm to compute the MEGB\nof PURSE samples, leading to an inner approximation. Our algorithm is named\nCLOSURE (enClosing baLl frOm purSe boUndaRy samplEs) and it enables computing a\ncertificate of approximation tightness by calculating the relative size ratio\nbetween the inner approximation and the outer approximation. Running on a\nsingle RTX 3090 GPU, CLOSURE achieves the relative ratio of 92.8% on the LM-O\nobject pose estimation dataset and 91.4% on the 3DMatch point cloud\nregistration dataset with the average runtime less than 0.2 second. Obtaining\ncomparable worst-case error bound but 398x and 833x faster than the outer\napproximation GRCC, CLOSURE enables uncertainty quantification of 6D pose\nestimation to be implemented in real-time robot perception applications.\n","authors":["Yihuai Gao","Yukai Tang","Han Qi","Heng Yang"],"pdf_url":"https://arxiv.org/pdf/2403.09990v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09988v1","updated":"2024-03-15T03:17:50Z","published":"2024-03-15T03:17:50Z","title":"Interactive Distance Field Mapping and Planning to Enable Human-Robot\n  Collaboration","summary":"  Human-robot collaborative applications require scene representations that are\nkept up-to-date and facilitate safe motions in dynamic scenes. In this letter,\nwe present an interactive distance field mapping and planning (IDMP) framework\nthat handles dynamic objects and collision avoidance through an efficient\nrepresentation. We define \\textit{interactive} mapping and planning as the\nprocess of creating and updating the representation of the scene online while\nsimultaneously planning and adapting the robot's actions based on that\nrepresentation. Given depth sensor data, our framework builds a continuous\nfield that allows to query the distance and gradient to the closest obstacle at\nany required position in 3D space. The key aspect of this work is an efficient\nGaussian Process field that performs incremental updates and implicitly handles\ndynamic objects with a simple and elegant formulation based on a temporary\nlatent model. In terms of mapping, IDMP is able to fuse point cloud data from\nsingle and multiple sensors, query the free space at any spatial resolution,\nand deal with moving objects without semantics. In terms of planning, IDMP\nallows seamless integration with gradient-based motion planners facilitating\nfast re-planning for collision-free navigation. The framework is evaluated on\nboth real and synthetic datasets. A comparison with similar state-of-the-art\nframeworks shows superior performance when handling dynamic objects and\ncomparable or better performance in the accuracy of the computed distance and\ngradient field. Finally, we show how the framework can be used for fast motion\nplanning in the presence of moving objects. An accompanying video, code, and\ndatasets are made publicly available https://uts-ri.github.io/IDMP.\n","authors":["Usama Ali","Lan Wu","Adrian Mueller","Fouad Sukkar","Tobias Kaupp","Teresa Vidal-Calleja"],"pdf_url":"https://arxiv.org/pdf/2403.09988v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09975v1","updated":"2024-03-15T02:42:28Z","published":"2024-03-15T02:42:28Z","title":"Skeleton-Based Human Action Recognition with Noisy Labels","summary":"  Understanding human actions from body poses is critical for assistive robots\nsharing space with humans in order to make informed and safe decisions about\nthe next interaction. However, precise temporal localization and annotation of\nactivity sequences is time-consuming and the resulting labels are often noisy.\nIf not effectively addressed, label noise negatively affects the model's\ntraining, resulting in lower recognition quality. Despite its importance,\naddressing label noise for skeleton-based action recognition has been\noverlooked so far. In this study, we bridge this gap by implementing a\nframework that augments well-established skeleton-based human action\nrecognition methods with label-denoising strategies from various research areas\nto serve as the initial benchmark. Observations reveal that these baselines\nyield only marginal performance when dealing with sparse skeleton data.\nConsequently, we introduce a novel methodology, NoiseEraSAR, which integrates\nglobal sample selection, co-teaching, and Cross-Modal Mixture-of-Experts\n(CM-MOE) strategies, aimed at mitigating the adverse impacts of label noise.\nOur proposed approach demonstrates better performance on the established\nbenchmark, setting new state-of-the-art standards. The source code for this\nstudy will be made accessible at https://github.com/xuyizdby/NoiseEraSAR.\n","authors":["Yi Xu","Kunyu Peng","Di Wen","Ruiping Liu","Junwei Zheng","Yufan Chen","Jiaming Zhang","Alina Roitberg","Kailun Yang","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2403.09975v1.pdf","comment":"The source code will be made accessible at\n  https://github.com/xuyizdby/NoiseEraSAR"},{"id":"http://arxiv.org/abs/2403.09971v1","updated":"2024-03-15T02:28:26Z","published":"2024-03-15T02:28:26Z","title":"Advancing Object Goal Navigation Through LLM-enhanced Object Affinities\n  Transfer","summary":"  In object goal navigation, agents navigate towards objects identified by\ncategory labels using visual and spatial information. Previously, solely\nnetwork-based methods typically rely on historical data for object affinities\nestimation, lacking adaptability to new environments and unseen targets.\nSimultaneously, employing Large Language Models (LLMs) for navigation as either\nplanners or agents, though offering a broad knowledge base, is cost-inefficient\nand lacks targeted historical experience. Addressing these challenges, we\npresent the LLM-enhanced Object Affinities Transfer (LOAT) framework,\nintegrating LLM-derived object semantics with network-based approaches to\nleverage experiential object affinities, thus improving adaptability in\nunfamiliar settings. LOAT employs a dual-module strategy: a generalized\naffinities module for accessing LLMs' vast knowledge and an experiential\naffinities module for applying learned object semantic relationships,\ncomplemented by a dynamic fusion module harmonizing these information sources\nbased on temporal context. The resulting scores activate semantic maps before\nfeeding into downstream policies, enhancing navigation systems with\ncontext-aware inputs. Our evaluations in AI2-THOR and Habitat simulators\ndemonstrate improvements in both navigation success rates and efficiency,\nvalidating the LOAT's efficacy in integrating LLM insights for improved object\ngoal navigation.\n","authors":["Mengying Lin","Yaran Chen","Dongbin Zhao","Zhaoran Wang"],"pdf_url":"https://arxiv.org/pdf/2403.09971v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.17420v2","updated":"2024-03-15T01:04:33Z","published":"2023-12-29T01:28:40Z","title":"Exact Consistency Tests for Gaussian Mixture Filters using Normalized\n  Deviation Squared Statistics","summary":"  We consider the problem of evaluating dynamic consistency in discrete time\nprobabilistic filters that approximate stochastic system state densities with\nGaussian mixtures. Dynamic consistency means that the estimated probability\ndistributions correctly describe the actual uncertainties. As such, the problem\nof consistency testing naturally arises in applications with regards to\nestimator tuning and validation. However, due to the general complexity of the\ndensity functions involved, straightforward approaches for consistency testing\nof mixture-based estimators have remained challenging to define and implement.\nThis paper derives a new exact result for Gaussian mixture consistency testing\nwithin the framework of normalized deviation squared (NDS) statistics. It is\nshown that NDS test statistics for generic multivariate Gaussian mixture models\nexactly follow mixtures of generalized chi-square distributions, for which\nefficient computational tools are available. The accuracy and utility of the\nresulting consistency tests are numerically demonstrated on static and dynamic\nmixture estimation examples.\n","authors":["Nisar Ahmed","Luke Burks","Kailah Cabral","Alyssa Bekai Rose"],"pdf_url":"https://arxiv.org/pdf/2312.17420v2.pdf","comment":"8 pages, 4 figures; final manuscript to be published 2024 American\n  Control Conference (ACC 2024), corrected small typos and updated Fig. 1 for\n  clarity"},{"id":"http://arxiv.org/abs/2403.09933v1","updated":"2024-03-15T00:17:38Z","published":"2024-03-15T00:17:38Z","title":"Design and Control Co-Optimization for Automated Design Iteration of\n  Dexterous Anthropomorphic Soft Robotic Hands","summary":"  We automate soft robotic hand design iteration by co-optimizing design and\ncontrol policy for dexterous manipulation skills in simulation. Our design\niteration pipeline combines genetic algorithms and policy transfer to learn\ncontrol policies for nearly 400 hand designs, testing grasp quality under\nexternal force disturbances. We validate the optimized designs in the real\nworld through teleoperation of pickup and reorient manipulation tasks. Our real\nworld evaluation, from over 900 teleoperated tasks, shows that the trend in\ndesign performance in simulation resembles that of the real world. Furthermore,\nwe show that optimized hand designs from our approach outperform existing soft\nrobot hands from prior work in the real world. The results highlight the\nusefulness of simulation in guiding parameter choices for anthropomorphic soft\nrobotic hand systems, and the effectiveness of our automated design iteration\napproach, despite the sim-to-real gap.\n","authors":["Pragna Mannam","Xingyu Liu","Ding Zhao","Jean Oh","Nancy Pollard"],"pdf_url":"https://arxiv.org/pdf/2403.09933v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.04784v3","updated":"2024-03-15T00:03:33Z","published":"2023-06-07T21:07:56Z","title":"Designing Anthropomorphic Soft Hands through Interaction","summary":"  Modeling and simulating soft robot hands can aid in design iteration for\ncomplex and high degree-of-freedom (DoF) morphologies. This can be further\nsupplemented by iterating on the design based on its performance in real world\nmanipulation tasks. However, iterating in the real world requires an approach\nthat allows us to test new designs quickly at low costs. In this paper, we\nleverage rapid prototyping of the hand using 3D-printing, and utilize\nteleoperation to evaluate the hand in real world manipulation tasks. Using this\nmethod, we design a 3D-printed 16-DoF dexterous anthropomorphic soft hand\n(DASH) and iteratively improve its design over five iterations. Rapid\nprototyping techniques such as 3D-printing allow us to directly evaluate the\nfabricated hand without modeling it in simulation. We show that the design\nimproves over five design iterations through evaluating the hand's performance\nin 30 real-world teleoperated manipulation tasks. Testing over 900\ndemonstrations shows that our final version of DASH can solve 19 of the 30\ntasks compared to Allegro, a popular rigid hand in the market, which can only\nsolve 7 tasks. We open-source our CAD models as well as the teleoperated\ndataset for further study.\n","authors":["Pragna Mannam","Kenneth Shaw","Dominik Bauer","Jean Oh","Deepak Pathak","Nancy Pollard"],"pdf_url":"https://arxiv.org/pdf/2306.04784v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10736v1","updated":"2024-03-15T23:45:53Z","published":"2024-03-15T23:45:53Z","title":"Stackelberg Meta-Learning Based Shared Control for Assistive Driving","summary":"  Shared control allows the human driver to collaborate with an assistive\ndriving system while retaining the ability to make decisions and take control\nif necessary. However, human-vehicle teaming and planning are challenging due\nto environmental uncertainties, the human's bounded rationality, and the\nvariability in human behaviors. An effective collaboration plan needs to learn\nand adapt to these uncertainties. To this end, we develop a Stackelberg\nmeta-learning algorithm to create automated learning-based planning for shared\ncontrol. The Stackelberg games are used to capture the leader-follower\nstructure in the asymmetric interactions between the human driver and the\nassistive driving system. The meta-learning algorithm generates a common\nbehavioral model, which is capable of fast adaptation using a small amount of\ndriving data to assist optimal decision-making. We use a case study of an\nobstacle avoidance driving scenario to corroborate that the adapted human\nbehavioral model can successfully assist the human driver in reaching the\ntarget destination. Besides, it saves driving time compared with a driver-only\nscheme and is also robust to drivers' bounded rationality and errors.\n","authors":["Yuhan Zhao","Quanyan Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.10736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10733v1","updated":"2024-03-15T23:38:21Z","published":"2024-03-15T23:38:21Z","title":"Incentive-Compatible and Distributed Allocation for Robotic Service\n  Provision Through Contract Theory","summary":"  Robot allocation plays an essential role in facilitating robotic service\nprovision across various domains. Yet the increasing number of users and the\nuncertainties regarding the users' true service requirements have posed\nchallenges for the service provider in effectively allocating service robots to\nusers to meet their needs. In this work, we first propose a contract-based\napproach to enable incentive-compatible service selection so that the service\nprovider can effectively reduce the user's service uncertainties for precise\nservice provision. Then, we develop a distributed allocation algorithm that\nincorporates robot dynamics and collision avoidance to allocate service robots\nand address scalability concerns associated with increasing numbers of service\nrobots and users. We conduct simulations in eight scenarios to validate our\napproach. Comparative analysis against the robust allocation paradigm and two\nalternative uncertainty reduction strategies demonstrates that our approach\nachieves better allocation efficiency and accuracy.\n","authors":["Yuhan Zhao","Quanyan Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.10733v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06074v2","updated":"2024-03-15T22:48:02Z","published":"2023-10-09T18:30:36Z","title":"Momentum-Aware Trajectory Optimisation using Full-Centroidal Dynamics\n  and Implicit Inverse Kinematics","summary":"  The current state-of-the-art gradient-based optimisation frameworks are able\nto produce impressive dynamic manoeuvres such as linear and rotational jumps.\nHowever, these methods, which optimise over the full rigid-body dynamics of the\nrobot, often require precise foothold locations apriori, while real-time\nperformance is not guaranteed without elaborate regularisation and tuning of\nthe cost function. In contrast, we investigate the advantages of a task-space\noptimisation framework, with special focus on acrobatic motions. Our proposed\nformulation exploits the system's high-order nonlinearities, such as the\nnonholonomy of the angular momentum, in order to produce feasible,\nhigh-acceleration manoeuvres. By leveraging the full-centroidal dynamics of the\nquadruped ANYmal C and directly optimising its footholds and contact forces,\nthe framework is capable of producing efficient motion plans with low\ncomputational overhead. Finally, we deploy our proposed framework on the ANYmal\nC platform, and demonstrate its true capabilities through real-world\nexperiments, with the successful execution of high-acceleration motions, such\nas linear and rotational jumps. Extensive analysis of these shows that the\nrobot's dynamics can be exploited to surpass its hardware limitations of having\na high mass and low-torque limits.\n","authors":["Aristotelis Papatheodorou","Wolfgang Merkt","Alexander L. Mitchell","Ioannis Havoutis"],"pdf_url":"https://arxiv.org/pdf/2310.06074v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.12046v2","updated":"2024-03-15T22:39:18Z","published":"2024-01-22T15:38:29Z","title":"Fourier Transporter: Bi-Equivariant Robotic Manipulation in 3D","summary":"  Many complex robotic manipulation tasks can be decomposed as a sequence of\npick and place actions. Training a robotic agent to learn this sequence over\nmany different starting conditions typically requires many iterations or\ndemonstrations, especially in 3D environments. In this work, we propose Fourier\nTransporter (FourTran) which leverages the two-fold SE(d)xSE(d) symmetry in the\npick-place problem to achieve much higher sample efficiency. FourTran is an\nopen-loop behavior cloning method trained using expert demonstrations to\npredict pick-place actions on new environments. FourTran is constrained to\nincorporate symmetries of the pick and place actions independently. Our method\nutilizes a fiber space Fourier transformation that allows for memory-efficient\nconstruction. We test our proposed network on the RLbench benchmark and achieve\nstate-of-the-art results across various tasks.\n","authors":["Haojie Huang","Owen Howell","Dian Wang","Xupeng Zhu","Robin Walters","Robert Platt"],"pdf_url":"https://arxiv.org/pdf/2401.12046v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2312.16016v2","updated":"2024-03-15T21:57:21Z","published":"2023-12-26T12:00:24Z","title":"V-STRONG: Visual Self-Supervised Traversability Learning for Off-road\n  Navigation","summary":"  Reliable estimation of terrain traversability is critical for the successful\ndeployment of autonomous systems in wild, outdoor environments. Given the lack\nof large-scale annotated datasets for off-road navigation, strictly-supervised\nlearning approaches remain limited in their generalization ability. To this\nend, we introduce a novel, image-based self-supervised learning method for\ntraversability prediction, leveraging a state-of-the-art vision foundation\nmodel for improved out-of-distribution performance. Our method employs\ncontrastive representation learning using both human driving data and\ninstance-based segmentation masks during training. We show that this simple,\nyet effective, technique drastically outperforms recent methods in predicting\ntraversability for both on- and off-trail driving scenarios. We compare our\nmethod with recent baselines on both a common benchmark as well as our own\ndatasets, covering a diverse range of outdoor environments and varied terrain\ntypes. We also demonstrate the compatibility of resulting costmap predictions\nwith a model-predictive controller. Finally, we evaluate our approach on zero-\nand few-shot tasks, demonstrating unprecedented performance for generalization\nto new environments. Videos and additional material can be found here:\nhttps://sites.google.com/view/visual-traversability-learning.\n","authors":["Sanghun Jung","JoonHo Lee","Xiangyun Meng","Byron Boots","Alexander Lambert"],"pdf_url":"https://arxiv.org/pdf/2312.16016v2.pdf","comment":"ICRA 2024; 8 pages"},{"id":"http://arxiv.org/abs/2403.10700v1","updated":"2024-03-15T21:36:15Z","published":"2024-03-15T21:36:15Z","title":"Mind the Error! Detection and Localization of Instruction Errors in\n  Vision-and-Language Navigation","summary":"  Vision-and-Language Navigation in Continuous Environments (VLN-CE) is one of\nthe most intuitive yet challenging embodied AI tasks. Agents are tasked to\nnavigate towards a target goal by executing a set of low-level actions,\nfollowing a series of natural language instructions. All VLN-CE methods in the\nliterature assume that language instructions are exact. However, in practice,\ninstructions given by humans can contain errors when describing a spatial\nenvironment due to inaccurate memory or confusion. Current VLN-CE benchmarks do\nnot address this scenario, making the state-of-the-art methods in VLN-CE\nfragile in the presence of erroneous instructions from human users. For the\nfirst time, we propose a novel benchmark dataset that introduces various types\nof instruction errors considering potential human causes. This benchmark\nprovides valuable insight into the robustness of VLN systems in continuous\nenvironments. We observe a noticeable performance drop (up to -25%) in Success\nRate when evaluating the state-of-the-art VLN-CE methods on our benchmark.\nMoreover, we formally define the task of Instruction Error Detection and\nLocalization, and establish an evaluation protocol on top of our benchmark\ndataset. We also propose an effective method, based on a cross-modal\ntransformer architecture, that achieves the best performance in error detection\nand localization, compared to baselines. Surprisingly, our proposed method has\nrevealed errors in the validation set of the two commonly used datasets for\nVLN-CE, i.e., R2R-CE and RxR-CE, demonstrating the utility of our technique in\nother tasks. Code and dataset will be made available upon acceptance at\nhttps://intelligolabs.github.io/R2RIE-CE\n","authors":["Francesco Taioli","Stefano Rosa","Alberto Castellini","Lorenzo Natale","Alessio Del Bue","Alessandro Farinelli","Marco Cristani","Yiming Wang"],"pdf_url":"https://arxiv.org/pdf/2403.10700v1.pdf","comment":"3 figures, 8 pages"},{"id":"http://arxiv.org/abs/2305.14392v2","updated":"2024-03-15T21:28:59Z","published":"2023-05-22T22:59:05Z","title":"FEDORA: Flying Event Dataset fOr Reactive behAvior","summary":"  The ability of resource-constrained biological systems such as fruitflies to\nperform complex and high-speed maneuvers in cluttered environments has been one\nof the prime sources of inspiration for developing vision-based autonomous\nsystems. To emulate this capability, the perception pipeline of such systems\nmust integrate information cues from tasks including optical flow and depth\nestimation, object detection and tracking, and segmentation, among others.\nHowever, the conventional approach of employing slow, synchronous inputs from\nstandard frame-based cameras constrains these perception capabilities,\nparticularly during high-speed maneuvers. Recently, event-based sensors have\nemerged as low latency and low energy alternatives to standard frame-based\ncameras for capturing high-speed motion, effectively speeding up perception and\nhence navigation. For coherence, all the perception tasks must be trained on\nthe same input data. However, present-day datasets are curated mainly for a\nsingle or a handful of tasks and are limited in the rate of the provided ground\ntruths. To address these limitations, we present Flying Event Dataset fOr\nReactive behAviour (FEDORA) - a fully synthetic dataset for perception tasks,\nwith raw data from frame-based cameras, event-based cameras, and Inertial\nMeasurement Units (IMU), along with ground truths for depth, pose, and optical\nflow at a rate much higher than existing datasets.\n","authors":["Amogh Joshi","Adarsh Kosta","Wachirawit Ponghiran","Manish Nagaraj","Kaushik Roy"],"pdf_url":"https://arxiv.org/pdf/2305.14392v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10689v1","updated":"2024-03-15T21:18:14Z","published":"2024-03-15T21:18:14Z","title":"Latent Object Characteristics Recognition with Visual to Haptic-Audio\n  Cross-modal Transfer Learning","summary":"  Recognising the characteristics of objects while a robot handles them is\ncrucial for adjusting motions that ensure stable and efficient interactions\nwith containers. Ahead of realising stable and efficient robot motions for\nhandling/transferring the containers, this work aims to recognise the latent\nunobservable object characteristics. While vision is commonly used for object\nrecognition by robots, it is ineffective for detecting hidden objects. However,\nrecognising objects indirectly using other sensors is a challenging task. To\naddress this challenge, we propose a cross-modal transfer learning approach\nfrom vision to haptic-audio. We initially train the model with vision, directly\nobserving the target object. Subsequently, we transfer the latent space learned\nfrom vision to a second module, trained only with haptic-audio and motor data.\nThis transfer learning framework facilitates the representation of object\ncharacteristics using indirect sensor data, thereby improving recognition\naccuracy. For evaluating the recognition accuracy of our proposed learning\nframework we selected shape, position, and orientation as the object\ncharacteristics. Finally, we demonstrate online recognition of both trained and\nuntrained objects using the humanoid robot Nextage Open.\n","authors":["Namiko Saito","Joao Moura","Hiroki Uchida","Sethu Vijayakumar"],"pdf_url":"https://arxiv.org/pdf/2403.10689v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2303.05764v2","updated":"2024-03-15T20:57:34Z","published":"2023-03-10T08:04:38Z","title":"DAVIS-Ag: A Synthetic Plant Dataset for Prototyping Domain-Inspired\n  Active Vision in Agricultural Robots","summary":"  In agricultural environments, viewpoint planning can be a critical\nfunctionality for a robot with visual sensors to obtain informative\nobservations of objects of interest (e.g., fruits) from complex structures of\nplant with random occlusions. Although recent studies on active vision have\nshown some potential for agricultural tasks, each model has been designed and\nvalidated on a unique environment that would not easily be replicated for\nbenchmarking novel methods being developed later. In this paper, we introduce a\ndataset, so-called DAVIS-Ag, for promoting more extensive research on\nDomain-inspired Active VISion in Agriculture. To be specific, we leveraged our\nopen-source \"AgML\" framework and 3D plant simulator of \"Helios\" to produce 502K\nRGB images from 30K densely sampled spatial locations in 632 synthetic\norchards. Moreover, plant environments of strawberries, tomatoes, and grapes\nare considered at two different scales (i.e., Single-Plant and Multi-Plant).\nUseful labels are also provided for each image, including (1) bounding boxes\nand (2) instance segmentation masks for all identifiable fruits, and also (3)\npointers to other images of the viewpoints that are reachable by an execution\nof action so as to simulate active viewpoint selections at each time step.\nUsing DAVIS-Ag, we visualize motivating examples where fruit detection rates\ncan dramatically change depending on the pose of the camera view primarily due\nto occlusions by other components, such as leaves. Furthermore, we present\nseveral baseline models with experiment results for benchmarking in the task of\ntarget visibility maximization. Transferability to real strawberry environments\nis also investigated to demonstrate the feasibility of using the dataset for\nprototyping real-world solutions. For future research, our dataset is made\npublicly available online: https://github.com/ctyeong/DAVIS-Ag.\n","authors":["Taeyeong Choi","Dario Guevara","Zifei Cheng","Grisha Bandodkar","Chonghan Wang","Brian N. Bailey","Mason Earles","Xin Liu"],"pdf_url":"https://arxiv.org/pdf/2303.05764v2.pdf","comment":"6 pages, 6 figures, 5 tables. Submitted to CASE2024"},{"id":"http://arxiv.org/abs/2403.10677v1","updated":"2024-03-15T20:53:10Z","published":"2024-03-15T20:53:10Z","title":"Spiking Neural Networks for Fast-Moving Object Detection on Neuromorphic\n  Hardware Devices Using an Event-Based Camera","summary":"  Table tennis is a fast-paced and exhilarating sport that demands agility,\nprecision, and fast reflexes. In recent years, robotic table tennis has become\na popular research challenge for robot perception algorithms. Fast and accurate\nball detection is crucial for enabling a robotic arm to rally the ball back\nsuccessfully. Previous approaches have employed conventional frame-based\ncameras with Convolutional Neural Networks (CNNs) or traditional computer\nvision methods. In this paper, we propose a novel solution that combines an\nevent-based camera with Spiking Neural Networks (SNNs) for ball detection. We\nuse multiple state-of-the-art SNN frameworks and develop a SNN architecture for\neach of them, complying with their corresponding constraints. Additionally, we\nimplement the SNN solution across multiple neuromorphic edge devices,\nconducting comparisons of their accuracies and run-times. This furnishes\nrobotics researchers with a benchmark illustrating the capabilities achievable\nwith each SNN framework and a corresponding neuromorphic edge device. Next to\nthis comparison of SNN solutions for robots, we also show that an SNN on a\nneuromorphic edge device is able to run in real-time in a closed loop robotic\nsystem, a table tennis robot in our use case.\n","authors":["Andreas Ziegler","Karl Vetter","Thomas Gossard","Jonas Tebbe","Andreas Zell"],"pdf_url":"https://arxiv.org/pdf/2403.10677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10672v1","updated":"2024-03-15T20:48:41Z","published":"2024-03-15T20:48:41Z","title":"Riemannian Flow Matching Policy for Robot Motion Learning","summary":"  We introduce Riemannian Flow Matching Policies (RFMP), a novel model for\nlearning and synthesizing robot visuomotor policies. RFMP leverages the\nefficient training and inference capabilities of flow matching methods. By\ndesign, RFMP inherits the strengths of flow matching: the ability to encode\nhigh-dimensional multimodal distributions, commonly encountered in robotic\ntasks, and a very simple and fast inference process. We demonstrate the\napplicability of RFMP to both state-based and vision-conditioned robot motion\npolicies. Notably, as the robot state resides on a Riemannian manifold, RFMP\ninherently incorporates geometric awareness, which is crucial for realistic\nrobotic tasks. To evaluate RFMP, we conduct two proof-of-concept experiments,\ncomparing its performance against Diffusion Policies. Although both approaches\nsuccessfully learn the considered tasks, our results show that RFMP provides\nsmoother action trajectories with significantly lower inference times.\n","authors":["Max Braun","Noémie Jaquier","Leonel Rozo","Tamim Asfour"],"pdf_url":"https://arxiv.org/pdf/2403.10672v1.pdf","comment":"8 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.06091v2","updated":"2024-03-15T20:24:31Z","published":"2024-03-10T04:24:29Z","title":"Fish-inspired tracking of underwater turbulent plumes","summary":"  Autonomous ocean-exploring vehicles have begun to take advantage of onboard\nsensor measurements of water properties such as salinity and temperature to\nlocate oceanic features in real time. Such targeted sampling strategies enable\nmore rapid study of ocean environments by actively steering towards areas of\nhigh scientific value. Inspired by the ability of aquatic animals to navigate\nvia flow sensing, this work investigates hydrodynamic cues for accomplishing\ntargeted sampling using a palm-sized robotic swimmer. As proof-of-concept\nanalogy for tracking hydrothermal vent plumes in the ocean, the robot is tasked\nwith locating the center of turbulent jet flows in a 13,000-liter water tank\nusing data from onboard pressure sensors. To learn a navigation strategy, we\nfirst implemented Reinforcement Learning (RL) on a simulated version of the\nrobot navigating in proximity to turbulent jets. After training, the RL\nalgorithm discovered an effective strategy for locating the jets by following\ntransverse velocity gradients sensed by pressure sensors located on opposite\nsides of the robot. When implemented on the physical robot, this gradient\nfollowing strategy enabled the robot to successfully locate the turbulent\nplumes at more than twice the rate of random searching. Additionally, we found\nthat navigation performance improved as the distance between the pressure\nsensors increased, which can inform the design of distributed flow sensors in\nocean robots. Our results demonstrate the effectiveness and limits of\nflow-based navigation for autonomously locating hydrodynamic features of\ninterest.\n","authors":["Peter Gunnarson","John O. Dabiri"],"pdf_url":"https://arxiv.org/pdf/2403.06091v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.08731v2","updated":"2024-03-15T20:21:26Z","published":"2023-09-15T19:37:58Z","title":"Pointing the Way: Refining Radar-Lidar Localization Using Learned ICP\n  Weights","summary":"  This paper presents a novel deep-learning-based approach to improve\nlocalizing radar measurements against lidar maps. Although the state of the art\nfor localization is matching lidar data to lidar maps, radar has been\nconsidered as a promising alternative. This is largely due to radar being more\nresilient against adverse weather such as precipitation and heavy fog. To make\nuse of existing high-quality lidar maps, while maintaining performance in\nadverse weather, it is of interest to match radar data to lidar maps. However,\nowing in part to the unique artefacts present in radar measurements,\nradar-lidar localization has struggled to achieve comparable performance to\nlidar-lidar systems, preventing it from being viable for autonomous driving.\nThis work builds on an ICP-based radar-lidar localization system by including a\nlearned preprocessing step that weights radar points based on high-level scan\ninformation. Combining a proven analytical approach with a learned weight\nreduces localization errors in radar-lidar ICP results run on real-world\nautonomous driving data by up to 54.94% in translation and 68.39% in rotation,\nwhile maintaining interpretability and robustness.\n","authors":["Daniil Lisus","Johann Laconte","Keenan Burnett","Timothy D. Barfoot"],"pdf_url":"https://arxiv.org/pdf/2309.08731v2.pdf","comment":"8 pages (6 content, 2 references). 4 figures"},{"id":"http://arxiv.org/abs/2402.16690v2","updated":"2024-03-15T19:00:42Z","published":"2024-02-26T16:05:53Z","title":"Risk-Aware Non-Myopic Motion Planner for Large-Scale Robotic Swarm Using\n  CVaR Constraints","summary":"  Swarm robotics has garnered significant attention due to its ability to\naccomplish elaborate and synchronized tasks. Existing methodologies for motion\nplanning of swarm robotic systems mainly encounter difficulties in scalability\nand safety guarantee. To address these limitations, we propose a Risk-aware\nswarm mOtion planner using conditional ValuE at Risk (ROVER) that\nsystematically navigates large-scale swarms through cluttered environments\nwhile ensuring safety. ROVER formulates a finite-time model predictive control\n(FTMPC) problem predicated upon the macroscopic state of the robot swarm\nrepresented by a Gaussian Mixture Model (GMM) and integrates conditional\nvalue-at-risk (CVaR) to ensure collision avoidance. The key component of ROVER\nis imposing a CVaR constraint on the distribution of the Signed Distance\nFunction between the swarm GMM and obstacles in the FTMPC to enforce collision\navoidance. Utilizing the analytical expression of CVaR of a GMM derived in this\nwork, we develop a computationally efficient solution to solve the non-linear\nconstrained FTMPC through sequential linear programming. Simulations and\ncomparisons with representative benchmark approaches demonstrate the\neffectiveness of ROVER in flexibility, scalability, and risk mitigation.\n","authors":["Xuru Yang","Yunze Hu","Han Gao","Kang Ding","Zhaoyang Li","Pingping Zhu","Ying Sun","Chang Liu"],"pdf_url":"https://arxiv.org/pdf/2402.16690v2.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.10629v1","updated":"2024-03-15T18:52:03Z","published":"2024-03-15T18:52:03Z","title":"Virtual Elastic Tether: a New Approach for Multi-agent Navigation in\n  Confined Aquatic Environments","summary":"  Underwater navigation is a challenging area in the field of mobile robotics\ndue to inherent constraints in self-localisation and communication in\nunderwater environments. Some of these challenges can be mitigated by using\ncollaborative multi-agent teams. However, when applied underwater, the\nrobustness of traditional multi-agent collaborative control approaches is\nhighly limited due to the unavailability of reliable measurements. In this\npaper, the concept of a Virtual Elastic Tether (VET) is introduced in the\ncontext of incomplete state measurements, which represents an innovative\napproach to underwater navigation in confined spaces. The concept of VET is\nformulated and validated using the Cooperative Aquatic Vehicle Exploration\nSystem (CAVES), which is a sim-to-real multi-agent aquatic robotic platform.\nWithin this framework, a vision-based Autonomous Underwater Vehicle-Autonomous\nSurface Vehicle leader-follower formulation is developed. Experiments were\nconducted in both simulation and on a physical platform, benchmarked against a\ntraditional Image-Based Visual Servoing approach. Results indicate that the\nformation of the baseline approach fails under discrete disturbances, when\ninduced distances between the robots exceeds 0.6 m in simulation and 0.3 m in\nthe real world. In contrast, the VET-enhanced system recovers to\npre-perturbation distances within 5 seconds. Furthermore, results illustrate\nthe successful navigation of VET-enhanced CAVES in a confined water pond where\nthe baseline approach fails to perform adequately.\n","authors":["Kanzhong Yao","Xueliang Cheng","Keir Groves","Barry Lennox","Ognjen Marjanovic","Simon Watson"],"pdf_url":"https://arxiv.org/pdf/2403.10629v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2207.03386v3","updated":"2024-03-15T18:51:52Z","published":"2022-07-07T15:53:05Z","title":"Egocentric Visual Self-Modeling for Autonomous Robot Dynamics Prediction\n  and Adaptation","summary":"  The ability of robots to model their own dynamics is key to autonomous\nplanning and learning, as well as for autonomous damage detection and recovery.\nTraditionally, dynamic models are pre-programmed or learned from external\nobservations. Here, we demonstrate for the first time how a task-agnostic\ndynamic self-model can be learned using only a single first-person-view camera\nin a self-supervised manner, without any prior knowledge of robot morphology,\nkinematics, or task. Through experiments on a 12-DoF robot, we demonstrate the\ncapabilities of the model in basic locomotion tasks using visual input.\nNotably, the robot can autonomously detect anomalies, such as damaged\ncomponents, and adapt its behavior, showcasing resilience in dynamic\nenvironments. Furthermore, the model's generalizability was validated across\nrobots with different configurations, emphasizing its potential as a universal\ntool for diverse robotic systems. The egocentric visual self-model proposed in\nour work paves the way for more autonomous, adaptable, and resilient robotic\nsystems.\n","authors":["Yuhang Hu","Boyuan Chen","Hod Lipson"],"pdf_url":"https://arxiv.org/pdf/2207.03386v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.04791v2","updated":"2024-03-15T18:40:19Z","published":"2024-01-09T19:34:47Z","title":"SOS-Match: Segmentation for Open-Set Robust Correspondence Search and\n  Robot Localization in Unstructured Environments","summary":"  We present SOS-Match, a novel framework for detecting and matching objects in\nunstructured environments. Our system consists of 1) a front-end mapping\npipeline using a zero-shot segmentation model to extract object masks from\nimages and track them across frames and 2) a frame alignment pipeline that uses\nthe geometric consistency of object relationships to efficiently localize\nacross a variety of conditions. We evaluate SOS-Match on the Batvik seasonal\ndataset which includes drone flights collected over a coastal plot of southern\nFinland during different seasons and lighting conditions. Results show that our\napproach is more robust to changes in lighting and appearance than classical\nimage feature-based approaches or global descriptor methods, and it provides\nmore viewpoint invariance than learning-based feature detection and description\napproaches. SOS-Match localizes within a reference map up to 46x faster than\nother feature-based approaches and has a map size less than 0.5% the size of\nthe most compact other maps. SOS-Match is a promising new approach for landmark\ndetection and correspondence search in unstructured environments that is robust\nto changes in lighting and appearance and is more computationally efficient\nthan other approaches, suggesting that the geometric arrangement of segments is\na valuable localization cue in unstructured environments. We release our\ndatasets at https://acl.mit.edu/SOS-Match/.\n","authors":["Annika Thomas","Jouko Kinnari","Parker Lusk","Kota Kondo","Jonathan P. How"],"pdf_url":"https://arxiv.org/pdf/2401.04791v2.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2402.11768v2","updated":"2024-03-15T18:39:27Z","published":"2024-02-19T01:47:45Z","title":"Targeted Parallelization of Conflict-Based Search for Multi-Robot Path\n  Planning","summary":"  Multi-Robot Path Planning (MRPP) on graphs, equivalently known as Multi-Agent\nPath Finding (MAPF), is a well-established NP-hard problem with critically\nimportant applications. As serial computation in (near)-optimally solving MRPP\napproaches the computation efficiency limit, parallelization offers a promising\nroute to push the limit further, especially in handling hard or large MRPP\ninstances. In this study, we initiated a \\emph{targeted} parallelization effort\nto boost the performance of conflict-based search for MRPP. Specifically, when\ninstances are relatively small but robots are densely packed with strong\ninteractions, we apply a decentralized parallel algorithm that concurrently\nexplores multiple branches that leads to markedly enhanced solution discovery.\nOn the other hand, when instances are large with sparse robot-robot\ninteractions, we prioritize node expansion and conflict resolution. Our\ninnovative multi-threaded approach to parallelizing bounded-suboptimal conflict\nsearch-based algorithms demonstrates significant improvements over baseline\nserial methods in success rate or runtime. Our contribution further pushes the\nunderstanding of MRPP and charts a promising path for elevating solution\nquality and computational efficiency through parallel algorithmic strategies.\n","authors":["Teng Guo","Jingjin Yu"],"pdf_url":"https://arxiv.org/pdf/2402.11768v2.pdf","comment":"Submitted to IROS"},{"id":"http://arxiv.org/abs/2310.04566v2","updated":"2024-03-15T18:37:34Z","published":"2023-10-06T20:13:07Z","title":"Knolling Bot: Learning Robotic Object Arrangement from Tidy\n  Demonstrations","summary":"  Addressing the challenge of organizing scattered items in domestic spaces is\ncomplicated by the diversity and subjective nature of tidiness. Just as the\ncomplexity of human language allows for multiple expressions of the same idea,\nhousehold tidiness preferences and organizational patterns vary widely, so\npresetting object locations would limit the adaptability to new objects and\nenvironments. Inspired by advancements in natural language processing (NLP),\nthis paper introduces a self-supervised learning framework that allows robots\nto understand and replicate the concept of tidiness from demonstrations of\nwell-organized layouts, akin to using conversational datasets to train Large\nLanguage Models(LLM). We leverage a transformer neural network to predict the\nplacement of subsequent objects. We demonstrate a ``knolling'' system with a\nrobotic arm and an RGB camera to organize items of varying sizes and quantities\non a table. Our method not only trains a generalizable concept of tidiness,\nenabling the model to provide diverse solutions and adapt to different numbers\nof objects, but it can also incorporate human preferences to generate\ncustomized tidy tables without explicit target positions for each object.\n","authors":["Yuhang Hu","Zhizhuo Zhang","Xinyue Zhu","Ruibo Liu","Philippe Wyder","Hod Lipson"],"pdf_url":"https://arxiv.org/pdf/2310.04566v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2402.13195v2","updated":"2024-03-15T18:15:18Z","published":"2024-02-20T18:06:00Z","title":"Design and Flight Demonstration of a Quadrotor for Urban Mapping and\n  Target Tracking Research","summary":"  This paper describes the hardware design and flight demonstration of a small\nquadrotor with imaging sensors for urban mapping, hazard avoidance, and target\ntracking research. The vehicle is equipped with five cameras, including two\npairs of fisheye stereo cameras that enable a nearly omnidirectional view and a\ntwo-axis gimbaled camera. An onboard NVIDIA Jetson Orin Nano computer running\nthe Robot Operating System software is used for data collection. An autonomous\ntracking behavior was implemented to coordinate the motion of the quadrotor and\ngimbaled camera to track a moving GPS coordinate. The data collection system\nwas demonstrated through a flight test that tracked a moving GPS-tagged vehicle\nthrough a series of roads and parking lots. A map of the environment was\nreconstructed from the collected images using the Direct Sparse Odometry (DSO)\nalgorithm. The performance of the quadrotor was also characterized by acoustic\nnoise, communication range, battery voltage in hover, and maximum speed tests.\n","authors":["Collin Hague","Nick Kakavitsas","Jincheng Zhang","Chris Beam","Andrew Willis","Artur Wolek"],"pdf_url":"https://arxiv.org/pdf/2402.13195v2.pdf","comment":"7 pages, 10 figures, To be presented at IEEE SoutheastCon 2024"}]},"2024-03-18T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2403.11761v1","updated":"2024-03-18T13:14:46Z","published":"2024-03-18T13:14:46Z","title":"BEVCar: Camera-Radar Fusion for BEV Map and Object Segmentation","summary":"  Semantic scene segmentation from a bird's-eye-view (BEV) perspective plays a\ncrucial role in facilitating planning and decision-making for mobile robots.\nAlthough recent vision-only methods have demonstrated notable advancements in\nperformance, they often struggle under adverse illumination conditions such as\nrain or nighttime. While active sensors offer a solution to this challenge, the\nprohibitively high cost of LiDARs remains a limiting factor. Fusing camera data\nwith automotive radars poses a more inexpensive alternative but has received\nless attention in prior research. In this work, we aim to advance this\npromising avenue by introducing BEVCar, a novel approach for joint BEV object\nand map segmentation. The core novelty of our approach lies in first learning a\npoint-based encoding of raw radar data, which is then leveraged to efficiently\ninitialize the lifting of image features into the BEV space. We perform\nextensive experiments on the nuScenes dataset and demonstrate that BEVCar\noutperforms the current state of the art. Moreover, we show that incorporating\nradar information significantly enhances robustness in challenging\nenvironmental conditions and improves segmentation performance for distant\nobjects. To foster future research, we provide the weather split of the\nnuScenes dataset used in our experiments, along with our code and trained\nmodels at http://bevcar.cs.uni-freiburg.de.\n","authors":["Jonas Schramm","Niclas Vödisch","Kürsat Petek","B Ravi Kiran","Senthil Yogamani","Wolfram Burgard","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2403.11761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11742v1","updated":"2024-03-18T12:54:33Z","published":"2024-03-18T12:54:33Z","title":"Accelerating Model Predictive Control for Legged Robots through\n  Distributed Optimization","summary":"  This paper presents a novel approach to enhance Model Predictive Control\n(MPC) for legged robots through Distributed Optimization. Our method focuses on\ndecomposing the robot dynamics into smaller, parallelizable subsystems, and\nutilizing the Alternating Direction Method of Multipliers (ADMM) to ensure\nconsensus among them. Each subsystem is managed by its own \\gls{ocp}, with ADMM\nfacilitating consistency between their optimizations. This approach not only\ndecreases the computational time but also allows for effective scaling with\nmore complex robot configurations, facilitating the integration of additional\nsubsystems such as articulated arms on a quadruped robot. We demonstrate,\nthrough numerical evaluations, the convergence of our approach on two systems\nwith increasing complexity. In addition, we showcase that our approach\nconverges towards the same solution when compared to a state-of-the-art\ncentralized whole-body MPC implementation. Moreover, we quantitatively compare\nthe computational efficiency of our method to the centralized approach,\nrevealing up to a 75\\% reduction in computational time. Overall, our approach\noffers a promising avenue for accelerating MPC solutions for legged robots,\npaving the way for more effective utilization of the computational performance\nof modern hardware.\n","authors":["Lorenzo Amatucci","Giulio Turrisi","Angelo Bratta","Victor Barasuol","Claudio Semini"],"pdf_url":"https://arxiv.org/pdf/2403.11742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.07658v2","updated":"2024-03-18T12:47:27Z","published":"2024-01-15T13:00:35Z","title":"Robustness Evaluation of Localization Techniques for Autonomous Racing","summary":"  This work introduces SynPF, an MCL-based algorithm tailored for high-speed\nracing environments. Benchmarked against Cartographer, a state-of-the-art\npose-graph SLAM algorithm, SynPF leverages synergies from previous\nparticle-filtering methods and synthesizes them for the high-performance racing\ndomain. Our extensive in-field evaluations reveal that while Cartographer\nexcels under nominal conditions, it struggles when subjected to wheel-slip, a\ncommon phenomenon in a racing scenario due to varying grip levels and\naggressive driving behaviour. Conversely, SynPF demonstrates robustness in\nthese challenging conditions and a low-latency computation time of 1.25 ms on\non-board computers without a GPU. Using the F1TENTH platform, a 1:10 scaled\nautonomous racing vehicle, this work not only highlights the vulnerabilities of\nexisting algorithms in high-speed scenarios, tested up until 7.6 m/s, but also\nemphasizes the potential of SynPF as a viable alternative, especially in\ndeteriorating odometry conditions.\n","authors":["Tian Yi Lim","Edoardo Ghignone","Nicolas Baumann","Michele Magno"],"pdf_url":"https://arxiv.org/pdf/2401.07658v2.pdf","comment":"Accepted at the Design, Automation and Test in Europe Conference 2024\n  as an extended abstract"},{"id":"http://arxiv.org/abs/2403.11737v1","updated":"2024-03-18T12:44:54Z","published":"2024-03-18T12:44:54Z","title":"SMT-Based Dynamic Multi-Robot Task Allocation","summary":"  Multi-Robot Task Allocation (MRTA) is a problem that arises in many\napplication domains including package delivery, warehouse robotics, and\nhealthcare. In this work, we consider the problem of MRTA for a dynamic stream\nof tasks with task deadlines and capacitated agents (capacity for more than one\nsimultaneous task). Previous work commonly focuses on the static case, uses\nspecialized algorithms for restrictive task specifications, or lacks\nguarantees. We propose an approach to Dynamic MRTA for capacitated robots that\nis based on Satisfiability Modulo Theories (SMT) solving and addresses these\nconcerns. We show our approach is both sound and complete, and that the SMT\nencoding is general, enabling extension to a broader class of task\nspecifications. We show how to leverage the incremental solving capabilities of\nSMT solvers, keeping learned information when allocating new tasks arriving\nonline, and to solve non-incrementally, which we provide runtime comparisons\nof. Additionally, we provide an algorithm to start with a smaller but\npotentially incomplete encoding that can iteratively be adjusted to the\ncomplete encoding. We evaluate our method on a parameterized set of benchmarks\nencoding multi-robot delivery created from a graph abstraction of a\nhospital-like environment. The effectiveness of our approach is demonstrated\nusing a range of encodings, including quantifier-free theories of uninterpreted\nfunctions and linear or bitvector arithmetic across multiple solvers.\n","authors":["Victoria Marie Tuck","Pei-Wei Chen","Georgios Fainekos","Bardh Hoxha","Hideki Okamoto","S. Shankar Sastry","Sanjit A. Seshia"],"pdf_url":"https://arxiv.org/pdf/2403.11737v1.pdf","comment":"26 pages, 6 figures, to be published in NASA Formal Methods Symposium\n  2024"},{"id":"http://arxiv.org/abs/2403.11729v1","updated":"2024-03-18T12:38:30Z","published":"2024-03-18T12:38:30Z","title":"Hardware Design and Learning-Based Software Architecture of\n  Musculoskeletal Wheeled Robot Musashi-W for Real-World Applications","summary":"  Various musculoskeletal humanoids have been developed so far. While these\nhumanoids have the advantage of their flexible and redundant bodies that mimic\nthe human body, they are still far from being applied to real-world tasks. One\nof the reasons for this is the difficulty of bipedal walking in a flexible\nbody. Thus, we developed a musculoskeletal wheeled robot, Musashi-W, by\ncombining a wheeled base and musculoskeletal upper limbs for real-world\napplications. Also, we constructed its software system by combining static and\ndynamic body schema learning, reflex control, and visual recognition. We show\nthat the hardware and software of Musashi-W can make the most of the advantages\nof the musculoskeletal upper limbs, through several tasks of cleaning by human\nteaching, carrying a heavy object considering muscle addition, and setting a\ntable through dynamic cloth manipulation with variable stiffness.\n","authors":["Kento Kawaharazuka","Akihiro Miki","Masahiro Bando","Temma Suzuki","Yoshimoto Ribayashi","Yasunori Toshimitsu","Yuya Nagamatsu","Kei Okada","and Masayuki Inaba"],"pdf_url":"https://arxiv.org/pdf/2403.11729v1.pdf","comment":"Accepted at Humanoids2022"},{"id":"http://arxiv.org/abs/2403.11728v1","updated":"2024-03-18T12:37:41Z","published":"2024-03-18T12:37:41Z","title":"PITA: Physics-Informed Trajectory Autoencoder","summary":"  Validating robotic systems in safety-critical appli-cations requires testing\nin many scenarios including rare edgecases that are unlikely to occur,\nrequiring to complement real-world testing with testing in simulation.\nGenerative models canbe used to augment real-world datasets with generated data\ntoproduce edge case scenarios by sampling in a learned latentspace.\nAutoencoders can learn said latent representation for aspecific domain by\nlearning to reconstruct the input data froma lower-dimensional intermediate\nrepresentation. However, theresulting trajectories are not necessarily\nphysically plausible, butinstead typically contain noise that is not present in\nthe inputtrajectory. To resolve this issue, we propose the novel\nPhysics-Informed Trajectory Autoencoder (PITA) architecture, whichincorporates\na physical dynamics model into the loss functionof the autoencoder. This\nresults in smooth trajectories that notonly reconstruct the input trajectory\nbut also adhere to thephysical model. We evaluate PITA on a real-world dataset\nofvehicle trajectories and compare its performance to a normalautoencoder and a\nstate-of-the-art action-space autoencoder.\n","authors":["Johannes Fischer","Kevin Rösch","Martin Lauer","Christoph Stiller"],"pdf_url":"https://arxiv.org/pdf/2403.11728v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10397v2","updated":"2024-03-18T12:00:31Z","published":"2024-03-15T15:31:13Z","title":"Collaborative Aquatic Positioning System Utilising Multi-beam Sonar and\n  Depth Sensors","summary":"  Accurate positioning of remotely operated underwater vehicles (ROVs) in\nconfined environments is crucial for inspection and mapping tasks and is also a\nprerequisite for autonomous operations. Presently, there are no positioning\nsystems available that are suited for real-world use in confined underwater\nenvironments, unconstrained by environmental lighting and water turbidity\nlevels and have sufficient accuracy for long-term, reliable and repeatable\nnavigation. This shortage presents a significant barrier to enhancing the\ncapabilities of ROVs in such scenarios. This paper introduces an innovative\npositioning system for ROVs operating in confined, cluttered underwater\nsettings, achieved through the collaboration of an omnidirectional surface\nvehicle and an ROV. A formulation is proposed and evaluated in the simulation\nagainst ground truth. The experimental results from the simulation form a proof\nof principle of the proposed system and also demonstrate its deployability.\nUnlike many previous approaches, the system does not rely on fixed\ninfrastructure or tracking of features in the environment and can cover large\nenclosed areas without additional equipment.\n","authors":["{Xueliang Cheng","Barry Lennox","Keir Groves"],"pdf_url":"https://arxiv.org/pdf/2403.10397v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2403.11681v1","updated":"2024-03-18T11:35:18Z","published":"2024-03-18T11:35:18Z","title":"MASSTAR: A Multi-Modal and Large-Scale Scene Dataset with a Versatile\n  Toolchain for Surface Prediction and Completion","summary":"  Surface prediction and completion have been widely studied in various\napplications. Recently, research in surface completion has evolved from small\nobjects to complex large-scale scenes. As a result, researchers have begun\nincreasing the volume of data and leveraging a greater variety of data\nmodalities including rendered RGB images, descriptive texts, depth images, etc,\nto enhance algorithm performance. However, existing datasets suffer from a\ndeficiency in the amounts of scene-level models along with the corresponding\nmulti-modal information. Therefore, a method to scale the datasets and generate\nmulti-modal information in them efficiently is essential. To bridge this\nresearch gap, we propose MASSTAR: a Multi-modal lArge-scale Scene dataset with\na verSatile Toolchain for surfAce pRediction and completion. We develop a\nversatile and efficient toolchain for processing the raw 3D data from the\nenvironments. It screens out a set of fine-grained scene models and generates\nthe corresponding multi-modal data. Utilizing the toolchain, we then generate\nan example dataset composed of over a thousand scene-level models with partial\nreal-world data added. We compare MASSTAR with the existing datasets, which\nvalidates its superiority: the ability to efficiently extract high-quality\nmodels from complex scenarios to expand the dataset. Additionally, several\nrepresentative surface completion algorithms are benchmarked on MASSTAR, which\nreveals that existing algorithms can hardly deal with scene-level completion.\nWe will release the source code of our toolchain and the dataset. For more\ndetails, please see our project page at https://sysu-star.github.io/MASSTAR.\n","authors":["Guiyong Zheng","Jinqi Jiang","Chen Feng","Shaojie Shen","Boyu Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.11681v1.pdf","comment":"Submitted to IROS2024. Code: https://github.com/SYSU-STAR/MASSTAR.\n  Project Page: https://github.com/SYSU-STAR/MASSTAR"},{"id":"http://arxiv.org/abs/2403.11679v1","updated":"2024-03-18T11:31:03Z","published":"2024-03-18T11:31:03Z","title":"NEDS-SLAM: A Novel Neural Explicit Dense Semantic SLAM Framework using\n  3D Gaussian Splatting","summary":"  We propose NEDS-SLAM, an Explicit Dense semantic SLAM system based on 3D\nGaussian representation, that enables robust 3D semantic mapping, accurate\ncamera tracking, and high-quality rendering in real-time. In the system, we\npropose a Spatially Consistent Feature Fusion model to reduce the effect of\nerroneous estimates from pre-trained segmentation head on semantic\nreconstruction, achieving robust 3D semantic Gaussian mapping. Additionally, we\nemploy a lightweight encoder-decoder to compress the high-dimensional semantic\nfeatures into a compact 3D Gaussian representation, mitigating the burden of\nexcessive memory consumption. Furthermore, we leverage the advantage of 3D\nGaussian splatting, which enables efficient and differentiable novel view\nrendering, and propose a Virtual Camera View Pruning method to eliminate\noutlier GS points, thereby effectively enhancing the quality of scene\nrepresentations. Our NEDS-SLAM method demonstrates competitive performance over\nexisting dense semantic SLAM methods in terms of mapping and tracking accuracy\non Replica and ScanNet datasets, while also showing excellent capabilities in\n3D dense semantic mapping.\n","authors":["Yiming Ji","Yang Liu","Guanghu Xie","Boyu Ma","Zongwu Xie"],"pdf_url":"https://arxiv.org/pdf/2403.11679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11662v1","updated":"2024-03-18T11:07:15Z","published":"2024-03-18T11:07:15Z","title":"FE-DeTr: Keypoint Detection and Tracking in Low-quality Image Frames\n  with Events","summary":"  Keypoint detection and tracking in traditional image frames are often\ncompromised by image quality issues such as motion blur and extreme lighting\nconditions. Event cameras offer potential solutions to these challenges by\nvirtue of their high temporal resolution and high dynamic range. However, they\nhave limited performance in practical applications due to their inherent noise\nin event data. This paper advocates fusing the complementary information from\nimage frames and event streams to achieve more robust keypoint detection and\ntracking. Specifically, we propose a novel keypoint detection network that\nfuses the textural and structural information from image frames with the\nhigh-temporal-resolution motion information from event streams, namely FE-DeTr.\nThe network leverages a temporal response consistency for supervision, ensuring\nstable and efficient keypoint detection. Moreover, we use a spatio-temporal\nnearest-neighbor search strategy for robust keypoint tracking. Extensive\nexperiments are conducted on a new dataset featuring both image frames and\nevent data captured under extreme conditions. The experimental results confirm\nthe superior performance of our method over both existing frame-based and\nevent-based methods.\n","authors":["Xiangyuan Wang","Kuangyi Chen","Wen Yang","Lei Yu","Yannan Xing","Huai Yu"],"pdf_url":"https://arxiv.org/pdf/2403.11662v1.pdf","comment":"7 pages, Accepted by ICRA 2024"},{"id":"http://arxiv.org/abs/2403.11661v1","updated":"2024-03-18T11:04:21Z","published":"2024-03-18T11:04:21Z","title":"Combining Local and Global Perception for Autonomous Navigation on\n  Nano-UAVs","summary":"  A critical challenge in deploying unmanned aerial vehicles (UAVs) for\nautonomous tasks is their ability to navigate in an unknown environment. This\npaper introduces a novel vision-depth fusion approach for autonomous navigation\non nano-UAVs. We combine the visual-based PULP-Dronet convolutional neural\nnetwork for semantic information extraction, i.e., serving as the global\nperception, with 8x8px depth maps for close-proximity maneuvers, i.e., the\nlocal perception. When tested in-field, our integration strategy highlights the\ncomplementary strengths of both visual and depth sensory information. We\nachieve a 100% success rate over 15 flights in a complex navigation scenario,\nencompassing straight pathways, static obstacle avoidance, and 90{\\deg} turns.\n","authors":["Lorenzo Lamberti","Georg Rutishauser","Francesco Conti","Luca Benini"],"pdf_url":"https://arxiv.org/pdf/2403.11661v1.pdf","comment":"5 pages, 2 figures, 1 table, 1 video"},{"id":"http://arxiv.org/abs/2403.11643v1","updated":"2024-03-18T10:35:15Z","published":"2024-03-18T10:35:15Z","title":"Diffusion-Based Environment-Aware Trajectory Prediction","summary":"  The ability to predict the future trajectories of traffic participants is\ncrucial for the safe and efficient operation of autonomous vehicles. In this\npaper, a diffusion-based generative model for multi-agent trajectory prediction\nis proposed. The model is capable of capturing the complex interactions between\ntraffic participants and the environment, accurately learning the multimodal\nnature of the data. The effectiveness of the approach is assessed on\nlarge-scale datasets of real-world traffic scenarios, showing that our model\noutperforms several well-established methods in terms of prediction accuracy.\nBy the incorporation of differential motion constraints on the model output, we\nillustrate that our model is capable of generating a diverse set of realistic\nfuture trajectories. Through the use of an interaction-aware guidance signal,\nwe further demonstrate that the model can be adapted to predict the behavior of\nless cooperative agents, emphasizing its practical applicability under\nuncertain traffic conditions.\n","authors":["Theodor Westny","Björn Olofsson","Erik Frisk"],"pdf_url":"https://arxiv.org/pdf/2403.11643v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11639v1","updated":"2024-03-18T10:21:05Z","published":"2024-03-18T10:21:05Z","title":"An Accurate and Real-time Relative Pose Estimation from Triple\n  Point-line Images by Decoupling Rotation and Translation","summary":"  Line features are valid complements for point features in man-made\nenvironments. 3D-2D constraints provided by line features have been widely used\nin Visual Odometry (VO) and Structure-from-Motion (SfM) systems. However, how\nto accurately solve three-view relative motion only with 2D observations of\npoints and lines in real time has not been fully explored. In this paper, we\npropose a novel three-view pose solver based on rotation-translation decoupled\nestimation. First, a high-precision rotation estimation method based on normal\nvector coplanarity constraints that consider the uncertainty of observations is\nproposed, which can be solved by Levenberg-Marquardt (LM) algorithm\nefficiently. Second, a robust linear translation constraint that minimizes the\ndegree of the rotation components and feature observation components in\nequations is elaborately designed for estimating translations accurately.\nExperiments on synthetic data and real-world data show that the proposed\napproach improves both rotation and translation accuracy compared to the\nclassical trifocal-tensor-based method and the state-of-the-art two-view\nalgorithm in outdoor and indoor environments.\n","authors":["Zewen Xu","Yijia He","Hao Wei","Bo Xu","BinJian Xie","Yihong Wu"],"pdf_url":"https://arxiv.org/pdf/2403.11639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11623v1","updated":"2024-03-18T09:55:22Z","published":"2024-03-18T09:55:22Z","title":"Synthesizing multi-log grasp poses","summary":"  Multi-object grasping is a challenging task. It is important for energy and\ncost-efficient operation of industrial crane manipulators, such as those used\nto collect tree logs off the forest floor and onto forest machines. In this\nwork, we used synthetic data from physics simulations to explore how\ndata-driven modeling can be used to infer multi-object grasp poses from images.\nWe showed that convolutional neural networks can be trained specifically for\nsynthesizing multi-object grasps. Using RGB-Depth images and instance\nsegmentation masks as input, a U-Net model outputs grasp maps with\ncorresponding grapple orientation and opening width. Given an observation of a\npile of logs, the model can be used to synthesize and rate the possible grasp\nposes and select the most suitable one, with the possibility to respect\nchanging operational constraints such as lift capacity and reach. When tested\non previously unseen data, the proposed model found successful grasp poses with\nan accuracy of 95%.\n","authors":["Arvid Fälldin","Erik Wallin","Tommy Löfstedt","Martin Servin"],"pdf_url":"https://arxiv.org/pdf/2403.11623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01962v2","updated":"2024-03-18T09:52:10Z","published":"2024-03-04T12:01:11Z","title":"An Efficient Model-Based Approach on Learning Agile Motor Skills without\n  Reinforcement","summary":"  Learning-based methods have improved locomotion skills of quadruped robots\nthrough deep reinforcement learning. However, the sim-to-real gap and low\nsample efficiency still limit the skill transfer. To address this issue, we\npropose an efficient model-based learning framework that combines a world model\nwith a policy network. We train a differentiable world model to predict future\nstates and use it to directly supervise a Variational Autoencoder (VAE)-based\npolicy network to imitate real animal behaviors. This significantly reduces the\nneed for real interaction data and allows for rapid policy updates. We also\ndevelop a high-level network to track diverse commands and trajectories. Our\nsimulated results show a tenfold sample efficiency increase compared to\nreinforcement learning methods such as PPO. In real-world testing, our policy\nachieves proficient command-following performance with only a two-minute data\ncollection period and generalizes well to new speeds and paths.\n","authors":["Haojie Shi","Tingguang Li","Qingxu Zhu","Jiapeng Sheng","Lei Han","Max Q. -H. Meng"],"pdf_url":"https://arxiv.org/pdf/2403.01962v2.pdf","comment":"Accepted by ICRA2024"},{"id":"http://arxiv.org/abs/2403.11617v1","updated":"2024-03-18T09:50:05Z","published":"2024-03-18T09:50:05Z","title":"Frontier-Based Exploration for Multi-Robot Rendezvous in\n  Communication-Restricted Unknown Environments","summary":"  Multi-robot rendezvous and exploration are fundamental challenges in the\ndomain of mobile robotic systems. This paper addresses multi-robot rendezvous\nwithin an initially unknown environment where communication is only possible\nafter the rendezvous. Traditionally, exploration has been focused on rapidly\nmapping the environment, often leading to suboptimal rendezvous performance in\nlater stages. We adapt a standard frontier-based exploration technique to\nintegrate exploration and rendezvous into a unified strategy, with a mechanism\nthat allows robots to re-visit previously explored regions thus enhancing\nrendezvous opportunities. We validate our approach in 3D realistic simulations\nusing ROS, showcasing its effectiveness in achieving faster rendezvous times\ncompared to exploration strategies.\n","authors":["Mauro Tellaroli","Matteo Luperto","Michele Antonazzi","Nicola Basilico"],"pdf_url":"https://arxiv.org/pdf/2403.11617v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11607v1","updated":"2024-03-18T09:31:59Z","published":"2024-03-18T09:31:59Z","title":"AGRNav: Efficient and Energy-Saving Autonomous Navigation for Air-Ground\n  Robots in Occlusion-Prone Environments","summary":"  The exceptional mobility and long endurance of air-ground robots are raising\ninterest in their usage to navigate complex environments (e.g., forests and\nlarge buildings). However, such environments often contain occluded and unknown\nregions, and without accurate prediction of unobserved obstacles, the movement\nof the air-ground robot often suffers a suboptimal trajectory under existing\nmapping-based and learning-based navigation methods. In this work, we present\nAGRNav, a novel framework designed to search for safe and energy-saving\nair-ground hybrid paths. AGRNav contains a lightweight semantic scene\ncompletion network (SCONet) with self-attention to enable accurate obstacle\npredictions by capturing contextual information and occlusion area features.\nThe framework subsequently employs a query-based method for low-latency updates\nof prediction results to the grid map. Finally, based on the updated map, the\nhierarchical path planner efficiently searches for energy-saving paths for\nnavigation. We validate AGRNav's performance through benchmarks in both\nsimulated and real-world environments, demonstrating its superiority over\nclassical and state-of-the-art methods. The open-source code is available at\nhttps://github.com/jmwang0117/AGRNav.\n","authors":["Junming Wang","Zekai Sun","Xiuxian Guan","Tianxiang Shen","Zongyuan Zhang","Tianyang Duan","Dong Huang","Shixiong Zhao","Heming Cui"],"pdf_url":"https://arxiv.org/pdf/2403.11607v1.pdf","comment":"Accepted to ICRA 2024"},{"id":"http://arxiv.org/abs/2403.11577v1","updated":"2024-03-18T08:53:03Z","published":"2024-03-18T08:53:03Z","title":"3DGS-Calib: 3D Gaussian Splatting for Multimodal SpatioTemporal\n  Calibration","summary":"  Reliable multimodal sensor fusion algorithms re- quire accurate\nspatiotemporal calibration. Recently, targetless calibration techniques based\non implicit neural representations have proven to provide precise and robust\nresults. Nevertheless, such methods are inherently slow to train given the high\ncompu- tational overhead caused by the large number of sampled points required\nfor volume rendering. With the recent introduction of 3D Gaussian Splatting as\na faster alternative to implicit representation methods, we propose to leverage\nthis new ren- dering approach to achieve faster multi-sensor calibration. We\nintroduce 3DGS-Calib, a new calibration method that relies on the speed and\nrendering accuracy of 3D Gaussian Splatting to achieve multimodal\nspatiotemporal calibration that is accurate, robust, and with a substantial\nspeed-up compared to methods relying on implicit neural representations. We\ndemonstrate the superiority of our proposal with experimental results on\nsequences from KITTI-360, a widely used driving dataset.\n","authors":["Quentin Herau","Moussab Bennehar","Arthur Moreau","Nathan Piasco","Luis Roldao","Dzmitry Tsishkou","Cyrille Migniot","Pascal Vasseur","Cédric Demonceaux"],"pdf_url":"https://arxiv.org/pdf/2403.11577v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2403.11567v1","updated":"2024-03-18T08:41:36Z","published":"2024-03-18T08:41:36Z","title":"R2SNet: Scalable Domain Adaptation for Object Detection in Cloud-Based\n  Robots Ecosystems via Proposal Refinement","summary":"  We introduce a novel approach for scalable domain adaptation in cloud\nrobotics scenarios where robots rely on third-party AI inference services\npowered by large pre-trained deep neural networks. Our method is based on a\ndownstream proposal-refinement stage running locally on the robots, exploiting\na new lightweight DNN architecture, R2SNet. This architecture aims to mitigate\nperformance degradation from domain shifts by adapting the object detection\nprocess to the target environment, focusing on relabeling, rescoring, and\nsuppression of bounding-box proposals. Our method allows for local execution on\nrobots, addressing the scalability challenges of domain adaptation without\nincurring significant computational costs. Real-world results on mobile service\nrobots performing door detection show the effectiveness of the proposed method\nin achieving scalable domain adaptation.\n","authors":["Michele Antonazzi","Matteo Luperto","N. Alberto Borghese","Nicola Basilico"],"pdf_url":"https://arxiv.org/pdf/2403.11567v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11552v1","updated":"2024-03-18T08:03:47Z","published":"2024-03-18T08:03:47Z","title":"LLM^3:Large Language Model-based Task and Motion Planning with Motion\n  Failure Reasoning","summary":"  Conventional Task and Motion Planning (TAMP) approaches rely on manually\ncrafted interfaces connecting symbolic task planning with continuous motion\ngeneration. These domain-specific and labor-intensive modules are limited in\naddressing emerging tasks in real-world settings. Here, we present LLM^3, a\nnovel Large Language Model (LLM)-based TAMP framework featuring a\ndomain-independent interface. Specifically, we leverage the powerful reasoning\nand planning capabilities of pre-trained LLMs to propose symbolic action\nsequences and select continuous action parameters for motion planning.\nCrucially, LLM^3 incorporates motion planning feed- back through prompting,\nallowing the LLM to iteratively refine its proposals by reasoning about motion\nfailure. Consequently, LLM^3 interfaces between task planning and motion\nplanning, alleviating the intricate design process of handling domain- specific\nmessages between them. Through a series of simulations in a box-packing domain,\nwe quantitatively demonstrate the effectiveness of LLM^3 in solving TAMP\nproblems and the efficiency in selecting action parameters. Ablation studies\nun- derscore the significant contribution of motion failure reasoning to the\nsuccess of LLM^3. Furthermore, we conduct qualitative experiments on a physical\nmanipulator, demonstrating the practical applicability of our approach in\nreal-world settings.\n","authors":["Shu Wang","Muzhi Han","Ziyuan Jiao","Zeyu Zhang","Ying Nian Wu","Song-Chun Zhu","Hangxin Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11552v1.pdf","comment":"Submitted to IROS 2024. Codes available:\n  https://github.com/AssassinWS/LLM-TAMP"},{"id":"http://arxiv.org/abs/2310.16838v2","updated":"2024-03-18T07:20:41Z","published":"2023-10-25T17:59:41Z","title":"SparseDFF: Sparse-View Feature Distillation for One-Shot Dexterous\n  Manipulation","summary":"  Humans demonstrate remarkable skill in transferring manipulation abilities\nacross objects of varying shapes, poses, and appearances, a capability rooted\nin their understanding of semantic correspondences between different instances.\nTo equip robots with a similar high-level comprehension, we present SparseDFF,\na novel DFF for 3D scenes utilizing large 2D vision models to extract semantic\nfeatures from sparse RGBD images, a domain where research is limited despite\nits relevance to many tasks with fixed-camera setups. SparseDFF generates\nview-consistent 3D DFFs, enabling efficient one-shot learning of dexterous\nmanipulations by mapping image features to a 3D point cloud. Central to\nSparseDFF is a feature refinement network, optimized with a contrastive loss\nbetween views and a point-pruning mechanism for feature continuity. This\nfacilitates the minimization of feature discrepancies w.r.t. end-effector\nparameters, bridging demonstrations and target manipulations. Validated in\nreal-world scenarios with a dexterous hand, SparseDFF proves effective in\nmanipulating both rigid and deformable objects, demonstrating significant\ngeneralization capabilities across object and scene variations.\n","authors":["Qianxu Wang","Haotong Zhang","Congyue Deng","Yang You","Hao Dong","Yixin Zhu","Leonidas Guibas"],"pdf_url":"https://arxiv.org/pdf/2310.16838v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.17500v2","updated":"2024-03-18T07:10:02Z","published":"2024-01-30T23:18:35Z","title":"LeTO: Learning Constrained Visuomotor Policy with Differentiable\n  Trajectory Optimization","summary":"  This paper introduces LeTO, a method for learning constrained visuomotor\npolicy via differentiable trajectory optimization. Our approach uniquely\nintegrates a differentiable optimization layer into the neural network. By\nformulating the optimization layer as a trajectory optimization problem, we\nenable the model to end-to-end generate actions in a safe and controlled\nfashion without extra modules. Our method allows for the introduction of\nconstraints information during the training process, thereby balancing the\ntraining objectives of satisfying constraints, smoothing the trajectories, and\nminimizing errors with demonstrations. This \"gray box\" method marries the\noptimization-based safety and interpretability with the powerful\nrepresentational abilities of neural networks. We quantitatively evaluate LeTO\nin simulation and on the real robot. In simulation, LeTO achieves a success\nrate comparable to state-of-the-art imitation learning methods, but the\ngenerated trajectories are of less uncertainty, higher quality, and smoother.\nIn real-world experiments, we deployed LeTO to handle constraints-critical\ntasks. The results show the effectiveness of LeTO comparing with\nstate-of-the-art imitation learning approaches. We release our code at\nhttps://github.com/ZhengtongXu/LeTO.\n","authors":["Zhengtong Xu","Yu She"],"pdf_url":"https://arxiv.org/pdf/2401.17500v2.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.11515v1","updated":"2024-03-18T07:01:21Z","published":"2024-03-18T07:01:21Z","title":"SSAP: A Shape-Sensitive Adversarial Patch for Comprehensive Disruption\n  of Monocular Depth Estimation in Autonomous Navigation Applications","summary":"  Monocular depth estimation (MDE) has advanced significantly, primarily\nthrough the integration of convolutional neural networks (CNNs) and more\nrecently, Transformers. However, concerns about their susceptibility to\nadversarial attacks have emerged, especially in safety-critical domains like\nautonomous driving and robotic navigation. Existing approaches for assessing\nCNN-based depth prediction methods have fallen short in inducing comprehensive\ndisruptions to the vision system, often limited to specific local areas. In\nthis paper, we introduce SSAP (Shape-Sensitive Adversarial Patch), a novel\napproach designed to comprehensively disrupt monocular depth estimation (MDE)\nin autonomous navigation applications. Our patch is crafted to selectively\nundermine MDE in two distinct ways: by distorting estimated distances or by\ncreating the illusion of an object disappearing from the system's perspective.\nNotably, our patch is shape-sensitive, meaning it considers the specific shape\nand scale of the target object, thereby extending its influence beyond\nimmediate proximity. Furthermore, our patch is trained to effectively address\ndifferent scales and distances from the camera. Experimental results\ndemonstrate that our approach induces a mean depth estimation error surpassing\n0.5, impacting up to 99% of the targeted region for CNN-based MDE models.\nAdditionally, we investigate the vulnerability of Transformer-based MDE models\nto patch-based attacks, revealing that SSAP yields a significant error of 0.59\nand exerts substantial influence over 99% of the target region on these models.\n","authors":["Amira Guesmi","Muhammad Abdullah Hanif","Ihsen Alouani","Bassem Ouni","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2403.11515v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11513v1","updated":"2024-03-18T06:54:38Z","published":"2024-03-18T06:54:38Z","title":"Visual Preference Inference: An Image Sequence-Based Preference\n  Reasoning in Tabletop Object Manipulation","summary":"  In robotic object manipulation, human preferences can often be influenced by\nthe visual attributes of objects, such as color and shape. These properties\nplay a crucial role in operating a robot to interact with objects and align\nwith human intention. In this paper, we focus on the problem of inferring\nunderlying human preferences from a sequence of raw visual observations in\ntabletop manipulation environments with a variety of object types, named Visual\nPreference Inference (VPI). To facilitate visual reasoning in the context of\nmanipulation, we introduce the Chain-of-Visual-Residuals (CoVR) method. CoVR\nemploys a prompting mechanism that describes the difference between the\nconsecutive images (i.e., visual residuals) and incorporates such texts with a\nsequence of images to infer the user's preference. This approach significantly\nenhances the ability to understand and adapt to dynamic changes in its visual\nenvironment during manipulation tasks. Furthermore, we incorporate such texts\nalong with a sequence of images to infer the user's preferences. Our method\noutperforms baseline methods in terms of extracting human preferences from\nvisual sequences in both simulation and real-world environments. Code and\nvideos are available at:\n\\href{https://joonhyung-lee.github.io/vpi/}{https://joonhyung-lee.github.io/vpi/}\n","authors":["Joonhyung Lee","Sangbeom Park","Yongin Kwon","Jemin Lee","Minwook Ahn","Sungjoon Choi"],"pdf_url":"https://arxiv.org/pdf/2403.11513v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2403.11511v1","updated":"2024-03-18T06:42:38Z","published":"2024-03-18T06:42:38Z","title":"Sim-to-Real Grasp Detection with Global-to-Local RGB-D Adaptation","summary":"  This paper focuses on the sim-to-real issue of RGB-D grasp detection and\nformulates it as a domain adaptation problem. In this case, we present a\nglobal-to-local method to address hybrid domain gaps in RGB and depth data and\ninsufficient multi-modal feature alignment. First, a self-supervised rotation\npre-training strategy is adopted to deliver robust initialization for RGB and\ndepth networks. We then propose a global-to-local alignment pipeline with\nindividual global domain classifiers for scene features of RGB and depth images\nas well as a local one specifically working for grasp features in the two\nmodalities. In particular, we propose a grasp prototype adaptation module,\nwhich aims to facilitate fine-grained local feature alignment by dynamically\nupdating and matching the grasp prototypes from the simulation and real-world\nscenarios throughout the training process. Due to such designs, the proposed\nmethod substantially reduces the domain shift and thus leads to consistent\nperformance improvements. Extensive experiments are conducted on the\nGraspNet-Planar benchmark and physical environment, and superior results are\nachieved which demonstrate the effectiveness of our method.\n","authors":["Haoxiang Ma","Ran Qin","Modi shi","Boyang Gao","Di Huang"],"pdf_url":"https://arxiv.org/pdf/2403.11511v1.pdf","comment":"Accepted at ICRA 2024"},{"id":"http://arxiv.org/abs/2403.11496v1","updated":"2024-03-18T06:00:38Z","published":"2024-03-18T06:00:38Z","title":"MCD: Diverse Large-Scale Multi-Campus Dataset for Robot Perception","summary":"  Perception plays a crucial role in various robot applications. However,\nexisting well-annotated datasets are biased towards autonomous driving\nscenarios, while unlabelled SLAM datasets are quickly over-fitted, and often\nlack environment and domain variations. To expand the frontier of these fields,\nwe introduce a comprehensive dataset named MCD (Multi-Campus Dataset),\nfeaturing a wide range of sensing modalities, high-accuracy ground truth, and\ndiverse challenging environments across three Eurasian university campuses. MCD\ncomprises both CCS (Classical Cylindrical Spinning) and NRE (Non-Repetitive\nEpicyclic) lidars, high-quality IMUs (Inertial Measurement Units), cameras, and\nUWB (Ultra-WideBand) sensors. Furthermore, in a pioneering effort, we introduce\nsemantic annotations of 29 classes over 59k sparse NRE lidar scans across three\ndomains, thus providing a novel challenge to existing semantic segmentation\nresearch upon this largely unexplored lidar modality. Finally, we propose, for\nthe first time to the best of our knowledge, continuous-time ground truth based\non optimization-based registration of lidar-inertial data on large survey-grade\nprior maps, which are also publicly released, each several times the size of\nexisting ones. We conduct a rigorous evaluation of numerous state-of-the-art\nalgorithms on MCD, report their performance, and highlight the challenges\nawaiting solutions from the research community.\n","authors":["Thien-Minh Nguyen","Shenghai Yuan","Thien Hoang Nguyen","Pengyu Yin","Haozhi Cao","Lihua Xie","Maciej Wozniak","Patric Jensfelt","Marko Thiel","Justin Ziegenbein","Noel Blunder"],"pdf_url":"https://arxiv.org/pdf/2403.11496v1.pdf","comment":"Accepted by The IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition 2024"},{"id":"http://arxiv.org/abs/2403.11492v1","updated":"2024-03-18T05:53:20Z","published":"2024-03-18T05:53:20Z","title":"SmartRefine: An Scenario-Adaptive Refinement Framework for Efficient\n  Motion Prediction","summary":"  Predicting the future motion of surrounding agents is essential for\nautonomous vehicles (AVs) to operate safely in dynamic, human-robot-mixed\nenvironments. Context information, such as road maps and surrounding agents'\nstates, provides crucial geometric and semantic information for motion behavior\nprediction. To this end, recent works explore two-stage prediction frameworks\nwhere coarse trajectories are first proposed, and then used to select critical\ncontext information for trajectory refinement. However, they either incur a\nlarge amount of computation or bring limited improvement, if not both. In this\npaper, we introduce a novel scenario-adaptive refinement strategy, named\nSmartRefine, to refine prediction with minimal additional computation.\nSpecifically, SmartRefine can comprehensively adapt refinement configurations\nbased on each scenario's properties, and smartly chooses the number of\nrefinement iterations by introducing a quality score to measure the prediction\nquality and remaining refinement potential of each scenario. SmartRefine is\ndesigned as a generic and flexible approach that can be seamlessly integrated\ninto most state-of-the-art motion prediction models. Experiments on Argoverse\n(1 & 2) show that our method consistently improves the prediction accuracy of\nmultiple state-of-the-art prediction models. Specifically, by adding\nSmartRefine to QCNet, we outperform all published ensemble-free works on the\nArgoverse 2 leaderboard (single agent track) at submission. Comprehensive\nstudies are also conducted to ablate design choices and explore the mechanism\nbehind multi-iteration refinement. Codes are available at\nhttps://github.com/opendilab/SmartRefine/\n","authors":["Yang Zhou","Hao Shao","Letian Wang","Steven L. Waslander","Hongsheng Li","Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11492v1.pdf","comment":"Camera-ready version for CVPR 2024"},{"id":"http://arxiv.org/abs/2403.11487v1","updated":"2024-03-18T05:38:07Z","published":"2024-03-18T05:38:07Z","title":"Can LLMs Generate Human-Like Wayfinding Instructions? Towards\n  Platform-Agnostic Embodied Instruction Synthesis","summary":"  We present a novel approach to automatically synthesize \"wayfinding\ninstructions\" for an embodied robot agent. In contrast to prior approaches that\nare heavily reliant on human-annotated datasets designed exclusively for\nspecific simulation platforms, our algorithm uses in-context learning to\ncondition an LLM to generate instructions using just a few references. Using an\nLLM-based Visual Question Answering strategy, we gather detailed information\nabout the environment which is used by the LLM for instruction synthesis. We\nimplement our approach on multiple simulation platforms including Matterport3D,\nAI Habitat and ThreeDWorld, thereby demonstrating its platform-agnostic nature.\nWe subjectively evaluate our approach via a user study and observe that 83.3%\nof users find the synthesized instructions accurately capture the details of\nthe environment and show characteristics similar to those of human-generated\ninstructions. Further, we conduct zero-shot navigation with multiple approaches\non the REVERIE dataset using the generated instructions, and observe very close\ncorrelation with the baseline on standard success metrics (< 1% change in SR),\nquantifying the viability of generated instructions in replacing\nhuman-annotated data. To the best of our knowledge, ours is the first\nLLM-driven approach capable of generating \"human-like\" instructions in a\nplatform-agnostic manner, without requiring any form of training.\n","authors":["Vishnu Sashank Dorbala","Sanjoy Chowdhury","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2403.11487v1.pdf","comment":"13 Pages"},{"id":"http://arxiv.org/abs/2403.11484v1","updated":"2024-03-18T05:14:40Z","published":"2024-03-18T05:14:40Z","title":"Robot Navigation in Unknown and Cluttered Workspace with Dynamical\n  System Modulation in Starshaped Roadmap","summary":"  This paper presents a novel reactive motion planning framework for navigating\nrobots in unknown and cluttered 2D workspace. Typical existing methods are\ndeveloped by enforcing the robot staying in free regions represented by the\nlocally extracted ellipse or polygon. Instead, we navigate the robot in free\nspace with an alternate starshaped decomposition, which is calculated directly\nfrom real-time sensor data. Additionally, a roadmap is constructed\nincrementally to maintain the connectivity information of the starshaped\nregions. Compared to the roadmap built upon connected polygons or ellipses in\nthe conventional approaches, the concave starshaped region is better suited to\ncapture the natural distribution of sensor data, so that the perception\ninformation can be fully exploited for robot navigation. In this sense,\nconservative and myopic behaviors are avoided with the proposed approach, and\nintricate obstacle configurations can be suitably accommodated in unknown and\ncluttered environments. Then, we design a heuristic exploration algorithm on\nthe roadmap to determine the frontier points of the starshaped regions, from\nwhich short-term goals are selected to attract the robot towards the goal\nconfiguration. It is noteworthy that, a recovery mechanism is developed on the\nroadmap that is triggered once a non-extendable short-term goal is reached.\nThis mechanism renders it possible to deal with dead-end situations that can be\ntypically encountered in unknown and cluttered environments. Furthermore, safe\nand smooth motion within the starshaped regions is generated by employing the\nDynamical System Modulation (DSM) approach on the constructed roadmap. Through\ncomprehensive evaluation in both simulations and real-world experiments, the\nproposed method outperforms the benchmark methods in terms of success rate and\ntraveling time.\n","authors":["Kai Chen","Haichao Liu","Yulin Li","Jianghua Duan","Lei Zhu","Jun Ma"],"pdf_url":"https://arxiv.org/pdf/2403.11484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11459v1","updated":"2024-03-18T04:20:16Z","published":"2024-03-18T04:20:16Z","title":"ALDM-Grasping: Diffusion-aided Zero-Shot Sim-to-Real Transfer for Robot\n  Grasping","summary":"  To tackle the \"reality gap\" encountered in Sim-to-Real transfer, this study\nproposes a diffusion-based framework that minimizes inconsistencies in grasping\nactions between the simulation settings and realistic environments. The process\nbegins by training an adversarial supervision layout-to-image diffusion\nmodel(ALDM). Then, leverage the ALDM approach to enhance the simulation\nenvironment, rendering it with photorealistic fidelity, thereby optimizing\nrobotic grasp task training. Experimental results indicate this framework\noutperforms existing models in both success rates and adaptability to new\nenvironments through improvements in the accuracy and reliability of visual\ngrasping actions under a variety of conditions. Specifically, it achieves a\n75\\% success rate in grasping tasks under plain backgrounds and maintains a\n65\\% success rate in more complex scenarios. This performance demonstrates this\nframework excels at generating controlled image content based on text\ndescriptions, identifying object grasp points, and demonstrating zero-shot\nlearning in complex, unseen scenarios.\n","authors":["Yiwei Li","Zihao Wu","Huaqin Zhao","Tianze Yang","Zhengliang Liu","Peng Shu","Jin Sun","Ramviyas Parasuraman","Tianming Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11459v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14134v2","updated":"2024-03-18T04:08:54Z","published":"2023-12-21T18:55:05Z","title":"Diffusion Reward: Learning Rewards via Conditional Video Diffusion","summary":"  Learning rewards from expert videos offers an affordable and effective\nsolution to specify the intended behaviors for reinforcement learning tasks. In\nthis work, we propose Diffusion Reward, a novel framework that learns rewards\nfrom expert videos via conditional video diffusion models for solving complex\nvisual RL problems. Our key insight is that lower generative diversity is\nobserved when conditioned on expert trajectories. Diffusion Reward is\naccordingly formalized by the negative of conditional entropy that encourages\nproductive exploration of expert-like behaviors. We show the efficacy of our\nmethod over 10 robotic manipulation tasks from MetaWorld and Adroit with visual\ninput and sparse reward. Moreover, Diffusion Reward could even solve unseen\ntasks successfully and effectively, largely surpassing baseline methods.\nProject page and code: https://diffusion-reward.github.io/.\n","authors":["Tao Huang","Guangqi Jiang","Yanjie Ze","Huazhe Xu"],"pdf_url":"https://arxiv.org/pdf/2312.14134v2.pdf","comment":"Project page and code: https://diffusion-reward.github.io/"},{"id":"http://arxiv.org/abs/2310.14402v2","updated":"2024-03-18T03:35:28Z","published":"2023-10-22T20:25:08Z","title":"Value of Assistance for Grasping","summary":"  In multiple realistic settings, a robot is tasked with grasping an object\nwithout knowing its exact pose and relies on a probabilistic estimation of the\npose to decide how to attempt the grasp. We support settings in which it is\npossible to provide the robot with an observation of the object before a grasp\nis attempted but this possibility is limited and there is a need to decide\nwhich sensing action would be most beneficial. We support this decision by\noffering a novel Value of Assistance (VOA) measure for assessing the expected\neffect a specific observation will have on the robot's ability to complete its\ntask. We evaluate our suggested measure in simulated and real-world\ncollaborative grasping settings.\n","authors":["Mohammad Masarwy","Yuval Goshen","David Dovrat","Sarah Keren"],"pdf_url":"https://arxiv.org/pdf/2310.14402v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14150v7","updated":"2024-03-18T03:27:35Z","published":"2023-09-25T14:04:31Z","title":"Fast LiDAR Informed Visual Search in Unseen Indoor Environments","summary":"  This paper explores the problem of planning for visual search without prior\nmap information. We leverage the pixel-wise environment perception problem\nwhere one is given wide Field of View 2D scan data and must perform LiDAR\nsegmentation to contextually label points in the surroundings. These pixel\nclassifications provide an informed prior on which to plan next best viewpoints\nduring visual search tasks. We present LIVES: LiDAR Informed Visual Search, a\nmethod aimed at finding objects of interest in unknown indoor environments. A\nrobust map-free classifier is trained from expert data collected using a simple\ncart platform equipped with a map-based classifier. An autonomous exploration\nplanner takes the contextual data from scans and uses that prior to plan\nviewpoints more likely to yield detection of the search target. We propose a\nutility function that accounts for traditional metrics like information gain\nand path cost and for the contextual information. LIVES is baselined against\nseveral existing exploration methods in simulation to verify its performance.\nIt is validated in real-world experiments with single and multiple search\nobjects with a Spot robot in two unseen environments. Videos of experiments,\nimplementation details and open source code can be found at\nhttps://sites.google.com/view/lives-2024/home.\n","authors":["Ryan Gupta","Kyle Morgenstein","Steven Ortega","Luis Sentis"],"pdf_url":"https://arxiv.org/pdf/2309.14150v7.pdf","comment":"6 pages + references. 6 figures. 1 algorithm. 1 table"},{"id":"http://arxiv.org/abs/2403.11432v1","updated":"2024-03-18T02:59:13Z","published":"2024-03-18T02:59:13Z","title":"Demystifying Deep Reinforcement Learning-Based Autonomous Vehicle\n  Decision-Making","summary":"  With the advent of universal function approximators in the domain of\nreinforcement learning, the number of practical applications leveraging deep\nreinforcement learning (DRL) has exploded. Decision-making in automated driving\ntasks has emerged as a chief application among them, taking the sensor data or\nthe higher-order kinematic variables as the input and providing a discrete\nchoice or continuous control output. However, the black-box nature of the\nmodels presents an overwhelming limitation that restricts the real-world\ndeployment of DRL in autonomous vehicles (AVs). Therefore, in this research\nwork, we focus on the interpretability of an attention-based DRL framework. We\nuse a continuous proximal policy optimization-based DRL algorithm as the\nbaseline model and add a multi-head attention framework in an open-source AV\nsimulation environment. We provide some analytical techniques for discussing\nthe interpretability of the trained models in terms of explainability and\ncausality for spatial and temporal correlations. We show that the weights in\nthe first head encode the positions of the neighboring vehicles while the\nsecond head focuses on the leader vehicle exclusively. Also, the ego vehicle's\naction is causally dependent on the vehicles in the target lane spatially and\ntemporally. Through these findings, we reliably show that these techniques can\nhelp practitioners decipher the results of the DRL algorithms.\n","authors":["Hanxi Wan","Pei Li","Arpan Kusari"],"pdf_url":"https://arxiv.org/pdf/2403.11432v1.pdf","comment":"Submitted for peer-review"},{"id":"http://arxiv.org/abs/2403.11412v1","updated":"2024-03-18T02:00:28Z","published":"2024-03-18T02:00:28Z","title":"Expert Composer Policy: Scalable Skill Repertoire for Quadruped Robots","summary":"  We propose the expert composer policy, a framework to reliably expand the\nskill repertoire of quadruped agents. The composer policy links pair of experts\nvia transitions to a sampled target state, allowing experts to be composed\nsequentially. Each expert specializes in a single skill, such as a locomotion\ngait or a jumping motion. Instead of a hierarchical or mixture-of-experts\narchitecture, we train a single composer policy in an independent process that\nis not conditioned on the other expert policies. By reusing the same composer\npolicy, our approach enables adding new experts without affecting existing\nones, enabling incremental repertoire expansion and preserving original motion\nquality. We measured the transition success rate of 72 transition pairs and\nachieved an average success rate of 99.99\\%, which is over 10\\% higher than the\nbaseline random approach, and outperforms other state-of-the-art methods. Using\ndomain randomization during training we ensure a successful transfer to the\nreal world, where we achieve an average transition success rate of 97.22\\%\n(N=360) in our experiments.\n","authors":["Guilherme Christmann","Ying-Sheng Luo","Wei-Chao Chen"],"pdf_url":"https://arxiv.org/pdf/2403.11412v1.pdf","comment":"ICRA 2024"},{"id":"http://arxiv.org/abs/2403.11396v1","updated":"2024-03-18T01:08:18Z","published":"2024-03-18T01:08:18Z","title":"Beyond Uncertainty: Risk-Aware Active View Acquisition for Safe Robot\n  Navigation and 3D Scene Understanding with FisherRF","summary":"  This work proposes a novel approach to bolster both the robot's risk\nassessment and safety measures while deepening its understanding of 3D scenes,\nwhich is achieved by leveraging Radiance Field (RF) models and 3D Gaussian\nSplatting. To further enhance these capabilities, we incorporate additional\nsampled views from the environment with the RF model. One of our key\ncontributions is the introduction of Risk-aware Environment Masking (RaEM),\nwhich prioritizes crucial information by selecting the next-best-view that\nmaximizes the expected information gain. This targeted approach aims to\nminimize uncertainties surrounding the robot's path and enhance the safety of\nits navigation. Our method offers a dual benefit: improved robot safety and\nincreased efficiency in risk-aware 3D scene reconstruction and understanding.\nExtensive experiments in real-world scenarios demonstrate the effectiveness of\nour proposed approach, highlighting its potential to establish a robust and\nsafety-focused framework for active robot exploration and 3D scene\nunderstanding.\n","authors":["Guangyi Liu","Wen Jiang","Boshu Lei","Vivek Pandey","Kostas Daniilidis","Nader Motee"],"pdf_url":"https://arxiv.org/pdf/2403.11396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11384v1","updated":"2024-03-18T00:22:30Z","published":"2024-03-18T00:22:30Z","title":"A Systematic Review of XR-based Remote Human-Robot Interaction Systems","summary":"  This survey provides an exhaustive review of the applications of extended\nreality (XR) technologies in the field of remote human-computer interaction\n(HRI). We developed a systematic search strategy based on the PRISMA\nmethodology. From the initial 2,561 articles selected, 100 research papers that\nmet our inclusion criteria were included. We categorized and summarized the\ndomain in detail, delving into XR technologies, including augmented reality\n(AR), virtual reality (VR), and mixed reality (MR), and their applications in\nfacilitating intuitive and effective remote control and interaction with\nrobotic systems.The survey highlights existing articles on the application of\nXR technologies, user experience enhancement, and various interaction designs\nfor XR in remote HRI, providing insights into current trends and future\ndirections. We also identified potential gaps and opportunities for future\nresearch to improve remote HRI systems through XR technology to guide and\ninform future XR and robotics research.\n","authors":["Xian Wang","Luyao Shen","Lik-Hang Lee"],"pdf_url":"https://arxiv.org/pdf/2403.11384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11383v1","updated":"2024-03-18T00:19:52Z","published":"2024-03-18T00:19:52Z","title":"On the Benefits of GPU Sample-Based Stochastic Predictive Controllers\n  for Legged Locomotion","summary":"  Quadrupedal robots excel in mobility, navigating complex terrains with\nagility. However, their complex control systems present challenges that are\nstill far from being fully addressed. In this paper, we introduce the use of\nSample-Based Stochastic control strategies for quadrupedal robots, as an\nalternative to traditional optimal control laws. We show that Sample-Based\nStochastic methods, supported by GPU acceleration, can be effectively applied\nto real quadruped robots. In particular, in this work, we focus on achieving\ngait frequency adaptation, a notable challenge in quadrupedal locomotion for\ngradient-based methods. To validate the effectiveness of Sample-Based\nStochastic controllers we test two distinct approaches for quadrupedal robots\nand compare them against a conventional gradient-based Model Predictive Control\nsystem. Our findings, validated both in simulation and on a real 21Kg Aliengo\nquadruped, demonstrate that our method is on par with a traditional Model\nPredictive Control strategy when the robot is subject to zero or moderate\ndisturbance, while it surpasses gradient-based methods in handling sustained\nexternal disturbances, thanks to the straightforward gait adaptation strategy\nthat is possible to achieve within their formulation.\n","authors":["Giulio Turrisi","Valerio Modugno","Lorenzo Amatucci","Dimitrios Kanoulas","Claudio Semini"],"pdf_url":"https://arxiv.org/pdf/2403.11383v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.19115v2","updated":"2024-03-18T22:39:04Z","published":"2023-05-30T15:24:40Z","title":"High-Gain Disturbance Observer for Robust Trajectory Tracking of\n  Quadrotors","summary":"  This paper presents a simple method to boost the robustness of quadrotors in\ntrajectory tracking. The presented method features a high-gain disturbance\nobserver (HGDO) that provides disturbance estimates in real-time. The estimates\nare then used in a trajectory control law to compensate for disturbance\neffects. We present theoretical convergence results showing that the proposed\nHGDO can quickly converge to an adjustable neighborhood of actual disturbance\nvalues. We will then integrate the disturbance estimates with a typical robust\ntrajectory controller, namely sliding mode control (SMC), and present Lyapunov\nstability analysis to establish the boundedness of trajectory tracking errors.\nHowever, our stability analysis can be easily extended to other Lyapunov-based\ncontrollers to develop different HGDO-based controllers with formal stability\nguarantees. We evaluate the proposed HGDO-based control method using both\nsimulation and laboratory experiments in various scenarios and in the presence\nof external disturbances. Our results indicate that the addition of HGDO to a\nquadrotor trajectory controller can significantly improve the accuracy and\nprecision of trajectory tracking in the presence of external disturbances.\n","authors":["Mohammadreza Izadi","Reza Faieghi"],"pdf_url":"https://arxiv.org/pdf/2305.19115v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12280v1","updated":"2024-03-18T21:55:25Z","published":"2024-03-18T21:55:25Z","title":"Reachability-based Trajectory Design via Exact Formulation of Implicit\n  Neural Signed Distance Functions","summary":"  Generating receding-horizon motion trajectories for autonomous vehicles in\nreal-time while also providing safety guarantees is challenging. This is\nbecause a future trajectory needs to be planned before the previously computed\ntrajectory is completely executed. This becomes even more difficult if the\ntrajectory is required to satisfy continuous-time collision-avoidance\nconstraints while accounting for a large number of obstacles. To address these\nchallenges, this paper proposes a novel real-time, receding-horizon motion\nplanning algorithm named REachability-based trajectory Design via Exact\nFormulation of Implicit NEural signed Distance functions (REDEFINED). REDEFINED\nfirst applies offline reachability analysis to compute zonotope-based reachable\nsets that overapproximate the motion of the ego vehicle. During online\nplanning, REDEFINED leverages zonotope arithmetic to construct a neural\nimplicit representation that computes the exact signed distance between a\nparameterized swept volume of the ego vehicle and obstacle vehicles. REDEFINED\nthen implements a novel, real-time optimization framework that utilizes the\nneural network to construct a collision avoidance constraint. REDEFINED is\ncompared to a variety of state-of-the-art techniques and is demonstrated to\nsuccessfully enable the vehicle to safely navigate through complex\nenvironments. Code, data, and video demonstrations can be found at\nhttps://roahmlab.github.io/redefined/.\n","authors":["Jonathan Michaux","Qingyi Chen","Challen Enninful Adu","Jinsun Liu","Ram Vasudevan"],"pdf_url":"https://arxiv.org/pdf/2403.12280v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12279v1","updated":"2024-03-18T21:54:04Z","published":"2024-03-18T21:54:04Z","title":"Scalable Networked Feature Selection with Randomized Algorithm for Robot\n  Navigation","summary":"  We address the problem of sparse selection of visual features for localizing\na team of robots navigating an unknown environment, where robots can exchange\nrelative position measurements with neighbors. We select a set of the most\ninformative features by anticipating their importance in robots localization by\nsimulating trajectories of robots over a prediction horizon. Through\ntheoretical proofs, we establish a crucial connection between graph Laplacian\nand the importance of features. We show that strong network connectivity\ntranslates to uniformity in feature importance, which enables uniform random\nsampling of features and reduces the overall computational complexity. We\nleverage a scalable randomized algorithm for sparse sums of positive\nsemidefinite matrices to efficiently select the set of the most informative\nfeatures and significantly improve the probabilistic performance bounds.\nFinally, we support our findings with extensive simulations.\n","authors":["Vivek Pandey","Arash Amini","Guangyi Liu","Ufuk Topcu","Qiyu Sun","Kostas Daniilidis","Nader Motee"],"pdf_url":"https://arxiv.org/pdf/2403.12279v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12273v1","updated":"2024-03-18T21:41:09Z","published":"2024-03-18T21:41:09Z","title":"Multimodal Human-Autonomous Agents Interaction Using Pre-Trained\n  Language and Visual Foundation Models","summary":"  In this paper, we extended the method proposed in [17] to enable humans to\ninteract naturally with autonomous agents through vocal and textual\nconversations. Our extended method exploits the inherent capabilities of\npre-trained large language models (LLMs), multimodal visual language models\n(VLMs), and speech recognition (SR) models to decode the high-level natural\nlanguage conversations and semantic understanding of the robot's task\nenvironment, and abstract them to the robot's actionable commands or queries.\nWe performed a quantitative evaluation of our framework's natural vocal\nconversation understanding with participants from different racial backgrounds\nand English language accents. The participants interacted with the robot using\nboth spoken and textual instructional commands. Based on the logged interaction\ndata, our framework achieved 87.55% vocal commands decoding accuracy, 86.27%\ncommands execution success, and an average latency of 0.89 seconds from\nreceiving the participants' vocal chat commands to initiating the robot's\nactual physical action. The video demonstrations of this paper can be found at\nhttps://linusnep.github.io/MTCC-IRoNL/.\n","authors":["Linus Nwankwo","Elmar Rueckert"],"pdf_url":"https://arxiv.org/pdf/2403.12273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12996v2","updated":"2024-03-18T20:45:17Z","published":"2023-11-21T21:05:21Z","title":"RLIF: Interactive Imitation Learning as Reinforcement Learning","summary":"  Although reinforcement learning methods offer a powerful framework for\nautomatic skill acquisition, for practical learning-based control problems in\ndomains such as robotics, imitation learning often provides a more convenient\nand accessible alternative. In particular, an interactive imitation learning\nmethod such as DAgger, which queries a near-optimal expert to intervene online\nto collect correction data for addressing the distributional shift challenges\nthat afflict na\\\"ive behavioral cloning, can enjoy good performance both in\ntheory and practice without requiring manually specified reward functions and\nother components of full reinforcement learning methods. In this paper, we\nexplore how off-policy reinforcement learning can enable improved performance\nunder assumptions that are similar but potentially even more practical than\nthose of interactive imitation learning. Our proposed method uses reinforcement\nlearning with user intervention signals themselves as rewards. This relaxes the\nassumption that intervening experts in interactive imitation learning should be\nnear-optimal and enables the algorithm to learn behaviors that improve over the\npotential suboptimal human expert. We also provide a unified framework to\nanalyze our RL method and DAgger; for which we present the asymptotic analysis\nof the suboptimal gap for both methods as well as the non-asymptotic sample\ncomplexity bound of our method. We then evaluate our method on challenging\nhigh-dimensional continuous control simulation benchmarks as well as real-world\nrobotic vision-based manipulation tasks. The results show that it strongly\noutperforms DAgger-like approaches across the different tasks, especially when\nthe intervening experts are suboptimal. Code and videos can be found on the\nproject website: https://rlif-page.github.io\n","authors":["Jianlan Luo","Perry Dong","Yuexiang Zhai","Yi Ma","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2311.12996v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.12235v1","updated":"2024-03-18T20:33:06Z","published":"2024-03-18T20:33:06Z","title":"IKSPARK: An Inverse Kinematics Solver using Semidefinite Relaxation and\n  Rank Minimization","summary":"  Inverse kinematics (IK) is a fundamental problem frequently occurred in robot\ncontrol and motion planning. However, the problem is nonconvex because the\nkinematic map between the configuration and task spaces is generally nonlinear,\nwhich makes it challenging for fast and accurate solutions. The problem can be\nmore complicated with the existence of different physical constraints imposed\nby the robot structure. In this paper, we develop an inverse kinematics solver\nnamed IKSPARK (Inverse Kinematics using Semidefinite Programming And RanK\nminimization) that can find solutions for robots with various structures,\nincluding open/closed kinematic chains, spherical, revolute, and/or prismatic\njoints. The solver works in the space of rotation matrices of the link\nreference frames and involves solving only convex semidefinite problems (SDPs).\nSpecifically, the IK problem is formulated as an SDP with an additional rank-1\nconstraint on symmetric matrices with constant traces. The solver first solves\nthis SDP disregarding the rank constraint to get a start point and then finds\nthe rank-1 solution iteratively via a rank minimization algorithm with proven\nlocal convergence. Compared to other work that performs SDP relaxation for IK\nproblems, our formulation is simpler, and uses variables with smaller sizes. We\nvalidate our approach via simulations on different robots, comparing against a\nstandard IK method.\n","authors":["Liangting Wu","Roberto Tron"],"pdf_url":"https://arxiv.org/pdf/2403.12235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12223v1","updated":"2024-03-18T20:11:02Z","published":"2024-03-18T20:11:02Z","title":"HRI in Indian Education: Challenges Opportunities","summary":"  With the recent advancements in the field of robotics and the increased focus\non having general-purpose robots widely available to the general public, it has\nbecome increasingly necessary to pursue research into Human-robot interaction\n(HRI). While there have been a lot of works discussing frameworks for teaching\nHRI in educational institutions with a few institutions already offering\ncourses to students, a consensus on the course content still eludes the field.\nIn this work, we highlight a few challenges and opportunities while designing\nan HRI course from an Indian perspective. These topics warrant further\ndeliberations as they have a direct impact on the design of HRI courses and\nwider implications for the entire field.\n","authors":["Chinmaya Mishra","Anuj Nandanwar","Sashikala Mishra"],"pdf_url":"https://arxiv.org/pdf/2403.12223v1.pdf","comment":"Presented at the Designing an Intro to HRI Course Workshop at HRI\n  2024 (arXiv:2403.05588)"},{"id":"http://arxiv.org/abs/2403.12214v1","updated":"2024-03-18T19:58:23Z","published":"2024-03-18T19:58:23Z","title":"Architectural-Scale Artistic Brush Painting with a Hybrid Cable Robot","summary":"  Robot art presents an opportunity to both showcase and advance\nstate-of-the-art robotics through the challenging task of creating art.\nCreating large-scale artworks in particular engages the public in a way that\nsmall-scale works cannot, and the distinct qualities of brush strokes\ncontribute to an organic and human-like quality. Combining the large scale of\nmurals with the strokes of the brush medium presents an especially impactful\nresult, but also introduces unique challenges in maintaining precise, dextrous\nmotion control of the brush across such a large workspace. In this work, we\npresent the first robot to our knowledge that can paint architectural-scale\nmurals with a brush. We create a hybrid robot consisting of a cable-driven\nparallel robot and 4 degree of freedom (DoF) serial manipulator to paint a 27m\nby 3.7m mural on windows spanning 2-stories of a building. We discuss our\napproach to achieving both the scale and accuracy required for brush-painting a\nmural through a combination of novel mechanical design elements, coordinated\nplanning and control, and on-site calibration algorithms with experimental\nvalidations.\n","authors":["Gerry Chen","Tristan Al-Haddad","Frank Dellaert","Seth Hutchinson"],"pdf_url":"https://arxiv.org/pdf/2403.12214v1.pdf","comment":"8 pages IEEE conference format, submitted to IROS 2024,"},{"id":"http://arxiv.org/abs/2403.12203v1","updated":"2024-03-18T19:25:57Z","published":"2024-03-18T19:25:57Z","title":"Bootstrapping Reinforcement Learning with Imitation for Vision-Based\n  Agile Flight","summary":"  We combine the effectiveness of Reinforcement Learning (RL) and the\nefficiency of Imitation Learning (IL) in the context of vision-based,\nautonomous drone racing. We focus on directly processing visual input without\nexplicit state estimation. While RL offers a general framework for learning\ncomplex controllers through trial and error, it faces challenges regarding\nsample efficiency and computational demands due to the high dimensionality of\nvisual inputs. Conversely, IL demonstrates efficiency in learning from visual\ndemonstrations but is limited by the quality of those demonstrations and faces\nissues like covariate shift. To overcome these limitations, we propose a novel\ntraining framework combining RL and IL's advantages. Our framework involves\nthree stages: initial training of a teacher policy using privileged state\ninformation, distilling this policy into a student policy using IL, and\nperformance-constrained adaptive RL fine-tuning. Our experiments in both\nsimulated and real-world environments demonstrate that our approach achieves\nsuperior performance and robustness than IL or RL alone in navigating a\nquadrotor through a racing course using only visual information without\nexplicit state estimation.\n","authors":["Jiaxu Xing","Angel Romero","Leonard Bauersfeld","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2403.12203v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12194v1","updated":"2024-03-18T19:07:42Z","published":"2024-03-18T19:07:42Z","title":"The POLAR Traverse Dataset: A Dataset of Stereo Camera Images Simulating\n  Traverses across Lunar Polar Terrain under Extreme Lighting Conditions","summary":"  We present the POLAR Traverse Dataset: a dataset of high-fidelity stereo pair\nimages of lunar-like terrain under polar lighting conditions designed to\nsimulate a straight-line traverse. Images from individual traverses with\ndifferent camera heights and pitches were recorded at 1 m intervals by moving a\nsuspended stereo bar across a test bed filled with regolith simulant and shaped\nto mimic lunar south polar terrain. Ground truth geometry and camera position\ninformation was also recorded. This dataset is intended for developing and\ntesting software algorithms that rely on stereo or monocular camera images,\nsuch as visual odometry, for use in the lunar polar environment, as well as to\nprovide insight into the expected lighting conditions in lunar polar regions.\n","authors":["Margaret Hansen","Uland Wong","Terrence Fong"],"pdf_url":"https://arxiv.org/pdf/2403.12194v1.pdf","comment":"6 pages, 5 figures, 3 tables. Associated dataset can be found at\n  https://ti.arc.nasa.gov/dataset/PolarTrav/"},{"id":"http://arxiv.org/abs/2403.12193v1","updated":"2024-03-18T19:04:56Z","published":"2024-03-18T19:04:56Z","title":"Continual Domain Randomization","summary":"  Domain Randomization (DR) is commonly used for sim2real transfer of\nreinforcement learning (RL) policies in robotics. Most DR approaches require a\nsimulator with a fixed set of tunable parameters from the start of the\ntraining, from which the parameters are randomized simultaneously to train a\nrobust model for use in the real world. However, the combined randomization of\nmany parameters increases the task difficulty and might result in sub-optimal\npolicies. To address this problem and to provide a more flexible training\nprocess, we propose Continual Domain Randomization (CDR) for RL that combines\ndomain randomization with continual learning to enable sequential training in\nsimulation on a subset of randomization parameters at a time. Starting from a\nmodel trained in a non-randomized simulation where the task is easier to solve,\nthe model is trained on a sequence of randomizations, and continual learning is\nemployed to remember the effects of previous randomizations. Our robotic\nreaching and grasping tasks experiments show that the model trained in this\nfashion learns effectively in simulation and performs robustly on the real\nrobot while matching or outperforming baselines that employ combined\nrandomization or sequential randomization without continual learning. Our code\nand videos are available at https://continual-dr.github.io/.\n","authors":["Josip Josifovski","Sayantan Auddy","Mohammadhossein Malmir","Justus Piater","Alois Knoll","Nicolás Navarro-Guerrero"],"pdf_url":"https://arxiv.org/pdf/2403.12193v1.pdf","comment":"Under peer review"},{"id":"http://arxiv.org/abs/2310.00145v2","updated":"2024-03-18T18:51:09Z","published":"2023-09-29T21:09:02Z","title":"3D Reconstruction in Noisy Agricultural Environments: A Bayesian\n  Optimization Perspective for View Planning","summary":"  3D reconstruction is a fundamental task in robotics that gained attention due\nto its major impact in a wide variety of practical settings, including\nagriculture, underwater, and urban environments. This task can be carried out\nvia view planning (VP), which aims to optimally place a certain number of\ncameras in positions that maximize the visual information, improving the\nresulting 3D reconstruction. Nonetheless, in most real-world settings, existing\nenvironmental noise can significantly affect the performance of 3D\nreconstruction. To that end, this work advocates a novel geometric-based\nreconstruction quality function for VP, that accounts for the existing noise of\nthe environment, without requiring its closed-form expression. With no analytic\nexpression of the objective function, this work puts forth an adaptive Bayesian\noptimization algorithm for accurate 3D reconstruction in the presence of noise.\nNumerical tests on noisy agricultural environments showcase the merits of the\nproposed approach for 3D reconstruction with even a small number of available\ncameras.\n","authors":["Athanasios Bacharis","Konstantinos D. Polyzos","Henry J. Nelson","Georgios B. Giannakis","Nikolaos Papanikolopoulos"],"pdf_url":"https://arxiv.org/pdf/2310.00145v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12176v1","updated":"2024-03-18T18:49:20Z","published":"2024-03-18T18:49:20Z","title":"Safety Implications of Explainable Artificial Intelligence in End-to-End\n  Autonomous Driving","summary":"  The end-to-end learning pipeline is gradually creating a paradigm shift in\nthe ongoing development of highly autonomous vehicles, largely due to advances\nin deep learning, the availability of large-scale training datasets, and\nimprovements in integrated sensor devices. However, a lack of interpretability\nin real-time decisions with contemporary learning methods impedes user trust\nand attenuates the widespread deployment and commercialization of such\nvehicles. Moreover, the issue is exacerbated when these cars are involved in or\ncause traffic accidents. Such drawback raises serious safety concerns from\nsocietal and legal perspectives. Consequently, explainability in end-to-end\nautonomous driving is essential to enable the safety of vehicular automation.\nHowever, the safety and explainability aspects of autonomous driving have\ngenerally been investigated disjointly by researchers in today's state of the\nart. In this paper, we aim to bridge the gaps between these topics and seek to\nanswer the following research question: When and how can explanations improve\nsafety of autonomous driving? In this regard, we first revisit established\nsafety and state-of-the-art explainability techniques in autonomous driving.\nFurthermore, we present three critical case studies and show the pivotal role\nof explanations in enhancing self-driving safety. Finally, we describe our\nempirical investigation and reveal potential value, limitations, and caveats\nwith practical explainable AI methods on their role of assuring safety and\ntransparency for vehicle autonomy.\n","authors":["Shahin Atakishiyev","Mohammad Salameh","Randy Goebel"],"pdf_url":"https://arxiv.org/pdf/2403.12176v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2403.09875v2","updated":"2024-03-18T18:46:13Z","published":"2024-03-14T21:09:59Z","title":"Touch-GS: Visual-Tactile Supervised 3D Gaussian Splatting","summary":"  In this work, we propose a novel method to supervise 3D Gaussian Splatting\n(3DGS) scenes using optical tactile sensors. Optical tactile sensors have\nbecome widespread in their use in robotics for manipulation and object\nrepresentation; however, raw optical tactile sensor data is unsuitable to\ndirectly supervise a 3DGS scene. Our representation leverages a Gaussian\nProcess Implicit Surface to implicitly represent the object, combining many\ntouches into a unified representation with uncertainty. We merge this model\nwith a monocular depth estimation network, which is aligned in a two stage\nprocess, coarsely aligning with a depth camera and then finely adjusting to\nmatch our touch data. For every training image, our method produces a\ncorresponding fused depth and uncertainty map. Utilizing this additional\ninformation, we propose a new loss function, variance weighted depth supervised\nloss, for training the 3DGS scene model. We leverage the DenseTact optical\ntactile sensor and RealSense RGB-D camera to show that combining touch and\nvision in this manner leads to quantitatively and qualitatively better results\nthan vision or touch alone in a few-view scene syntheses on opaque as well as\non reflective and transparent objects. Please see our project page at\nhttp://armlabstanford.github.io/touch-gs\n","authors":["Aiden Swann","Matthew Strong","Won Kyung Do","Gadiel Sznaier Camps","Mac Schwager","Monroe Kennedy III"],"pdf_url":"https://arxiv.org/pdf/2403.09875v2.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.12170v1","updated":"2024-03-18T18:39:51Z","published":"2024-03-18T18:39:51Z","title":"Sim2Real Manipulation on Unknown Objects with Tactile-based\n  Reinforcement Learning","summary":"  Using tactile sensors for manipulation remains one of the most challenging\nproblems in robotics. At the heart of these challenges is generalization: How\ncan we train a tactile-based policy that can manipulate unseen and diverse\nobjects? In this paper, we propose to perform Reinforcement Learning with only\nvisual tactile sensing inputs on diverse objects in a physical simulator. By\ntraining with diverse objects in simulation, it enables the policy to\ngeneralize to unseen objects. However, leveraging simulation introduces the\nSim2Real transfer problem. To mitigate this problem, we study different tactile\nrepresentations and evaluate how each affects real-robot manipulation results\nafter transfer. We conduct our experiments on diverse real-world objects and\nshow significant improvements over baselines for the pivoting task. Our project\npage is available at https://tactilerl.github.io/.\n","authors":["Entong Su","Chengzhe Jia","Yuzhe Qin","Wenxuan Zhou","Annabella Macaluso","Binghao Huang","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2403.12170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12162v1","updated":"2024-03-18T18:23:36Z","published":"2024-03-18T18:23:36Z","title":"Intelligent Execution through Plan Analysis","summary":"  Intelligent robots need to generate and execute plans. In order to deal with\nthe complexity of real environments, planning makes some assumptions about the\nworld. When executing plans, the assumptions are usually not met. Most works\nhave focused on the negative impact of this fact and the use of replanning\nafter execution failures. Instead, we focus on the positive impact, or\nopportunities to find better plans. When planning, the proposed technique finds\nand stores those opportunities. Later, during execution, the monitoring system\ncan use them to focus perception and repair the plan, instead of replanning\nfrom scratch. Experiments in several paradigmatic robotic tasks show how the\napproach outperforms standard replanning strategies.\n","authors":["Daniel Borrajo","Manuela Veloso"],"pdf_url":"https://arxiv.org/pdf/2403.12162v1.pdf","comment":"Published at IROS 21, 6 pages"},{"id":"http://arxiv.org/abs/2403.12149v1","updated":"2024-03-18T18:03:08Z","published":"2024-03-18T18:03:08Z","title":"Ergonomic Optimization in Worker-Robot Bimanual Object Handover:\n  Implementing REBA Using Reinforcement Learning in Virtual Reality","summary":"  Robots can serve as safety catalysts on construction job sites by taking over\nhazardous and repetitive tasks while alleviating the risks associated with\nexisting manual workflows. Research on the safety of physical human-robot\ninteraction (pHRI) is traditionally focused on addressing the risks associated\nwith potential collisions. However, it is equally important to ensure that the\nworkflows involving a collaborative robot are inherently safe, even though they\nmay not result in an accident. For example, pHRI may require the human\ncounterpart to use non-ergonomic body postures to conform to the robot hardware\nand physical configurations. Frequent and long-term exposure to such situations\nmay result in chronic health issues. Safety and ergonomics assessment measures\ncan be understood by robots if they are presented in algorithmic fashions so\noptimization for body postures is attainable. While frameworks such as Rapid\nEntire Body Assessment (REBA) have been an industry standard for many decades,\nthey lack a rigorous mathematical structure which poses challenges in using\nthem immediately for pHRI safety optimization purposes. Furthermore, learnable\napproaches have limited robustness outside of their training data, reducing\ngeneralizability. In this paper, we propose a novel framework that approaches\noptimization through Reinforcement Learning, ensuring precise, online ergonomic\nscores as compared to approximations, while being able to generalize and tune\nthe regiment to any human and any task. To ensure practicality, the training is\ndone in virtual reality utilizing Inverse Kinematics to simulate human movement\nmechanics. Experimental findings are compared to ergonomically naive object\nhandover heuristics and indicate promising results where the developed\nframework can find the optimal object handover coordinates in pHRI contexts for\nmanual material handling exemplary situations.\n","authors":["Mani Amani","Reza Akhavian"],"pdf_url":"https://arxiv.org/pdf/2403.12149v1.pdf","comment":"Submitted to Safety Science"},{"id":"http://arxiv.org/abs/2403.12039v1","updated":"2024-03-18T17:59:48Z","published":"2024-03-18T17:59:48Z","title":"StereoNavNet: Learning to Navigate using Stereo Cameras with Auxiliary\n  Occupancy Voxels","summary":"  Visual navigation has received significant attention recently. Most of the\nprior works focus on predicting navigation actions based on semantic features\nextracted from visual encoders. However, these approaches often rely on large\ndatasets and exhibit limited generalizability. In contrast, our approach draws\ninspiration from traditional navigation planners that operate on geometric\nrepresentations, such as occupancy maps. We propose StereoNavNet (SNN), a novel\nvisual navigation approach employing a modular learning framework comprising\nperception and policy modules. Within the perception module, we estimate an\nauxiliary 3D voxel occupancy grid from stereo RGB images and extract geometric\nfeatures from it. These features, along with user-defined goals, are utilized\nby the policy module to predict navigation actions. Through extensive empirical\nevaluation, we demonstrate that SNN outperforms baseline approaches in terms of\nsuccess rates, success weighted by path length, and navigation error.\nFurthermore, SNN exhibits better generalizability, characterized by maintaining\nleading performance when navigating across previously unseen environments.\n","authors":["Hongyu Li","Taskin Padir","Huaizu Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.12039v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12023v1","updated":"2024-03-18T17:55:52Z","published":"2024-03-18T17:55:52Z","title":"Aligning Learning with Communication in Shared Autonomy","summary":"  Assistive robot arms can help humans by partially automating their desired\ntasks. Consider an adult with motor impairments controlling an assistive robot\narm to eat dinner. The robot can reduce the number of human inputs -- and how\nprecise those inputs need to be -- by recognizing what the human wants (e.g., a\nfork) and assisting for that task (e.g., moving towards the fork). Prior\nresearch has largely focused on learning the human's task and providing\nmeaningful assistance. But as the robot learns and assists, we also need to\nensure that the human understands the robot's intent (e.g., does the human know\nthe robot is reaching for a fork?). In this paper, we study the effects of\ncommunicating learned assistance from the robot back to the human operator. We\ndo not focus on the specific interfaces used for communication. Instead, we\ndevelop experimental and theoretical models of a) how communication changes the\nway humans interact with assistive robot arms, and b) how robots can harness\nthese changes to better align with the human's intent. We first conduct online\nand in-person user studies where participants operate robots that provide\npartial assistance, and we measure how the human's inputs change with and\nwithout communication. With communication, we find that humans are more likely\nto intervene when the robot incorrectly predicts their intent, and more likely\nto release control when the robot correctly understands their task. We then use\nthese findings to modify an established robot learning algorithm so that the\nrobot can correctly interpret the human's inputs when communication is present.\nOur results from a second in-person user study suggest that this combination of\ncommunication and learning outperforms assistive systems that isolate either\nlearning or communication.\n","authors":["Joshua Hoegerman","Shahabedin Sagheb","Benjamin A. Christie","Dylan P. Losey"],"pdf_url":"https://arxiv.org/pdf/2403.12023v1.pdf","comment":"7 pages, under review for IROS 2024"},{"id":"http://arxiv.org/abs/2310.15928v2","updated":"2024-03-18T17:36:33Z","published":"2023-10-24T15:26:57Z","title":"AO-Grasp: Articulated Object Grasp Generation","summary":"  We introduce AO-Grasp, a grasp proposal method that generates 6 DoF grasps\nthat enable robots to interact with articulated objects, such as opening and\nclosing cabinets and appliances. AO-Grasp consists of two main contributions:\nthe AO-Grasp Model and the AO-Grasp Dataset. Given a segmented partial point\ncloud of a single articulated object, the AO-Grasp Model predicts the best\ngrasp points on the object with an Actionable Grasp Point Predictor. Then, it\nfinds corresponding grasp orientations for each of these points, resulting in\nstable and actionable grasp proposals. We train the AO-Grasp Model on our new\nAO-Grasp Dataset, which contains 78K actionable parallel-jaw grasps on\nsynthetic articulated objects. In simulation, AO-Grasp achieves a 45.0 % grasp\nsuccess rate, whereas the highest performing baseline achieves a 35.0% success\nrate. Additionally, we evaluate AO-Grasp on 120 real-world scenes of objects\nwith varied geometries, articulation axes, and joint states, where AO-Grasp\nproduces successful grasps on 67.5% of scenes, while the baseline only produces\nsuccessful grasps on 33.3% of scenes. To the best of our knowledge, AO-Grasp is\nthe first method for generating 6 DoF grasps on articulated objects directly\nfrom partial point clouds without requiring part detection or hand-designed\ngrasp heuristics. Project website: https://stanford-iprl-lab.github.io/ao-grasp\n","authors":["Carlota Parés Morlans","Claire Chen","Yijia Weng","Michelle Yi","Yuying Huang","Nick Heppert","Linqi Zhou","Leonidas Guibas","Jeannette Bohg"],"pdf_url":"https://arxiv.org/pdf/2310.15928v2.pdf","comment":"Project website: https://stanford-iprl-lab.github.io/ao-grasp"},{"id":"http://arxiv.org/abs/2403.11985v1","updated":"2024-03-18T17:22:43Z","published":"2024-03-18T17:22:43Z","title":"SceneSense: Diffusion Models for 3D Occupancy Synthesis from Partial\n  Observation","summary":"  When exploring new areas, robotic systems generally exclusively plan and\nexecute controls over geometry that has been directly measured. When entering\nspace that was previously obstructed from view such as turning corners in\nhallways or entering new rooms, robots often pause to plan over the newly\nobserved space. To address this we present SceneScene, a real-time 3D diffusion\nmodel for synthesizing 3D occupancy information from partial observations that\neffectively predicts these occluded or out of view geometries for use in future\nplanning and control frameworks. SceneSense uses a running occupancy map and a\nsingle RGB-D camera to generate predicted geometry around the platform at\nruntime, even when the geometry is occluded or out of view. Our architecture\nensures that SceneSense never overwrites observed free or occupied space. By\npreserving the integrity of the observed map, SceneSense mitigates the risk of\ncorrupting the observed space with generative predictions. While SceneSense is\nshown to operate well using a single RGB-D camera, the framework is flexible\nenough to extend to additional modalities. SceneSense operates as part of any\nsystem that generates a running occupancy map `out of the box', removing\nconditioning from the framework. Alternatively, for maximum performance in new\nmodalities, the perception backbone can be replaced and the model retrained for\ninference in new applications. Unlike existing models that necessitate multiple\nviews and offline scene synthesis, or are focused on filling gaps in observed\ndata, our findings demonstrate that SceneSense is an effective approach to\nestimating unobserved local occupancy information at runtime. Local occupancy\npredictions from SceneSense are shown to better represent the ground truth\noccupancy distribution during the test exploration trajectories than the\nrunning occupancy map.\n","authors":["Alec Reed","Brendan Crowe","Doncey Albin","Lorin Achey","Bradley Hayes","Christoffer Heckman"],"pdf_url":"https://arxiv.org/pdf/2403.11985v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2306.16316v2","updated":"2024-03-18T17:02:28Z","published":"2023-06-28T15:47:49Z","title":"Learning Continuous Control with Geometric Regularity from Robot\n  Intrinsic Symmetry","summary":"  Geometric regularity, which leverages data symmetry, has been successfully\nincorporated into deep learning architectures such as CNNs, RNNs, GNNs, and\nTransformers. While this concept has been widely applied in robotics to address\nthe curse of dimensionality when learning from high-dimensional data, the\ninherent reflectional and rotational symmetry of robot structures has not been\nadequately explored. Drawing inspiration from cooperative multi-agent\nreinforcement learning, we introduce novel network structures for single-agent\ncontrol learning that explicitly capture these symmetries. Moreover, we\ninvestigate the relationship between the geometric prior and the concept of\nParameter Sharing in multi-agent reinforcement learning. Last but not the\nleast, we implement the proposed framework in online and offline learning\nmethods to demonstrate its ease of use. Through experiments conducted on\nvarious challenging continuous control tasks on simulators and real robots, we\nhighlight the significant potential of the proposed geometric regularity in\nenhancing robot learning capabilities.\n","authors":["Shengchao Yan","Baohe Zhang","Yuan Zhang","Joschka Boedecker","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2306.16316v2.pdf","comment":"accepted by ICRA 2024"},{"id":"http://arxiv.org/abs/2403.11955v1","updated":"2024-03-18T16:52:45Z","published":"2024-03-18T16:52:45Z","title":"Inferring Belief States in Partially-Observable Human-Robot Teams","summary":"  We investigate the real-time estimation of human situation awareness using\nobservations from a robot teammate with limited visibility. In human factors\nand human-autonomy teaming, it is recognized that individuals navigate their\nenvironments using an internal mental simulation, or mental model. The mental\nmodel informs cognitive processes including situation awareness, contextual\nreasoning, and task planning. In teaming domains, the mental model includes a\nteam model of each teammate's beliefs and capabilities, enabling fluent\nteamwork without the need for explicit communication. However, little work has\napplied team models to human-robot teaming. We compare the performance of two\ncurrent methods at estimating user situation awareness over varying visibility\nconditions. Our results indicate that the methods are largely resilient to\nlow-visibility conditions in our domain, however opportunities exist to improve\ntheir overall performance.\n","authors":["Jack Kolb","Karen M. Feigh"],"pdf_url":"https://arxiv.org/pdf/2403.11955v1.pdf","comment":"Under review, project page: https://jackkolb.com/tmm-hri"},{"id":"http://arxiv.org/abs/2403.11948v1","updated":"2024-03-18T16:42:39Z","published":"2024-03-18T16:42:39Z","title":"Learning Dynamical Systems Encoding Non-Linearity within Space Curvature","summary":"  Dynamical Systems (DS) are an effective and powerful means of shaping\nhigh-level policies for robotics control. They provide robust and reactive\ncontrol while ensuring the stability of the driving vector field. The\nincreasing complexity of real-world scenarios necessitates DS with a higher\ndegree of non-linearity, along with the ability to adapt to potential changes\nin environmental conditions, such as obstacles. Current learning strategies for\nDSs often involve a trade-off, sacrificing either stability guarantees or\noffline computational efficiency in order to enhance the capabilities of the\nlearned DS. Online local adaptation to environmental changes is either not\ntaken into consideration or treated as a separate problem. In this paper, our\nobjective is to introduce a method that enhances the complexity of the learned\nDS without compromising efficiency during training or stability guarantees.\nFurthermore, we aim to provide a unified approach for seamlessly integrating\nthe initially learned DS's non-linearity with any local non-linearities that\nmay arise due to changes in the environment. We propose a geometrical approach\nto learn asymptotically stable non-linear DS for robotics control. Each DS is\nmodeled as a harmonic damped oscillator on a latent manifold. By learning the\nmanifold's Euclidean embedded representation, our approach encodes the\nnon-linearity of the DS within the curvature of the space. Having an explicit\nembedded representation of the manifold allows us to showcase obstacle\navoidance by directly inducing local deformations of the space. We demonstrate\nthe effectiveness of our methodology through two scenarios: first, the 2D\nlearning of synthetic vector fields, and second, the learning of 3D robotic\nend-effector motions in real-world settings.\n","authors":["Bernardo Fichera","Aude Billard"],"pdf_url":"https://arxiv.org/pdf/2403.11948v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11914v1","updated":"2024-03-18T16:13:02Z","published":"2024-03-18T16:13:02Z","title":"Single-Agent Actor Critic for Decentralized Cooperative Driving","summary":"  Active traffic management incorporating autonomous vehicles (AVs) promises a\nfuture with diminished congestion and enhanced traffic flow. However,\ndeveloping algorithms for real-world application requires addressing the\nchallenges posed by continuous traffic flow and partial observability. To\nbridge this gap and advance the field of active traffic management towards\ngreater decentralization, we introduce a novel asymmetric actor-critic model\naimed at learning decentralized cooperative driving policies for autonomous\nvehicles using single-agent reinforcement learning. Our approach employs\nattention neural networks with masking to handle the dynamic nature of\nreal-world traffic flow and partial observability. Through extensive\nevaluations against baseline controllers across various traffic scenarios, our\nmodel shows great potential for improving traffic flow at diverse bottleneck\nlocations within the road system. Additionally, we explore the challenge\nassociated with the conservative driving behaviors of autonomous vehicles that\nadhere strictly to traffic regulations. The experiment results illustrate that\nour proposed cooperative policy can mitigate potential traffic slowdowns\nwithout compromising safety.\n","authors":["Shengchao Yan","Lukas König","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2403.11914v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11898v1","updated":"2024-03-18T15:56:44Z","published":"2024-03-18T15:56:44Z","title":"Visuo-Tactile Pretraining for Cable Plugging","summary":"  Tactile information is a critical tool for fine-grain manipulation. As\nhumans, we rely heavily on tactile information to understand objects in our\nenvironments and how to interact with them. We use touch not only to perform\nmanipulation tasks but also to learn how to perform these tasks. Therefore, to\ncreate robotic agents that can learn to complete manipulation tasks at a human\nor super-human level of performance, we need to properly incorporate tactile\ninformation into both skill execution and skill learning. In this paper, we\ninvestigate how we can incorporate tactile information into imitation learning\nplatforms to improve performance on complex tasks. To do this, we tackle the\nchallenge of plugging in a USB cable, a dexterous manipulation task that relies\non fine-grain visuo-tactile serving. By incorporating tactile information into\nimitation learning frameworks, we are able to train a robotic agent to plug in\na USB cable - a first for imitation learning. Additionally, we explore how\ntactile information can be used to train non-tactile agents through a\ncontrastive-loss pretraining process. Our results show that by pretraining with\ntactile information, the performance of a non-tactile agent can be\nsignificantly improved, reaching a level on par with visuo-tactile agents.\n  For demonstration videos and access to our codebase, see the project website:\nhttps://sites.google.com/andrew.cmu.edu/visuo-tactile-cable-plugging/home\n","authors":["Abraham George","Selam Gano","Pranav Katragadda","Amir Barati Farimani"],"pdf_url":"https://arxiv.org/pdf/2403.11898v1.pdf","comment":"8 pages, 6 figures, submitted to IROS 2024"},{"id":"http://arxiv.org/abs/2211.13024v3","updated":"2024-03-18T15:39:47Z","published":"2022-11-23T15:27:31Z","title":"Comparison of Motion Encoding Frameworks on Human Manipulation Actions","summary":"  Movement generation, and especially generalisation to unseen situations,\nplays an important role in robotics. Different types of movement generation\nmethods exist such as spline based methods, dynamical system based methods, and\nmethods based on Gaussian mixture models (GMMs). Using a large, new dataset on\nhuman manipulations, in this paper we provide a highly detailed comparison of\nfive fundamentally different and widely used movement encoding and generation\nframeworks: dynamic movement primitives (DMPs), time based Gaussian mixture\nregression (tbGMR), stable estimator of dynamical systems (SEDS), Probabilistic\nMovement Primitives (ProMP) and Optimal Control Primitives (OCP). We compare\nthese frameworks with respect to their movement encoding efficiency,\nreconstruction accuracy, and movement generalisation capabilities. The new\ndataset consists of nine object manipulation actions performed by 12 humans:\npick and place, put on top/take down, put inside/take out, hide/uncover, and\npush/pull with a total of 7,652 movement examples. Our analysis shows that for\nmovement encoding and reconstruction DMPs and OCPs are the most efficient with\nrespect to the number of parameters and reconstruction accuracy, if a\nsufficient number of kernels is used. In case of movement generalisation to new\nstart- and end-point situations, DMPs, OCPs and task parameterized GMM (TP-GMM,\nmovement generalisation framework based on tbGMR) lead to similar performance,\nwhich ProMPs only achieve when using many demonstrations for learning. All\nmodels outperform SEDS, which additionally proves to be difficult to fit.\nFurthermore we observe that TP-GMM and SEDS suffer from problems reaching the\nend-points of generalizations.These different quantitative results will help\nselecting the most appropriate models and designing trajectory representations\nin an improved task-dependent way in future robotic applications.\n","authors":["Lennart Jahn","Florentin Wörgötter","Tomas Kulvicius"],"pdf_url":"https://arxiv.org/pdf/2211.13024v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11876v1","updated":"2024-03-18T15:28:35Z","published":"2024-03-18T15:28:35Z","title":"Deep Bayesian Future Fusion for Self-Supervised, High-Resolution,\n  Off-Road Mapping","summary":"  The limited sensing resolution of resource-constrained off-road vehicles\nposes significant challenges towards reliable off-road autonomy. To overcome\nthis limitation, we propose a general framework based on fusing the future\ninformation (i.e. future fusion) for self-supervision. Recent approaches\nexploit this future information alongside the hand-crafted heuristics to\ndirectly supervise the targeted downstream tasks (e.g. traversability\nestimation). However, in this paper, we opt for a more general line of\ndevelopment - time-efficient completion of the highest resolution (i.e. 2cm per\npixel) BEV map in a self-supervised manner via future fusion, which can be used\nfor any downstream tasks for better longer range prediction. To this end,\nfirst, we create a high-resolution future-fusion dataset containing pairs of\n(RGB / height) raw sparse and noisy inputs and map-based dense labels. Next, to\naccommodate the noise and sparsity of the sensory information, especially in\nthe distal regions, we design an efficient realization of the Bayes filter onto\nthe vanilla convolutional network via the recurrent mechanism. Equipped with\nthe ideas from SOTA generative models, our Bayesian structure effectively\npredicts high-quality BEV maps in the distal regions. Extensive evaluation on\nboth the quality of completion and downstream task on our future-fusion dataset\ndemonstrates the potential of our approach.\n","authors":["Shubhra Aich","Wenshan Wang","Parv Maheshwari","Matthew Sivaprakasam","Samuel Triest","Cherie Ho","Jason M. Gregory","John G. Rogers III","Sebastian Scherer"],"pdf_url":"https://arxiv.org/pdf/2403.11876v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11863v1","updated":"2024-03-18T15:17:15Z","published":"2024-03-18T15:17:15Z","title":"Context-aware LLM-based Safe Control Against Latent Risks","summary":"  It is challenging for autonomous control systems to perform complex tasks in\nthe presence of latent risks. Motivated by this challenge, this paper proposes\nan integrated framework that involves Large Language Models (LLMs), stochastic\ngradient descent (SGD), and optimization-based control. In the first phrase,\nthe proposed framework breaks down complex tasks into a sequence of smaller\nsubtasks, whose specifications account for contextual information and latent\nrisks. In the second phase, these subtasks and their parameters are refined\nthrough a dual process involving LLMs and SGD. LLMs are used to generate rough\nguesses and failure explanations, and SGD is used to fine-tune parameters. The\nproposed framework is tested using simulated case studies of robots and\nvehicles. The experiments demonstrate that the proposed framework can mediate\nactions based on the context and latent risks and learn complex behaviors\nefficiently.\n","authors":["Quan Khanh Luu","Xiyu Deng","Anh Van Ho","Yorie Nakahira"],"pdf_url":"https://arxiv.org/pdf/2403.11863v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.13139v2","updated":"2024-03-18T14:57:03Z","published":"2023-09-22T18:48:54Z","title":"Exposing the Unseen: Exposure Time Emulation for Offline Benchmarking of\n  Vision Algorithms","summary":"  Visual Odometry (VO) is one of the fundamental tasks in computer vision for\nrobotics. However, its performance is deeply affected by High Dynamic Range\n(HDR) scenes, omnipresent outdoor. While new Automatic-Exposure (AE) approaches\nto mitigate this have appeared, their comparison in a reproducible manner is\nproblematic. This stems from the fact that the behavior of AE depends on the\nenvironment, and it affects the image acquisition process. Consequently, AE has\ntraditionally only been benchmarked in an online manner, making the experiments\nnon-reproducible. To solve this, we propose a new methodology based on an\nemulator that can generate images at any exposure time. It leverages BorealHDR,\na unique multi-exposure stereo dataset collected over 10 km, on 55 trajectories\nwith challenging illumination conditions. Moreover, it includes\nlidar-inertial-based global maps with pose estimation for each image frame as\nwell as Global Navigation Satellite System (GNSS) data, for comparison. We show\nthat using these images acquired at different exposure times, we can emulate\nrealistic images, keeping a Root-Mean-Square Error (RMSE) below 1.78 % compared\nto ground truth images. To demonstrate the practicality of our approach for\noffline benchmarking, we compared three state-of-the-art AE algorithms on key\nelements of Visual Simultaneous Localization And Mapping (VSLAM) pipeline,\nagainst four baselines. Consequently, reproducible evaluation of AE is now\npossible, speeding up the development of future approaches. Our code and\ndataset are available online at this link:\nhttps://github.com/norlab-ulaval/BorealHDR\n","authors":["Olivier Gamache","Jean-Michel Fortin","Matěj Boxan","Maxime Vaidis","François Pomerleau","Philippe Giguère"],"pdf_url":"https://arxiv.org/pdf/2309.13139v2.pdf","comment":"6 pages, 6 figures, submitted to 2024 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS 2024)"},{"id":"http://arxiv.org/abs/2403.11796v1","updated":"2024-03-18T13:53:48Z","published":"2024-03-18T13:53:48Z","title":"OpenOcc: Open Vocabulary 3D Scene Reconstruction via Occupancy\n  Representation","summary":"  3D reconstruction has been widely used in autonomous navigation fields of\nmobile robotics. However, the former research can only provide the basic\ngeometry structure without the capability of open-world scene understanding,\nlimiting advanced tasks like human interaction and visual navigation. Moreover,\ntraditional 3D scene understanding approaches rely on expensive labeled 3D\ndatasets to train a model for a single task with supervision. Thus, geometric\nreconstruction with zero-shot scene understanding i.e. Open vocabulary 3D\nUnderstanding and Reconstruction, is crucial for the future development of\nmobile robots. In this paper, we propose OpenOcc, a novel framework unifying\nthe 3D scene reconstruction and open vocabulary understanding with neural\nradiance fields. We model the geometric structure of the scene with occupancy\nrepresentation and distill the pre-trained open vocabulary model into a 3D\nlanguage field via volume rendering for zero-shot inference. Furthermore, a\nnovel semantic-aware confidence propagation (SCP) method has been proposed to\nrelieve the issue of language field representation degeneracy caused by\ninconsistent measurements in distilled features. Experimental results show that\nour approach achieves competitive performance in 3D scene understanding tasks,\nespecially for small and long-tail objects.\n","authors":["Haochen Jiang","Yueming Xu","Yihan Zeng","Hang Xu","Wei Zhang","Jianfeng Feng","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.11796v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02599v2","updated":"2024-03-18T13:44:22Z","published":"2023-12-05T09:18:12Z","title":"MAINS: A Magnetic Field Aided Inertial Navigation System for Indoor\n  Positioning","summary":"  A Magnetic field Aided Inertial Navigation System (MAINS) for indoor\nnavigation is proposed in this paper. MAINS leverages an array of magnetometers\nto measure spatial variations in the magnetic field, which are then used to\nestimate the displacement and orientation changes of the system, thereby aiding\nthe inertial navigation system (INS). Experiments show that MAINS significantly\noutperforms the stand-alone INS, demonstrating a remarkable two orders of\nmagnitude reduction in position error. Furthermore, when compared to the\nstate-of-the-art magnetic-field-aided navigation approach, the proposed method\nexhibits slightly improved horizontal position accuracy. On the other hand, it\nhas noticeably larger vertical error on datasets with large magnetic field\nvariations. However, one of the main advantages of MAINS compared to the\nstate-of-the-art is that it enables flexible sensor configurations. The\nexperimental results show that the position error after 2 minutes of navigation\nin most cases is less than 3 meters when using an array of 30 magnetometers.\nThus, the proposed navigation solution has the potential to solve one of the\nkey challenges faced with current magnetic-field simultaneous localization and\nmapping (SLAM) solutions: the very limited allowable length of the exploration\nphase during which unvisited areas are mapped.\n","authors":["Chuan Huang","Gustaf Hendeby","Hassen Fourati","Christophe Prieur","Isaac Skog"],"pdf_url":"https://arxiv.org/pdf/2312.02599v2.pdf","comment":"Accepted to IEEE Sensors Journal"},{"id":"http://arxiv.org/abs/2403.11784v1","updated":"2024-03-18T13:42:06Z","published":"2024-03-18T13:42:06Z","title":"ForzaETH Race Stack -- Scaled Autonomous Head-to-Head Racing on Fully\n  Commercial off-the-Shelf Hardware","summary":"  Autonomous racing in robotics combines high-speed dynamics with the necessity\nfor reliability and real-time decision-making. While such racing pushes\nsoftware and hardware to their limits, many existing full-system solutions\nnecessitate complex, custom hardware and software, and usually focus on\nTime-Trials rather than full unrestricted Head-to-Head racing, due to financial\nand safety constraints. This limits their reproducibility, making advancements\nand replication feasible mostly for well-resourced laboratories with\ncomprehensive expertise in mechanical, electrical, and robotics fields.\nResearchers interested in the autonomy domain but with only partial experience\nin one of these fields, need to spend significant time with familiarization and\nintegration. The ForzaETH Race Stack addresses this gap by providing an\nautonomous racing software platform designed for F1TENTH, a 1:10 scaled\nHead-to-Head autonomous racing competition, which simplifies replication by\nusing commercial off-the-shelf hardware. This approach enhances the competitive\naspect of autonomous racing and provides an accessible platform for research\nand development in the field. The ForzaETH Race Stack is designed with\nmodularity and operational ease of use in mind, allowing customization and\nadaptability to various environmental conditions, such as track friction and\nlayout. Capable of handling both Time-Trials and Head-to-Head racing, the stack\nhas demonstrated its effectiveness, robustness, and adaptability in the field\nby winning the official F1TENTH international competition multiple times.\n","authors":["Nicolas Baumann","Edoardo Ghignone","Jonas Kühne","Niklas Bastuck","Jonathan Becker","Nadine Imholz","Tobias Kränzlin","Tian Yi Lim","Michael Lötscher","Luca Schwarzenbach","Luca Tognoni","Christian Vogt","Andrea Carron","Michele Magno"],"pdf_url":"https://arxiv.org/pdf/2403.11784v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.13694v2","updated":"2024-03-18T13:38:35Z","published":"2023-10-20T17:57:30Z","title":"Studying speed-accuracy trade-offs in best-of-n collective\n  decision-making through heterogeneous mean-field modeling","summary":"  To succeed in their objectives, groups of individuals must be able to make\nquick and accurate collective decisions on the best option among a set of\nalternatives with different qualities. Group-living animals aim to do that all\nthe time. Plants and fungi are thought to do so too. Swarms of autonomous\nrobots can also be programmed to make best-of-n decisions for solving tasks\ncollaboratively. Ultimately, humans critically need it and so many times they\nshould be better at it. Thanks to their mathematical tractability, simple\nmodels like the voter model and the local majority rule model have proven\nuseful to describe the dynamics of such collective decision-making processes.\nTo reach a consensus, individuals change their opinion by interacting with\nneighbors in their social network. At least among animals and robots, options\nwith a better quality are exchanged more often and therefore spread faster than\nlower-quality options, leading to the collective selection of the best option.\nWith our work, we study the impact of individuals making errors in pooling\nothers' opinions caused, for example, by the need to reduce the cognitive load.\nOur analysis is grounded on the introduction of a model that generalizes the\ntwo existing models (local majority rule and voter model), showing a\nspeed-accuracy trade-off regulated by the cognitive effort of individuals. We\nalso investigate the impact of the interaction network topology on the\ncollective dynamics. To do so, we extend our model and, by using the\nheterogeneous mean-field approach, we show the presence of another\nspeed-accuracy trade-off regulated by network connectivity. An interesting\nresult is that reduced network connectivity corresponds to an increase in\ncollective decision accuracy.\n","authors":["Andreagiovanni Reina","Thierry Njougouo","Elio Tuci","Timoteo Carletti"],"pdf_url":"https://arxiv.org/pdf/2310.13694v2.pdf","comment":"29 pages, 18 figures"},{"id":"http://arxiv.org/abs/2306.05716v4","updated":"2024-03-18T11:57:09Z","published":"2023-06-09T07:22:12Z","title":"Transferring Foundation Models for Generalizable Robotic Manipulation","summary":"  Improving the generalization capabilities of general-purpose robotic\nmanipulation agents in the real world has long been a significant challenge.\nExisting approaches often rely on collecting large-scale robotic data which is\ncostly and time-consuming, such as the RT-1 dataset. However, due to\ninsufficient diversity of data, these approaches typically suffer from limiting\ntheir capability in open-domain scenarios with new objects and diverse\nenvironments. In this paper, we propose a novel paradigm that effectively\nleverages language-reasoning segmentation mask generated by internet-scale\nfoundation models, to condition robot manipulation tasks. By integrating the\nmask modality, which incorporates semantic, geometric, and temporal correlation\npriors derived from vision foundation models, into the end-to-end policy model,\nour approach can effectively and robustly perceive object pose and enable\nsample-efficient generalization learning, including new object instances,\nsemantic categories, and unseen backgrounds. We first introduce a series of\nfoundation models to ground natural language demands across multiple tasks.\nSecondly, we develop a two-stream 2D policy model based on imitation learning,\nwhich processes raw images and object masks to predict robot actions with a\nlocal-global perception manner. Extensive realworld experiments conducted on a\nFranka Emika robot arm demonstrate the effectiveness of our proposed paradigm\nand policy architecture. Demos can be found in our submitted video, and more\ncomprehensive ones can be found in link1 or link2.\n","authors":["Jiange Yang","Wenhui Tan","Chuhao Jin","Keling Yao","Bei Liu","Jianlong Fu","Ruihua Song","Gangshan Wu","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2306.05716v4.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.11577v1","updated":"2024-03-18T08:53:03Z","published":"2024-03-18T08:53:03Z","title":"3DGS-Calib: 3D Gaussian Splatting for Multimodal SpatioTemporal\n  Calibration","summary":"  Reliable multimodal sensor fusion algorithms require accurate spatiotemporal\ncalibration. Recently, targetless calibration techniques based on implicit\nneural representations have proven to provide precise and robust results.\nNevertheless, such methods are inherently slow to train given the high\ncomputational overhead caused by the large number of sampled points required\nfor volume rendering. With the recent introduction of 3D Gaussian Splatting as\na faster alternative to implicit representation methods, we propose to leverage\nthis new rendering approach to achieve faster multi-sensor calibration. We\nintroduce 3DGS-Calib, a new calibration method that relies on the speed and\nrendering accuracy of 3D Gaussian Splatting to achieve multimodal\nspatiotemporal calibration that is accurate, robust, and with a substantial\nspeed-up compared to methods relying on implicit neural representations. We\ndemonstrate the superiority of our proposal with experimental results on\nsequences from KITTI-360, a widely used driving dataset.\n","authors":["Quentin Herau","Moussab Bennehar","Arthur Moreau","Nathan Piasco","Luis Roldao","Dzmitry Tsishkou","Cyrille Migniot","Pascal Vasseur","Cédric Demonceaux"],"pdf_url":"https://arxiv.org/pdf/2403.11577v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2403.11552v1","updated":"2024-03-18T08:03:47Z","published":"2024-03-18T08:03:47Z","title":"LLM^3:Large Language Model-based Task and Motion Planning with Motion\n  Failure Reasoning","summary":"  Conventional Task and Motion Planning (TAMP) approaches rely on manually\ncrafted interfaces connecting symbolic task planning with continuous motion\ngeneration. These domain-specific and labor-intensive modules are limited in\naddressing emerging tasks in real-world settings. Here, we present LLM^3, a\nnovel Large Language Model (LLM)-based TAMP framework featuring a\ndomain-independent interface. Specifically, we leverage the powerful reasoning\nand planning capabilities of pre-trained LLMs to propose symbolic action\nsequences and select continuous action parameters for motion planning.\nCrucially, LLM^3 incorporates motion planning feedback through prompting,\nallowing the LLM to iteratively refine its proposals by reasoning about motion\nfailure. Consequently, LLM^3 interfaces between task planning and motion\nplanning, alleviating the intricate design process of handling domain-specific\nmessages between them. Through a series of simulations in a box-packing domain,\nwe quantitatively demonstrate the effectiveness of LLM^3 in solving TAMP\nproblems and the efficiency in selecting action parameters. Ablation studies\nunderscore the significant contribution of motion failure reasoning to the\nsuccess of LLM^3. Furthermore, we conduct qualitative experiments on a physical\nmanipulator, demonstrating the practical applicability of our approach in\nreal-world settings.\n","authors":["Shu Wang","Muzhi Han","Ziyuan Jiao","Zeyu Zhang","Ying Nian Wu","Song-Chun Zhu","Hangxin Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11552v1.pdf","comment":"Submitted to IROS 2024. Codes available:\n  https://github.com/AssassinWS/LLM-TAMP"}]},"2024-03-19T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2403.11461v2","updated":"2024-03-19T03:32:15Z","published":"2024-03-18T04:26:52Z","title":"VIHE: Virtual In-Hand Eye Transformer for 3D Robotic Manipulation","summary":"  In this work, we introduce the Virtual In-Hand Eye Transformer (VIHE), a\nnovel method designed to enhance 3D manipulation capabilities through\naction-aware view rendering. VIHE autoregressively refines actions in multiple\nstages by conditioning on rendered views posed from action predictions in the\nearlier stages. These virtual in-hand views provide a strong inductive bias for\neffectively recognizing the correct pose for the hand, especially for\nchallenging high-precision tasks such as peg insertion. On 18 manipulation\ntasks in RLBench simulated environments, VIHE achieves a new state-of-the-art,\nwith a 12% absolute improvement, increasing from 65% to 77% over the existing\nstate-of-the-art model using 100 demonstrations per task. In real-world\nscenarios, VIHE can learn manipulation tasks with just a handful of\ndemonstrations, highlighting its practical utility. Videos and code\nimplementation can be found at our project site: https://vihe-3d.github.io.\n","authors":["Weiyao Wang","Yutian Lei","Shiyu Jin","Gregory D. Hager","Liangjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.11461v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11370v2","updated":"2024-03-19T09:12:00Z","published":"2024-03-17T23:23:40Z","title":"DynamicGlue: Epipolar and Time-Informed Data Association in Dynamic\n  Environments using Graph Neural Networks","summary":"  The assumption of a static environment is common in many geometric computer\nvision tasks like SLAM but limits their applicability in highly dynamic scenes.\nSince these tasks rely on identifying point correspondences between input\nimages within the static part of the environment, we propose a graph neural\nnetwork-based sparse feature matching network designed to perform robust\nmatching under challenging conditions while excluding keypoints on moving\nobjects. We employ a similar scheme of attentional aggregation over graph edges\nto enhance keypoint representations as state-of-the-art feature-matching\nnetworks but augment the graph with epipolar and temporal information and\nvastly reduce the number of graph edges. Furthermore, we introduce a\nself-supervised training scheme to extract pseudo labels for image pairs in\ndynamic environments from exclusively unprocessed visual-inertial data. A\nseries of experiments show the superior performance of our network as it\nexcludes keypoints on moving objects compared to state-of-the-art feature\nmatching networks while still achieving similar results regarding conventional\nmatching metrics. When integrated into a SLAM system, our network significantly\nimproves performance, especially in highly dynamic scenes.\n","authors":["Theresa Huber","Simon Schaefer","Stefan Leutenegger"],"pdf_url":"https://arxiv.org/pdf/2403.11370v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12959v1","updated":"2024-03-19T17:58:02Z","published":"2024-03-19T17:58:02Z","title":"WHAC: World-grounded Humans and Cameras","summary":"  Estimating human and camera trajectories with accurate scale in the world\ncoordinate system from a monocular video is a highly desirable yet challenging\nand ill-posed problem. In this study, we aim to recover expressive parametric\nhuman models (i.e., SMPL-X) and corresponding camera poses jointly, by\nleveraging the synergy between three critical players: the world, the human,\nand the camera. Our approach is founded on two key observations. Firstly,\ncamera-frame SMPL-X estimation methods readily recover absolute human depth.\nSecondly, human motions inherently provide absolute spatial cues. By\nintegrating these insights, we introduce a novel framework, referred to as\nWHAC, to facilitate world-grounded expressive human pose and shape estimation\n(EHPS) alongside camera pose estimation, without relying on traditional\noptimization techniques. Additionally, we present a new synthetic dataset,\nWHAC-A-Mole, which includes accurately annotated humans and cameras, and\nfeatures diverse interactive human motions as well as realistic camera\ntrajectories. Extensive experiments on both standard and newly established\nbenchmarks highlight the superiority and efficacy of our framework. We will\nmake the code and dataset publicly available.\n","authors":["Wanqi Yin","Zhongang Cai","Ruisi Wang","Fanzhou Wang","Chen Wei","Haiyi Mei","Weiye Xiao","Zhitao Yang","Qingping Sun","Atsushi Yamashita","Ziwei Liu","Lei Yang"],"pdf_url":"https://arxiv.org/pdf/2403.12959v1.pdf","comment":"Homepage: https://wqyin.github.io/projects/WHAC/"},{"id":"http://arxiv.org/abs/2403.06420v2","updated":"2024-03-19T17:52:09Z","published":"2024-03-11T04:13:26Z","title":"RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic\n  Manipulations With Large Language Models","summary":"  Reinforcement learning (RL) has demonstrated its capability in solving\nvarious tasks but is notorious for its low sample efficiency. In this paper, we\npropose RLingua, a framework that can leverage the internal knowledge of large\nlanguage models (LLMs) to reduce the sample complexity of RL in robotic\nmanipulations. To this end, we first present a method for extracting the prior\nknowledge of LLMs by prompt engineering so that a preliminary rule-based robot\ncontroller for a specific task can be generated in a user-friendly manner.\nDespite being imperfect, the LLM-generated robot controller is utilized to\nproduce action samples during rollouts with a decaying probability, thereby\nimproving RL's sample efficiency. We employ TD3, the widely-used RL baseline\nmethod, and modify the actor loss to regularize the policy learning towards the\nLLM-generated controller. RLingua also provides a novel method of improving the\nimperfect LLM-generated robot controllers by RL. We demonstrate that RLingua\ncan significantly reduce the sample complexity of TD3 in four robot tasks of\npanda_gym and achieve high success rates in 12 sampled sparsely rewarded robot\ntasks in RLBench, where the standard TD3 fails. Additionally, We validated\nRLingua's effectiveness in real-world robot experiments through Sim2Real,\ndemonstrating that the learned policies are effectively transferable to real\nrobot tasks. Further details about our work are available at our project\nwebsite https://rlingua.github.io.\n","authors":["Liangliang Chen","Yutian Lei","Shiyu Jin","Ying Zhang","Liangjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.06420v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12945v1","updated":"2024-03-19T17:48:38Z","published":"2024-03-19T17:48:38Z","title":"DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset","summary":"  The creation of large, diverse, high-quality robot manipulation datasets is\nan important stepping stone on the path toward more capable and robust robotic\nmanipulation policies. However, creating such datasets is challenging:\ncollecting robot manipulation data in diverse environments poses logistical and\nsafety challenges and requires substantial investments in hardware and human\nlabour. As a result, even the most general robot manipulation policies today\nare mostly trained on data collected in a small number of environments with\nlimited scene and task diversity. In this work, we introduce DROID (Distributed\nRobot Interaction Dataset), a diverse robot manipulation dataset with 76k\ndemonstration trajectories or 350 hours of interaction data, collected across\n564 scenes and 84 tasks by 50 data collectors in North America, Asia, and\nEurope over the course of 12 months. We demonstrate that training with DROID\nleads to policies with higher performance and improved generalization ability.\nWe open source the full dataset, policy learning code, and a detailed guide for\nreproducing our robot hardware setup.\n","authors":["Alexander Khazatsky","Karl Pertsch","Suraj Nair","Ashwin Balakrishna","Sudeep Dasari","Siddharth Karamcheti","Soroush Nasiriany","Mohan Kumar Srirama","Lawrence Yunliang Chen","Kirsty Ellis","Peter David Fagan","Joey Hejna","Masha Itkina","Marion Lepert","Yecheng Jason Ma","Patrick Tree Miller","Jimmy Wu","Suneel Belkhale","Shivin Dass","Huy Ha","Arhan Jain","Abraham Lee","Youngwoon Lee","Marius Memmel","Sungjae Park","Ilija Radosavovic","Kaiyuan Wang","Albert Zhan","Kevin Black","Cheng Chi","Kyle Beltran Hatch","Shan Lin","Jingpei Lu","Jean Mercat","Abdul Rehman","Pannag R Sanketi","Archit Sharma","Cody Simpson","Quan Vuong","Homer Rich Walke","Blake Wulfe","Ted Xiao","Jonathan Heewon Yang","Arefeh Yavary","Tony Z. Zhao","Christopher Agia","Rohan Baijal","Mateo Guaman Castro","Daphne Chen","Qiuyu Chen","Trinity Chung","Jaimyn Drake","Ethan Paul Foster","Jensen Gao","David Antonio Herrera","Minho Heo","Kyle Hsu","Jiaheng Hu","Donovon Jackson","Charlotte Le","Yunshuang Li","Kevin Lin","Roy Lin","Zehan Ma","Abhiram Maddukuri","Suvir Mirchandani","Daniel Morton","Tony Nguyen","Abigail O'Neill","Rosario Scalise","Derick Seale","Victor Son","Stephen Tian","Emi Tran","Andrew E. Wang","Yilin Wu","Annie Xie","Jingyun Yang","Patrick Yin","Yunchu Zhang","Osbert Bastani","Glen Berseth","Jeannette Bohg","Ken Goldberg","Abhinav Gupta","Abhishek Gupta","Dinesh Jayaraman","Joseph J Lim","Jitendra Malik","Roberto Martín-Martín","Subramanian Ramamoorthy","Dorsa Sadigh","Shuran Song","Jiajun Wu","Michael C. Yip","Yuke Zhu","Thomas Kollar","Sergey Levine","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2403.12945v1.pdf","comment":"Project website: https://droid-dataset.github.io/"},{"id":"http://arxiv.org/abs/2403.12943v1","updated":"2024-03-19T17:47:37Z","published":"2024-03-19T17:47:37Z","title":"Vid2Robot: End-to-end Video-conditioned Policy Learning with\n  Cross-Attention Transformers","summary":"  While large-scale robotic systems typically rely on textual instructions for\ntasks, this work explores a different approach: can robots infer the task\ndirectly from observing humans? This shift necessitates the robot's ability to\ndecode human intent and translate it into executable actions within its\nphysical constraints and environment. We introduce Vid2Robot, a novel\nend-to-end video-based learning framework for robots. Given a video\ndemonstration of a manipulation task and current visual observations, Vid2Robot\ndirectly produces robot actions. This is achieved through a unified\nrepresentation model trained on a large dataset of human video and robot\ntrajectory. The model leverages cross-attention mechanisms to fuse prompt video\nfeatures to the robot's current state and generate appropriate actions that\nmimic the observed task. To further improve policy performance, we propose\nauxiliary contrastive losses that enhance the alignment between human and robot\nvideo representations. We evaluate Vid2Robot on real-world robots,\ndemonstrating a 20% improvement in performance compared to other\nvideo-conditioned policies when using human demonstration videos. Additionally,\nour model exhibits emergent capabilities, such as successfully transferring\nobserved motions from one object to another, and long-horizon composition, thus\nshowcasing its potential for real-world applications. Project website:\nvid2robot.github.io\n","authors":["Vidhi Jain","Maria Attarian","Nikhil J Joshi","Ayzaan Wahid","Danny Driess","Quan Vuong","Pannag R Sanketi","Pierre Sermanet","Stefan Welker","Christine Chan","Igor Gilitschenski","Yonatan Bisk","Debidatta Dwibedi"],"pdf_url":"https://arxiv.org/pdf/2403.12943v1.pdf","comment":"Robot learning: Imitation Learning, Robot Perception, Sensing &\n  Vision, Grasping & Manipulation"},{"id":"http://arxiv.org/abs/2403.12920v1","updated":"2024-03-19T17:23:44Z","published":"2024-03-19T17:23:44Z","title":"Semantic Layering in Room Segmentation via LLMs","summary":"  In this paper, we introduce Semantic Layering in Room Segmentation via LLMs\n(SeLRoS), an advanced method for semantic room segmentation by integrating\nLarge Language Models (LLMs) with traditional 2D map-based segmentation. Unlike\nprevious approaches that solely focus on the geometric segmentation of indoor\nenvironments, our work enriches segmented maps with semantic data, including\nobject identification and spatial relationships, to enhance robotic navigation.\nBy leveraging LLMs, we provide a novel framework that interprets and organizes\ncomplex information about each segmented area, thereby improving the accuracy\nand contextual relevance of room segmentation. Furthermore, SeLRoS overcomes\nthe limitations of existing algorithms by using a semantic evaluation method to\naccurately distinguish true room divisions from those erroneously generated by\nfurniture and segmentation inaccuracies. The effectiveness of SeLRoS is\nverified through its application across 30 different 3D environments. Source\ncode and experiment videos for this work are available at:\nhttps://sites.google.com/view/selros.\n","authors":["Taehyeon Kim","Byung-Cheol Min"],"pdf_url":"https://arxiv.org/pdf/2403.12920v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12910v1","updated":"2024-03-19T17:08:24Z","published":"2024-03-19T17:08:24Z","title":"Yell At Your Robot: Improving On-the-Fly from Language Corrections","summary":"  Hierarchical policies that combine language and low-level control have been\nshown to perform impressively long-horizon robotic tasks, by leveraging either\nzero-shot high-level planners like pretrained language and vision-language\nmodels (LLMs/VLMs) or models trained on annotated robotic demonstrations.\nHowever, for complex and dexterous skills, attaining high success rates on\nlong-horizon tasks still represents a major challenge -- the longer the task\nis, the more likely it is that some stage will fail. Can humans help the robot\nto continuously improve its long-horizon task performance through intuitive and\nnatural feedback? In this paper, we make the following observation: high-level\npolicies that index into sufficiently rich and expressive low-level\nlanguage-conditioned skills can be readily supervised with human feedback in\nthe form of language corrections. We show that even fine-grained corrections,\nsuch as small movements (\"move a bit to the left\"), can be effectively\nincorporated into high-level policies, and that such corrections can be readily\nobtained from humans observing the robot and making occasional suggestions.\nThis framework enables robots not only to rapidly adapt to real-time language\nfeedback, but also incorporate this feedback into an iterative training scheme\nthat improves the high-level policy's ability to correct errors in both\nlow-level execution and high-level decision-making purely from verbal feedback.\nOur evaluation on real hardware shows that this leads to significant\nperformance improvement in long-horizon, dexterous manipulation tasks without\nthe need for any additional teleoperation. Videos and code are available at\nhttps://yay-robot.github.io/.\n","authors":["Lucy Xiaoyang Shi","Zheyuan Hu","Tony Z. Zhao","Archit Sharma","Karl Pertsch","Jianlan Luo","Sergey Levine","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2403.12910v1.pdf","comment":"Project website: https://yay-robot.github.io/"},{"id":"http://arxiv.org/abs/2403.11492v2","updated":"2024-03-19T17:04:35Z","published":"2024-03-18T05:53:20Z","title":"SmartRefine: A Scenario-Adaptive Refinement Framework for Efficient\n  Motion Prediction","summary":"  Predicting the future motion of surrounding agents is essential for\nautonomous vehicles (AVs) to operate safely in dynamic, human-robot-mixed\nenvironments. Context information, such as road maps and surrounding agents'\nstates, provides crucial geometric and semantic information for motion behavior\nprediction. To this end, recent works explore two-stage prediction frameworks\nwhere coarse trajectories are first proposed, and then used to select critical\ncontext information for trajectory refinement. However, they either incur a\nlarge amount of computation or bring limited improvement, if not both. In this\npaper, we introduce a novel scenario-adaptive refinement strategy, named\nSmartRefine, to refine prediction with minimal additional computation.\nSpecifically, SmartRefine can comprehensively adapt refinement configurations\nbased on each scenario's properties, and smartly chooses the number of\nrefinement iterations by introducing a quality score to measure the prediction\nquality and remaining refinement potential of each scenario. SmartRefine is\ndesigned as a generic and flexible approach that can be seamlessly integrated\ninto most state-of-the-art motion prediction models. Experiments on Argoverse\n(1 & 2) show that our method consistently improves the prediction accuracy of\nmultiple state-of-the-art prediction models. Specifically, by adding\nSmartRefine to QCNet, we outperform all published ensemble-free works on the\nArgoverse 2 leaderboard (single agent track) at submission. Comprehensive\nstudies are also conducted to ablate design choices and explore the mechanism\nbehind multi-iteration refinement. Codes are available at\nhttps://github.com/opendilab/SmartRefine/\n","authors":["Yang Zhou","Hao Shao","Letian Wang","Steven L. Waslander","Hongsheng Li","Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11492v2.pdf","comment":"Camera-ready version for CVPR 2024"},{"id":"http://arxiv.org/abs/2403.12892v1","updated":"2024-03-19T16:44:53Z","published":"2024-03-19T16:44:53Z","title":"Uoc luong kenh truyen trong he thong da robot su dung SDR","summary":"  This study focuses on developing an experimental system for estimating\ncommunication channels in a multi-robot mobile system using software-defined\nradio (SDR) devices. The system consists of two mobile robots programmed for\ntwo scenarios: one where the robot remains stationary and another where it\nfollows a predefined trajectory. Communication within the system is conducted\nthrough orthogonal frequency-division multiplexing (OFDM) to mitigate the\neffects of multipath propagation in indoor environments. The system's\nperformance is evaluated using the bit error rate (BER). Connections related to\nrobot motion and communication are implemented using Raspberry Pi 3 and BladeRF\nx115, respectively. The least squares (LS) technique is employed to estimate\nthe channel with a bit error rate of approximately 10^(-2).\n","authors":["Do Hai Son","Nguyen Huu Hung","Pham Duy Hung","Tran Thi Thuy Quynh"],"pdf_url":"https://arxiv.org/pdf/2403.12892v1.pdf","comment":"in Vietnamese language"},{"id":"http://arxiv.org/abs/2403.12891v1","updated":"2024-03-19T16:40:57Z","published":"2024-03-19T16:40:57Z","title":"Adaptive Visual Imitation Learning for Robotic Assisted Feeding Across\n  Varied Bowl Configurations and Food Types","summary":"  In this study, we introduce a novel visual imitation network with a spatial\nattention module for robotic assisted feeding (RAF). The goal is to acquire\n(i.e., scoop) food items from a bowl. However, achieving robust and adaptive\nfood manipulation is particularly challenging. To deal with this, we propose a\nframework that integrates visual perception with imitation learning to enable\nthe robot to handle diverse scenarios during scooping. Our approach, named AVIL\n(adaptive visual imitation learning), exhibits adaptability and robustness\nacross different bowl configurations in terms of material, size, and position,\nas well as diverse food types including granular, semi-solid, and liquid, even\nin the presence of distractors. We validate the effectiveness of our approach\nby conducting experiments on a real robot. We also compare its performance with\na baseline. The results demonstrate improvement over the baseline across all\nscenarios, with an enhancement of up to 2.5 times in terms of a success metric.\nNotably, our model, trained solely on data from a transparent glass bowl\ncontaining granular cereals, showcases generalization ability when tested\nzero-shot on other bowl configurations with different types of food.\n","authors":["Rui Liu","Amisha Bhaskar","Pratap Tokekar"],"pdf_url":"https://arxiv.org/pdf/2403.12891v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12876v1","updated":"2024-03-19T16:21:40Z","published":"2024-03-19T16:21:40Z","title":"LAVA: Long-horizon Visual Action based Food Acquisition","summary":"  Robotic Assisted Feeding (RAF) addresses the fundamental need for individuals\nwith mobility impairments to regain autonomy in feeding themselves. The goal of\nRAF is to use a robot arm to acquire and transfer food to individuals from the\ntable. Existing RAF methods primarily focus on solid foods, leaving a gap in\nmanipulation strategies for semi-solid and deformable foods. This study\nintroduces Long-horizon Visual Action (LAVA) based food acquisition of liquid,\nsemisolid, and deformable foods. Long-horizon refers to the goal of \"clearing\nthe bowl\" by sequentially acquiring the food from the bowl. LAVA employs a\nhierarchical policy for long-horizon food acquisition tasks. The framework uses\nhigh-level policy to determine primitives by leveraging ScoopNet. At the\nmid-level, LAVA finds parameters for primitives using vision. To carry out\nsequential plans in the real world, LAVA delegates action execution which is\ndriven by Low-level policy that uses parameters received from mid-level policy\nand behavior cloning ensuring precise trajectory execution. We validate our\napproach on complex real-world acquisition trials involving granular, liquid,\nsemisolid, and deformable food types along with fruit chunks and soup\nacquisition. Across 46 bowls, LAVA acquires much more efficiently than\nbaselines with a success rate of 89 +/- 4% and generalizes across realistic\nplate variations such as different positions, varieties, and amount of food in\nthe bowl. Code, datasets, videos, and supplementary materials can be found on\nour website.\n","authors":["Amisha Bhaskar","Rui Liu","Vishnu D. Sharma","Guangyao Shi","Pratap Tokekar"],"pdf_url":"https://arxiv.org/pdf/2403.12876v1.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2310.12320v2","updated":"2024-03-19T16:18:21Z","published":"2023-10-18T20:44:16Z","title":"Asynchronous Distributed Smoothing and Mapping via On-Manifold Consensus\n  ADMM","summary":"  In this paper we present a fully distributed, asynchronous, and general\npurpose optimization algorithm for Consensus Simultaneous Localization and\nMapping (CSLAM). Multi-robot teams require that agents have timely and accurate\nsolutions to their state as well as the states of the other robots in the team.\nTo optimize this solution we develop a CSLAM back-end based on Consensus ADMM\ncalled MESA (Manifold, Edge-based, Separable ADMM). MESA is fully distributed\nto tolerate failures of individual robots, asynchronous to tolerate\ncommunication delays and outages, and general purpose to handle any CSLAM\nproblem formulation. We demonstrate that MESA exhibits superior convergence\nrates and accuracy compare to existing state-of-the art CSLAM back-end\noptimizers.\n","authors":["Daniel McGann","Kyle Lassak","Michael Kaess"],"pdf_url":"https://arxiv.org/pdf/2310.12320v2.pdf","comment":"Accepted to ICRA 2024"},{"id":"http://arxiv.org/abs/2306.06192v4","updated":"2024-03-19T16:16:16Z","published":"2023-06-09T18:45:15Z","title":"Ada-NAV: Adaptive Trajectory Length-Based Sample Efficient Policy\n  Learning for Robotic Navigation","summary":"  Trajectory length stands as a crucial hyperparameter within reinforcement\nlearning (RL) algorithms, significantly contributing to the sample inefficiency\nin robotics applications. Motivated by the pivotal role trajectory length plays\nin the training process, we introduce Ada-NAV, a novel adaptive trajectory\nlength scheme designed to enhance the training sample efficiency of RL\nalgorithms in robotic navigation tasks. Unlike traditional approaches that\ntreat trajectory length as a fixed hyperparameter, we propose to dynamically\nadjust it based on the entropy of the underlying navigation policy.\nInterestingly, Ada-NAV can be applied to both existing on-policy and off-policy\nRL methods, which we demonstrate by empirically validating its efficacy on\nthree popular RL methods: REINFORCE, Proximal Policy Optimization (PPO), and\nSoft Actor-Critic (SAC). We demonstrate through simulated and real-world\nrobotic experiments that Ada-NAV outperforms conventional methods that employ\nconstant or randomly sampled trajectory lengths. Specifically, for a fixed\nsample budget, Ada-NAV achieves an 18\\% increase in navigation success rate, a\n20-38\\% reduction in navigation path length, and a 9.32\\% decrease in elevation\ncosts. Furthermore, we showcase the versatility of Ada-NAV by integrating it\nwith the Clearpath Husky robot, illustrating its applicability in complex\noutdoor environments.\n","authors":["Bhrij Patel","Kasun Weerakoon","Wesley A. Suttle","Alec Koppel","Brian M. Sadler","Tianyi Zhou","Amrit Singh Bedi","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2306.06192v4.pdf","comment":"11 pages, 9 figures, 2 tables"},{"id":"http://arxiv.org/abs/2403.12865v1","updated":"2024-03-19T16:09:30Z","published":"2024-03-19T16:09:30Z","title":"PE-Planner: A Performance-Enhanced Quadrotor Motion Planner for\n  Autonomous Flight in Complex and Dynamic Environments","summary":"  The role of a motion planner is pivotal in quadrotor applications, yet\nexisting methods often struggle to adapt to complex environments, limiting\ntheir ability to achieve fast, safe, and robust flight. In this letter, we\nintroduce a performance-enhanced quadrotor motion planner designed for\nautonomous flight in complex environments including dense obstacles, dynamic\nobstacles, and unknown disturbances. The global planner generates an initial\ntrajectory through kinodynamic path searching and refines it using B-spline\ntrajectory optimization. Subsequently, the local planner takes into account the\nquadrotor dynamics, estimated disturbance, global reference trajectory, control\ncost, time cost, and safety constraints to generate real-time control inputs,\nutilizing the framework of model predictive contouring control. Both\nsimulations and real-world experiments corroborate the heightened robustness,\nsafety, and speed of the proposed motion planner. Additionally, our motion\nplanner achieves flights at more than 6.8 m/s in a challenging and complex\nracing scenario.\n","authors":["Jiaxin Qiu","Qingchen Liu","Jiahu Qin","Dewang Cheng","Yawei Tian","Qichao Ma"],"pdf_url":"https://arxiv.org/pdf/2403.12865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12861v1","updated":"2024-03-19T16:05:51Z","published":"2024-03-19T16:05:51Z","title":"D-Cubed: Latent Diffusion Trajectory Optimisation for Dexterous\n  Deformable Manipulation","summary":"  Mastering dexterous robotic manipulation of deformable objects is vital for\novercoming the limitations of parallel grippers in real-world applications.\nCurrent trajectory optimisation approaches often struggle to solve such tasks\ndue to the large search space and the limited task information available from a\ncost function. In this work, we propose D-Cubed, a novel trajectory\noptimisation method using a latent diffusion model (LDM) trained from a\ntask-agnostic play dataset to solve dexterous deformable object manipulation\ntasks. D-Cubed learns a skill-latent space that encodes short-horizon actions\nin the play dataset using a VAE and trains a LDM to compose the skill latents\ninto a skill trajectory, representing a long-horizon action trajectory in the\ndataset. To optimise a trajectory for a target task, we introduce a novel\ngradient-free guided sampling method that employs the Cross-Entropy method\nwithin the reverse diffusion process. In particular, D-Cubed samples a small\nnumber of noisy skill trajectories using the LDM for exploration and evaluates\nthe trajectories in simulation. Then, D-Cubed selects the trajectory with the\nlowest cost for the subsequent reverse process. This effectively explores\npromising solution areas and optimises the sampled trajectories towards a\ntarget task throughout the reverse diffusion process. Through empirical\nevaluation on a public benchmark of dexterous deformable object manipulation\ntasks, we demonstrate that D-Cubed outperforms traditional trajectory\noptimisation and competitive baseline approaches by a significant margin. We\nfurther demonstrate that trajectories found by D-Cubed readily transfer to a\nreal-world LEAP hand on a folding task.\n","authors":["Jun Yamada","Shaohong Zhong","Jack Collins","Ingmar Posner"],"pdf_url":"https://arxiv.org/pdf/2403.12861v1.pdf","comment":"https://applied-ai-lab.github.io/D-cubed/"},{"id":"http://arxiv.org/abs/2403.12856v1","updated":"2024-03-19T16:01:25Z","published":"2024-03-19T16:01:25Z","title":"Equivariant Ensembles and Regularization for Reinforcement Learning in\n  Map-based Path Planning","summary":"  In reinforcement learning (RL), exploiting environmental symmetries can\nsignificantly enhance efficiency, robustness, and performance. However,\nensuring that the deep RL policy and value networks are respectively\nequivariant and invariant to exploit these symmetries is a substantial\nchallenge. Related works try to design networks that are equivariant and\ninvariant by construction, limiting them to a very restricted library of\ncomponents, which in turn hampers the expressiveness of the networks. This\npaper proposes a method to construct equivariant policies and invariant value\nfunctions without specialized neural network components, which we term\nequivariant ensembles. We further add a regularization term for adding\ninductive bias during training. In a map-based path planning case study, we\nshow how equivariant ensembles and regularization benefit sample efficiency and\nperformance.\n","authors":["Mirco Theile","Hongpeng Cao","Marco Caccamo","Alberto L. Sangiovanni-Vincentelli"],"pdf_url":"https://arxiv.org/pdf/2403.12856v1.pdf","comment":"submitted for possible publication. A video can be found here:\n  https://youtu.be/L6NOdvU7n7s"},{"id":"http://arxiv.org/abs/2403.12853v1","updated":"2024-03-19T15:57:32Z","published":"2024-03-19T15:57:32Z","title":"RASP: A Drone-based Reconfigurable Actuation and Sensing Platform\n  Towards Ambient Intelligent Systems","summary":"  Realizing consumer-grade drones that are as useful as robot vacuums\nthroughout our homes or personal smartphones in our daily lives requires drones\nto sense, actuate, and respond to general scenarios that may arise. Towards\nthis vision, we propose RASP, a modular and reconfigurable sensing and\nactuation platform that allows drones to autonomously swap onboard sensors and\nactuators in only 25 seconds, allowing a single drone to quickly adapt to a\ndiverse range of tasks. RASP consists of a mechanical layer to physically swap\nsensor modules, an electrical layer to maintain power and communication lines\nto the sensor/actuator, and a software layer to maintain a common interface\nbetween the drone and any sensor module in our platform. Leveraging recent\nadvances in large language and visual language models, we further introduce the\narchitecture, implementation, and real-world deployments of a personal\nassistant system utilizing RASP. We demonstrate that RASP can enable a diverse\nrange of useful tasks in home, office, lab, and other indoor settings.\n","authors":["Minghui Zhao","Junxi Xia","Kaiyuan Hou","Yanchen Liu","Stephen Xia","Xiaofan Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.12853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12842v1","updated":"2024-03-19T15:49:32Z","published":"2024-03-19T15:49:32Z","title":"The Interplay Between Symmetries and Impact Effects on Hybrid Mechanical\n  Systems","summary":"  Hybrid systems are dynamical systems with continuous-time and discrete-time\ncomponents in their dynamics. When hybrid systems are defined on a principal\nbundle we are able to define two classes of impacts for the discrete-time\ntransition of the dynamics: interior impacts and exterior impacts. In this\npaper we define hybrid systems on principal bundles, study the underlying\ngeometry on the switching surface where impacts occur and we find conditions\nfor which both exterior and interior impacts are preserved by the mechanical\nconnection induced in the principal bundle.\n","authors":["William Clark","Leonardo Colombo","Anthony Bloch"],"pdf_url":"https://arxiv.org/pdf/2403.12842v1.pdf","comment":"6 pages. To be presented at a conference. Comments welcome"},{"id":"http://arxiv.org/abs/2403.12837v1","updated":"2024-03-19T15:42:46Z","published":"2024-03-19T15:42:46Z","title":"Opti-Acoustic Semantic SLAM with Unknown Objects in Underwater\n  Environments","summary":"  Despite recent advances in semantic Simultaneous Localization and Mapping\n(SLAM) for terrestrial and aerial applications, underwater semantic SLAM\nremains an open and largely unaddressed research problem due to the unique\nsensing modalities and the object classes found underwater. This paper presents\nan object-based semantic SLAM method for underwater environments that can\nidentify, localize, classify, and map a wide variety of marine objects without\na priori knowledge of the object classes present in the scene. The method\nperforms unsupervised object segmentation and object-level feature aggregation,\nand then uses opti-acoustic sensor fusion for object localization.\nProbabilistic data association is used to determine observation to landmark\ncorrespondences. Given such correspondences, the method then jointly optimizes\nlandmark and vehicle position estimates. Indoor and outdoor underwater datasets\nwith a wide variety of objects and challenging acoustic and lighting conditions\nare collected for evaluation and made publicly available. Quantitative and\nqualitative results show the proposed method achieves reduced trajectory error\ncompared to baseline methods, and is able to obtain comparable map accuracy to\na baseline closed-set method that requires hand-labeled data of all objects in\nthe scene.\n","authors":["Kurran Singh","Jungseok Hong","Nicholas R. Rypkema","John J. Leonard"],"pdf_url":"https://arxiv.org/pdf/2403.12837v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12835v1","updated":"2024-03-19T15:41:39Z","published":"2024-03-19T15:41:39Z","title":"AnySkill: Learning Open-Vocabulary Physical Skill for Interactive Agents","summary":"  Traditional approaches in physics-based motion generation, centered around\nimitation learning and reward shaping, often struggle to adapt to new\nscenarios. To tackle this limitation, we propose AnySkill, a novel hierarchical\nmethod that learns physically plausible interactions following open-vocabulary\ninstructions. Our approach begins by developing a set of atomic actions via a\nlow-level controller trained via imitation learning. Upon receiving an\nopen-vocabulary textual instruction, AnySkill employs a high-level policy that\nselects and integrates these atomic actions to maximize the CLIP similarity\nbetween the agent's rendered images and the text. An important feature of our\nmethod is the use of image-based rewards for the high-level policy, which\nallows the agent to learn interactions with objects without manual reward\nengineering. We demonstrate AnySkill's capability to generate realistic and\nnatural motion sequences in response to unseen instructions of varying lengths,\nmarking it the first method capable of open-vocabulary physical skill learning\nfor interactive humanoid agents.\n","authors":["Jieming Cui","Tengyu Liu","Nian Liu","Yaodong Yang","Yixin Zhu","Siyuan Huang"],"pdf_url":"https://arxiv.org/pdf/2403.12835v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.02021v4","updated":"2024-03-19T15:05:05Z","published":"2022-09-05T15:41:13Z","title":"When Robotics Meets Wireless Communications: An Introductory Tutorial","summary":"  The importance of ground Mobile Robots (MRs) and Unmanned Aerial Vehicles\n(UAVs) within the research community, industry, and society is growing fast.\nMany of these agents are nowadays equipped with communication systems that are,\nin some cases, essential to successfully achieve certain tasks. In this\ncontext, we have begun to witness the development of a new interdisciplinary\nresearch field at the intersection of robotics and communications. This\nresearch field has been boosted by the intention of integrating UAVs within the\n5G and 6G communication networks. This research will undoubtedly lead to many\nimportant applications in the near future. Nevertheless, one of the main\nobstacles to the development of this research area is that most researchers\naddress these problems by oversimplifying either the robotics or the\ncommunications aspect. This impedes the ability of reaching the full potential\nof this new interdisciplinary research area. In this tutorial, we present some\nof the modelling tools necessary to address problems involving both robotics\nand communication from an interdisciplinary perspective. As an illustrative\nexample of such problems, we focus in this tutorial on the issue of\ncommunication-aware trajectory planning.\n","authors":["Daniel Bonilla Licea","Mounir Ghogho","Martin Saska"],"pdf_url":"https://arxiv.org/pdf/2209.02021v4.pdf","comment":"35 pages, 192 references"},{"id":"http://arxiv.org/abs/2403.12798v1","updated":"2024-03-19T15:00:53Z","published":"2024-03-19T15:00:53Z","title":"Introducing Combi-Stations in Robotic Mobile Fulfilment Systems: A\n  Queueing-Theory-Based Efficiency Analysis","summary":"  In the era of digital commerce, the surge in online shopping and the\nexpectation for rapid delivery have placed unprecedented demands on warehouse\noperations. The traditional method of order fulfilment, where human order\npickers traverse large storage areas to pick items, has become a bottleneck,\nconsuming valuable time and resources. Robotic Mobile Fulfilment Systems (RMFS)\noffer a solution by using robots to transport storage racks directly to\nhuman-operated picking stations, eliminating the need for pickers to travel.\nThis paper introduces combi-stations, a novel type of station that enables both\nitem picking and replenishment, as opposed to traditional separate stations. We\nanalyse the efficiency of combi-stations using queueing theory and demonstrate\ntheir potential to streamline warehouse operations. Our results suggest that\ncombi-stations can reduce the number of robots required for stability and\nsignificantly reduce order turnover time, indicating a promising direction for\nfuture warehouse automation.\n","authors":["Lin Xie","Sonja Otten"],"pdf_url":"https://arxiv.org/pdf/2403.12798v1.pdf","comment":"15 pages, 7 figures. arXiv admin note: text overlap with\n  arXiv:1912.01782"},{"id":"http://arxiv.org/abs/2401.16205v2","updated":"2024-03-19T14:54:29Z","published":"2024-01-29T15:00:27Z","title":"CognitiveOS: Large Multimodal Model based System to Endow Any Type of\n  Robot with Generative AI","summary":"  This paper introduces CognitiveOS, the first operating system designed for\ncognitive robots capable of functioning across diverse robotic platforms.\nCognitiveOS is structured as a multi-agent system comprising modules built upon\na transformer architecture, facilitating communication through an internal\nmonologue format. These modules collectively empower the robot to tackle\nintricate real-world tasks. The paper delineates the operational principles of\nthe system along with descriptions of its nine distinct modules. The modular\ndesign endows the system with distinctive advantages over traditional\nend-to-end methodologies, notably in terms of adaptability and scalability. The\nsystem's modules are configurable, modifiable, or deactivatable depending on\nthe task requirements, while new modules can be seamlessly integrated. This\nsystem serves as a foundational resource for researchers and developers in the\ncognitive robotics domain, alleviating the burden of constructing a cognitive\nrobot system from scratch. Experimental findings demonstrate the system's\nadvanced task comprehension and adaptability across varied tasks, robotic\nplatforms, and module configurations, underscoring its potential for real-world\napplications. Moreover, in the category of Reasoning it outperformed\nCognitiveDog (by 15%) and RT2 (by 31%), achieving the highest to date rate of\n77%. We provide a code repository and dataset for the replication of\nCognitiveOS: link will be provided in camera-ready submission.\n","authors":["Artem Lykov","Mikhail Konenkov","Koffivi Fidèle Gbagbe","Mikhail Litvinov","Denis Davletshin","Aleksey Fedoseev","Miguel Altamirano Cabrera","Robinroy Peter","Dzmitry Tsetserukou"],"pdf_url":"https://arxiv.org/pdf/2401.16205v2.pdf","comment":"The paper is submitted to the IEEE conference"},{"id":"http://arxiv.org/abs/2403.12761v1","updated":"2024-03-19T14:27:31Z","published":"2024-03-19T14:27:31Z","title":"BTGenBot: Behavior Tree Generation for Robotic Tasks with Lightweight\n  LLMs","summary":"  This paper presents a novel approach to generating behavior trees for robots\nusing lightweight large language models (LLMs) with a maximum of 7 billion\nparameters. The study demonstrates that it is possible to achieve satisfying\nresults with compact LLMs when fine-tuned on a specific dataset. The key\ncontributions of this research include the creation of a fine-tuning dataset\nbased on existing behavior trees using GPT-3.5 and a comprehensive comparison\nof multiple LLMs (namely llama2, llama-chat, and code-llama) across nine\ndistinct tasks. To be thorough, we evaluated the generated behavior trees using\nstatic syntactical analysis, a validation system, a simulated environment, and\na real robot. Furthermore, this work opens the possibility of deploying such\nsolutions directly on the robot, enhancing its practical applicability.\nFindings from this study demonstrate the potential of LLMs with a limited\nnumber of parameters in generating effective and efficient robot behaviors.\n","authors":["Riccardo Andrea Izzo","Gianluca Bardaro","Matteo Matteucci"],"pdf_url":"https://arxiv.org/pdf/2403.12761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.04792v2","updated":"2024-03-19T14:13:53Z","published":"2023-08-09T08:31:05Z","title":"A Fast and Optimal Learning-based Path Planning Method for Planetary\n  Rovers","summary":"  Intelligent autonomous path planning is crucial to improve the exploration\nefficiency of planetary rovers. In this paper, we propose a learning-based\nmethod to quickly search for optimal paths in an elevation map, which is called\nNNPP. The NNPP model learns semantic information about start and goal\nlocations, as well as map representations, from numerous pre-annotated optimal\npath demonstrations, and produces a probabilistic distribution over each pixel\nrepresenting the likelihood of it belonging to an optimal path on the map. More\nspecifically, the paper computes the traversal cost for each grid cell from the\nslope, roughness and elevation difference obtained from the DEM. Subsequently,\nthe start and goal locations are encoded using a Gaussian distribution and\ndifferent location encoding parameters are analyzed for their effect on model\nperformance. After training, the NNPP model is able to perform path planning on\nnovel maps. Experiments show that the guidance field generated by the NNPP\nmodel can significantly reduce the search time for optimal paths under the same\nhardware conditions, and the advantage of NNPP increases with the scale of the\nmap.\n","authors":["Yiming Ji","Yang Liu","Guanghu Xie","Boyu Ma","Zongwu Xie","Baoshi Cao"],"pdf_url":"https://arxiv.org/pdf/2308.04792v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12725v1","updated":"2024-03-19T13:41:49Z","published":"2024-03-19T13:41:49Z","title":"Some geometric and topological data-driven methods in robot motion path\n  planning","summary":"  Motion path planning is an intrinsically geometric problem which is central\nfor design of robot systems. Since the early years of AI, robotics together\nwith computer vision have been the areas of computer science that drove its\ndevelopment. Many questions that arise, such as existence, optimality, and\ndiversity of motion paths in the configuration space that describes feasible\nrobot configurations, are of topological nature. The recent advances in\ntopological data analysis and related metric geometry, topology and\ncombinatorics have provided new tools to address these engineering tasks. We\nwill survey some questions, issues, recent work and promising directions in\ndata-driven geometric and topological methods with some emphasis on the use of\ndiscrete Morse theory.\n","authors":["Boris Goldfarb"],"pdf_url":"https://arxiv.org/pdf/2403.12725v1.pdf","comment":"21 pages, 6 figures, to appear in a book project on Topology,\n  Geometry and AI in the EMS Series in Industrial and Applied Mathematics,\n  edited by Michael Farber and Jes\\'us Gonz\\'alez"},{"id":"http://arxiv.org/abs/2403.12720v1","updated":"2024-03-19T13:29:44Z","published":"2024-03-19T13:29:44Z","title":"Shared Autonomy via Variable Impedance Control and Virtual Potential\n  Fields for Encoding Human Demonstration","summary":"  This article introduces a framework for complex human-robot collaboration\ntasks, such as the co-manufacturing of furniture. For these tasks, it is\nessential to encode tasks from human demonstration and reproduce these skills\nin a compliant and safe manner. Therefore, two key components are addressed in\nthis work: motion generation and shared autonomy. We propose a motion generator\nbased on a time-invariant potential field, capable of encoding wrench profiles,\ncomplex and closed-loop trajectories, and additionally incorporates obstacle\navoidance. Additionally, the paper addresses shared autonomy (SA) which enables\nsynergetic collaboration between human operators and robots by dynamically\nallocating authority. Variable impedance control (VIC) and force control are\nemployed, where impedance and wrench are adapted based on the human-robot\nautonomy factor derived from interaction forces. System passivity is ensured by\nan energy-tank based task passivation strategy. The framework's efficacy is\nvalidated through simulations and an experimental study employing a Franka\nEmika Research 3 robot.\n","authors":["Shail Jadav","Johannes Heidersberger","Christian Ott","Dongheui Lee"],"pdf_url":"https://arxiv.org/pdf/2403.12720v1.pdf","comment":"Accepted to ICRA 2024"},{"id":"http://arxiv.org/abs/2403.12686v1","updated":"2024-03-19T12:45:18Z","published":"2024-03-19T12:45:18Z","title":"WaterVG: Waterway Visual Grounding based on Text-Guided Vision and\n  mmWave Radar","summary":"  The perception of waterways based on human intent holds significant\nimportance for autonomous navigation and operations of Unmanned Surface\nVehicles (USVs) in water environments. Inspired by visual grounding, in this\npaper, we introduce WaterVG, the first visual grounding dataset designed for\nUSV-based waterway perception based on human intention prompts. WaterVG\nencompasses prompts describing multiple targets, with annotations at the\ninstance level including bounding boxes and masks. Notably, WaterVG includes\n11,568 samples with 34,950 referred targets, which integrates both visual and\nradar characteristics captured by monocular camera and millimeter-wave (mmWave)\nradar, enabling a finer granularity of text prompts. Furthermore, we propose a\nnovel multi-modal visual grounding model, Potamoi, which is a multi-modal and\nmulti-task model based on the one-stage paradigm with a designed Phased\nHeterogeneous Modality Fusion (PHMF) structure, including Adaptive Radar\nWeighting (ARW) and Multi-Head Slim Cross Attention (MHSCA). In specific, MHSCA\nis a low-cost and efficient fusion module with a remarkably small parameter\ncount and FLOPs, elegantly aligning and fusing scenario context information\ncaptured by two sensors with linguistic features, which can effectively address\ntasks of referring expression comprehension and segmentation based on\nfine-grained prompts. Comprehensive experiments and evaluations have been\nconducted on WaterVG, where our Potamoi archives state-of-the-art performances\ncompared with counterparts.\n","authors":["Runwei Guan","Liye Jia","Fengyufan Yang","Shanliang Yao","Erick Purwanto","Xiaohui Zhu","Eng Gee Lim","Jeremy Smith","Ka Lok Man","Yutao Yue"],"pdf_url":"https://arxiv.org/pdf/2403.12686v1.pdf","comment":"10 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.12685v1","updated":"2024-03-19T12:45:00Z","published":"2024-03-19T12:45:00Z","title":"Dynamic Manipulation of Deformable Objects using Imitation Learning with\n  Adaptation to Hardware Constraints","summary":"  Imitation Learning (IL) is a promising paradigm for learning dynamic\nmanipulation of deformable objects since it does not depend on\ndifficult-to-create accurate simulations of such objects. However, the\ntranslation of motions demonstrated by a human to a robot is a challenge for\nIL, due to differences in the embodiments and the robot's physical limits.\nThese limits are especially relevant in dynamic manipulation where high\nvelocities and accelerations are typical. To address this problem, we propose a\nframework that first maps a dynamic demonstration into a motion that respects\nthe robot's constraints using a constrained Dynamic Movement Primitive. Second,\nthe resulting object state is further optimized by quasi-static refinement\nmotions to optimize task performance metrics. This allows both efficiently\naltering the object state by dynamic motions and stable small-scale\nrefinements. We evaluate the framework in the challenging task of bag opening,\ndesigning the system BILBO: Bimanual dynamic manipulation using Imitation\nLearning for Bag Opening. Our results show that BILBO can successfully open a\nwide range of crumpled bags, using a demonstration with a single bag. See\nsupplementary material at https://sites.google.com/view/bilbo-bag.\n","authors":["Eric Hannus","Tran Nguyen Le","David Blanco-Mulero","Ville Kyrki"],"pdf_url":"https://arxiv.org/pdf/2403.12685v1.pdf","comment":"Submitted to 2024 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2024). 8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.12682v1","updated":"2024-03-19T12:36:51Z","published":"2024-03-19T12:36:51Z","title":"IFFNeRF: Initialisation Free and Fast 6DoF pose estimation from a single\n  image and a NeRF model","summary":"  We introduce IFFNeRF to estimate the six degrees-of-freedom (6DoF) camera\npose of a given image, building on the Neural Radiance Fields (NeRF)\nformulation. IFFNeRF is specifically designed to operate in real-time and\neliminates the need for an initial pose guess that is proximate to the sought\nsolution. IFFNeRF utilizes the Metropolis-Hasting algorithm to sample surface\npoints from within the NeRF model. From these sampled points, we cast rays and\ndeduce the color for each ray through pixel-level view synthesis. The camera\npose can then be estimated as the solution to a Least Squares problem by\nselecting correspondences between the query image and the resulting bundle. We\nfacilitate this process through a learned attention mechanism, bridging the\nquery image embedding with the embedding of parameterized rays, thereby\nmatching rays pertinent to the image. Through synthetic and real evaluation\nsettings, we show that our method can improve the angular and translation error\naccuracy by 80.1% and 67.3%, respectively, compared to iNeRF while performing\nat 34fps on consumer hardware and not requiring the initial pose guess.\n","authors":["Matteo Bortolon","Theodore Tsesmelis","Stuart James","Fabio Poiesi","Alessio Del Bue"],"pdf_url":"https://arxiv.org/pdf/2403.12682v1.pdf","comment":"Accepted ICRA 2024, Project page:\n  https://mbortolon97.github.io/iffnerf/"},{"id":"http://arxiv.org/abs/2403.12676v1","updated":"2024-03-19T12:21:23Z","published":"2024-03-19T12:21:23Z","title":"In-Hand Following of Deformable Linear Objects Using Dexterous Fingers\n  with Tactile Sensing","summary":"  Most research on deformable linear object (DLO) manipulation assumes rigid\ngrasping. However, beyond rigid grasping and re-grasping, in-hand following is\nalso an essential skill that humans use to dexterously manipulate DLOs, which\nrequires continuously changing the grasp point by in-hand sliding while holding\nthe DLO to prevent it from falling. Achieving such a skill is very challenging\nfor robots without using specially designed but not versatile end-effectors.\nPrevious works have attempted using generic parallel grippers, but their\nrobustness is unsatisfactory owing to the conflict between following and\nholding, which is hard to balance with a one-degree-of-freedom gripper. In this\nwork, inspired by how humans use fingers to follow DLOs, we explore the usage\nof a generic dexterous hand with tactile sensing to imitate human skills and\nachieve robust in-hand DLO following. To enable the hardware system to function\nin the real world, we develop a framework that includes Cartesian-space\narm-hand control, tactile-based in-hand 3-D DLO pose estimation, and\ntask-specific motion design. Experimental results demonstrate the significant\nsuperiority of our method over using parallel grippers, as well as its great\nrobustness, generalizability, and efficiency.\n","authors":["Mingrui Yu","Boyuan Liang","Xiang Zhang","Xinghao Zhu","Xiang Li","Masayoshi Tomizuka"],"pdf_url":"https://arxiv.org/pdf/2403.12676v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12670v1","updated":"2024-03-19T12:11:57Z","published":"2024-03-19T12:11:57Z","title":"Driving Animatronic Robot Facial Expression From Speech","summary":"  Animatronic robots aim to enable natural human-robot interaction through\nlifelike facial expressions. However, generating realistic, speech-synchronized\nrobot expressions is challenging due to the complexities of facial biomechanics\nand responsive motion synthesis. This paper presents a principled,\nskinning-centric approach to drive animatronic robot facial expressions from\nspeech. The proposed approach employs linear blend skinning (LBS) as the core\nrepresentation to guide tightly integrated innovations in embodiment design and\nmotion synthesis. LBS informs the actuation topology, enables human expression\nretargeting, and allows speech-driven facial motion generation. The proposed\napproach is capable of generating highly realistic, real-time facial\nexpressions from speech on an animatronic face, significantly advancing robots'\nability to replicate nuanced human expressions for natural interaction.\n","authors":["Boren Li","Hang Li","Hangxin Liu"],"pdf_url":"https://arxiv.org/pdf/2403.12670v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2401.07603v3","updated":"2024-03-19T11:17:00Z","published":"2024-01-15T11:20:34Z","title":"Multi-task real-robot data with gaze attention for dual-arm fine\n  manipulation","summary":"  In the field of robotic manipulation, deep imitation learning is recognized\nas a promising approach for acquiring manipulation skills. Additionally,\nlearning from diverse robot datasets is considered a viable method to achieve\nversatility and adaptability. In such research, by learning various tasks,\nrobots achieved generality across multiple objects. However, such multi-task\nrobot datasets have mainly focused on single-arm tasks that are relatively\nimprecise, not addressing the fine-grained object manipulation that robots are\nexpected to perform in the real world. This paper introduces a dataset of\ndiverse object manipulations that includes dual-arm tasks and/or tasks\nrequiring fine manipulation. To this end, we have generated dataset with 224k\nepisodes (150 hours, 1,104 language instructions) which includes dual-arm fine\ntasks such as bowl-moving, pencil-case opening or banana-peeling, and this data\nis publicly available. Additionally, this dataset includes visual attention\nsignals as well as dual-action labels, a signal that separates actions into a\nrobust reaching trajectory and precise interaction with objects, and language\ninstructions to achieve robust and precise object manipulation. We applied the\ndataset to our Dual-Action and Attention (DAA), a model designed for\nfine-grained dual arm manipulation tasks and robust against covariate shifts.\nThe model was tested with over 7k total trials in real robot manipulation\ntasks, demonstrating its capability in fine manipulation.\n","authors":["Heecheol Kim","Yoshiyuki Ohmura","Yasuo Kuniyoshi"],"pdf_url":"https://arxiv.org/pdf/2401.07603v3.pdf","comment":"10 pages, The dataset is available at\n  https://sites.google.com/view/multi-task-fine"},{"id":"http://arxiv.org/abs/2310.12547v2","updated":"2024-03-19T11:09:12Z","published":"2023-10-19T07:54:30Z","title":"PGA: Personalizing Grasping Agents with Single Human-Robot Interaction","summary":"  Language-Conditioned Robotic Grasping (LCRG) aims to develop robots that\ncomprehend and grasp objects based on natural language instructions. While the\nability to understand personal objects like my wallet facilitates more natural\ninteraction with human users, current LCRG systems only allow generic language\ninstructions, e.g., the black-colored wallet next to the laptop. To this end,\nwe introduce a task scenario GraspMine alongside a novel dataset aimed at\npinpointing and grasping personal objects given personal indicators via\nlearning from a single human-robot interaction, rather than a large labeled\ndataset. Our proposed method, Personalized Grasping Agent (PGA), addresses\nGraspMine by leveraging the unlabeled image data of the user's environment,\ncalled Reminiscence. Specifically, PGA acquires personal object information by\na user presenting a personal object with its associated indicator, followed by\nPGA inspecting the object by rotating it. Based on the acquired information,\nPGA pseudo-labels objects in the Reminiscence by our proposed label propagation\nalgorithm. Harnessing the information acquired from the interactions and the\npseudo-labeled objects in the Reminiscence, PGA adapts the object grounding\nmodel to grasp personal objects. This results in significant efficiency while\nprevious LCRG systems rely on resource-intensive human annotations --\nnecessitating hundreds of labeled data to learn my wallet. Moreover, PGA\noutperforms baseline methods across all metrics and even shows comparable\nperformance compared to the fully-supervised method, which learns from 9k\nannotated data samples. We further validate PGA's real-world applicability by\nemploying a physical robot to execute GrsapMine. Code and data are publicly\navailable at https://github.com/JHKim-snu/PGA.\n","authors":["Junghyun Kim","Gi-Cheon Kang","Jaein Kim","Seoyun Yang","Minjoon Jung","Byoung-Tak Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.12547v2.pdf","comment":"8 pages, under review"},{"id":"http://arxiv.org/abs/2403.12631v1","updated":"2024-03-19T10:59:21Z","published":"2024-03-19T10:59:21Z","title":"PointGrasp: Point Cloud-based Grasping for Tendon-driven Soft Robotic\n  Glove Applications","summary":"  Controlling hand exoskeletons to assist individuals with grasping tasks poses\na challenge due to the difficulty in understanding user intentions. We propose\nthat most daily grasping tasks during activities of daily living (ADL) can be\ndeduced by analyzing object geometries (simple and complex) from 3D point\nclouds. The study introduces PointGrasp, a real-time system designed for\nidentifying household scenes semantically, aiming to support and enhance\nassistance during ADL for tailored end-to-end grasping tasks. The system\ncomprises an RGB-D camera with an inertial measurement unit and a\nmicroprocessor integrated into a tendon-driven soft robotic glove. The RGB-D\ncamera processes 3D scenes at a rate exceeding 30 frames per second. The\nproposed pipeline demonstrates an average RMSE of 0.8 $\\pm$ 0.39 cm for simple\nand 0.11 $\\pm$ 0.06 cm for complex geometries. Within each mode, it identifies\nand pinpoints reachable objects. This system shows promise in end-to-end\nvision-driven robotic-assisted rehabilitation manual tasks.\n","authors":["Chen Hu","Shirui Lyu","Eojin Rho","Daekyum Kim","Shan Luo","Letizia Gionfrida"],"pdf_url":"https://arxiv.org/pdf/2403.12631v1.pdf","comment":"6 pages, 8 figures, conference"},{"id":"http://arxiv.org/abs/2203.09749v2","updated":"2024-03-19T10:56:12Z","published":"2022-03-18T05:17:00Z","title":"Goal-conditioned dual-action imitation learning for dexterous dual-arm\n  robot manipulation","summary":"  Long-horizon dexterous robot manipulation of deformable objects, such as\nbanana peeling, is a problematic task because of the difficulties in object\nmodeling and a lack of knowledge about stable and dexterous manipulation\nskills. This paper presents a goal-conditioned dual-action (GC-DA) deep\nimitation learning (DIL) approach that can learn dexterous manipulation skills\nusing human demonstration data. Previous DIL methods map the current sensory\ninput and reactive action, which often fails because of compounding errors in\nimitation learning caused by the recurrent computation of actions. The method\npredicts reactive action only when the precise manipulation of the target\nobject is required (local action) and generates the entire trajectory when\nprecise manipulation is not required (global action). This dual-action\nformulation effectively prevents compounding error in the imitation learning\nusing the trajectory-based global action while responding to unexpected changes\nin the target object during the reactive local action. The proposed method was\ntested in a real dual-arm robot and successfully accomplished the\nbanana-peeling task.\n","authors":["Heecheol Kim","Yoshiyuki Ohmura","Yasuo Kuniyoshi"],"pdf_url":"https://arxiv.org/pdf/2203.09749v2.pdf","comment":"19 pages, published in Transactions on Robotics (T-RO)"},{"id":"http://arxiv.org/abs/2403.12607v1","updated":"2024-03-19T10:18:03Z","published":"2024-03-19T10:18:03Z","title":"Looking for the Human in HRI Teaching: User-Centered Course Design for\n  Tech-Savvy Students","summary":"  Top-down, user-centered thinking is not typically a strength of all students,\nespecially tech-savvy computer science-related ones. We propose Human-Robot\nInteraction (HRI) introductory courses as a highly suitable opportunity to\nfoster these important skills since the HRI discipline includes a focus on\nhumans as users. Our HRI course therefore contains elements like scenario-based\ndesign of laboratory projects, discussing and merging ideas and other\nself-empowerment techniques. Participants describe, implement and present\neveryday scenarios using Pepper robots and our customized open-source visual\nprogramming tool. We observe that students obtain a good grasp of the taught\ntopics and improve their user-centered thinking skills.\n","authors":["Tobias Doernbach"],"pdf_url":"https://arxiv.org/pdf/2403.12607v1.pdf","comment":"Presented at the Designing an Intro to HRI Course Workshop at HRI\n  2024 (arXiv:2403.05588)"},{"id":"http://arxiv.org/abs/2403.12589v1","updated":"2024-03-19T09:48:18Z","published":"2024-03-19T09:48:18Z","title":"FootstepNet: an Efficient Actor-Critic Method for Fast On-line Bipedal\n  Footstep Planning and Forecasting","summary":"  Designing a humanoid locomotion controller is challenging and classically\nsplit up in sub-problems. Footstep planning is one of those, where the sequence\nof footsteps is defined. Even in simpler environments, finding a minimal\nsequence, or even a feasible sequence, yields a complex optimization problem.\nIn the literature, this problem is usually addressed by search-based algorithms\n(e.g. variants of A*). However, such approaches are either computationally\nexpensive or rely on hand-crafted tuning of several parameters. In this work,\nat first, we propose an efficient footstep planning method to navigate in local\nenvironments with obstacles, based on state-of-the art Deep Reinforcement\nLearning (DRL) techniques, with very low computational requirements for on-line\ninference. Our approach is heuristic-free and relies on a continuous set of\nactions to generate feasible footsteps. In contrast, other methods necessitate\nthe selection of a relevant discrete set of actions. Second, we propose a\nforecasting method, allowing to quickly estimate the number of footsteps\nrequired to reach different candidates of local targets. This approach relies\non inherent computations made by the actor-critic DRL architecture. We\ndemonstrate the validity of our approach with simulation results, and by a\ndeployment on a kid-size humanoid robot during the RoboCup 2023 competition.\n","authors":["Clément Gaspard","Grégoire Passault","Mélodie Daniel","Olivier Ly"],"pdf_url":"https://arxiv.org/pdf/2403.12589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09786v2","updated":"2024-03-19T09:12:31Z","published":"2023-12-15T13:40:34Z","title":"Drones Guiding Drones: Cooperative Navigation of a Less-Equipped Micro\n  Aerial Vehicle in Cluttered Environments","summary":"  Reliable deployment of Unmanned Aerial Vehicles (UAVs) in cluttered unknown\nenvironments requires accurate sensors for Global Navigation Satellite System\n(GNSS)-denied localization and obstacle avoidance. Such a requirement limits\nthe usage of cheap and micro-scale vehicles with constrained payload capacity\nif industrial-grade reliability and precision are required. This paper\ninvestigates the possibility of offloading the necessity to carry heavy sensors\nto another member of the UAV team while preserving the desired capability of\nthe smaller robot intended for exploring narrow passages. A novel cooperative\nguidance framework offloading the sensing requirements from a minimalistic\nsecondary UAV to a superior primary UAV is proposed. The primary UAV constructs\na dense occupancy map of the environment and plans collision-free paths for\nboth UAVs to ensure reaching the desired secondary UAV's goals even in areas\nnot accessible by the bigger robot. The primary UAV guides the secondary UAV to\nfollow the planned path while tracking the UAV using Light Detection and\nRanging (LiDAR)-based relative localization. The proposed approach was verified\nin real-world experiments with a heterogeneous team of a 3D LiDAR-equipped\nprimary UAV and a micro-scale camera-equipped secondary UAV moving autonomously\nthrough unknown cluttered GNSS-denied environments with the proposed framework\nrunning fully on board the UAVs.\n","authors":["Václav Pritzl","Matouš Vrba","Yurii Stasinchuk","Vít Krátký","Jiří Horyna","Petr Štěpán","Martin Saska"],"pdf_url":"https://arxiv.org/pdf/2312.09786v2.pdf","comment":"8 pages, submitted to IROS 2024"},{"id":"http://arxiv.org/abs/2403.12552v1","updated":"2024-03-19T08:54:52Z","published":"2024-03-19T08:54:52Z","title":"M2DA: Multi-Modal Fusion Transformer Incorporating Driver Attention for\n  Autonomous Driving","summary":"  End-to-end autonomous driving has witnessed remarkable progress. However, the\nextensive deployment of autonomous vehicles has yet to be realized, primarily\ndue to 1) inefficient multi-modal environment perception: how to integrate data\nfrom multi-modal sensors more efficiently; 2) non-human-like scene\nunderstanding: how to effectively locate and predict critical risky agents in\ntraffic scenarios like an experienced driver. To overcome these challenges, in\nthis paper, we propose a Multi-Modal fusion transformer incorporating Driver\nAttention (M2DA) for autonomous driving. To better fuse multi-modal data and\nachieve higher alignment between different modalities, a novel\nLidar-Vision-Attention-based Fusion (LVAFusion) module is proposed. By\nincorporating driver attention, we empower the human-like scene understanding\nability to autonomous vehicles to identify crucial areas within complex\nscenarios precisely and ensure safety. We conduct experiments on the CARLA\nsimulator and achieve state-of-the-art performance with less data in\nclosed-loop benchmarks. Source codes are available at\nhttps://anonymous.4open.science/r/M2DA-4772.\n","authors":["Dongyang Xu","Haokun Li","Qingfan Wang","Ziying Song","Lei Chen","Hanming Deng"],"pdf_url":"https://arxiv.org/pdf/2403.12552v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.09007v2","updated":"2024-03-19T08:28:47Z","published":"2023-09-16T14:33:47Z","title":"MonoForce: Self-supervised Learning of Physics-aware Model for\n  Predicting Robot-terrain Interaction","summary":"  While autonomous navigation of mobile robots on rigid terrain is a\nwell-explored problem, navigating on deformable terrain such as tall grass or\nbushes remains a challenge. To address it, we introduce an explainable,\nphysics-aware and end-to-end differentiable model which predicts the outcome of\nrobot-terrain interaction from camera images, both on rigid and non-rigid\nterrain. The proposed MonoForce model consists of a black-box module which\npredicts robot-terrain interaction forces from onboard cameras, followed by a\nwhite-box module, which transforms these forces and a control signals into\npredicted trajectories, using only the laws of classical mechanics. The\ndifferentiable white-box module allows backpropagating the predicted trajectory\nerrors into the black-box module, serving as a self-supervised loss that\nmeasures consistency between the predicted forces and ground-truth trajectories\nof the robot. Experimental evaluation on a public dataset and our data has\nshown that while the prediction capabilities are comparable to state-of-the-art\nalgorithms on rigid terrain, MonoForce shows superior accuracy on non-rigid\nterrain such as tall grass or bushes. To facilitate the reproducibility of our\nresults, we release both the code and datasets.\n","authors":["Ruslan Agishev","Karel Zimmermann","Vladimír Kubelka","Martin Pecka","Tomáš Svoboda"],"pdf_url":"https://arxiv.org/pdf/2309.09007v2.pdf","comment":"8 pages, IROS-2024 submission"},{"id":"http://arxiv.org/abs/2403.12538v1","updated":"2024-03-19T08:25:42Z","published":"2024-03-19T08:25:42Z","title":"Multi-View Active Sensing for Human-Robot Interaction via Hierarchically\n  Connected Tree","summary":"  Comprehensive perception of human beings is the prerequisite to ensure the\nsafety of human-robot interaction. Currently, prevailing visual sensing\napproach typically involves a single static camera, resulting in a restricted\nand occluded field of view. In our work, we develop an active vision system\nusing multiple cameras to dynamically capture multi-source RGB-D data. An\nintegrated human sensing strategy based on a hierarchically connected tree\nstructure is proposed to fuse localized visual information. Constituting the\ntree model are the nodes representing keypoints and the edges representing\nkeyparts, which are consistently interconnected to preserve the structural\nconstraints during multi-source fusion. Utilizing RGB-D data and HRNet, the 3D\npositions of keypoints are analytically estimated, and their presence is\ninferred through a sliding widow of confidence scores. Subsequently, the point\nclouds of reliable keyparts are extracted by drawing occlusion-resistant masks,\nenabling fine registration between data clouds and cylindrical model following\nthe hierarchical order. Experimental results demonstrate that our method\nenhances keypart recognition recall from 69.20% to 90.10%, compared to\nemploying a single static camera. Furthermore, in overcoming challenges related\nto localized and occluded perception, the robotic arm's obstacle avoidance\ncapabilities are effectively improved.\n","authors":["Yuanjiong Ying","Xian Huang","Wei Dong"],"pdf_url":"https://arxiv.org/pdf/2403.12538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12535v1","updated":"2024-03-19T08:19:53Z","published":"2024-03-19T08:19:53Z","title":"High-Fidelity SLAM Using Gaussian Splatting with Rendering-Guided\n  Densification and Regularized Optimization","summary":"  We propose a dense RGBD SLAM system based on 3D Gaussian Splatting that\nprovides metrically accurate pose tracking and visually realistic\nreconstruction. To this end, we first propose a Gaussian densification strategy\nbased on the rendering loss to map unobserved areas and refine reobserved\nareas. Second, we introduce extra regularization parameters to alleviate the\nforgetting problem in the continuous mapping problem, where parameters tend to\noverfit the latest frame and result in decreasing rendering quality for\nprevious frames. Both mapping and tracking are performed with Gaussian\nparameters by minimizing re-rendering loss in a differentiable way. Compared to\nrecent neural and concurrently developed gaussian splatting RGBD SLAM\nbaselines, our method achieves state-of-the-art results on the synthetic\ndataset Replica and competitive results on the real-world dataset TUM.\n","authors":["Shuo Sun","Malcolm Mielle","Achim J. Lilienthal","Martin Magnusson"],"pdf_url":"https://arxiv.org/pdf/2403.12535v1.pdf","comment":"submitted to IROS24"},{"id":"http://arxiv.org/abs/2403.12533v1","updated":"2024-03-19T08:09:44Z","published":"2024-03-19T08:09:44Z","title":"To Help or Not to Help: LLM-based Attentive Support for Human-Robot\n  Group Interactions","summary":"  How can a robot provide unobtrusive physical support within a group of\nhumans? We present Attentive Support, a novel interaction concept for robots to\nsupport a group of humans. It combines scene perception, dialogue acquisition,\nsituation understanding, and behavior generation with the common-sense\nreasoning capabilities of Large Language Models (LLMs). In addition to\nfollowing user instructions, Attentive Support is capable of deciding when and\nhow to support the humans, and when to remain silent to not disturb the group.\nWith a diverse set of scenarios, we show and evaluate the robot's attentive\nbehavior, which supports and helps the humans when required, while not\ndisturbing if no help is needed.\n","authors":["Daniel Tanneberg","Felix Ocker","Stephan Hasler","Joerg Deigmoeller","Anna Belardinelli","Chao Wang","Heiko Wersing","Bernhard Sendhoff","Michael Gienger"],"pdf_url":"https://arxiv.org/pdf/2403.12533v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.12504v1","updated":"2024-03-19T07:11:00Z","published":"2024-03-19T07:11:00Z","title":"TON-VIO: Online Time Offset Modeling Networks for Robust Temporal\n  Alignment in High Dynamic Motion VIO","summary":"  Temporal misalignment (time offset) between sensors is common in low cost\nvisual-inertial odometry (VIO) systems. Such temporal misalignment introduces\ninconsistent constraints for state estimation, leading to a significant\npositioning drift especially in high dynamic motion scenarios. In this article,\nwe focus on online temporal calibration to reduce the positioning drift caused\nby the time offset for high dynamic motion VIO. For the time offset observation\nmodel, most existing methods rely on accurate state estimation or stable visual\ntracking. For the prediction model, current methods oversimplify the time\noffset as a constant value with white Gaussian noise. However, these ideal\nconditions are seldom satisfied in real high dynamic scenarios, resulting in\nthe poor performance. In this paper, we introduce online time offset modeling\nnetworks (TON) to enhance real-time temporal calibration. TON improves the\naccuracy of time offset observation and prediction modeling. Specifically, for\nobservation modeling, we propose feature velocity observation networks to\nenhance velocity computation for features in unstable visual tracking\nconditions. For prediction modeling, we present time offset prediction networks\nto learn its evolution pattern. To highlight the effectiveness of our method,\nwe integrate the proposed TON into both optimization-based and filter-based VIO\nsystems. Simulation and real-world experiments are conducted to demonstrate the\nenhanced performance of our approach. Additionally, to contribute to the VIO\ncommunity, we will open-source the code of our method on:\nhttps://github.com/Franky-X/FVON-TPN.\n","authors":["Chaoran Xiong","Guoqing Liu","Qi Wu","Songpengcheng Xia","Tong Hua","Kehui Ma","Zhen Sun","Yan Xiang","Ling Pei"],"pdf_url":"https://arxiv.org/pdf/2403.12504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12502v1","updated":"2024-03-19T07:10:04Z","published":"2024-03-19T07:10:04Z","title":"Under-actuated Robotic Gripper with Multiple Grasping Modes Inspired by\n  Human Finger","summary":"  Under-actuated robot grippers as a pervasive tool of robots have become a\nconsiderable research focus. Despite their simplicity of mechanical design and\ncontrol strategy, they suffer from poor versatility and weak adaptability,\nmaking widespread applications limited. To better relieve relevant research\ngaps, we present a novel 3-finger linkage-based gripper that realizes\nretractable and reconfigurable multi-mode grasps driven by a single motor.\nFirstly, inspired by the changes that occurred in the contact surface with a\nhuman finger moving, we artfully design a slider-slide rail mechanism as the\nphalanx to achieve retraction of each finger, allowing for better performance\nin the enveloping grasping mode. Secondly, a reconfigurable structure is\nconstructed to broaden the grasping range of objects' dimensions for the\nproposed gripper. By adjusting the configuration and gesture of each finger,\nthe gripper can achieve five grasping modes. Thirdly, the proposed gripper is\njust actuated by a single motor, yet it can be capable of grasping and\nreconfiguring simultaneously. Finally, various experiments on grasps of\nslender, thin, and large-volume objects are implemented to evaluate the\nperformance of the proposed gripper in practical scenarios, which demonstrates\nthe excellent grasping capabilities of the gripper.\n","authors":["Jihao Li","Tingbo Liao","Hassen Nigatu","Haotian Guo","Guodong Lu","Huixu Dong"],"pdf_url":"https://arxiv.org/pdf/2403.12502v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2403.12471v1","updated":"2024-03-19T06:07:01Z","published":"2024-03-19T06:07:01Z","title":"Theoretical Modeling and Bio-inspired Trajectory Optimization of A\n  Multiple-locomotion Origami Robot","summary":"  Recent research on mobile robots has focused on increasing their adaptability\nto unpredictable and unstructured environments using soft materials and\nstructures. However, the determination of key design parameters and control\nover these compliant robots are predominantly iterated through experiments,\nlacking a solid theoretical foundation. To improve their efficiency, this paper\naims to provide mathematics modeling over two locomotion, crawling and\nswimming. Specifically, a dynamic model is first devised to reveal the\ninfluence of the contact surfaces' frictional coefficients on displacements in\ndifferent motion phases. Besides, a swimming kinematics model is provided using\ncoordinate transformation, based on which, we further develop an algorithm that\nsystematically plans human-like swimming gaits, with maximum thrust obtained.\nThe proposed algorithm is highly generalizable and has the potential to be\napplied in other soft robots with multiple joints. Simulation experiments have\nbeen conducted to illustrate the effectiveness of the proposed modeling.\n","authors":["Keqi Zhu","Haotian Guo","Wei Yu","Hassen Nigatu","Tong Li","Huixu Dong"],"pdf_url":"https://arxiv.org/pdf/2403.12471v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2403.12465v1","updated":"2024-03-19T05:46:20Z","published":"2024-03-19T05:46:20Z","title":"Diagrammatic Instructions to Specify Spatial Objectives and Constraints\n  with Applications to Mobile Base Placement","summary":"  This paper introduces Spatial Diagrammatic Instructions (SDIs), an approach\nfor human operators to specify objectives and constraints that are related to\nspatial regions in the working environment. Human operators are enabled to\nsketch out regions directly on camera images that correspond to the objectives\nand constraints. These sketches are projected to 3D spatial coordinates, and\ncontinuous Spatial Instruction Maps (SIMs) are learned upon them. These maps\ncan then be integrated into optimization problems for tasks of robots. In\nparticular, we demonstrate how Spatial Diagrammatic Instructions can be applied\nto solve the Base Placement Problem of mobile manipulators, which concerns the\nbest place to put the manipulator to facilitate a certain task. Human operators\ncan specify, via sketch, spatial regions of interest for a manipulation task\nand permissible regions for the mobile manipulator to be at. Then, an\noptimization problem that maximizes the manipulator's reachability, or\ncoverage, over the designated regions of interest while remaining in the\npermissible regions is solved. We provide extensive empirical evaluations, and\nshow that our formulation of Spatial Instruction Maps provides accurate\nrepresentations of user-specified diagrammatic instructions. Furthermore, we\ndemonstrate that our diagrammatic approach to the Mobile Base Placement Problem\nenables higher quality solutions and faster run-time.\n","authors":["Qilin Sun","Weiming Zhi","Tianyi Zhang","Matthew Johnson-Roberson"],"pdf_url":"https://arxiv.org/pdf/2403.12465v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08460v2","updated":"2024-03-19T05:25:20Z","published":"2024-03-13T12:20:20Z","title":"Towards Dense and Accurate Radar Perception Via Efficient Cross-Modal\n  Diffusion Model","summary":"  Millimeter wave (mmWave) radars have attracted significant attention from\nboth academia and industry due to their capability to operate in extreme\nweather conditions. However, they face challenges in terms of sparsity and\nnoise interference, which hinder their application in the field of micro aerial\nvehicle (MAV) autonomous navigation. To this end, this paper proposes a novel\napproach to dense and accurate mmWave radar point cloud construction via\ncross-modal learning. Specifically, we introduce diffusion models, which\npossess state-of-the-art performance in generative modeling, to predict\nLiDAR-like point clouds from paired raw radar data. We also incorporate the\nmost recent diffusion model inference accelerating techniques to ensure that\nthe proposed method can be implemented on MAVs with limited computing\nresources.We validate the proposed method through extensive benchmark\ncomparisons and real-world experiments, demonstrating its superior performance\nand generalization ability. Code and pretrained models will be available at\nhttps://github.com/ZJU-FAST-Lab/Radar-Diffusion.\n","authors":["Ruibin Zhang","Donglai Xue","Yuhan Wang","Ruixu Geng","Fei Gao"],"pdf_url":"https://arxiv.org/pdf/2403.08460v2.pdf","comment":"8 pages, 6 figures, submitted to RA-L"},{"id":"http://arxiv.org/abs/2403.12449v1","updated":"2024-03-19T05:18:47Z","published":"2024-03-19T05:18:47Z","title":"Multi-Object RANSAC: Efficient Plane Clustering Method in a Clutter","summary":"  In this paper, we propose a novel method for plane clustering specialized in\ncluttered scenes using an RGB-D camera and validate its effectiveness through\nrobot grasping experiments. Unlike existing methods, which focus on large-scale\nindoor structures, our approach -- Multi-Object RANSAC emphasizes cluttered\nenvironments that contain a wide range of objects with different scales. It\nenhances plane segmentation by generating subplanes in Deep Plane Clustering\n(DPC) module, which are then merged with the final planes by post-processing.\nDPC rearranges the point cloud by voting layers to make subplane clusters,\ntrained in a self-supervised manner using pseudo-labels generated from RANSAC.\nMulti-Object RANSAC demonstrates superior plane instance segmentation\nperformances over other recent RANSAC applications. We conducted an experiment\non robot suction-based grasping, comparing our method with vision-based\ngrasping network and RANSAC applications. The results from this real-world\nscenario showed its remarkable performance surpassing the baseline methods,\nhighlighting its potential for advanced scene understanding and manipulation.\n","authors":["Seunghyeon Lim","Youngjae Yoo","Jun Ki Lee","Byoung-Tak Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.12449v1.pdf","comment":"7 pages, 6 figures"},{"id":"http://arxiv.org/abs/2309.14590v3","updated":"2024-03-19T04:21:24Z","published":"2023-09-26T00:45:04Z","title":"HeLiPR: Heterogeneous LiDAR Dataset for inter-LiDAR Place Recognition\n  under Spatiotemporal Variations","summary":"  Place recognition is crucial for robot localization and loop closure in\nsimultaneous localization and mapping (SLAM). Light Detection and Ranging\n(LiDAR), known for its robust sensing capabilities and measurement consistency\neven in varying illumination conditions, has become pivotal in various fields,\nsurpassing traditional imaging sensors in certain applications. Among various\ntypes of LiDAR, spinning LiDARs are widely used, while non-repetitive scanning\npatterns have recently been utilized in robotics applications. Some LiDARs\nprovide additional measurements such as reflectivity, Near Infrared (NIR), and\nvelocity from Frequency modulated continuous wave (FMCW) LiDARs. Despite these\nadvances, there is a lack of comprehensive datasets reflecting the broad\nspectrum of LiDAR configurations for place recognition. To tackle this issue,\nour paper proposes the HeLiPR dataset, curated especially for place recognition\nwith heterogeneous LiDARs, embodying spatiotemporal variations. To the best of\nour knowledge, the HeLiPR dataset is the first heterogeneous LiDAR dataset\nsupporting inter-LiDAR place recognition with both non-repetitive and spinning\nLiDARs, accommodating different field of view (FOV)s and varying numbers of\nrays. The dataset covers diverse environments, from urban cityscapes to\nhigh-dynamic freeways, over a month, enhancing adaptability and robustness\nacross scenarios. Notably, HeLiPR includes trajectories parallel to MulRan\nsequences, making it valuable for research in heterogeneous LiDAR place\nrecognition and long-term studies. The dataset is accessible at\nhttps://sites.google.com/view/heliprdataset.\n","authors":["Minwoo Jung","Wooseong Yang","Dongjae Lee","Hyeonjae Gil","Giseop Kim","Ayoung Kim"],"pdf_url":"https://arxiv.org/pdf/2309.14590v3.pdf","comment":"11 pages, 9 figures, 5 tables"},{"id":"http://arxiv.org/abs/2403.12421v1","updated":"2024-03-19T04:08:18Z","published":"2024-03-19T04:08:18Z","title":"UniDexFPM: Universal Dexterous Functional Pre-grasp Manipulation Via\n  Diffusion Policy","summary":"  Objects in the real world are often not naturally positioned for functional\ngrasping, which usually requires repositioning and reorientation before they\ncan be grasped, a process known as pre-grasp manipulation. However, effective\nlearning of universal dexterous functional pre-grasp manipulation necessitates\nprecise control over relative position, relative orientation, and contact\nbetween the hand and object, while generalizing to diverse dynamic scenarios\nwith varying objects and goal poses. We address the challenge by using\nteacher-student learning. We propose a novel mutual reward that incentivizes\nagents to jointly optimize three key criteria. Furthermore, we introduce a\npipeline that leverages a mixture-of-experts strategy to learn diverse\nmanipulation policies, followed by a diffusion policy to capture complex action\ndistributions from these experts. Our method achieves a success rate of 72.6%\nacross 30+ object categories encompassing 1400+ objects and 10k+ goal poses.\nNotably, our method relies solely on object pose information for universal\ndexterous functional pre-grasp manipulation by using extrinsic dexterity and\nadjusting from feedback. Additional experiments under noisy object pose\nobservation showcase the robustness of our method and its potential for\nreal-world applications. The demonstrations can be viewed at\nhttps://unidexfpm.github.io.\n","authors":["Tianhao Wu","Yunchong Gan","Mingdong Wu","Jingbo Cheng","Yaodong Yang","Yixin Zhu","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2403.12421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12420v1","updated":"2024-03-19T04:07:45Z","published":"2024-03-19T04:07:45Z","title":"Bin Packing Optimization via Deep Reinforcement Learning","summary":"  The Bin Packing Problem (BPP) has attracted enthusiastic research interest\nrecently, owing to widespread applications in logistics and warehousing\nenvironments. It is truly essential to optimize the bin packing to enable more\nobjects to be packed into boxes. Object packing order and placement strategy\nare the two crucial optimization objectives of the BPP. However, existing\noptimization methods for BPP, such as the genetic algorithm (GA), emerge as the\nmain issues in highly computational cost and relatively low accuracy, making it\ndifficult to implement in realistic scenarios. To well relieve the research\ngaps, we present a novel optimization methodology of two-dimensional (2D)-BPP\nand three-dimensional (3D)-BPP for objects with regular shapes via deep\nreinforcement learning (DRL), maximizing the space utilization and minimizing\nthe usage number of boxes. First, an end-to-end DRL neural network constructed\nby a modified Pointer Network consisting of an encoder, a decoder and an\nattention module is proposed to achieve the optimal object packing order.\nSecond, conforming to the top-down operation mode, the placement strategy based\non a height map is used to arrange the ordered objects in the boxes, preventing\nthe objects from colliding with boxes and other objects in boxes. Third, the\nreward and loss functions are defined as the indicators of the compactness,\npyramid, and usage number of boxes to conduct the training of the DRL neural\nnetwork based on an on-policy actor-critic framework. Finally, a series of\nexperiments are implemented to compare our method with conventional packing\nmethods, from which we conclude that our method outperforms these packing\nmethods in both packing accuracy and efficiency.\n","authors":["Baoying Wang","Huixu Dong"],"pdf_url":"https://arxiv.org/pdf/2403.12420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.09297v2","updated":"2024-03-19T03:25:50Z","published":"2023-09-17T15:14:01Z","title":"Chasing Day and Night: Towards Robust and Efficient All-Day Object\n  Detection Guided by an Event Camera","summary":"  The ability to detect objects in all lighting (i.e., normal-, over-, and\nunder-exposed) conditions is crucial for real-world applications, such as\nself-driving.Traditional RGB-based detectors often fail under such varying\nlighting conditions.Therefore, recent works utilize novel event cameras to\nsupplement or guide the RGB modality; however, these methods typically adopt\nasymmetric network structures that rely predominantly on the RGB modality,\nresulting in limited robustness for all-day detection. In this paper, we\npropose EOLO, a novel object detection framework that achieves robust and\nefficient all-day detection by fusing both RGB and event modalities. Our EOLO\nframework is built based on a lightweight spiking neural network (SNN) to\nefficiently leverage the asynchronous property of events. Buttressed by it, we\nfirst introduce an Event Temporal Attention (ETA) module to learn the high\ntemporal information from events while preserving crucial edge information.\nSecondly, as different modalities exhibit varying levels of importance under\ndiverse lighting conditions, we propose a novel Symmetric RGB-Event Fusion\n(SREF) module to effectively fuse RGB-Event features without relying on a\nspecific modality, thus ensuring a balanced and adaptive fusion for all-day\ndetection. In addition, to compensate for the lack of paired RGB-Event datasets\nfor all-day training and evaluation, we propose an event synthesis approach\nbased on the randomized optical flow that allows for directly generating the\nevent frame from a single exposure image. We further build two new datasets,\nE-MSCOCO and E-VOC based on the popular benchmarks MSCOCO and PASCAL VOC.\nExtensive experiments demonstrate that our EOLO outperforms the\nstate-of-the-art detectors,e.g.,RENet,by a substantial margin (+3.74% mAP50) in\nall lighting conditions.Our code and datasets will be available at\nhttps://vlislab22.github.io/EOLO/\n","authors":["Jiahang Cao","Xu Zheng","Yuanhuiyi Lyu","Jiaxu Wang","Renjing Xu","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2309.09297v2.pdf","comment":"Accepted by ICRA 2024"},{"id":"http://arxiv.org/abs/2403.12396v1","updated":"2024-03-19T03:09:24Z","published":"2024-03-19T03:09:24Z","title":"OV9D: Open-Vocabulary Category-Level 9D Object Pose and Size Estimation","summary":"  This paper studies a new open-set problem, the open-vocabulary category-level\nobject pose and size estimation. Given human text descriptions of arbitrary\nnovel object categories, the robot agent seeks to predict the position,\norientation, and size of the target object in the observed scene image. To\nenable such generalizability, we first introduce OO3D-9D, a large-scale\nphotorealistic dataset for this task. Derived from OmniObject3D, OO3D-9D is the\nlargest and most diverse dataset in the field of category-level object pose and\nsize estimation. It includes additional annotations for the symmetry axis of\neach category, which help resolve symmetric ambiguity. Apart from the\nlarge-scale dataset, we find another key to enabling such generalizability is\nleveraging the strong prior knowledge in pre-trained visual-language foundation\nmodels. We then propose a framework built on pre-trained DinoV2 and\ntext-to-image stable diffusion models to infer the normalized object coordinate\nspace (NOCS) maps of the target instances. This framework fully leverages the\nvisual semantic prior from DinoV2 and the aligned visual and language knowledge\nwithin the text-to-image diffusion model, which enables generalization to\nvarious text descriptions of novel categories. Comprehensive quantitative and\nqualitative experiments demonstrate that the proposed open-vocabulary method,\ntrained on our large-scale synthesized data, significantly outperforms the\nbaseline and can effectively generalize to real-world images of unseen\ncategories. The project page is at https://ov9d.github.io.\n","authors":["Junhao Cai","Yisheng He","Weihao Yuan","Siyu Zhu","Zilong Dong","Liefeng Bo","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2403.12396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10554v2","updated":"2024-03-19T03:05:53Z","published":"2024-03-13T04:40:53Z","title":"Safe Planning through Incremental Decomposition of Signal Temporal Logic\n  Specifications","summary":"  Trajectory planning is a critical process that enables autonomous systems to\nsafely navigate complex environments. Signal temporal logic (STL)\nspecifications are an effective way to encode complex temporally extended\nobjectives for trajectory planning in cyber-physical systems (CPS). However,\nplanning from these specifications using existing techniques scale\nexponentially with the number of nested operators and the horizon of\nspecification. Additionally, performance is exacerbated at runtime due to\nlimited computational budgets and compounding modeling errors. Decomposing a\ncomplex specification into smaller subtasks and incrementally planning for them\ncan remedy these issues. In this work, we present a way to decompose STL\nrequirements temporally to improve planning efficiency and performance. The key\ninsight in our work is to encode all specifications as a set of reachability\nand invariance constraints and scheduling these constraints sequentially at\nruntime. Our proposed technique outperforms the state-of-the-art trajectory\nsynthesis techniques for both linear and non linear dynamical systems.\n","authors":["Parv Kapoor","Eunsuk Kang","Romulo Meira-Goes"],"pdf_url":"https://arxiv.org/pdf/2403.10554v2.pdf","comment":"Accepted to Nasa Formal Methods (NFM) 2024"},{"id":"http://arxiv.org/abs/2403.11852v2","updated":"2024-03-19T02:46:55Z","published":"2024-03-18T15:02:46Z","title":"Reinforcement Learning with Latent State Inference for Autonomous\n  On-ramp Merging under Observation Delay","summary":"  This paper presents a novel approach to address the challenging problem of\nautonomous on-ramp merging, where a self-driving vehicle needs to seamlessly\nintegrate into a flow of vehicles on a multi-lane highway. We introduce the\nLane-keeping, Lane-changing with Latent-state Inference and Safety Controller\n(L3IS) agent, designed to perform the on-ramp merging task safely without\ncomprehensive knowledge about surrounding vehicles' intents or driving styles.\nWe also present an augmentation of this agent called AL3IS that accounts for\nobservation delays, allowing the agent to make more robust decisions in\nreal-world environments with vehicle-to-vehicle (V2V) communication delays. By\nmodeling the unobservable aspects of the environment through latent states,\nsuch as other drivers' intents, our approach enhances the agent's ability to\nadapt to dynamic traffic conditions, optimize merging maneuvers, and ensure\nsafe interactions with other vehicles. We demonstrate the effectiveness of our\nmethod through extensive simulations generated from real traffic data and\ncompare its performance with existing approaches. L3IS shows a 99.90% success\nrate in a challenging on-ramp merging case generated from the real US Highway\n101 data. We further perform a sensitivity analysis on AL3IS to evaluate its\nrobustness against varying observation delays, which demonstrates an acceptable\nperformance of 93.84% success rate in 1-second V2V communication delay.\n","authors":["Amin Tabrizian","Zhitong Huang","Peng Wei"],"pdf_url":"https://arxiv.org/pdf/2403.11852v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12377v1","updated":"2024-03-19T02:40:51Z","published":"2024-03-19T02:40:51Z","title":"Online Multi-Agent Pickup and Delivery with Task Deadlines","summary":"  Managing delivery deadlines in automated warehouses and factories is crucial\nfor maintaining customer satisfaction and ensuring seamless production. This\nstudy introduces the problem of online multi-agent pickup and delivery with\ntask deadlines (MAPD-D), which is an advanced variant of the online MAPD\nproblem incorporating delivery deadlines. MAPD-D presents a dynamic\ndeadline-driven approach that includes task deadlines, with tasks being added\nat any time (online), thus challenging conventional MAPD frameworks. To tackle\nMAPD-D, we propose a novel algorithm named deadline-aware token passing (D-TP).\nThe D-TP algorithm is designed to calculate pickup deadlines and assign tasks\nwhile balancing execution cost and deadline proximity. Additionally, we\nintroduce the D-TP with task swaps (D-TPTS) method to further reduce task\ntardiness, enhancing flexibility and efficiency via task-swapping strategies.\nNumerical experiments were conducted in simulated warehouse environments to\nshowcase the effectiveness of the proposed methods. Both D-TP and D-TPTS\ndemonstrate significant reductions in task tardiness compared to existing\nmethods, thereby contributing to efficient operations in automated warehouses\nand factories with delivery deadlines.\n","authors":["Hiroya Makino","Seigo Ito"],"pdf_url":"https://arxiv.org/pdf/2403.12377v1.pdf","comment":"6 pages, 2 figures, submitted to IROS 2024"},{"id":"http://arxiv.org/abs/2310.07968v3","updated":"2024-03-19T01:32:19Z","published":"2023-10-12T01:17:56Z","title":"Think, Act, and Ask: Open-World Interactive Personalized Robot\n  Navigation","summary":"  Zero-Shot Object Navigation (ZSON) enables agents to navigate towards\nopen-vocabulary objects in unknown environments. The existing works of ZSON\nmainly focus on following individual instructions to find generic object\nclasses, neglecting the utilization of natural language interaction and the\ncomplexities of identifying user-specific objects. To address these\nlimitations, we introduce Zero-shot Interactive Personalized Object Navigation\n(ZIPON), where robots need to navigate to personalized goal objects while\nengaging in conversations with users. To solve ZIPON, we propose a new\nframework termed Open-woRld Interactive persOnalized Navigation (ORION), which\nuses Large Language Models (LLMs) to make sequential decisions to manipulate\ndifferent modules for perception, navigation and communication. Experimental\nresults show that the performance of interactive agents that can leverage user\nfeedback exhibits significant improvement. However, obtaining a good balance\nbetween task completion and the efficiency of navigation and interaction\nremains challenging for all methods. We further provide more findings on the\nimpact of diverse user feedback forms on the agents' performance.\n","authors":["Yinpei Dai","Run Peng","Sikai Li","Joyce Chai"],"pdf_url":"https://arxiv.org/pdf/2310.07968v3.pdf","comment":"Video URL: https://www.youtube.com/watch?v=rN5S8QIhhQc Code URL:\n  https://github.com/sled-group/navchat"},{"id":"http://arxiv.org/abs/2305.19157v2","updated":"2024-03-19T01:09:34Z","published":"2023-05-30T15:58:56Z","title":"Sensor Fault Detection and Compensation with Performance Prescription\n  for Robotic Manipulators","summary":"  This paper focuses on sensor fault detection and compensation for robotic\nmanipulators. The proposed method features a new adaptive observer and a new\nterminal sliding mode control law established on a second-order integral\nsliding surface. The method enables sensor fault detection without the need to\nknow the bounds on fault value and/or its derivative. It also enables fast and\nfixed-time fault-tolerant control whose performance can be prescribed\nbeforehand by defining funnel bounds on the tracking error. The ultimate\nboundedness of the estimation errors for the proposed observer and the\nfixed-time stability of the control system are shown using Lyapunov stability\nanalysis. The effectiveness of the proposed method is verified using numerical\nsimulations on two different robotic manipulators, and the results are compared\nwith existing methods. Our results demonstrate performance gains obtained by\nthe proposed method compared to the existing results.\n","authors":["S. Mohammadreza Ebrahimi","Farid Norouzi","Hossein Dastres","Reza Faieghi","Mehdi Naderi","Milad Malekzadeh"],"pdf_url":"https://arxiv.org/pdf/2305.19157v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.09509v2","updated":"2024-03-19T00:05:21Z","published":"2023-06-15T21:06:54Z","title":"Granger-Causal Hierarchical Skill Discovery","summary":"  Reinforcement Learning (RL) has demonstrated promising results in learning\npolicies for complex tasks, but it often suffers from low sample efficiency and\nlimited transferability. Hierarchical RL (HRL) methods aim to address the\ndifficulty of learning long-horizon tasks by decomposing policies into skills,\nabstracting states, and reusing skills in new tasks. However, many HRL methods\nrequire some initial task success to discover useful skills, which\nparadoxically may be very unlikely without access to useful skills. On the\nother hand, reward-free HRL methods often need to learn far too many skills to\nachieve proper coverage in high-dimensional domains. In contrast, we introduce\nthe Chain of Interaction Skills (COInS) algorithm, which focuses on\ncontrollability in factored domains to identify a small number of task-agnostic\nskills that still permit a high degree of control. COInS uses learned detectors\nto identify interactions between state factors and then trains a chain of\nskills to control each of these factors successively. We evaluate COInS on a\nrobotic pushing task with obstacles-a challenging domain where other RL and HRL\nmethods fall short. We also demonstrate the transferability of skills learned\nby COInS, using variants of Breakout, a common RL benchmark, and show 2-3x\nimprovement in both sample efficiency and final performance compared to\nstandard RL baselines.\n","authors":["Caleb Chuck","Kevin Black","Aditya Arjun","Yuke Zhu","Scott Niekum"],"pdf_url":"https://arxiv.org/pdf/2306.09509v2.pdf","comment":"Accepted TMLR 2024"},{"id":"http://arxiv.org/abs/2403.11788v2","updated":"2024-03-19T18:20:59Z","published":"2024-03-18T13:46:32Z","title":"Locomotion Generation for a Rat Robot based on Environmental Changes via\n  Reinforcement Learning","summary":"  This research focuses on developing reinforcement learning approaches for the\nlocomotion generation of small-size quadruped robots. The rat robot NeRmo is\nemployed as the experimental platform. Due to the constrained volume,\nsmall-size quadruped robots typically possess fewer and weaker sensors,\nresulting in difficulty in accurately perceiving and responding to\nenvironmental changes. In this context, insufficient and imprecise feedback\ndata from sensors makes it difficult to generate adaptive locomotion based on\nreinforcement learning. To overcome these challenges, this paper proposes a\nnovel reinforcement learning approach that focuses on extracting effective\nperceptual information to enhance the environmental adaptability of small-size\nquadruped robots. According to the frequency of a robot's gait stride, key\ninformation of sensor data is analyzed utilizing sinusoidal functions derived\nfrom Fourier transform results. Additionally, a multifunctional reward\nmechanism is proposed to generate adaptive locomotion in different tasks.\nExtensive simulations are conducted to assess the effectiveness of the proposed\nreinforcement learning approach in generating rat robot locomotion in various\nenvironments. The experiment results illustrate the capability of the proposed\napproach to maintain stable locomotion of a rat robot across different\nterrains, including ramps, stairs, and spiral stairs.\n","authors":["Xinhui Shan","Yuhong Huang","Zhenshan Bing","Zitao Zhang","Xiangtong Yao","Kai Huang","Alois Knoll"],"pdf_url":"https://arxiv.org/pdf/2403.11788v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13208v1","updated":"2024-03-19T23:57:55Z","published":"2024-03-19T23:57:55Z","title":"CaDRE: Controllable and Diverse Generation of Safety-Critical Driving\n  Scenarios using Real-World Trajectories","summary":"  Simulation is an indispensable tool in the development and testing of\nautonomous vehicles (AVs), offering an efficient and safe alternative to road\ntesting by allowing the exploration of a wide range of scenarios. Despite its\nadvantages, a significant challenge within simulation-based testing is the\ngeneration of safety-critical scenarios, which are essential to ensure that AVs\ncan handle rare but potentially fatal situations. This paper addresses this\nchallenge by introducing a novel generative framework, CaDRE, which is\nspecifically designed for generating diverse and controllable safety-critical\nscenarios using real-world trajectories. Our approach optimizes for both the\nquality and diversity of scenarios by employing a unique formulation and\nalgorithm that integrates real-world data, domain knowledge, and black-box\noptimization techniques. We validate the effectiveness of our framework through\nextensive testing in three representative types of traffic scenarios. The\nresults demonstrate superior performance in generating diverse and high-quality\nscenarios with greater sample efficiency than existing reinforcement learning\nand sampling-based methods.\n","authors":["Peide Huang","Wenhao Ding","Jonathan Francis","Bingqing Chen","Ding Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.13208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13198v1","updated":"2024-03-19T23:18:40Z","published":"2024-03-19T23:18:40Z","title":"Towards Robots That Know When They Need Help: Affordance-Based\n  Uncertainty for Large Language Model Planners","summary":"  Large language models (LLMs) showcase many desirable traits for intelligent\nand helpful robots. However, they are also known to hallucinate predictions.\nThis issue is exacerbated in consumer robotics where LLM hallucinations may\nresult in robots confidently executing plans that are contrary to user goals,\nrelying more frequently on human assistance, or preventing the robot from\nasking for help at all. In this work, we present LAP, a novel approach for\nutilizing off-the-shelf LLM's, alongside scene and object Affordances, in\nrobotic Planners that minimize harmful hallucinations and know when to ask for\nhelp. Our key finding is that calculating and leveraging a scene affordance\nscore, a measure of whether a given action is possible in the provided scene,\nhelps to mitigate hallucinations in LLM predictions and better align the LLM's\nconfidence measure with the probability of success. We specifically propose and\ntest three different affordance scores, which can be used independently or in\ntandem to improve performance across different use cases. The most successful\nof these individual scores involves prompting an LLM to determine if a given\naction is possible and safe in the given scene and uses the LLM's response to\ncompute the score. Through experiments in both simulation and the real world,\non tasks with a variety of ambiguities, we show that LAP significantly\nincreases success rate and decreases the amount of human intervention required\nrelative to prior art. For example, in our real-world testing paradigm, LAP\ndecreases the human help rate of previous methods by over 33% at a success rate\nof 70%.\n","authors":["James F. Mullen Jr.","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2403.13198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13188v1","updated":"2024-03-19T22:57:03Z","published":"2024-03-19T22:57:03Z","title":"Reflectivity Is All You Need!: Advancing LiDAR Semantic Segmentation","summary":"  LiDAR semantic segmentation frameworks predominantly leverage geometry-based\nfeatures to differentiate objects within a scan. While these methods excel in\nscenarios with clear boundaries and distinct shapes, their performance declines\nin environments where boundaries are blurred, particularly in off-road\ncontexts. To address this, recent strides in 3D segmentation algorithms have\nfocused on harnessing raw LiDAR intensity measurements to improve prediction\naccuracy. Despite these efforts, current learning-based models struggle to\ncorrelate the intricate connections between raw intensity and factors such as\ndistance, incidence angle, material reflectivity, and atmospheric conditions.\nBuilding upon our prior work, this paper delves into the advantages of\nemploying calibrated intensity (also referred to as reflectivity) within\nlearning-based LiDAR semantic segmentation frameworks. We initially establish\nthat incorporating reflectivity as an input enhances the existing LiDAR\nsemantic segmentation model. Furthermore, we present findings that enable the\nmodel to learn to calibrate intensity can boost its performance. Through\nextensive experimentation on the off-road dataset Rellis-3D, we demonstrate\nnotable improvements. Specifically, converting intensity to reflectivity\nresults in a 4% increase in mean Intersection over Union (mIoU) when compared\nto using raw intensity in Off-road scenarios. Additionally, we also investigate\nthe possible benefits of using calibrated intensity in semantic segmentation in\nurban environments (SemanticKITTI) and cross-sensor domain adaptation.\n","authors":["Kasi Viswanath","Peng Jiang","Srikanth Saripalli"],"pdf_url":"https://arxiv.org/pdf/2403.13188v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13177v1","updated":"2024-03-19T22:06:37Z","published":"2024-03-19T22:06:37Z","title":"User-customizable Shared Control for Fine Teleoperation via Virtual\n  Reality","summary":"  Shared control can ease and enhance a human operator's ability to teleoperate\nrobots, particularly for intricate tasks demanding fine control over multiple\ndegrees of freedom. However, the arbitration process dictating how much\nautonomous assistance to administer in shared control can confuse novice\noperators and impede their understanding of the robot's behavior. To overcome\nthese adverse side-effects, we propose a novel formulation of shared control\nthat enables operators to tailor the arbitration to their unique capabilities\nand preferences. Unlike prior approaches to customizable shared control where\nusers could indirectly modify the latent parameters of the arbitration function\nby issuing a feedback command, we instead make these parameters observable and\ndirectly editable via a virtual reality (VR) interface. We present our\nuser-customizable shared control method for a teleoperation task in SE(3),\nknown as the buzz wire game. A user study is conducted with participants\nteleoperating a robotic arm in VR to complete the game. The experiment spanned\ntwo weeks per subject to investigate longitudinal trends. Our findings reveal\nthat users allowed to interactively tune the arbitration parameters across\ntrials generalize well to adaptations in the task, exhibiting improvements in\nprecision and fluency over direct teleoperation and conventional shared\ncontrol.\n","authors":["Rui Luo","Mark Zolotas","Drake Moore","Taskin Padir"],"pdf_url":"https://arxiv.org/pdf/2403.13177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13170v1","updated":"2024-03-19T21:49:26Z","published":"2024-03-19T21:49:26Z","title":"On Designing Consistent Covariance Recovery from a Deep Learning Visual\n  Odometry Engine","summary":"  Deep learning techniques have significantly advanced in providing accurate\nvisual odometry solutions by leveraging large datasets. However, generating\nuncertainty estimates for these methods remains a challenge. Traditional sensor\nfusion approaches in a Bayesian framework are well-established, but deep\nlearning techniques with millions of parameters lack efficient methods for\nuncertainty estimation.\n  This paper addresses the issue of uncertainty estimation for pre-trained\ndeep-learning models in monocular visual odometry. We propose formulating a\nfactor graph on an implicit layer of the deep learning network to recover\nrelative covariance estimates, which allows us to determine the covariance of\nthe Visual Odometry (VO) solution. We showcase the consistency of the deep\nlearning engine's covariance approximation with an empirical analysis of the\ncovariance model on the EUROC datasets to demonstrate the correctness of our\nformulation.\n","authors":["Jagatpreet Singh Nir","Dennis Giaya","Hanumant Singh"],"pdf_url":"https://arxiv.org/pdf/2403.13170v1.pdf","comment":"Submitted to IROS 2024"},{"id":"http://arxiv.org/abs/2403.13147v1","updated":"2024-03-19T20:50:20Z","published":"2024-03-19T20:50:20Z","title":"Meta-Learning for Fast Adaptation in Intent Inferral on a Robotic Hand\n  Orthosis for Stroke","summary":"  We propose MetaEMG, a meta-learning approach for fast adaptation in intent\ninferral on a robotic hand orthosis for stroke. One key challenge in machine\nlearning for assistive and rehabilitative robotics with disabled-bodied\nsubjects is the difficulty of collecting labeled training data. Muscle tone and\nspasticity often vary significantly among stroke subjects, and hand function\ncan even change across different use sessions of the device for the same\nsubject. We investigate the use of meta-learning to mitigate the burden of data\ncollection needed to adapt high-capacity neural networks to a new session or\nsubject. Our experiments on real clinical data collected from five stroke\nsubjects show that MetaEMG can improve the intent inferral accuracy with a\nsmall session- or subject-specific dataset and very few fine-tuning epochs. To\nthe best of our knowledge, we are the first to formulate intent inferral on\nstroke subjects as a meta-learning problem and demonstrate fast adaptation to a\nnew session or subject for controlling a robotic hand orthosis with EMG\nsignals.\n","authors":["Pedro Leandro La Rotta","Jingxi Xu","Ava Chen","Lauren Winterbottom","Wenxi Chen","Dawn Nilsen","Joel Stein","Matei Ciocarlie"],"pdf_url":"https://arxiv.org/pdf/2403.13147v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13144v1","updated":"2024-03-19T20:41:06Z","published":"2024-03-19T20:41:06Z","title":"Interactive Robot-Environment Self-Calibration via Compliant Exploratory\n  Actions","summary":"  Calibrating robots into their workspaces is crucial for manipulation tasks.\nExisting calibration techniques often rely on sensors external to the robot\n(cameras, laser scanners, etc.) or specialized tools. This reliance complicates\nthe calibration process and increases the costs and time requirements.\nFurthermore, the associated setup and measurement procedures require\nsignificant human intervention, which makes them more challenging to operate.\nUsing the built-in force-torque sensors, which are nowadays a default component\nin collaborative robots, this work proposes a self-calibration framework where\nrobot-environmental spatial relations are automatically estimated through\ncompliant exploratory actions by the robot itself. The self-calibration\napproach converges, verifies its own accuracy, and terminates upon completion,\nautonomously purely through interactive exploration of the environment's\ngeometries. Extensive experiments validate the effectiveness of our\nself-calibration approach in accurately establishing the robot-environment\nspatial relationships without the need for additional sensing equipment or any\nhuman intervention.\n","authors":["Podshara Chanrungmaneekul","Kejia Ren","Joshua T. Grace","Aaron M. Dollar","Kaiyu Hang"],"pdf_url":"https://arxiv.org/pdf/2403.13144v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13132v1","updated":"2024-03-19T20:04:35Z","published":"2024-03-19T20:04:35Z","title":"Wearable Roller Rings to Enable Robot Dexterous In-Hand Manipulation\n  through Active Surfaces","summary":"  In-hand manipulation is a crucial ability for reorienting and repositioning\nobjects within grasps. The main challenges are not only the complexity in the\ncomputational models, but also the risks of grasp instability caused by active\nfinger motions, such as rolling, sliding, breaking, and remaking contacts.\nBased on the idea of manipulation without lifting a finger, this paper presents\nthe development of Roller Rings (RR), a modular robotic attachment with active\nsurfaces that is wearable by both robot and human hands. By installing and\nangling the RRs on grasping systems, such that their spatial motions are not\nco-linear, we derive a general differential motion model for the object\nactuated by the active surfaces. Our motion model shows that complete in-hand\nmanipulation skill sets can be provided by as few as only 2 RRs through\nnon-holonomic object motions, while more RRs can enable enhanced manipulation\ndexterity with fewer motion constraints. Through extensive experiments, we wear\nRRs on both a robot hand and a human hand to evaluate their manipulation\ncapabilities, and show that the RRs can be employed to manipulate arbitrary\nobject shapes to provide dexterous in-hand manipulation.\n","authors":["Hayden Webb","Podshara Chanrungmaneekul","Shenli Yuan","Kaiyu Hang"],"pdf_url":"https://arxiv.org/pdf/2403.13132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13129v1","updated":"2024-03-19T19:58:54Z","published":"2024-03-19T19:58:54Z","title":"Better Call SAL: Towards Learning to Segment Anything in Lidar","summary":"  We propose $\\texttt{SAL}$ ($\\texttt{S}$egment $\\texttt{A}$nything in\n$\\texttt{L}$idar) method consisting of a text-promptable zero-shot model for\nsegmenting and classifying any object in Lidar, and a pseudo-labeling engine\nthat facilitates model training without manual supervision. While the\nestablished paradigm for $\\textit{Lidar Panoptic Segmentation}$ (LPS) relies on\nmanual supervision for a handful of object classes defined a priori, we utilize\n2D vision foundation models to generate 3D supervision \"for free\". Our\npseudo-labels consist of instance masks and corresponding CLIP tokens, which we\nlift to Lidar using calibrated multi-modal data. By training our model on these\nlabels, we distill the 2D foundation models into our Lidar $\\texttt{SAL}$\nmodel. Even without manual labels, our model achieves $91\\%$ in terms of\nclass-agnostic segmentation and $44\\%$ in terms of zero-shot LPS of the fully\nsupervised state-of-the-art. Furthermore, we outperform several baselines that\ndo not distill but only lift image features to 3D. More importantly, we\ndemonstrate that $\\texttt{SAL}$ supports arbitrary class prompts, can be easily\nextended to new datasets, and shows significant potential to improve with\nincreasing amounts of self-labeled data.\n","authors":["Aljoša Ošep","Tim Meinhardt","Francesco Ferroni","Neehar Peri","Deva Ramanan","Laura Leal-Taixé"],"pdf_url":"https://arxiv.org/pdf/2403.13129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13124v1","updated":"2024-03-19T19:54:32Z","published":"2024-03-19T19:54:32Z","title":"Cooperative Modular Manipulation with Numerous Cable-Driven Robots for\n  Assistive Construction and Gap Crossing","summary":"  Soldiers in the field often need to cross negative obstacles, such as rivers\nor canyons, to reach goals or safety. Military gap crossing involves on-site\ntemporary bridges construction. However, this procedure is conducted with\ndangerous, time and labor intensive operations, and specialized machinery. We\nenvision a scalable robotic solution inspired by advancements in\nforce-controlled and Cable Driven Parallel Robots (CDPRs); this solution can\naddress the challenges inherent in this transportation problem, achieving fast,\nefficient, and safe deployment and field operations. We introduce the embodied\nvision in Co3MaNDR, a solution to the military gap crossing problem, a\ndistributed robot consisting of several modules simultaneously pulling on a\ncentral payload, controlling the cables' tensions to achieve complex\nobjectives, such as precise trajectory tracking or force amplification.\nHardware experiments demonstrate teleoperation of a payload, trajectory\nfollowing, and the sensing and amplification of operators' applied physical\nforces during slow operations. An operator was shown to manipulate a 27.2 kg\n(60 lb) payload with an average force utilization of 14.5\\% of its weight.\nResults indicate that the system can be scaled up to heavier payloads without\ncompromising performance or introducing superfluous complexity. This research\nlays a foundation to expand CDPR technology to uncoordinated and unstable\nmobile platforms in unknown environments.\n","authors":["Kevin Murphy","Joao C. V. Soares","Justin K. Yim","Dustin Nottage","Ahmet Soylemezoglu","Joao Ramos"],"pdf_url":"https://arxiv.org/pdf/2403.13124v1.pdf","comment":"8 pages, 9 figures. Submit to IROS 2024"},{"id":"http://arxiv.org/abs/2305.03001v2","updated":"2024-03-19T19:40:44Z","published":"2023-05-04T17:19:47Z","title":"OSDaR23: Open Sensor Data for Rail 2023","summary":"  To achieve a driverless train operation on mainline railways, actual and\npotential obstacles for the train's driveway must be detected automatically by\nappropriate sensor systems. Machine learning algorithms have proven to be\npowerful tools for this task during the last years. However, these algorithms\nrequire large amounts of high-quality annotated data containing\nrailway-specific objects as training data. Unfortunately, all of the publicly\navailable datasets that tackle this requirement are restricted in some way.\nTherefore, this paper presents OSDaR23, a multi-sensor dataset of 45\nsubsequences acquired in Hamburg, Germany, in September 2021, that was created\nto foster driverless train operation on mainline railways. The sensor setup\nconsists of multiple calibrated and synchronized infrared (IR) and visual (RGB)\ncameras, lidars, a radar, and position and acceleration sensors mounted on the\nfront of a rail vehicle. In addition to the raw data, the dataset contains\n204091 polyline, polygonal, rectangle, and cuboid annotations in total for 20\ndifferent object classes. It is the first publicly available multi-sensor\ndataset annotated with a variety of object classes that are relevant for the\nrailway context. OSDaR23, available at data.fid-move.de/dataset/osdar23, can\nalso be used for tasks beyond collision prediction, which are listed in this\npaper.\n","authors":["Rustam Tagiew","Martin Köppel","Karsten Schwalbe","Patrick Denzler","Philipp Neumaier","Tobias Klockau","Martin Boekhoff","Pavel Klasek","Roman Tilly"],"pdf_url":"https://arxiv.org/pdf/2305.03001v2.pdf","comment":"7 pages, 11 images, 5 tables"},{"id":"http://arxiv.org/abs/2311.00112v2","updated":"2024-03-19T19:37:22Z","published":"2023-10-31T19:39:44Z","title":"Hierarchical Optimization-based Control for Whole-body Loco-manipulation\n  of Heavy Objects","summary":"  In recent years, the field of legged robotics has seen growing interest in\nenhancing the capabilities of these robots through the integration of\narticulated robotic arms. However, achieving successful loco-manipulation,\nespecially involving interaction with heavy objects, is far from\nstraightforward, as object manipulation can introduce substantial disturbances\nthat impact the robot's locomotion. This paper presents a novel framework for\nlegged loco-manipulation that considers whole-body coordination through a\nhierarchical optimization-based control framework. First, an online\nmanipulation planner computes the manipulation forces and manipulated object\ntask-based reference trajectory. Then, pose optimization aligns the robot's\ntrajectory with kinematic constraints. The resultant robot reference trajectory\nis executed via a linear MPC controller incorporating the desired manipulation\nforces into its prediction model. Our approach has been validated in simulation\nand hardware experiments, highlighting the necessity of whole-body optimization\ncompared to the baseline locomotion MPC when interacting with heavy objects.\nExperimental results with Unitree Aliengo, equipped with a custom-made robotic\narm, showcase its ability to lift and carry an 8kg payload and manipulate\ndoors.\n","authors":["Alberto Rigo","Muqun Hu","Satyandra K. Gupta","Quan Nguyen"],"pdf_url":"https://arxiv.org/pdf/2311.00112v2.pdf","comment":"7 pages, 7 figures"},{"id":"http://arxiv.org/abs/2310.09442v2","updated":"2024-03-19T19:11:32Z","published":"2023-10-13T23:23:39Z","title":"Learning Agile Locomotion and Adaptive Behaviors via RL-augmented MPC","summary":"  In the context of legged robots, adaptive behavior involves adaptive\nbalancing and adaptive swing foot reflection. While adaptive balancing\ncounteracts perturbations to the robot, adaptive swing foot reflection helps\nthe robot to navigate intricate terrains without foot entrapment. In this\npaper, we manage to bring both aspects of adaptive behavior to quadruped\nlocomotion by combining RL and MPC while improving the robustness and agility\nof blind legged locomotion. This integration leverages MPC's strength in\npredictive capabilities and RL's adeptness in drawing from past experiences.\nUnlike traditional locomotion controls that separate stance foot control and\nswing foot trajectory, our innovative approach unifies them, addressing their\nlack of synchronization. At the heart of our contribution is the synthesis of\nstance foot control with swing foot reflection, improving agility and\nrobustness in locomotion with adaptive behavior. A hallmark of our approach is\nrobust blind stair climbing through swing foot reflection. Moreover, we\nintentionally designed the learning module as a general plugin for different\nrobot platforms. We trained the policy and implemented our approach on the\nUnitree A1 robot, achieving impressive results: a peak turn rate of 8.5 rad/s,\na peak running speed of 3 m/s, and steering at a speed of 2.5 m/s. Remarkably,\nthis framework also allows the robot to maintain stable locomotion while\nbearing an unexpected load of 10 kg, or 83\\% of its body mass. We further\ndemonstrate the generalizability and robustness of the same policy where it\nrealizes zero-shot transfer to different robot platforms like Go1 and AlienGo\nrobots for load carrying. Code is made available for the use of the research\ncommunity at https://github.com/DRCL-USC/RL_augmented_MPC.git\n","authors":["Yiyu Chen","Quan Nguyen"],"pdf_url":"https://arxiv.org/pdf/2310.09442v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13093v1","updated":"2024-03-19T18:42:22Z","published":"2024-03-19T18:42:22Z","title":"Graph Neural Network-based Multi-agent Reinforcement Learning for\n  Resilient Distributed Coordination of Multi-Robot Systems","summary":"  Existing multi-agent coordination techniques are often fragile and vulnerable\nto anomalies such as agent attrition and communication disturbances, which are\nquite common in the real-world deployment of systems like field robotics. To\nbetter prepare these systems for the real world, we present a graph neural\nnetwork (GNN)-based multi-agent reinforcement learning (MARL) method for\nresilient distributed coordination of a multi-robot system. Our method,\nMulti-Agent Graph Embedding-based Coordination (MAGEC), is trained using\nmulti-agent proximal policy optimization (PPO) and enables distributed\ncoordination around global objectives under agent attrition, partial\nobservability, and limited or disturbed communications. We use a multi-robot\npatrolling scenario to demonstrate our MAGEC method in a ROS 2-based simulator\nand then compare its performance with prior coordination approaches. Results\ndemonstrate that MAGEC outperforms existing methods in several experiments\ninvolving agent attrition and communication disturbance, and provides\ncompetitive results in scenarios without such anomalies.\n","authors":["Anthony Goeckner","Yueyuan Sui","Nicolas Martinet","Xinliang Li","Qi Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.13093v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2403.13090v1","updated":"2024-03-19T18:38:50Z","published":"2024-03-19T18:38:50Z","title":"Digital Twin-Driven Reinforcement Learning for Obstacle Avoidance in\n  Robot Manipulators: A Self-Improving Online Training Framework","summary":"  The evolution and growing automation of collaborative robots introduce more\ncomplexity and unpredictability to systems, highlighting the crucial need for\nrobot's adaptability and flexibility to address the increasing complexities of\ntheir environment. In typical industrial production scenarios, robots are often\nrequired to be re-programmed when facing a more demanding task or even a few\nchanges in workspace conditions. To increase productivity, efficiency and\nreduce human effort in the design process, this paper explores the potential of\nusing digital twin combined with Reinforcement Learning (RL) to enable robots\nto generate self-improving collision-free trajectories in real time. The\ndigital twin, acting as a virtual counterpart of the physical system, serves as\na 'forward run' for monitoring, controlling, and optimizing the physical system\nin a safe and cost-effective manner. The physical system sends data to\nsynchronize the digital system through the video feeds from cameras, which\nallows the virtual robot to update its observation and policy based on real\nscenarios. The bidirectional communication between digital and physical systems\nprovides a promising platform for hardware-in-the-loop RL training through\ntrial and error until the robot successfully adapts to its new environment. The\nproposed online training framework is demonstrated on the Unfactory Xarm5\ncollaborative robot, where the robot end-effector aims to reach the target\nposition while avoiding obstacles. The experiment suggest that proposed\nframework is capable of performing policy online training, and that there\nremains significant room for improvement.\n","authors":["Yuzhu Sun","Mien Van","Stephen McIlvanna","Nguyen Minh Nhat","Kabirat Olayemi","Jack Close","Seán McLoone"],"pdf_url":"https://arxiv.org/pdf/2403.13090v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2403.13085v1","updated":"2024-03-19T18:32:10Z","published":"2024-03-19T18:32:10Z","title":"Subgoal Diffuser: Coarse-to-fine Subgoal Generation to Guide Model\n  Predictive Control for Robot Manipulation","summary":"  Manipulation of articulated and deformable objects can be difficult due to\ntheir compliant and under-actuated nature. Unexpected disturbances can cause\nthe object to deviate from a predicted state, making it necessary to use\nModel-Predictive Control (MPC) methods to plan motion. However, these methods\nneed a short planning horizon to be practical. Thus, MPC is ill-suited for\nlong-horizon manipulation tasks due to local minima. In this paper, we present\na diffusion-based method that guides an MPC method to accomplish long-horizon\nmanipulation tasks by dynamically specifying sequences of subgoals for the MPC\nto follow. Our method, called Subgoal Diffuser, generates subgoals in a\ncoarse-to-fine manner, producing sparse subgoals when the task is easily\naccomplished by MPC and more dense subgoals when the MPC method needs more\nguidance. The density of subgoals is determined dynamically based on a learned\nestimate of reachability, and subgoals are distributed to focus on challenging\nparts of the task. We evaluate our method on two robot manipulation tasks and\nfind it improves the planning performance of an MPC method, and also\noutperforms prior diffusion-based methods.\n","authors":["Zixuan Huang","Yating Lin","Fan Yang","Dmitry Berenson"],"pdf_url":"https://arxiv.org/pdf/2403.13085v1.pdf","comment":"ICRA 2024"},{"id":"http://arxiv.org/abs/2403.13079v1","updated":"2024-03-19T18:15:35Z","published":"2024-03-19T18:15:35Z","title":"Current-Based Impedance Control for Interacting with Mobile Manipulators","summary":"  As robots shift from industrial to human-centered spaces, adopting mobile\nmanipulators, which expand workspace capabilities, becomes crucial. In these\nsettings, seamless interaction with humans necessitates compliant control. Two\ncommon methods for safe interaction, admittance, and impedance control, require\nforce or torque sensors, often absent in lower-cost or lightweight robots. This\npaper presents an adaption of impedance control that can be used on\ncurrent-controlled robots without the use of force or torque sensors and its\napplication for compliant control of a mobile manipulator. A calibration method\nis designed that enables estimation of the actuators' current/torque ratios and\nfrictions, used by the adapted impedance controller, and that can handle model\nerrors. The calibration method and the performance of the designed controller\nare experimentally validated using the Kinova GEN3 Lite arm. Results show that\nthe calibration method is consistent and that the designed controller for the\narm is compliant while also being able to track targets with five-millimeter\nprecision when no interaction is present. Additionally, this paper presents two\noperational modes for interacting with the mobile manipulator: one for guiding\nthe robot around the workspace through interacting with the arm and another for\nexecuting a tracking task, both maintaining compliance to external forces.\nThese operational modes were tested in real-world experiments, affirming their\npractical applicability and effectiveness.\n","authors":["Jelmer de Wolde","Luzia Knoedler","Gianluca Garofalo","Javier Alonso-Mora"],"pdf_url":"https://arxiv.org/pdf/2403.13079v1.pdf","comment":"8 pages, 13 figures, under review for IROS 2024"},{"id":"http://arxiv.org/abs/2310.00481v2","updated":"2024-03-19T18:05:18Z","published":"2023-09-30T20:26:00Z","title":"LANCAR: Leveraging Language for Context-Aware Robot Locomotion in\n  Unstructured Environments","summary":"  Navigating robots through unstructured terrains is challenging, primarily due\nto the dynamic environmental changes. While humans adeptly navigate such\nterrains by using context from their observations, creating a similar\ncontext-aware navigation system for robots is difficult. The essence of the\nissue lies in the acquisition and interpretation of contextual information, a\ntask complicated by the inherent ambiguity of human language. In this work, we\nintroduce LANCAR, which addresses this issue by combining a context translator\nwith reinforcement learning (RL) agents for context-aware locomotion. LANCAR\nallows robots to comprehend contextual information through Large Language\nModels (LLMs) sourced from human observers and convert this information into\nactionable contextual embeddings. These embeddings, combined with the robot's\nsensor data, provide a complete input for the RL agent's policy network. We\nprovide an extensive evaluation of LANCAR under different levels of contextual\nambiguity and compare with alternative methods. The experimental results\nshowcase the superior generalizability and adaptability across different\nterrains. Notably, LANCAR shows at least a 7.4% increase in episodic reward\nover the best alternatives, highlighting its potential to enhance robotic\nnavigation in unstructured environments. More details and experiment videos\ncould be found in http://raaslab.org/projects/LLM_Context_Estimation/.\n","authors":["Chak Lam Shek","Xiyang Wu","Wesley A. Suttle","Carl Busart","Erin Zaroukian","Dinesh Manocha","Pratap Tokekar","Amrit Singh Bedi"],"pdf_url":"https://arxiv.org/pdf/2310.00481v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13042v1","updated":"2024-03-19T17:57:09Z","published":"2024-03-19T17:57:09Z","title":"TAPTR: Tracking Any Point with Transformers as Detection","summary":"  In this paper, we propose a simple and strong framework for Tracking Any\nPoint with TRansformers (TAPTR). Based on the observation that point tracking\nbears a great resemblance to object detection and tracking, we borrow designs\nfrom DETR-like algorithms to address the task of TAP. In the proposed\nframework, in each video frame, each tracking point is represented as a point\nquery, which consists of a positional part and a content part. As in DETR, each\nquery (its position and content feature) is naturally updated layer by layer.\nIts visibility is predicted by its updated content feature. Queries belonging\nto the same tracking point can exchange information through self-attention\nalong the temporal dimension. As all such operations are well-designed in\nDETR-like algorithms, the model is conceptually very simple. We also adopt some\nuseful designs such as cost volume from optical flow models and develop simple\ndesigns to provide long temporal information while mitigating the feature\ndrifting issue. Our framework demonstrates strong performance with\nstate-of-the-art performance on various TAP datasets with faster inference\nspeed.\n","authors":["Hongyang Li","Hao Zhang","Shilong Liu","Zhaoyang Zeng","Tianhe Ren","Feng Li","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.13042v1.pdf","comment":null}]},"2024-03-17T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2403.11368v1","updated":"2024-03-17T23:07:13Z","published":"2024-03-17T23:07:13Z","title":"Driving Style Alignment for LLM-powered Driver Agent","summary":"  Recently, LLM-powered driver agents have demonstrated considerable potential\nin the field of autonomous driving, showcasing human-like reasoning and\ndecision-making abilities.However, current research on aligning driver agent\nbehaviors with human driving styles remains limited, partly due to the scarcity\nof high-quality natural language data from human driving behaviors.To address\nthis research gap, we propose a multi-alignment framework designed to align\ndriver agents with human driving styles through demonstrations and feedback.\nNotably, we construct a natural language dataset of human driver behaviors\nthrough naturalistic driving experiments and post-driving interviews, offering\nhigh-quality human demonstrations for LLM alignment. The framework's\neffectiveness is validated through simulation experiments in the CARLA urban\ntraffic simulator and further corroborated by human evaluations. Our research\noffers valuable insights into designing driving agents with diverse driving\nstyles.The implementation of the framework and details of the dataset can be\nfound at the link.\n","authors":["Ruoxuan Yang","Xinyue Zhang","Anais Fernandez-Laaksonen","Xin Ding","Jiangtao Gong"],"pdf_url":"https://arxiv.org/pdf/2403.11368v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11367v1","updated":"2024-03-17T23:06:12Z","published":"2024-03-17T23:06:12Z","title":"3DGS-ReLoc: 3D Gaussian Splatting for Map Representation and Visual\n  ReLocalization","summary":"  This paper presents a novel system designed for 3D mapping and visual\nrelocalization using 3D Gaussian Splatting. Our proposed method uses LiDAR and\ncamera data to create accurate and visually plausible representations of the\nenvironment. By leveraging LiDAR data to initiate the training of the 3D\nGaussian Splatting map, our system constructs maps that are both detailed and\ngeometrically accurate. To mitigate excessive GPU memory usage and facilitate\nrapid spatial queries, we employ a combination of a 2D voxel map and a KD-tree.\nThis preparation makes our method well-suited for visual localization tasks,\nenabling efficient identification of correspondences between the query image\nand the rendered image from the Gaussian Splatting map via normalized\ncross-correlation (NCC). Additionally, we refine the camera pose of the query\nimage using feature-based matching and the Perspective-n-Point (PnP) technique.\nThe effectiveness, adaptability, and precision of our system are demonstrated\nthrough extensive evaluation on the KITTI360 dataset.\n","authors":["Peng Jiang","Gaurav Pandey","Srikanth Saripalli"],"pdf_url":"https://arxiv.org/pdf/2403.11367v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2308.08978v2","updated":"2024-03-17T22:17:41Z","published":"2023-08-17T13:33:15Z","title":"Quantifying the biomimicry gap in biohybrid robot-fish pairs","summary":"  Biohybrid systems in which robotic lures interact with animals have become\ncompelling tools for probing and identifying the mechanisms underlying\ncollective animal behavior. One key challenge lies in the transfer of social\ninteraction models from simulations to reality, using robotics to validate the\nmodeling hypotheses. This challenge arises in bridging what we term the\n\"biomimicry gap\", which is caused by imperfect robotic replicas, communication\ncues and physics constraints not incorporated in the simulations, that may\nelicit unrealistic behavioral responses in animals. In this work, we used a\nbiomimetic lure of a rummy-nose tetra fish (Hemigrammus rhodostomus) and a\nneural network (NN) model for generating biomimetic social interactions.\nThrough experiments with a biohybrid pair comprising a fish and the robotic\nlure, a pair of real fish, and simulations of pairs of fish, we demonstrate\nthat our biohybrid system generates social interactions mirroring those of\ngenuine fish pairs. Our analyses highlight that: 1) the lure and NN maintain\nminimal deviation in real-world interactions compared to simulations and\nfish-only experiments, 2) our NN controls the robot efficiently in real-time,\nand 3) a comprehensive validation is crucial to bridge the biomimicry gap,\nensuring realistic biohybrid systems.\n","authors":["Vaios Papaspyros","Guy Theraulaz","Clément Sire","Francesco Mondada"],"pdf_url":"https://arxiv.org/pdf/2308.08978v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.04619v2","updated":"2024-03-17T20:46:14Z","published":"2023-07-10T15:07:29Z","title":"Learning Fine Pinch-Grasp Skills using Tactile Sensing from A Few\n  Real-world Demonstrations","summary":"  Imitation learning for robot dexterous manipulation, especially with a real\nrobot setup, typically requires a large number of demonstrations. In this\npaper, we present a data-efficient learning from demonstration framework which\nexploits the use of rich tactile sensing data and achieves fine bimanual pinch\ngrasping. Specifically, we employ a convolutional autoencoder network that can\neffectively extract and encode high-dimensional tactile information. Further,\nWe develop a framework that achieves efficient multi-sensor fusion for\nimitation learning, allowing the robot to learn contact-aware sensorimotor\nskills from demonstrations. Our comparision study against the framework without\nusing encoded tactile features highlighted the effectiveness of incorporating\nrich contact information, which enabled dexterous bimanual grasping with active\ncontact searching. Extensive experiments demonstrated the robustness of the\nfine pinch grasp policy directly learned from few-shot demonstration, including\ngrasping of the same object with different initial poses, generalizing to ten\nunseen new objects, robust and firm grasping against external pushes, as well\nas contact-aware and reactive re-grasping in case of dropping objects under\nvery large perturbations. Furthermore, the saliency map analysis method is used\nto describe weight distribution across various modalities during pinch\ngrasping, confirming the effectiveness of our framework at leveraging\nmultimodal information.\n","authors":["Xiaofeng Mao","Yucheng Xu","Ruoshi Wen","Mohammadreza Kasaei","Wanming Yu","Efi Psomopoulou","Nathan F. Lepora","Zhibin Li"],"pdf_url":"https://arxiv.org/pdf/2307.04619v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17892v2","updated":"2024-03-17T20:44:31Z","published":"2024-02-27T21:12:31Z","title":"SWTrack: Multiple Hypothesis Sliding Window 3D Multi-Object Tracking","summary":"  Modern robotic systems are required to operate in dense dynamic environments,\nrequiring highly accurate real-time track identification and estimation. For 3D\nmulti-object tracking, recent approaches process a single measurement frame\nrecursively with greedy association and are prone to errors in ambiguous\nassociation decisions. Our method, Sliding Window Tracker (SWTrack), yields\nmore accurate association and state estimation by batch processing many frames\nof sensor data while being capable of running online in real-time. The most\nprobable track associations are identified by evaluating all possible track\nhypotheses across the temporal sliding window. A novel graph optimization\napproach is formulated to solve the multidimensional assignment problem with\nlifted graph edges introduced to account for missed detections and graph\nsparsity enforced to retain real-time efficiency. We evaluate our SWTrack\nimplementation$^{2}$ on the NuScenes autonomous driving dataset to demonstrate\nimproved tracking performance.\n","authors":["Sandro Papais","Robert Ren","Steven Waslander"],"pdf_url":"https://arxiv.org/pdf/2402.17892v2.pdf","comment":"Accepted to ICRA 2024"},{"id":"http://arxiv.org/abs/2403.11334v1","updated":"2024-03-17T20:29:01Z","published":"2024-03-17T20:29:01Z","title":"Bridging the Gap between Discrete Agent Strategies in Game Theory and\n  Continuous Motion Planning in Dynamic Environments","summary":"  Generating competitive strategies and performing continuous motion planning\nsimultaneously in an adversarial setting is a challenging problem. In addition,\nunderstanding the intent of other agents is crucial to deploying autonomous\nsystems in adversarial multi-agent environments. Existing approaches either\ndiscretize agent action by grouping similar control inputs, sacrificing\nperformance in motion planning, or plan in uninterpretable latent spaces,\nproducing hard-to-understand agent behaviors. This paper proposes an agent\nstrategy representation via Policy Characteristic Space that maps the agent\npolicies to a pre-specified low-dimensional space. Policy Characteristic Space\nenables the discretization of agent policy switchings while preserving\ncontinuity in control. Also, it provides intepretability of agent policies and\nclear intentions of policy switchings. Then, regret-based game-theoretic\napproaches can be applied in the Policy Characteristic Space to obtain high\nperformance in adversarial environments. Our proposed method is assessed by\nconducting experiments in an autonomous racing scenario using scaled vehicles.\nStatistical evidence shows that our method significantly improves the win rate\nof ego agent and the method also generalizes well to unseen environments.\n","authors":["Hongrui Zheng","Zhijun Zhuang","Stephanie Wu","Shuo Yang","Rahul Mangharam"],"pdf_url":"https://arxiv.org/pdf/2403.11334v1.pdf","comment":"Submitted to RA-L"},{"id":"http://arxiv.org/abs/2209.14875v2","updated":"2024-03-17T20:10:16Z","published":"2022-09-29T15:40:12Z","title":"Accelerating Laboratory Automation Through Robot Skill Learning For\n  Sample Scraping","summary":"  The use of laboratory robotics for autonomous experiments offers an\nattractive route to alleviate scientists from tedious tasks while accelerating\nmaterial discovery for topical issues such as climate change and\npharmaceuticals. While some experimental workflows can already benefit from\nautomation, sample preparation is still carried out manually due to the high\nlevel of motor function and dexterity required when dealing with different\ntools, chemicals, and glassware. A fundamental workflow in chemical fields is\ncrystallisation, where one application is polymorph screening, i.e., obtaining\na three dimensional molecular structure from a crystal. For this process, it is\nof utmost importance to recover as much of the sample as possible since\nsynthesising molecules is both costly in time and money. To this aim, chemists\nscrape vials to retrieve sample contents prior to imaging plate transfer.\nAutomating this process is challenging as it goes beyond robotic insertion\ntasks due to a fundamental requirement of having to execute fine-granular\nmovements within a constrained environment (sample vial). Motivated by how\nhuman chemists carry out this process of scraping powder from vials, our work\nproposes a model-free reinforcement learning method for learning a scraping\npolicy, leading to a fully autonomous sample scraping procedure. We first\ncreate a scenario-specific simulation environment with a Panda Franka Emika\nrobot using a laboratory scraper that is inserted into a simulated vial, to\ndemonstrate how a scraping policy can be learned successfully in simulation. We\nthen train and evaluate our method on a real robotic manipulator in laboratory\nsettings, and show that our method can autonomously scrape powder across\nvarious setups.\n","authors":["Gabriella Pizzuto","Hetong Wang","Hatem Fakhruldeen","Bei Peng","Kevin S. Luck","Andrew I. Cooper"],"pdf_url":"https://arxiv.org/pdf/2209.14875v2.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.05570v2","updated":"2024-03-17T19:49:58Z","published":"2024-02-22T02:38:51Z","title":"A Motion Planning Algorithm in a Figure Eight Track","summary":"  We design a motion planning algorithm to coordinate the movements of two\nrobots along a figure eight track, in such a way that no collisions occur. We\nuse a topological approach to robot motion planning that relates instabilities\nin motion planning algorithms to topological features of configuration spaces.\nThe topological complexity of a configuration space is an invariant that\nmeasures the complexity of motion planning algorithms. We show that the\ntopological complexity of our problem is 3 and construct an explicit algorithm\nwith three continuous instructions.\n","authors":["Cristian Jardon","Brian Sheppard","Veet Zaveri"],"pdf_url":"https://arxiv.org/pdf/2403.05570v2.pdf","comment":"25 pages, 45 figures, First published in PUMP Journal of\n  Undergraduate Research. This research paper was completed under the\n  supervision of Prof. Hellen Colman at Wilbur Wright College"},{"id":"http://arxiv.org/abs/2403.11313v1","updated":"2024-03-17T19:23:46Z","published":"2024-03-17T19:23:46Z","title":"Leveraging Simulation-Based Model Preconditions for Fast Action\n  Parameter Optimization with Multiple Models","summary":"  Optimizing robotic action parameters is a significant challenge for\nmanipulation tasks that demand high levels of precision and generalization.\nUsing a model-based approach, the robot must quickly reason about the outcomes\nof different actions using a predictive model to find a set of parameters that\nwill have the desired effect. The model may need to capture the behaviors of\nrigid and deformable objects, as well as objects of various shapes and sizes.\nPredictive models often need to trade-off speed for prediction accuracy and\ngeneralization. This paper proposes a framework that leverages the strengths of\nmultiple predictive models, including analytical, learned, and simulation-based\nmodels, to enhance the efficiency and accuracy of action parameter\noptimization. Our approach uses Model Deviation Estimators (MDEs) to determine\nthe most suitable predictive model for any given state-action parameters,\nallowing the robot to select models to make fast and precise predictions. We\nextend the MDE framework by not only learning sim-to-real MDEs, but also\nsim-to-sim MDEs. Our experiments show that these sim-to-sim MDEs provide\nsignificantly faster parameter optimization as well as a basis for efficiently\nlearning sim-to-real MDEs through finetuning. The ease of collecting sim-to-sim\ntraining data also allows the robot to learn MDEs based directly on visual\ninputs and local material properties.\n","authors":["M. Yunus Seker","Oliver Kroemer"],"pdf_url":"https://arxiv.org/pdf/2403.11313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11304v1","updated":"2024-03-17T18:53:46Z","published":"2024-03-17T18:53:46Z","title":"Pioneering SE(2)-Equivariant Trajectory Planning for Automated Driving","summary":"  Planning the trajectory of the controlled ego vehicle is a key challenge in\nautomated driving. As for human drivers, predicting the motions of surrounding\nvehicles is important to plan the own actions. Recent motion prediction methods\nutilize equivariant neural networks to exploit geometric symmetries in the\nscene. However, no existing method combines motion prediction and trajectory\nplanning in a joint step while guaranteeing equivariance under\nroto-translations of the input space. We address this gap by proposing a\nlightweight equivariant planning model that generates multi-modal joint\npredictions for all vehicles and selects one mode as the ego plan. The\nequivariant network design improves sample efficiency, guarantees output\nstability, and reduces model parameters. We further propose equivariant route\nattraction to guide the ego vehicle along a high-level route provided by an\noff-the-shelf GPS navigation system. This module creates a momentum from\nembedded vehicle positions toward the route in latent space while keeping the\nequivariance property. Route attraction enables goal-oriented behavior without\nforcing the vehicle to stick to the exact route. We conduct experiments on the\nchallenging nuScenes dataset to investigate the capability of our planner. The\nresults show that the planned trajectory is stable under roto-translations of\nthe input scene which demonstrates the equivariance of our model. Despite using\nonly a small split of the dataset for training, our method improves L2 distance\nat 3 s by 20.6 % and surpasses the state of the art.\n","authors":["Steffen Hagedorn","Marcel Milich","Alexandru P. Condurache"],"pdf_url":"https://arxiv.org/pdf/2403.11304v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11298v1","updated":"2024-03-17T18:40:29Z","published":"2024-03-17T18:40:29Z","title":"Multi-Sample Long Range Path Planning under Sensing Uncertainty for\n  Off-Road Autonomous Driving","summary":"  We focus on the problem of long-range dynamic replanning for off-road\nautonomous vehicles, where a robot plans paths through a previously unobserved\nenvironment while continuously receiving noisy local observations. An effective\napproach for planning under sensing uncertainty is determinization, where one\nconverts a stochastic world into a deterministic one and plans under this\nsimplification. This makes the planning problem tractable, but the cost of\nfollowing the planned path in the real world may be different than in the\ndeterminized world. This causes collisions if the determinized world\noptimistically ignores obstacles, or causes unnecessarily long routes if the\ndeterminized world pessimistically imagines more obstacles. We aim to be robust\nto uncertainty over potential worlds while still achieving the efficiency\nbenefits of determinization. We evaluate algorithms for dynamic replanning on a\nlarge real-world dataset of challenging long-range planning problems from the\nDARPA RACER program. Our method, Dynamic Replanning via Evaluating and\nAggregating Multiple Samples (DREAMS), outperforms other determinization-based\napproaches in terms of combined traversal time and collision cost.\nhttps://sites.google.com/cs.washington.edu/dreams/\n","authors":["Matt Schmittle","Rohan Baijal","Brian Hou","Siddhartha Srinivasa","Byron Boots"],"pdf_url":"https://arxiv.org/pdf/2403.11298v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11289v1","updated":"2024-03-17T17:59:59Z","published":"2024-03-17T17:59:59Z","title":"ManipVQA: Injecting Robotic Affordance and Physically Grounded\n  Information into Multi-Modal Large Language Models","summary":"  The integration of Multimodal Large Language Models (MLLMs) with robotic\nsystems has significantly enhanced the ability of robots to interpret and act\nupon natural language instructions. Despite these advancements, conventional\nMLLMs are typically trained on generic image-text pairs, lacking essential\nrobotics knowledge such as affordances and physical knowledge, which hampers\ntheir efficacy in manipulation tasks. To bridge this gap, we introduce\nManipVQA, a novel framework designed to endow MLLMs with Manipulation-centric\nknowledge through a Visual Question-Answering format. This approach not only\nencompasses tool detection and affordance recognition but also extends to a\ncomprehensive understanding of physical concepts. Our approach starts with\ncollecting a varied set of images displaying interactive objects, which\npresents a broad range of challenges in tool object detection, affordance, and\nphysical concept predictions. To seamlessly integrate this robotic-specific\nknowledge with the inherent vision-reasoning capabilities of MLLMs, we adopt a\nunified VQA format and devise a fine-tuning strategy that preserves the\noriginal vision-reasoning abilities while incorporating the new robotic\ninsights. Empirical evaluations conducted in robotic simulators and across\nvarious vision task benchmarks demonstrate the robust performance of ManipVQA.\nCode and dataset will be made publicly available at\nhttps://github.com/SiyuanHuang95/ManipVQA.\n","authors":["Siyuan Huang","Iaroslav Ponomarenko","Zhengkai Jiang","Xiaoqi Li","Xiaobin Hu","Peng Gao","Hongsheng Li","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2403.11289v1.pdf","comment":"Code and dataset will be made publicly available at\n  https://github.com/SiyuanHuang95/ManipVQA"},{"id":"http://arxiv.org/abs/2403.11279v1","updated":"2024-03-17T17:37:21Z","published":"2024-03-17T17:37:21Z","title":"Hybrid Feedback for Three-dimensional Convex Obstacle Avoidance","summary":"  We propose a hybrid feedback control scheme for the autonomous robot\nnavigation problem in three-dimensional environments with arbitrarily-shaped\nconvex obstacles. The proposed hybrid control strategy, which consists in\nswitching between the move-to-target mode and the obstacle-avoidance mode,\nguarantees global asymptotic stability of the target location in the\nobstacle-free workspace. We also provide a procedure for the implementation of\nthe proposed hybrid controller in a priori unknown environments and validate\nits effectiveness through simulation results.\n","authors":["Mayur Sawant","Ilia Polushin","Abdelhamid Tayebi"],"pdf_url":"https://arxiv.org/pdf/2403.11279v1.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.11252v1","updated":"2024-03-17T15:59:41Z","published":"2024-03-17T15:59:41Z","title":"Zutu: A Platform for Localization and Navigation of Swarm Robots Using\n  Virtual Grids","summary":"  Swarm robots, which are inspired from the way insects behave collectively in\norder to achieve a common goal, have become a major part of research with\napplications involving search and rescue, area exploration, surveillance etc.\nIn this paper, we present a swarm of robots that do not require individual\nextrinsic sensors to sense the environment but instead use a single central\ncamera to locate and map the swarm. The robots can be easily built using\nreadily available components with the main chassis being 3D printed, making the\nsystem low-cost, low-maintenance, and easy to replicate. We describe Zutu's\nhardware and software architecture, the algorithms to map the robots to the\nreal world, and some experiments conducted using four of our robots.\nEventually, we conclude the possible applications of our system in research,\neducation, and industries.\n","authors":[" Prateek","Pawan Wadhwani","Reshesh Kumar Pathak","Mayur Bhosale","Dr. A Helen Victoria"],"pdf_url":"https://arxiv.org/pdf/2403.11252v1.pdf","comment":"Accepted at 7th International Conference on Robotics and Automation\n  Engineering, ICRAE 2022, Singapore, November 18 - November 20, 2022"},{"id":"http://arxiv.org/abs/2403.11247v1","updated":"2024-03-17T15:41:35Z","published":"2024-03-17T15:41:35Z","title":"Compact 3D Gaussian Splatting For Dense Visual SLAM","summary":"  Recent work has shown that 3D Gaussian-based SLAM enables high-quality\nreconstruction, accurate pose estimation, and real-time rendering of scenes.\nHowever, these approaches are built on a tremendous number of redundant 3D\nGaussian ellipsoids, leading to high memory and storage costs, and slow\ntraining speed. To address the limitation, we propose a compact 3D Gaussian\nSplatting SLAM system that reduces the number and the parameter size of\nGaussian ellipsoids. A sliding window-based masking strategy is first proposed\nto reduce the redundant ellipsoids. Then we observe that the covariance matrix\n(geometry) of most 3D Gaussian ellipsoids are extremely similar, which\nmotivates a novel geometry codebook to compress 3D Gaussian geometric\nattributes, i.e., the parameters. Robust and accurate pose estimation is\nachieved by a global bundle adjustment method with reprojection loss. Extensive\nexperiments demonstrate that our method achieves faster training and rendering\nspeed while maintaining the state-of-the-art (SOTA) quality of the scene\nrepresentation.\n","authors":["Tianchen Deng","Yaohui Chen","Leyan Zhang","Jianfei Yang","Shenghai Yuan","Danwei Wang","Weidong Chen"],"pdf_url":"https://arxiv.org/pdf/2403.11247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11233v1","updated":"2024-03-17T14:42:05Z","published":"2024-03-17T14:42:05Z","title":"STAIR: Semantic-Targeted Active Implicit Reconstruction","summary":"  Many autonomous robotic applications require object-level understanding when\ndeployed. Actively reconstructing objects of interest, i.e. objects with\nspecific semantic meanings, is therefore relevant for a robot to perform\ndownstream tasks in an initially unknown environment. In this work, we propose\na novel framework for semantic-targeted active reconstruction using posed RGB-D\nmeasurements and 2D semantic labels as input. The key components of our\nframework are a semantic implicit neural representation and a compatible\nplanning utility function based on semantic rendering and uncertainty\nestimation, enabling adaptive view planning to target objects of interest. Our\nplanning approach achieves better reconstruction performance in terms of mesh\nand novel view rendering quality compared to implicit reconstruction baselines\nthat do not consider semantics for view planning. Our framework further\noutperforms a state-of-the-art semantic-targeted active reconstruction pipeline\nbased on explicit maps, justifying our choice of utilising implicit neural\nrepresentations to tackle semantic-targeted active reconstruction problems.\n","authors":["Liren Jin","Haofei Kuang","Yue Pan","Cyrill Stachniss","Marija Popović"],"pdf_url":"https://arxiv.org/pdf/2403.11233v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11205v1","updated":"2024-03-17T13:08:41Z","published":"2024-03-17T13:08:41Z","title":"Continuous Jumping of a Parallel Wire-Driven Monopedal Robot RAMIEL\n  Using Reinforcement Learning","summary":"  We have developed a parallel wire-driven monopedal robot, RAMIEL, which has\nboth speed and power due to the parallel wire mechanism and a long acceleration\ndistance. RAMIEL is capable of jumping high and continuously, and so has high\nperformance in traveling. On the other hand, one of the drawbacks of a minimal\nparallel wire-driven robot without joint encoders is that the current joint\nvelocities estimated from the wire lengths oscillate due to the elongation of\nthe wires, making the values unreliable. Therefore, despite its high\nperformance, the control of the robot is unstable, and in 10 out of 16 jumps,\nthe robot could only jump up to two times continuously. In this study, we\npropose a method to realize a continuous jumping motion by reinforcement\nlearning in simulation, and its application to the actual robot. Because the\njoint velocities oscillate with the elongation of the wires, they are not used\ndirectly, but instead are inferred from the time series of joint angles. At the\nsame time, noise that imitates the vibration caused by the elongation of the\nwires is added for transfer to the actual robot. The results show that the\nsystem can be applied to the actual robot RAMIEL as well as to the stable\ncontinuous jumping motion in simulation.\n","authors":["Kento Kawaharazuka","Temma Suzuki","Kei Okada","Masayuki Inaba"],"pdf_url":"https://arxiv.org/pdf/2403.11205v1.pdf","comment":"Accepted at Humanoids2022"},{"id":"http://arxiv.org/abs/2403.11198v1","updated":"2024-03-17T12:50:05Z","published":"2024-03-17T12:50:05Z","title":"Learning-Based Wiping Behavior of Low-Rigidity Robots Considering\n  Various Surface Materials and Task Definitions","summary":"  Wiping behavior is a task of tracing the surface of an object while feeling\nthe force with the palm of the hand. It is necessary to adjust the force and\nposture appropriately considering the various contact conditions felt by the\nhand. Several studies have been conducted on the wiping motion, however, these\nstudies have only dealt with a single surface material, and have only\nconsidered the application of the amount of appropriate force, lacking\nintelligent movements to ensure that the force is applied either evenly to the\nentire surface or to a certain area. Depending on the surface material, the\nhand posture and pressing force should be varied appropriately, and this is\nhighly dependent on the definition of the task. Also, most of the movements are\nexecuted by high-rigidity robots that are easy to model, and few movements are\nexecuted by robots that are low-rigidity but therefore have a small risk of\ndamage due to excessive contact. So, in this study, we develop a method of\nmotion generation based on the learned prediction of contact force during the\nwiping motion of a low-rigidity robot. We show that MyCobot, which is made of\nlow-rigidity resin, can appropriately perform wiping behaviors on a plane with\nmultiple surface materials based on various task definitions.\n","authors":["Kento Kawaharazuka","Naoaki Kanazawa","Kei Okada","Masayuki Inaba"],"pdf_url":"https://arxiv.org/pdf/2403.11198v1.pdf","comment":"Accepted at Humanoids2022"},{"id":"http://arxiv.org/abs/2307.00533v3","updated":"2024-03-17T12:34:00Z","published":"2023-07-02T10:02:08Z","title":"Representing Robot Geometry as Distance Fields: Applications to\n  Whole-body Manipulation","summary":"  In this work, we propose a novel approach to represent robot geometry as\ndistance fields (RDF) that extends the principle of signed distance fields\n(SDFs) to articulated kinematic chains. Our method employs a combination of\nBernstein polynomials to encode the signed distance for each robot link with\nhigh accuracy and efficiency while ensuring the mathematical continuity and\ndifferentiability of SDFs. We further leverage the kinematics chain of the\nrobot to produce the SDF representation in joint space, allowing robust\ndistance queries in arbitrary joint configurations. The proposed RDF\nrepresentation is differentiable and smooth in both task and joint spaces,\nenabling its direct integration to optimization problems. Additionally, the\n0-level set of the robot corresponds to the robot surface, which can be\nseamlessly integrated into whole-body manipulation tasks. We conduct various\nexperiments in both simulations and with 7-axis Franka Emika robots, comparing\nagainst baseline methods, and demonstrating its effectiveness in collision\navoidance and whole-body manipulation tasks. Project page:\nhttps://sites.google.com/view/lrdf/home\n","authors":["Yiming Li","Yan Zhang","Amirreza Razmjoo","Sylvain Calinon"],"pdf_url":"https://arxiv.org/pdf/2307.00533v3.pdf","comment":"IEEE International Conference on Robotics and Automation, ICRA, 2024"},{"id":"http://arxiv.org/abs/2307.00085v2","updated":"2024-03-17T12:32:08Z","published":"2023-06-30T18:43:24Z","title":"Parallel Self-assembly for a Multi-USV System on Water Surface with\n  Obstacles","summary":"  Parallel self-assembly is an efficient approach to accelerate the assembly\nprocess for modular robots. However, these approaches cannot accommodate\ncomplicated environments with obstacles, which restricts their applications.\nThis paper considers the surrounding stationary obstacles and proposes a\nparallel self-assembly planning algorithm named SAPOA. With this algorithm,\nmodular robots can avoid immovable obstacles when performing docking actions,\nwhich adapts the parallel self-assembly process to complex scenes. To validate\nthe efficiency and scalability, we have designed 25 distinct grid maps with\ndifferent obstacle configurations to simulate the algorithm. From the results\ncompared to the existing parallel self-assembly algorithms, our algorithm shows\na significantly higher success rate, which is more than 80%. For verification\nin real-world applications, a multi-agent hardware testbed system is developed.\nThe algorithm is successfully deployed on four omnidirectional unmanned surface\nvehicles, CuBoats. The navigation strategy that translates the discrete\nplanner, SAPOA, to the continuous controller on the CuBoats is presented. The\nalgorithm's feasibility and flexibility were demonstrated through successful\nself-assembly experiments on 5 maps with varying obstacle configurations.\n","authors":["Lianxin Zhang","Yihan Huang","Zhongzhong Cao","Yang Jiao","Huihuan Qian"],"pdf_url":"https://arxiv.org/pdf/2307.00085v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02044v3","updated":"2024-03-17T10:37:08Z","published":"2023-10-03T13:35:49Z","title":"How Physics and Background Attributes Impact Video Transformers in\n  Robotic Manipulation: A Case Study on Planar Pushing","summary":"  As model and dataset sizes continue to scale in robot learning, the need to\nunderstand what is the specific factor in the dataset that affects model\nperformance becomes increasingly urgent to ensure cost-effective data\ncollection and model performance. In this work, we empirically investigate how\nphysics attributes (color, friction coefficient, shape) and scene background\ncharacteristics, such as the complexity and dynamics of interactions with\nbackground objects, influence the performance of Video Transformers in\npredicting planar pushing trajectories. We aim to investigate three primary\nquestions: How do physics attributes and background scene characteristics\ninfluence model performance? What kind of changes in attributes are most\ndetrimental to model generalization? What proportion of fine-tuning data is\nrequired to adapt models to novel scenarios? To facilitate this research, we\npresent CloudGripper-Push-1K, a large real-world vision-based robot pushing\ndataset comprising 1278 hours and 460,000 videos of planar pushing interactions\nwith objects with different physics and background attributes. We also propose\nVideo Occlusion Transformer (VOT), a generic modular video-transformer-based\ntrajectory prediction framework which features 3 choices of 2D-spatial encoders\nas the subject of our case study. Dataset and codes will be available at\nhttps://cloudgripper.org.\n","authors":["Shutong Jin","Ruiyu Wang","Muhammad Zahid","Florian T. Pokorny"],"pdf_url":"https://arxiv.org/pdf/2310.02044v3.pdf","comment":"Under review at IEEE/RSJ IROS 2024"},{"id":"http://arxiv.org/abs/2310.00398v2","updated":"2024-03-17T09:59:40Z","published":"2023-09-30T14:40:12Z","title":"Optimal Impact Angle Guidance via First-Order Optimization under\n  Nonconvex Constraints","summary":"  Most of the optimal guidance problems can be formulated as nonconvex\noptimization problems, which can be solved indirectly by relaxation,\nconvexification, or linearization. Although these methods are guaranteed to\nconverge to the global optimum of the modified problems, the obtained solution\nmay not guarantee global optimality or even the feasibility of the original\nnonconvex problems. In this paper, we propose a computational optimal guidance\napproach that directly handles the nonconvex constraints encountered in\nformulating the guidance problems. The proposed computational guidance approach\nalternately solves the least squares problems and projects the solution onto\nnonconvex feasible sets, which rapidly converges to feasible suboptimal\nsolutions or sometimes to the globally optimal solutions. The proposed\nalgorithm is verified via a series of numerical simulations on impact angle\nguidance problems under state dependent maneuver vector constraints, and it is\ndemonstrated that the proposed algorithm provides superior guidance performance\nthan conventional techniques.\n","authors":["Gyubin Park","Jiwoo Choi","Da Hoon Jeong","Jong-Han Kim"],"pdf_url":"https://arxiv.org/pdf/2310.00398v2.pdf","comment":"To appear at 2024 American Control Conference"},{"id":"http://arxiv.org/abs/2403.11146v1","updated":"2024-03-17T08:51:17Z","published":"2024-03-17T08:51:17Z","title":"Toward Adaptive Cooperation: Model-Based Shared Control Using\n  LQ-Differential Games","summary":"  This paper introduces a novel model-based adaptive shared control to allow\nfor the identification and design challenge for shared-control systems, in\nwhich humans and automation share control tasks. The main challenge is the\nadaptive behavior of the human in such shared control interactions.\nConsequently, merely identifying human behavior without considering automation\nis insufficient and often leads to inadequate automation design. Therefore,\nthis paper proposes a novel solution involving online identification of the\nhuman and the adaptation of shared control using Linear-Quadratic differential\ngames. The effectiveness of the proposed online adaptation is analyzed in\nsimulations and compared with a non-adaptive shared control from the state of\nthe art. Finally, the proposed approach is tested through human-in-the-loop\nexperiments, highlighting its suitability for real-time applications.\n","authors":["Balint Varga"],"pdf_url":"https://arxiv.org/pdf/2403.11146v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15241v2","updated":"2024-03-17T05:30:40Z","published":"2023-11-26T08:59:30Z","title":"CalibFormer: A Transformer-based Automatic LiDAR-Camera Calibration\n  Network","summary":"  The fusion of LiDARs and cameras has been increasingly adopted in autonomous\ndriving for perception tasks. The performance of such fusion-based algorithms\nlargely depends on the accuracy of sensor calibration, which is challenging due\nto the difficulty of identifying common features across different data\nmodalities. Previously, many calibration methods involved specific targets\nand/or manual intervention, which has proven to be cumbersome and costly.\nLearning-based online calibration methods have been proposed, but their\nperformance is barely satisfactory in most cases. These methods usually suffer\nfrom issues such as sparse feature maps, unreliable cross-modality association,\ninaccurate calibration parameter regression, etc. In this paper, to address\nthese issues, we propose CalibFormer, an end-to-end network for automatic\nLiDAR-camera calibration. We aggregate multiple layers of camera and LiDAR\nimage features to achieve high-resolution representations. A multi-head\ncorrelation module is utilized to identify correlations between features more\naccurately. Lastly, we employ transformer architectures to estimate accurate\ncalibration parameters from the correlation information. Our method achieved a\nmean translation error of $0.8751 \\mathrm{cm}$ and a mean rotation error of\n$0.0562 ^{\\circ}$ on the KITTI dataset, surpassing existing state-of-the-art\nmethods and demonstrating strong robustness, accuracy, and generalization\ncapabilities.\n","authors":["Yuxuan Xiao","Yao Li","Chengzhen Meng","Xingchen Li","Jianmin Ji","Yanyong Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.15241v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11095v1","updated":"2024-03-17T05:23:43Z","published":"2024-03-17T05:23:43Z","title":"PyroTrack: Belief-Based Deep Reinforcement Learning Path Planning for\n  Aerial Wildfire Monitoring in Partially Observable Environments","summary":"  Motivated by agility, 3D mobility, and low-risk operation compared to\nhuman-operated management systems of autonomous unmanned aerial vehicles\n(UAVs), this work studies UAV-based active wildfire monitoring where a UAV\ndetects fire incidents in remote areas and tracks the fire frontline. A UAV\npath planning solution is proposed considering realistic wildfire management\nmissions, where a single low-altitude drone with limited power and flight time\nis available. Noting the limited field of view of commercial low-altitude UAVs,\nthe problem formulates as a partially observable Markov decision process\n(POMDP), in which wildfire progression outside the field of view causes\ninaccurate state representation that prevents the UAV from finding the optimal\npath to track the fire front in limited time. Common deep reinforcement\nlearning (DRL)-based trajectory planning solutions require diverse\ndrone-recorded wildfire data to generalize pre-trained models to real-time\nsystems, which is not currently available at a diverse and standard scale. To\nnarrow down the gap caused by partial observability in the space of possible\npolicies, a belief-based state representation with broad, extensive simulated\ndata is proposed where the beliefs (i.e., ignition probabilities of different\ngrid areas) are updated using a Bayesian framework for the cells within the\nfield of view. The performance of the proposed solution in terms of the ratio\nof detected fire cells and monitored ignited area (MIA) is evaluated in a\ncomplex fire scenario with multiple rapidly growing fire batches, indicating\nthat the belief state representation outperforms the observation state\nrepresentation both in fire coverage and the distance to fire frontline.\n","authors":["Sahand Khoshdel","Qi Luo","Fatemeh Afghah"],"pdf_url":"https://arxiv.org/pdf/2403.11095v1.pdf","comment":"7 pages, Accepted in American Control Conference (ACC) 2024, July\n  10-12th, Toronto, ON, Canada"},{"id":"http://arxiv.org/abs/2311.03351v4","updated":"2024-03-17T04:40:06Z","published":"2023-11-06T18:58:59Z","title":"Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with\n  Multi-Step On-Policy Optimization","summary":"  Combining offline and online reinforcement learning (RL) is crucial for\nefficient and safe learning. However, previous approaches treat offline and\nonline learning as separate procedures, resulting in redundant designs and\nlimited performance. We ask: Can we achieve straightforward yet effective\noffline and online learning without introducing extra conservatism or\nregularization? In this study, we propose Uni-o4, which utilizes an on-policy\nobjective for both offline and online learning. Owning to the alignment of\nobjectives in two phases, the RL agent can transfer between offline and online\nlearning seamlessly. This property enhances the flexibility of the learning\nparadigm, allowing for arbitrary combinations of pretraining, fine-tuning,\noffline, and online learning. In the offline phase, specifically, Uni-o4\nleverages diverse ensemble policies to address the mismatch issues between the\nestimated behavior policy and the offline dataset. Through a simple offline\npolicy evaluation (OPE) approach, Uni-o4 can achieve multi-step policy\nimprovement safely. We demonstrate that by employing the method above, the\nfusion of these two paradigms can yield superior offline initialization as well\nas stable and rapid online fine-tuning capabilities. Through real-world robot\ntasks, we highlight the benefits of this paradigm for rapid deployment in\nchallenging, previously unseen real-world environments. Additionally, through\ncomprehensive evaluations using numerous simulated benchmarks, we substantiate\nthat our method achieves state-of-the-art performance in both offline and\noffline-to-online fine-tuning learning. Our website:\nhttps://lei-kun.github.io/uni-o4/ .\n","authors":["Kun Lei","Zhengmao He","Chenhao Lu","Kaizhe Hu","Yang Gao","Huazhe Xu"],"pdf_url":"https://arxiv.org/pdf/2311.03351v4.pdf","comment":"Our website: https://lei-kun.github.io/uni-o4/"},{"id":"http://arxiv.org/abs/2403.11057v1","updated":"2024-03-17T02:06:49Z","published":"2024-03-17T02:06:49Z","title":"Large Language Models Powered Context-aware Motion Prediction","summary":"  Motion prediction is among the most fundamental tasks in autonomous driving.\nTraditional methods of motion forecasting primarily encode vector information\nof maps and historical trajectory data of traffic participants, lacking a\ncomprehensive understanding of overall traffic semantics, which in turn affects\nthe performance of prediction tasks. In this paper, we utilized Large Language\nModels (LLMs) to enhance the global traffic context understanding for motion\nprediction tasks. We first conducted systematic prompt engineering, visualizing\ncomplex traffic environments and historical trajectory information of traffic\nparticipants into image prompts -- Transportation Context Map (TC-Map),\naccompanied by corresponding text prompts. Through this approach, we obtained\nrich traffic context information from the LLM. By integrating this information\ninto the motion prediction model, we demonstrate that such context can enhance\nthe accuracy of motion predictions. Furthermore, considering the cost\nassociated with LLMs, we propose a cost-effective deployment strategy:\nenhancing the accuracy of motion prediction tasks at scale with 0.7\\%\nLLM-augmented datasets. Our research offers valuable insights into enhancing\nthe understanding of traffic scenes of LLMs and the motion prediction\nperformance of autonomous driving.\n","authors":["Xiaoji Zheng","Lixiu Wu","Zhijie Yan","Yuanrong Tang","Hao Zhao","Chen Zhong","Bokui Chen","Jiangtao Gong"],"pdf_url":"https://arxiv.org/pdf/2403.11057v1.pdf","comment":"6 pages,4 figures"}]},"2024-03-16T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2303.05333v2","updated":"2024-03-16T23:09:07Z","published":"2023-03-09T15:24:21Z","title":"A Convex Hull Cheapest Insertion Heuristic for Precedence Constrained\n  Traveling Salesperson Problems or Sequential Ordering Problems","summary":"  The convex hull cheapest insertion heuristic is a well-known method that\nefficiently generates good solutions to the Traveling Salesperson Problem.\nHowever, this heuristic has not been adapted to account for precedence\nconstraints that restrict the order in which locations can be visited. Such\nconstraints result in the precedence constrained traveling salesperson problem\nor the sequential ordering problem, which are commonly encountered in\napplications where items have to be picked up before they are delivered. In\nthis paper, we present an adapted version of this heuristic that accounts for\nprecedence constraints in the problem definition. This algorithm is compared\nwith the widely used Nearest Neighbor heuristic on the TSPLIB benchmark data\nwith added precedence constraints. It is seen that the proposed algorithm is\nparticularly well suited to cases where delivery nodes are centrally\npositioned, with pickup nodes located in the periphery, outperforming the\nNearest Neighbor algorithm in 97\\% of the examined instances.\n","authors":["Mithun Goutham","Stephanie Stockar"],"pdf_url":"https://arxiv.org/pdf/2303.05333v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2302.06582"},{"id":"http://arxiv.org/abs/2403.11034v1","updated":"2024-03-16T22:46:12Z","published":"2024-03-16T22:46:12Z","title":"Resilient Fleet Management for Energy-Aware Intra-Factory Logistics","summary":"  This paper presents a novel fleet management strategy for battery-powered\nrobot fleets tasked with intra-factory logistics in an autonomous manufacturing\nfacility. In this environment, repetitive material handling operations are\nsubject to real-world uncertainties such as blocked passages, and equipment or\nrobot malfunctions. In such cases, centralized approaches enhance resilience by\nimmediately adjusting the task allocation between the robots. To overcome the\ncomputational expense, a two-step methodology is proposed where the nominal\nproblem is solved a priori using a Monte Carlo Tree Search algorithm for task\nallocation, resulting in a nominal search tree. When a disruption occurs, the\nnominal search tree is rapidly updated a posteriori with costs to the new\nproblem while simultaneously generating feasible solutions. Computational\nexperiments prove the real-time capability of the proposed algorithm for\nvarious scenarios and compare it with the case where the search tree is not\nused and the decentralized approach that does not attempt task reassignment.\n","authors":["Mithun Goutham","Stephanie Stockar"],"pdf_url":"https://arxiv.org/pdf/2403.11034v1.pdf","comment":"This manuscript was accepted to the 2024 American Control Conference\n  (ACC) which will be held Wednesday through Friday, July 10-12, 2024 in\n  Toronto, ON, Canada. arXiv admin note: text overlap with arXiv:2304.11444"},{"id":"http://arxiv.org/abs/2310.18247v2","updated":"2024-03-16T21:21:18Z","published":"2023-10-27T16:34:00Z","title":"Guided Data Augmentation for Offline Reinforcement Learning and\n  Imitation Learning","summary":"  In offline reinforcement learning (RL), an RL agent learns to solve a task\nusing only a fixed dataset of previously collected data. While offline RL has\nbeen successful in learning real-world robot control policies, it typically\nrequires large amounts of expert-quality data to learn effective policies that\ngeneralize to out-of-distribution states. Unfortunately, such data is often\ndifficult and expensive to acquire in real-world tasks. Several recent works\nhave leveraged data augmentation (DA) to inexpensively generate additional\ndata, but most DA works apply augmentations in a random fashion and ultimately\nproduce highly suboptimal augmented experience. In this work, we propose Guided\nData Augmentation (GuDA), a human-guided DA framework that generates\nexpert-quality augmented data. The key insight behind GuDA is that while it may\nbe difficult to demonstrate the sequence of actions required to produce expert\ndata, a user can often easily characterize when an augmented trajectory segment\nrepresents progress toward task completion. Thus, a user can restrict the space\nof possible augmentations to automatically reject suboptimal augmented data. To\nextract a policy from GuDA, we use off-the-shelf offline reinforcement learning\nand behavior cloning algorithms. We evaluate GuDA on a physical robot soccer\ntask as well as simulated D4RL navigation tasks, a simulated autonomous driving\ntask, and a simulated soccer task. Empirically, GuDA enables learning given a\nsmall initial dataset of potentially suboptimal experience and outperforms a\nrandom DA strategy as well as a model-based DA strategy.\n","authors":["Nicholas E. Corrado","Yuxiao Qu","John U. Balis","Adam Labiosa","Josiah P. Hanna"],"pdf_url":"https://arxiv.org/pdf/2310.18247v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11000v1","updated":"2024-03-16T19:08:24Z","published":"2024-03-16T19:08:24Z","title":"Quantifying the Sim2real Gap for GPS and IMU Sensors","summary":"  Simulation can and should play a critical role in the development and testing\nof algorithms for autonomous agents. What might reduce its impact is the\n``sim2real'' gap -- the algorithm response differs between operation in\nsimulated versus real-world environments. This paper introduces an approach to\nevaluate this gap, focusing on the accuracy of sensor simulation --\nspecifically IMU and GPS -- in velocity estimation tasks for autonomous agents.\nUsing a scaled autonomous vehicle, we conduct 40 real-world experiments across\ndiverse environments then replicate the experiments in simulation with five\ndistinct sensor noise models. We note that direct comparison of raw simulation\nand real sensor data fails to quantify the sim2real gap for robotics\napplications. We demonstrate that by using a state of the art state-estimation\npackage as a ``judge'', and by evaluating the performance of this\nstate-estimator in both real and simulated scenarios, we can isolate the\nsim2real discrepancies stemming from sensor simulations alone. The dataset\ngenerated is open-source and publicly available for unfettered use.\n","authors":["Ishaan Mahajan","Huzaifa Unjhawala","Harry Zhang","Zhenhao Zhou","Aaron Young","Alexis Ruiz","Stefan Caldararu","Nevindu Batagoda","Sriram Ashokkumar","Dan Negrut"],"pdf_url":"https://arxiv.org/pdf/2403.11000v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10996v1","updated":"2024-03-16T18:47:04Z","published":"2024-03-16T18:47:04Z","title":"A Scalable and Parallelizable Digital Twin Framework for Sustainable\n  Sim2Real Transition of Multi-Agent Reinforcement Learning Systems","summary":"  This work presents a sustainable multi-agent deep reinforcement learning\nframework capable of selectively scaling parallelized training workloads\non-demand, and transferring the trained policies from simulation to reality\nusing minimal hardware resources. We introduce AutoDRIVE Ecosystem as an\nenabling digital twin framework to train, deploy, and transfer cooperative as\nwell as competitive multi-agent reinforcement learning policies from simulation\nto reality. Particularly, we first investigate an intersection traversal\nproblem of 4 cooperative vehicles (Nigel) that share limited state information\nin single as well as multi-agent learning settings using a common policy\napproach. We then investigate an adversarial autonomous racing problem of 2\nvehicles (F1TENTH) using an individual policy approach. In either set of\nexperiments, a decentralized learning architecture was adopted, which allowed\nrobust training and testing of the policies in stochastic environments. The\nagents were provided with realistically sparse observation spaces, and were\nrestricted to sample control actions that implicitly satisfied the imposed\nkinodynamic and safety constraints. The experimental results for both problem\nstatements are reported in terms of quantitative metrics and qualitative\nremarks for training as well as deployment phases. We also discuss agent and\nenvironment parallelization techniques adopted to efficiently accelerate MARL\ntraining, while analyzing their computational performance. Finally, we\ndemonstrate a resource-aware transition of the trained policies from simulation\nto reality using the proposed digital twin framework.\n","authors":["Chinmay Vilas Samak","Tanmay Vilas Samak","Venkat Krovi"],"pdf_url":"https://arxiv.org/pdf/2403.10996v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2309.10007"},{"id":"http://arxiv.org/abs/2403.10994v1","updated":"2024-03-16T18:44:21Z","published":"2024-03-16T18:44:21Z","title":"SSUP-HRI: Social Signaling in Urban Public Human-Robot Interaction\n  dataset","summary":"  This paper introduces our dataset featuring human-robot interactions (HRI) in\nurban public environments. This dataset is rich with social signals that we\nbelieve can be modeled to help understand naturalistic human-robot interaction.\nOur dataset currently comprises approximately 15 hours of video footage\nrecorded from the robots' perspectives, within which we annotated a total of\n274 observable interactions featuring a wide range of naturalistic human-robot\ninteractions. The data was collected by two mobile trash barrel robots deployed\nin Astor Place, New York City, over the course of a week. We invite the HRI\ncommunity to access and utilize our dataset. To the best of our knowledge, this\nis the first dataset showcasing robot deployments in a complete public,\nnon-controlled setting involving urban residents.\n","authors":["Fanjun Bu","Wendy Ju"],"pdf_url":"https://arxiv.org/pdf/2403.10994v1.pdf","comment":"Workshop on Social Signal Modelling (SS4HRI '24) at HRI 2024"},{"id":"http://arxiv.org/abs/2403.10991v1","updated":"2024-03-16T18:23:15Z","published":"2024-03-16T18:23:15Z","title":"Inverse Submodular Maximization with Application to Human-in-the-Loop\n  Multi-Robot Multi-Objective Coverage Control","summary":"  We consider a new type of inverse combinatorial optimization, Inverse\nSubmodular Maximization (ISM), for human-in-the-loop multi-robot coordination.\n  Forward combinatorial optimization, defined as the process of solving a\ncombinatorial problem given the reward (cost)-related parameters, is widely\nused in multi-robot coordination. In the standard pipeline, the reward\n(cost)-related parameters are designed offline by domain experts first and then\nthese parameters are utilized for coordinating robots online. What if we need\nto change these parameters by non-expert human supervisors who watch over the\nrobots during tasks to adapt to some new requirements? We are interested in the\ncase where human supervisors can suggest what actions to take, and the robots\nneed to change the internal parameters based on such suggestions. We study such\nproblems from the perspective of inverse combinatorial optimization, i.e., the\nprocess of finding parameters given solutions to the problem. Specifically, we\npropose a new formulation for ISM, in which we aim to find a new set of\nparameters that minimally deviate from the current parameters and can make the\ngreedy algorithm output actions the same as those suggested by humans. We show\nthat such problems can be formulated as a Mixed Integer Quadratic Program\n(MIQP). However, MIQP involves exponentially many binary variables, making it\nintractable for the existing solver when the problem size is large. We propose\na new algorithm under the Branch $\\&$ Bound paradigm to solve such problems. In\nnumerical simulations, we demonstrate how to use ISM in multi-robot\nmulti-objective coverage control, and we show that the proposed algorithm\nachieves significant advantages in running time and peak memory usage compared\nto directly using an existing solver.\n","authors":["Guangyao Shi","Gaurav S. Sukhatme"],"pdf_url":"https://arxiv.org/pdf/2403.10991v1.pdf","comment":"submitted to IROS2024"},{"id":"http://arxiv.org/abs/2403.10981v1","updated":"2024-03-16T17:24:46Z","published":"2024-03-16T17:24:46Z","title":"Automatic Spatial Calibration of Near-Field MIMO Radar With Respect to\n  Optical Sensors","summary":"  Despite an emerging interest in MIMO radar, the utilization of its\ncomplementary strengths in combination with optical sensors has so far been\nlimited to far-field applications, due to the challenges that arise from mutual\nsensor calibration in the near field. In fact, most related approaches in the\nautonomous industry propose target-based calibration methods using corner\nreflectors that have proven to be unsuitable for the near field. In contrast,\nwe propose a novel, joint calibration approach for optical RGB-D sensors and\nMIMO radars that is designed to operate in the radar's near-field range, within\ndecimeters from the sensors. Our pipeline consists of a bespoke calibration\ntarget, allowing for automatic target detection and localization, followed by\nthe spatial calibration of the two sensor coordinate systems through target\nregistration. We validate our approach using two different depth sensing\ntechnologies from the optical domain. The experiments show the efficiency and\naccuracy of our calibration for various target displacements, as well as its\nrobustness of our localization in terms of signal ambiguities.\n","authors":["Vanessa Wirth","Johanna Bräunig","Danti Khouri","Florian Gutsche","Martin Vossiek","Tim Weyrich","Marc Stamminger"],"pdf_url":"https://arxiv.org/pdf/2403.10981v1.pdf","comment":"8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.10966v1","updated":"2024-03-16T16:25:34Z","published":"2024-03-16T16:25:34Z","title":"Robust Co-Design of Canonical Underactuated Systems for Increased\n  Certifiable Stability","summary":"  Optimal behaviours of a system to perform a specific task can be achieved by\nleveraging the coupling between trajectory optimization, stabilization, and\ndesign optimization. This approach is particularly advantageous for\nunderactuated systems, which are systems that have fewer actuators than degrees\nof freedom and thus require for more elaborate control systems. This paper\nproposes a novel co-design algorithm, namely Robust Trajectory Control with\nDesign optimization (RTC-D). An inner optimization layer (RTC) simultaneously\nperforms direct transcription (DIRTRAN) to find a nominal trajectory while\ncomputing optimal hyperparameters for a stabilizing time-varying linear\nquadratic regulator (TVLQR). RTC-D augments RTC with a design optimization\nlayer, maximizing the system's robustness through a time-varying Lyapunov-based\nregion of attraction (ROA) analysis. This analysis provides a formal guarantee\nof stability for a set of off-nominal states. The proposed algorithm has been\ntested on two different underactuated systems: the torque-limited simple\npendulum and the cart-pole. Extensive simulations of off-nominal initial\nconditions demonstrate improved robustness, while real-system experiments show\nincreased insensitivity to torque disturbances.\n","authors":["Federico Girlanda","Lasse Shala","Shivesh Kumar","Frank Kirchner"],"pdf_url":"https://arxiv.org/pdf/2403.10966v1.pdf","comment":"Copr. 2024 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works. PREPRINT"},{"id":"http://arxiv.org/abs/2403.10955v1","updated":"2024-03-16T15:44:27Z","published":"2024-03-16T15:44:27Z","title":"Agonist-Antagonist Pouch Motors: Bidirectional Soft Actuators Enhanced\n  by Thermally Responsive Peltier Elements","summary":"  In this study, we introduce a novel Mylar-based pouch motor design that\nleverages the reversible actuation capabilities of Peltier junctions to enable\nagonist-antagonist muscle mimicry in soft robotics. Addressing the limitations\nof traditional silicone-based materials, such as leakage and phase-change fluid\ndegradation, our pouch motors filled with Novec 7000 provide a durable and\nleak-proof solution for geometric modeling. The integration of flexible Peltier\njunctions offers a significant advantage over conventional Joule heating\nmethods by allowing active and reversible heating and cooling cycles. This\ninnovation not only enhances the reliability and longevity of soft robotic\napplications but also broadens the scope of design possibilities, including the\ndevelopment of agonist-antagonist artificial muscles, grippers with can\nmanipulate through flexion and extension, and an anchor-slip style simple\ncrawler design. Our findings indicate that this approach could lead to more\nefficient, versatile, and durable robotic systems, marking a significant\nadvancement in the field of soft robotics.\n","authors":["Trevor Exley","Rashmi Wijesundara","Nathan Tan","Akshay Sunkara","Xinyu He","Shuopu Wang","Bonnie Chan","Aditya Jain","Luis Espinosa","Amir Jafari"],"pdf_url":"https://arxiv.org/pdf/2403.10955v1.pdf","comment":"submitted to IROS 2024, 7 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.10951v1","updated":"2024-03-16T15:35:56Z","published":"2024-03-16T15:35:56Z","title":"TVIM: Thermo-Active Variable Impedance Module: Evaluating Shear-Mode\n  Capabilities of Polycaprolactone","summary":"  In this work, we introduce an advanced thermo-active variable impedance\nmodule which builds upon our previous innovation in thermal-based impedance\nadjustment for actuation systems. Our initial design harnessed the\ntemperature-responsive, viscoelastic properties of Polycaprolactone (PCL) to\nmodulate stiffness and damping, facilitated by integrated flexible Peltier\nelements. While effective, the reliance on compressing and the inherent stress\nrelaxation characteristics of PCL led to suboptimal response times in impedance\nadjustments. Addressing these limitations, the current iteration of our module\npivots to a novel 'shear-mode' operation. By conducting comprehensive shear\nrheology analyses on PCL, we have identified a configuration that eliminates\nthe viscoelastic delay, offering a faster response with improved heat transfer\nefficiency. A key advantage of our module lies in its scalability and\nelimination of additional mechanical actuators for impedance adjustment. The\ncompactness and efficiency of thermal actuation through Peltier elements allow\nfor significant downsizing, making these thermal, variable impedance modules\nexceptionally well-suited for applications where space constraints and actuator\nweight are critical considerations. This development represents a significant\nleap forward in the design of variable impedance actuators, offering a more\nversatile, responsive, and compact solution for a wide range of robotic and\nbiomechanical applications.\n","authors":["Trevor Exley","Rashmi Wijesundara","Shuopu Wang","Arian Moridani","Amir Jafari"],"pdf_url":"https://arxiv.org/pdf/2403.10951v1.pdf","comment":"Submitted to IROS 2024, 7 pages, 10 figures"},{"id":"http://arxiv.org/abs/2309.09810v2","updated":"2024-03-16T15:35:00Z","published":"2023-09-18T14:31:37Z","title":"Learning Inertial Parameter Identification of Unknown Object with\n  Humanoid Robot using Real-to-Sim Adaptation","summary":"  We present a fast learning-based inertial parameters estimation framework\ncapable of understanding the dynamics of an unknown object to enable a humanoid\n(or manipulator) to more safely and accurately interact with its surrounding\nenvironments. Unlike most relevant literature, our framework doesn't require to\nuse of a force/torque sensor, vision system, and a long-horizon trajectory. To\nachieve fast inertia parameter estimation, a time-series data-driven regression\nmodel is utilized rather than solving a constrained optimization problem. Due\nto the challenge of obtaining a large number of the ground truth of inertia\nparameters in the real world, we acquire a reliable dataset in a high-fidelity\nsimulation that is developed using a real-to-sim adaptation. The adaptation\nmethod we introduced consists of two components: 1) \\textit{Robot System\nIdentification} and 2) \\textit{Gaussian Processes}. We demonstrate our method\nwith a 4-DOF single manipulator of a wheeled humanoid robot, SATYRR. Results\nshow that our method can identify the inertial parameters of various unknown\nobjects quickly while maintaining sufficient accuracy compared to other\nmethods. Manipulation and locomotion experiments were also carried out to show\nthe benefit of using the estimated inertia parameters from control perspective.\n","authors":["Donghoon Baek","Bo Peng","Saurabh Gupta","Joao Ramos"],"pdf_url":"https://arxiv.org/pdf/2309.09810v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10948v1","updated":"2024-03-16T15:29:40Z","published":"2024-03-16T15:29:40Z","title":"Real-to-Sim Adaptation via High-Fidelity Simulation to Control a\n  Wheeled-Humanoid Robot with Unknown Dynamics","summary":"  Model-based controllers using a linearized model around the system's\nequilibrium point is a common approach in the control of a wheeled humanoid due\nto their less computational load and ease of stability analysis. However,\ncontrolling a wheeled humanoid robot while it lifts an unknown object presents\nsignificant challenges, primarily due to the lack of knowledge in object\ndynamics. This paper presents a framework designed for predicting the new\nequilibrium point explicitly to control a wheeled-legged robot with unknown\ndynamics. We estimated the total mass and center of mass of the system from its\nresponse to initially unknown dynamics, then calculated the new equilibrium\npoint accordingly. To avoid using additional sensors (e.g., force torque\nsensor) and reduce the effort of obtaining expensive real data, a data-driven\napproach is utilized with a novel real-to-sim adaptation. A more accurate\nnonlinear dynamics model, offering a closer representation of real-world\nphysics, is injected into a rigid-body simulation for real-to-sim adaptation.\nThe nonlinear dynamics model parameters were optimized using Particle Swarm\nOptimization. The efficacy of this framework was validated on a physical\nwheeled inverted pendulum, a simplified model of a wheeled-legged robot. The\nexperimental results indicate that employing a more precise analytical model\nwith optimized parameters significantly reduces the gap between simulation and\nreality, thus improving the efficiency of a model-based controller in\ncontrolling a wheeled robot with unknown dynamics.\n","authors":["Donghoon Baek","Youngwoo Sim","Amartya Purushottam","Saurabh Gupta","Joao Ramos"],"pdf_url":"https://arxiv.org/pdf/2403.10948v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06171v2","updated":"2024-03-16T15:06:07Z","published":"2023-10-09T21:49:48Z","title":"Memory-Consistent Neural Networks for Imitation Learning","summary":"  Imitation learning considerably simplifies policy synthesis compared to\nalternative approaches by exploiting access to expert demonstrations. For such\nimitation policies, errors away from the training samples are particularly\ncritical. Even rare slip-ups in the policy action outputs can compound quickly\nover time, since they lead to unfamiliar future states where the policy is\nstill more likely to err, eventually causing task failures. We revisit simple\nsupervised ``behavior cloning'' for conveniently training the policy from\nnothing more than pre-recorded demonstrations, but carefully design the model\nclass to counter the compounding error phenomenon. Our ``memory-consistent\nneural network'' (MCNN) outputs are hard-constrained to stay within clearly\nspecified permissible regions anchored to prototypical ``memory'' training\nsamples. We provide a guaranteed upper bound for the sub-optimality gap induced\nby MCNN policies. Using MCNNs on 10 imitation learning tasks, with MLP,\nTransformer, and Diffusion backbones, spanning dexterous robotic manipulation\nand driving, proprioceptive inputs and visual inputs, and varying sizes and\ntypes of demonstration data, we find large and consistent gains in performance,\nvalidating that MCNNs are better-suited than vanilla deep neural networks for\nimitation learning applications. Website:\nhttps://sites.google.com/view/mcnn-imitation\n","authors":["Kaustubh Sridhar","Souradeep Dutta","Dinesh Jayaraman","James Weimer","Insup Lee"],"pdf_url":"https://arxiv.org/pdf/2310.06171v2.pdf","comment":"ICLR 2024. 26 pages (9 main pages)"},{"id":"http://arxiv.org/abs/2403.10940v1","updated":"2024-03-16T14:52:26Z","published":"2024-03-16T14:52:26Z","title":"ViSaRL: Visual Reinforcement Learning Guided by Human Saliency","summary":"  Training robots to perform complex control tasks from high-dimensional pixel\ninput using reinforcement learning (RL) is sample-inefficient, because image\nobservations are comprised primarily of task-irrelevant information. By\ncontrast, humans are able to visually attend to task-relevant objects and\nareas. Based on this insight, we introduce Visual Saliency-Guided Reinforcement\nLearning (ViSaRL). Using ViSaRL to learn visual representations significantly\nimproves the success rate, sample efficiency, and generalization of an RL agent\non diverse tasks including DeepMind Control benchmark, robot manipulation in\nsimulation and on a real robot. We present approaches for incorporating\nsaliency into both CNN and Transformer-based encoders. We show that visual\nrepresentations learned using ViSaRL are robust to various sources of visual\nperturbations including perceptual noise and scene variations. ViSaRL nearly\ndoubles success rate on the real-robot tasks compared to the baseline which\ndoes not use saliency.\n","authors":["Anthony Liang","Jesse Thomason","Erdem Bıyık"],"pdf_url":"https://arxiv.org/pdf/2403.10940v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10934v1","updated":"2024-03-16T14:20:46Z","published":"2024-03-16T14:20:46Z","title":"Quaternion-Based Sliding Mode Control for Six Degrees of Freedom Flight\n  Control of Quadrotors","summary":"  Despite extensive research on sliding mode control (SMC) design for\nquadrotors, the existing approaches suffer from certain limitations. Euler\nangle-based SMC formulations suffer from poor performance in high-pitch or\n-roll maneuvers. Quaternion-based SMC approaches have unwinding issues and\ncomplex architecture. Coordinate-free methods are slow and only almost globally\nstable. This paper presents a new six degrees of freedom SMC flight controller\nto address the above limitations. We use a cascaded architecture with a\nposition controller in the outer loop and a quaternion-based attitude\ncontroller in the inner loop. The position controller generates the desired\ntrajectory for the attitude controller using a coordinate-free approach. The\nquaternion-based attitude controller uses the natural characteristics of the\nquaternion hypersphere, featuring a simple structure while providing global\nstability and avoiding unwinding issues. We compare our controller with three\nother common control methods conducting challenging maneuvers like flip-over\nand high-speed trajectory tracking in the presence of model uncertainties and\ndisturbances. Our controller consistently outperforms the benchmark approaches\nwith less control effort and actuator saturation, offering highly effective and\nefficient flight control.\n","authors":["Amin Yazdanshenas","Reza Faieghi"],"pdf_url":"https://arxiv.org/pdf/2403.10934v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10932v1","updated":"2024-03-16T14:13:08Z","published":"2024-03-16T14:13:08Z","title":"Learning-Based Design of Off-Policy Gaussian Controllers: Integrating\n  Model Predictive Control and Gaussian Process Regression","summary":"  This paper presents an off-policy Gaussian Predictive Control (GPC) framework\naimed at solving optimal control problems with a smaller computational\nfootprint, thereby facilitating real-time applicability while ensuring critical\nsafety considerations. The proposed controller imitates classical control\nmethodologies by modeling the optimization process through a Gaussian process\nand employs Gaussian Process Regression to learn from the Model Predictive\nControl (MPC) algorithm. Notably, the Gaussian Process setup does not\nincorporate a built-in model, enhancing its applicability to a broad range of\ncontrol problems. We applied this framework experimentally to a differential\ndrive mobile robot, tasking it with trajectory tracking and obstacle avoidance.\nLeveraging the off-policy aspect, the controller demonstrated adaptability to\ndiverse trajectories and obstacle behaviors. Simulation experiments confirmed\nthe effectiveness of the proposed GPC method, emphasizing its ability to learn\nthe dynamics of optimal control strategies. Consequently, our findings\nhighlight the significant potential of off-policy Gaussian Predictive Control\nin achieving real-time optimal control for handling of robotic systems in\nsafety-critical scenarios.\n","authors":["Shiva Kumar Tekumatla","Varun Gampa","Siavash Farzan"],"pdf_url":"https://arxiv.org/pdf/2403.10932v1.pdf","comment":"Accepted to ACC 2024. 8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.10924v1","updated":"2024-03-16T13:44:00Z","published":"2024-03-16T13:44:00Z","title":"PAAMP: Polytopic Action-Set And Motion Planning For Long Horizon Dynamic\n  Motion Planning via Mixed Integer Linear Programming","summary":"  Optimization methods for long-horizon, dynamically feasible motion planning\nin robotics tackle challenging non-convex and discontinuous optimization\nproblems. Traditional methods often falter due to the nonlinear characteristics\nof these problems. We introduce a technique that utilizes learned\nrepresentations of the system, known as Polytopic Action Sets, to efficiently\ncompute long-horizon trajectories. By employing a suitable sequence of\nPolytopic Action Sets, we transform the long-horizon dynamically feasible\nmotion planning problem into a Linear Program. This reformulation enables us to\naddress motion planning as a Mixed Integer Linear Program (MILP). We\ndemonstrate the effectiveness of a Polytopic Action-Set and Motion Planning\n(PAAMP) approach by identifying swing-up motions for a torque-constrained\npendulum within approximately 0.75 milliseconds. This approach is well-suited\nfor solving complex motion planning and long-horizon Constraint Satisfaction\nProblems (CSPs) in dynamic and underactuated systems such as legged and aerial\nrobots.\n","authors":["Akshay Jaitly","Siavash Farzan"],"pdf_url":"https://arxiv.org/pdf/2403.10924v1.pdf","comment":"8 pages, 10 figures, under review"},{"id":"http://arxiv.org/abs/2203.02511v4","updated":"2024-03-16T11:20:44Z","published":"2022-03-04T15:53:04Z","title":"Self-Supervised Learning for Joint Pushing and Grasping Policies in\n  Highly Cluttered Environments","summary":"  Robots often face situations where grasping a goal object is desirable but\nnot feasible due to other present objects preventing the grasp action. We\npresent a deep Reinforcement Learning approach to learn grasping and pushing\npolicies for manipulating a goal object in highly cluttered environments to\naddress this problem. In particular, a dual Reinforcement Learning model\napproach is proposed, which presents high resilience in handling complicated\nscenes, reaching an average of 98% task completion using primitive objects in a\nsimulation environment. To evaluate the performance of the proposed approach,\nwe performed two extensive sets of experiments in packed objects and a pile of\nobject scenarios with a total of 1000 test runs in simulation. Experimental\nresults showed that the proposed method worked very well in both scenarios and\noutperformed the recent state-of-the-art approaches. Demo video, trained\nmodels, and source code for the results reproducibility purpose are publicly\navailable. https://sites.google.com/view/pushandgrasp/home\n","authors":["Yongliang Wang","Kamal Mokhtar","Cock Heemskerk","Hamidreza Kasaei"],"pdf_url":"https://arxiv.org/pdf/2203.02511v4.pdf","comment":"This paper has been accepted for publication at the ICRA2024\n  conference"},{"id":"http://arxiv.org/abs/2403.10874v1","updated":"2024-03-16T09:53:22Z","published":"2024-03-16T09:53:22Z","title":"Robotic Task Success Evaluation Under Multi-modal Non-Parametric Object\n  Pose Uncertainty","summary":"  Accurate 6D object pose estimation is essential for various robotic tasks.\nUncertain pose estimates can lead to task failures; however, a certain degree\nof error in the pose estimates is often acceptable. Hence, by quantifying\nerrors in the object pose estimate and acceptable errors for task success,\nrobots can make informed decisions. This is a challenging problem as both the\nobject pose uncertainty and acceptable error for the robotic task are often\nmulti-modal and cannot be parameterized with commonly used uni-modal\ndistributions. In this paper, we introduce a framework for evaluating robotic\ntask success under object pose uncertainty, representing both the estimated\nerror space of the object pose and the acceptable error space for task success\nusing multi-modal non-parametric probability distributions. The proposed\nframework pre-computes the acceptable error space for task success using\ndynamic simulations and subsequently integrates the pre-computed acceptable\nerror space over the estimated error space of the object pose to predict the\nlikelihood of the task success. We evaluated the proposed framework on two\nmobile manipulation tasks. Our results show that by representing the estimated\nand the acceptable error space using multi-modal non-parametric distributions,\nwe achieve higher task success rates and fewer failures.\n","authors":["Lakshadeep Naik","Thorbjørn Mosekjær Iversen","Aljaz Kramberger","Norbert Krüger"],"pdf_url":"https://arxiv.org/pdf/2403.10874v1.pdf","comment":"Submitted to IROS 2024"},{"id":"http://arxiv.org/abs/2403.10855v1","updated":"2024-03-16T08:30:55Z","published":"2024-03-16T08:30:55Z","title":"Reinforcement Learning with Options","summary":"  The current thesis aims to explore the reinforcement learning field and build\non existing methods to produce improved ones to tackle the problem of learning\nin high-dimensional and complex environments. It addresses such goals by\ndecomposing learning tasks in a hierarchical fashion known as Hierarchical\nReinforcement Learning.\n  We start in the first chapter by getting familiar with the Markov Decision\nProcess framework and presenting some of its recent techniques that the\nfollowing chapters use. We then proceed to build our Hierarchical Policy\nlearning as an answer to the limitations of a single primitive policy. The\nhierarchy is composed of a manager agent at the top and employee agents at the\nlower level.\n  In the last chapter, which is the core of this thesis, we attempt to learn\nlower-level elements of the hierarchy independently of the manager level in\nwhat is known as the \"Eigenoption\". Based on the graph structure of the\nenvironment, Eigenoptions allow us to build agents that are aware of the\ngeometric and dynamic properties of the environment. Their decision-making has\na special property: it is invariant to symmetric transformations of the\nenvironment, allowing as a consequence to greatly reduce the complexity of the\nlearning task.\n","authors":["Ayoub Ghriss","Masashi Sugiyama","Alessandro Lazaric"],"pdf_url":"https://arxiv.org/pdf/2403.10855v1.pdf","comment":"Master Thesis 2018, MVA ENS Paris-Saclay, Tokyo RIKEN AIP"},{"id":"http://arxiv.org/abs/2403.10850v1","updated":"2024-03-16T08:10:23Z","published":"2024-03-16T08:10:23Z","title":"GAgent: An Adaptive Rigid-Soft Gripping Agent with Vision Language\n  Models for Complex Lighting Environments","summary":"  This paper introduces GAgent: an Gripping Agent designed for open-world\nenvironments that provides advanced cognitive abilities via VLM agents and\nflexible grasping abilities with variable stiffness soft grippers. GAgent\ncomprises three primary components - Prompt Engineer module, Visual-Language\nModel (VLM) core and Workflow module. These three modules enhance gripper\nsuccess rates by recognizing objects and materials and accurately estimating\ngrasp area even under challenging lighting conditions. As part of creativity,\nresearchers also created a bionic hybrid soft gripper with variable stiffness\ncapable of gripping heavy loads while still gently engaging objects. This\nintelligent agent, featuring VLM-based cognitive processing with bionic design,\nshows promise as it could potentially benefit UAVs in various scenarios.\n","authors":["Zhuowei Li","Miao Zhang","Xiaotian Lin","Meng Yin","Shuai Lu","Xueqian Wang"],"pdf_url":"https://arxiv.org/pdf/2403.10850v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.08865v2","updated":"2024-03-16T07:28:52Z","published":"2023-09-16T04:01:34Z","title":"ARTEMIS: AI-driven Robotic Triage Labeling and Emergency Medical\n  Information System","summary":"  Mass casualty incidents (MCIs) pose a significant challenge to emergency\nmedical services by overwhelming available resources and personnel. Effective\nvictim assessment is the key to minimizing casualties during such a crisis. We\nintroduce ARTEMIS, an AI-driven Robotic Triage Labeling and Emergency Medical\nInformation System, to aid first responders in MCI events. It leverages speech\nprocessing, natural language processing, and deep learning to help with acuity\nclassification. This is deployed on a quadruped that performs victim\nlocalization and preliminary injury severity assessment. First responders\naccess victim information through a Graphical User Interface that is updated in\nreal-time. To validate our proposed algorithmic triage protocol, we used the\nUnitree Go1 quadruped. The robot identifies humans, interacts with them, gets\nvitals and information, and assigns an acuity label. Simulations of an MCI in\nsoftware and a controlled environment outdoors were conducted. The system\nachieved a triage-level classification precision of over 74% on average and 99%\nfor the most critical victims, i.e. level 1 acuity, outperforming\nstate-of-the-art deep learning-based triage labeling systems. In this paper, we\nshowcase the potential of human-robot interaction in assisting medical\npersonnel in MCI events.\n","authors":["Revanth Krishna Senthilkumaran","Mridu Prashanth","Hrishikesh Viswanath","Sathvika Kotha","Kshitij Tiwari","Aniket Bera"],"pdf_url":"https://arxiv.org/pdf/2309.08865v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10840v1","updated":"2024-03-16T07:26:50Z","published":"2024-03-16T07:26:50Z","title":"MSI-NeRF: Linking Omni-Depth with View Synthesis through Multi-Sphere\n  Image aided Generalizable Neural Radiance Field","summary":"  Panoramic observation using fisheye cameras is significant in robot\nperception, reconstruction, and remote operation. However, panoramic images\nsynthesized by traditional methods lack depth information and can only provide\nthree degrees-of-freedom (3DoF) rotation rendering in virtual reality\napplications. To fully preserve and exploit the parallax information within the\noriginal fisheye cameras, we introduce MSI-NeRF, which combines deep learning\nomnidirectional depth estimation and novel view rendering. We first construct a\nmulti-sphere image as a cost volume through feature extraction and warping of\nthe input images. It is then processed by geometry and appearance decoders,\nrespectively. Unlike methods that regress depth maps directly, we further build\nan implicit radiance field using spatial points and interpolated 3D feature\nvectors as input. In this way, we can simultaneously realize omnidirectional\ndepth estimation and 6DoF view synthesis. Our method is trained in a\nsemi-self-supervised manner. It does not require target view images and only\nuses depth data for supervision. Our network has the generalization ability to\nreconstruct unknown scenes efficiently using only four images. Experimental\nresults show that our method outperforms existing methods in depth estimation\nand novel view synthesis tasks.\n","authors":["Dongyu Yan","Guanyu Huang","Fengyu Quan","Haoyao Chen"],"pdf_url":"https://arxiv.org/pdf/2403.10840v1.pdf","comment":"8 pages, 7 figures, Submitted to IEEE/RSJ International Conference on\n  Intelligent Robots and Systems 2024"},{"id":"http://arxiv.org/abs/2401.08948v2","updated":"2024-03-16T07:00:52Z","published":"2024-01-17T03:50:16Z","title":"PINSAT: Parallelized Interleaving of Graph Search and Trajectory\n  Optimization for Kinodynamic Motion Planning","summary":"  Trajectory optimization is a widely used technique in robot motion planning\nfor letting the dynamics and constraints on the system shape and synthesize\ncomplex behaviors. Several previous works have shown its benefits in\nhigh-dimensional continuous state spaces and under differential constraints.\nHowever, long time horizons and planning around obstacles in non-convex spaces\npose challenges in guaranteeing convergence or finding optimal solutions. As a\nresult, discrete graph search planners and sampling-based planers are preferred\nwhen facing obstacle-cluttered environments. A recently developed algorithm\ncalled INSAT effectively combines graph search in the low-dimensional subspace\nand trajectory optimization in the full-dimensional space for global\nkinodynamic planning over long horizons. Although INSAT successfully reasoned\nabout and solved complex planning problems, the numerous expensive calls to an\noptimizer resulted in large planning times, thereby limiting its practical use.\nInspired by the recent work on edge-based parallel graph search, we present\nPINSAT, which introduces systematic parallelization in INSAT to achieve lower\nplanning times and higher success rates, while maintaining significantly lower\ncosts over relevant baselines. We demonstrate PINSAT by evaluating it on 6 DoF\nkinodynamic manipulation planning with obstacles.\n","authors":["Ramkumar Natarajan","Shohin Mukherjee","Howie Choset","Maxim Likhachev"],"pdf_url":"https://arxiv.org/pdf/2401.08948v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2403.10833v1","updated":"2024-03-16T06:56:32Z","published":"2024-03-16T06:56:32Z","title":"Deep Reinforcement Learning-based Large-scale Robot Exploration","summary":"  In this work, we propose a deep reinforcement learning (DRL) based reactive\nplanner to solve large-scale Lidar-based autonomous robot exploration problems\nin 2D action space. Our DRL-based planner allows the agent to reactively plan\nits exploration path by making implicit predictions about unknown areas, based\non a learned estimation of the underlying transition model of the environment.\nTo this end, our approach relies on learned attention mechanisms for their\npowerful ability to capture long-term dependencies at different spatial scales\nto reason about the robot's entire belief over known areas. Our approach relies\non ground truth information (i.e., privileged learning) to guide the\nenvironment estimation during training, as well as on a graph rarefaction\nalgorithm, which allows models trained in small-scale environments to scale to\nlarge-scale ones. Simulation results show that our model exhibits better\nexploration efficiency (12% in path length, 6% in makespan) and lower planning\ntime (60%) than the state-of-the-art planners in a 130m x 100m benchmark\nscenario. We also validate our learned model on hardware.\n","authors":["Yuhong Cao","Rui Zhao","Yizhuo Wang","Bairan Xiang","Guillaume Sartoretti"],"pdf_url":"https://arxiv.org/pdf/2403.10833v1.pdf","comment":"\\c{opyright} 20XX IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2401.08022v3","updated":"2024-03-16T06:28:41Z","published":"2024-01-16T00:19:40Z","title":"Preprocessing-based Kinodynamic Motion Planning Framework for\n  Intercepting Projectiles using a Robot Manipulator","summary":"  We are interested in studying sports with robots and starting with the\nproblem of intercepting a projectile moving toward a robot manipulator equipped\nwith a shield. To successfully perform this task, the robot needs to (i) detect\nthe incoming projectile, (ii) predict the projectile's future motion, (iii)\nplan a minimum-time rapid trajectory that can evade obstacles and intercept the\nprojectile, and (iv) execute the planned trajectory. These four steps must be\nperformed under the manipulator's dynamic limits and extreme time constraints\n(<350ms in our setting) to successfully intercept the projectile. In addition,\nwe want these trajectories to be smooth to reduce the robot's joint torques and\nthe impulse on the platform on which it is mounted. To this end, we propose a\nkinodynamic motion planning framework that preprocesses smooth trajectories\noffline to allow real-time collision-free executions online. We present an\nend-to-end pipeline along with our planning framework, including perception,\nprediction, and execution modules. We evaluate our framework experimentally in\nsimulation and show that it has a higher blocking success rate than the\nbaselines. Further, we deploy our pipeline on a robotic system comprising an\nindustrial arm (ABB IRB-1600) and an onboard stereo camera (ZED 2i), which\nachieves a 78% success rate in projectile interceptions.\n","authors":["Ramkumar Natarajan","Hanlan Yang","Qintong Xie","Yash Oza","Manash Pratim Das","Fahad Islam","Muhammad Suhail Saleem","Howie Choset","Maxim Likhachev"],"pdf_url":"https://arxiv.org/pdf/2401.08022v3.pdf","comment":"Proceedings of the IEEE International Conference on Robotics and\n  Automation (ICRA) 2024"},{"id":"http://arxiv.org/abs/2310.00397v2","updated":"2024-03-16T06:21:55Z","published":"2023-09-30T14:39:54Z","title":"Powered Descent Guidance via First-Order Optimization with Expansive\n  Projection","summary":"  This paper introduces a first-order method for solving optimal powered\ndescent guidance (PDG) problems, that directly handles the nonconvex\nconstraints associated with the maximum and minimum thrust bounds with varying\nmass and the pointing angle constraints on thrust vectors. This issue has been\nconventionally circumvented via lossless convexification (LCvx), which lifts a\nnonconvex feasible set to a higher-dimensional convex set, and via linear\napproximation of another nonconvex feasible set defined by exponential\nfunctions. However, this approach sometimes results in an infeasible solution\nwhen the solution obtained from the higher-dimensional space is projected back\nto the original space, especially when the problem involves a nonoptimal time\nof flight. Additionally, the Taylor series approximation introduces an\napproximation error that grows with both flight time and deviation from the\nreference trajectory. In this paper, we introduce a first-order approach that\nmakes use of orthogonal projections onto nonconvex sets, allowing expansive\nprojection (ExProj). We show that 1) this approach produces a feasible solution\nwith better performance even for the nonoptimal time of flight cases for which\nconventional techniques fail to generate achievable trajectories and 2) the\nproposed method compensates for the linearization error that arises from Taylor\nseries approximation, thus generating a superior guidance solution with less\nfuel consumption. We provide numerical examples featuring quantitative\nassessments to elucidate the effectiveness of the proposed methodology,\nparticularly in terms of fuel consumption and flight time. Our analysis\nsubstantiates the assertion that the proposed approach affords enhanced\nflexibility in devising viable trajectories for a diverse array of planetary\nsoft landing scenarios.\n","authors":["Jiwoo Choi","Jong-Han Kim"],"pdf_url":"https://arxiv.org/pdf/2310.00397v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10821v1","updated":"2024-03-16T06:14:04Z","published":"2024-03-16T06:14:04Z","title":"H3-Mapping: Quasi-Heterogeneous Feature Grids for Real-time Dense\n  Mapping Using Hierarchical Hybrid Representation","summary":"  In recent years, implicit online dense mapping methods have achieved\nhigh-quality reconstruction results, showcasing great potential in robotics,\nAR/VR, and digital twins applications. However, existing methods struggle with\nslow texture modeling which limits their real-time performance. To address\nthese limitations, we propose a NeRF-based dense mapping method that enables\nfaster and higher-quality reconstruction. To improve texture modeling, we\nintroduce quasi-heterogeneous feature grids, which inherit the fast querying\nability of uniform feature grids while adapting to varying levels of texture\ncomplexity. Besides, we present a gradient-aided coverage-maximizing strategy\nfor keyframe selection that enables the selected keyframes to exhibit a closer\nfocus on rich-textured regions and a broader scope for weak-textured areas.\nExperimental results demonstrate that our method surpasses existing NeRF-based\napproaches in texture fidelity, geometry accuracy, and time consumption. The\ncode for our method will be available at:\nhttps://github.com/SYSU-STAR/H3-Mapping.\n","authors":["Chenxing Jiang","Yiming Luo","Boyu Zhou","Shaojie Shen"],"pdf_url":"https://arxiv.org/pdf/2403.10821v1.pdf","comment":"8 pages, 11 figures, submitted to IEEE Robotics and Automation\n  Letters"},{"id":"http://arxiv.org/abs/2403.10814v1","updated":"2024-03-16T05:21:42Z","published":"2024-03-16T05:21:42Z","title":"DarkGS: Learning Neural Illumination and 3D Gaussians Relighting for\n  Robotic Exploration in the Dark","summary":"  Humans have the remarkable ability to construct consistent mental models of\nan environment, even under limited or varying levels of illumination. We wish\nto endow robots with this same capability. In this paper, we tackle the\nchallenge of constructing a photorealistic scene representation under poorly\nilluminated conditions and with a moving light source. We approach the task of\nmodeling illumination as a learning problem, and utilize the developed\nillumination model to aid in scene reconstruction. We introduce an innovative\nframework that uses a data-driven approach, Neural Light Simulators (NeLiS), to\nmodel and calibrate the camera-light system. Furthermore, we present DarkGS, a\nmethod that applies NeLiS to create a relightable 3D Gaussian scene model\ncapable of real-time, photorealistic rendering from novel viewpoints. We show\nthe applicability and robustness of our proposed simulator and system in a\nvariety of real-world environments.\n","authors":["Tianyi Zhang","Kaining Huang","Weiming Zhi","Matthew Johnson-Roberson"],"pdf_url":"https://arxiv.org/pdf/2403.10814v1.pdf","comment":"8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2312.03297v2","updated":"2024-03-16T05:01:26Z","published":"2023-12-06T05:36:55Z","title":"SoftMAC: Differentiable Soft Body Simulation with Forecast-based Contact\n  Model and Two-way Coupling with Articulated Rigid Bodies and Clothes","summary":"  Differentiable physics simulation provides an avenue to tackle previously\nintractable challenges through gradient-based optimization, thereby greatly\nimproving the efficiency of solving robotics-related problems. To apply\ndifferentiable simulation in diverse robotic manipulation scenarios, a key\nchallenge is to integrate various materials in a unified framework. We present\nSoftMAC, a differentiable simulation framework that couples soft bodies with\narticulated rigid bodies and clothes. SoftMAC simulates soft bodies with the\ncontinuum-mechanics-based Material Point Method (MPM). We provide a novel\nforecast-based contact model for MPM, which effectively reduces penetration\nwithout introducing other artifacts like unnatural rebound. To couple MPM\nparticles with deformable and non-volumetric clothes meshes, we also propose a\npenetration tracing algorithm that reconstructs the signed distance field in\nlocal area. Diverging from previous works, SoftMAC simulates the complete\ndynamics of each modality and incorporates them into a cohesive system with an\nexplicit and differentiable coupling mechanism. The feature empowers SoftMAC to\nhandle a broader spectrum of interactions, such as soft bodies serving as\nmanipulators and engaging with underactuated systems. We conducted\ncomprehensive experiments to validate the effectiveness and accuracy of the\nproposed differentiable pipeline in downstream robotic manipulation\napplications. Supplementary materials and videos are available on our project\nwebsite at https://sites.google.com/view/softmac.\n","authors":["Min Liu","Gang Yang","Siyuan Luo","Lin Shao"],"pdf_url":"https://arxiv.org/pdf/2312.03297v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10809v1","updated":"2024-03-16T04:57:23Z","published":"2024-03-16T04:57:23Z","title":"Efficient Trajectory Forecasting and Generation with Conditional Flow\n  Matching","summary":"  Trajectory prediction and generation are vital for autonomous robots\nnavigating dynamic environments. While prior research has typically focused on\neither prediction or generation, our approach unifies these tasks to provide a\nversatile framework and achieve state-of-the-art performance. Diffusion models,\nwhich are currently state-of-the-art for learned trajectory generation in\nlong-horizon planning and offline reinforcement learning tasks, rely on a\ncomputationally intensive iterative sampling process. This slow process impedes\nthe dynamic capabilities of robotic systems. In contrast, we introduce\nTrajectory Conditional Flow Matching (T-CFM), a novel data-driven approach that\nutilizes flow matching techniques to learn a solver time-varying vector field\nfor efficient and fast trajectory generation. We demonstrate the effectiveness\nof T-CFM on three separate tasks: adversarial tracking, real-world aircraft\ntrajectory forecasting, and long-horizon planning. Our model outperforms\nstate-of-the-art baselines with an increase of 35% in predictive accuracy and\n142% increase in planning performance. Notably, T-CFM achieves up to\n100$\\times$ speed-up compared to diffusion-based models without sacrificing\naccuracy, which is crucial for real-time decision making in robotics.\n","authors":["Sean Ye","Matthew Gombolay"],"pdf_url":"https://arxiv.org/pdf/2403.10809v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10795v1","updated":"2024-03-16T03:54:38Z","published":"2024-03-16T03:54:38Z","title":"From Words to Routes: Applying Large Language Models to Vehicle Routing","summary":"  LLMs have shown impressive progress in robotics (e.g., manipulation and\nnavigation) with natural language task descriptions. The success of LLMs in\nthese tasks leads us to wonder: What is the ability of LLMs to solve vehicle\nrouting problems (VRPs) with natural language task descriptions? In this work,\nwe study this question in three steps. First, we construct a dataset with 21\ntypes of single- or multi-vehicle routing problems. Second, we evaluate the\nperformance of LLMs across four basic prompt paradigms of text-to-code\ngeneration, each involving different types of text input. We find that the\nbasic prompt paradigm, which generates code directly from natural language task\ndescriptions, performs the best for GPT-4, achieving 56% feasibility, 40%\noptimality, and 53% efficiency. Third, based on the observation that LLMs may\nnot be able to provide correct solutions at the initial attempt, we propose a\nframework that enables LLMs to refine solutions through self-reflection,\nincluding self-debugging and self-verification. With GPT-4, our proposed\nframework achieves a 16% increase in feasibility, a 7% increase in optimality,\nand a 15% increase in efficiency. Moreover, we examine the sensitivity of GPT-4\nto task descriptions, specifically focusing on how its performance changes when\ncertain details are omitted from the task descriptions, yet the core meaning is\npreserved. Our findings reveal that such omissions lead to a notable decrease\nin performance: 4% in feasibility, 4% in optimality, and 5% in efficiency.\nWebsite: https://sites.google.com/view/words-to-routes/\n","authors":["Zhehui Huang","Guangyao Shi","Gaurav S. Sukhatme"],"pdf_url":"https://arxiv.org/pdf/2403.10795v1.pdf","comment":"Submitted to IEEE Robotics and Automation Society (IROS 2024)"},{"id":"http://arxiv.org/abs/2403.10794v1","updated":"2024-03-16T03:53:55Z","published":"2024-03-16T03:53:55Z","title":"Diffusion-Reinforcement Learning Hierarchical Motion Planning in\n  Adversarial Multi-agent Games","summary":"  Reinforcement Learning- (RL-)based motion planning has recently shown the\npotential to outperform traditional approaches from autonomous navigation to\nrobot manipulation. In this work, we focus on a motion planning task for an\nevasive target in a partially observable multi-agent adversarial\npursuit-evasion games (PEG). These pursuit-evasion problems are relevant to\nvarious applications, such as search and rescue operations and surveillance\nrobots, where robots must effectively plan their actions to gather intelligence\nor accomplish mission tasks while avoiding detection or capture themselves. We\npropose a hierarchical architecture that integrates a high-level diffusion\nmodel to plan global paths responsive to environment data while a low-level RL\nalgorithm reasons about evasive versus global path-following behavior. Our\napproach outperforms baselines by 51.2% by leveraging the diffusion model to\nguide the RL algorithm for more efficient exploration and improves the\nexplanability and predictability.\n","authors":["Zixuan Wu","Sean Ye","Manisha Natarajan","Matthew C. Gombolay"],"pdf_url":"https://arxiv.org/pdf/2403.10794v1.pdf","comment":"This work has been submitted to the IEEE Robotics and Automation\n  Letters (RA-L) for possible publication. Copyright may be transferred without\n  notice, after which this version may no longer be accessible"},{"id":"http://arxiv.org/abs/2403.10784v1","updated":"2024-03-16T03:07:28Z","published":"2024-03-16T03:07:28Z","title":"Identifying Optimal Launch Sites of High-Altitude Latex-Balloons using\n  Bayesian Optimisation for the Task of Station-Keeping","summary":"  Station-keeping tasks for high-altitude balloons show promise in areas such\nas ecological surveys, atmospheric analysis, and communication relays. However,\nidentifying the optimal time and position to launch a latex high-altitude\nballoon is still a challenging and multifaceted problem. For example, tasks\nsuch as forest fire tracking place geometric constraints on the launch location\nof the balloon. Furthermore, identifying the most optimal location also heavily\ndepends on atmospheric conditions. We first illustrate how reinforcement\nlearning-based controllers, frequently used for station-keeping tasks, can\nexploit the environment. This exploitation can degrade performance on unseen\nweather patterns and affect station-keeping performance when identifying an\noptimal launch configuration. Valuing all states equally in the region, the\nagent exploits the region's geometry by flying near the edge, leading to risky\nbehaviours. We propose a modification which compensates for this exploitation\nand finds this leads to, on average, higher steps within the target region on\nunseen data. Then, we illustrate how Bayesian Optimisation (BO) can identify\nthe optimal launch location to perform station-keeping tasks, maximising the\nexpected undiscounted return from a given rollout. We show BO can find this\nlaunch location in fewer steps compared to other optimisation methods. Results\nindicate that, surprisingly, the most optimal location to launch from is not\ncommonly within the target region. Please find further information about our\nproject at https://sites.google.com/view/bo-lauch-balloon/.\n","authors":["Jack Saunders","Sajad Saeedi","Adam Hartshorne","Binbin Xu","Özgur Şimşek","Alan Hunter","Wenbin Li"],"pdf_url":"https://arxiv.org/pdf/2403.10784v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10773v1","updated":"2024-03-16T02:22:10Z","published":"2024-03-16T02:22:10Z","title":"DPPE: Dense Pose Estimation in a Plenoxels Environment using Gradient\n  Approximation","summary":"  We present DPPE, a dense pose estimation algorithm that functions over a\nPlenoxels environment. Recent advances in neural radiance field techniques have\nshown that it is a powerful tool for environment representation. More recent\nneural rendering algorithms have significantly improved both training duration\nand rendering speed. Plenoxels introduced a fully-differentiable radiance field\ntechnique that uses Plenoptic volume elements contained in voxels for\nrendering, offering reduced training times and better rendering accuracy, while\nalso eliminating the neural net component. In this work, we introduce a 6-DoF\nmonocular RGB-only pose estimation procedure for Plenoxels, which seeks to\nrecover the ground truth camera pose after a perturbation. We employ a\nvariation on classical template matching techniques, using stochastic gradient\ndescent to optimize the pose by minimizing errors in re-rendering. In\nparticular, we examine an approach that takes advantage of the rapid rendering\nspeed of Plenoxels to numerically approximate part of the pose gradient, using\na central differencing technique. We show that such methods are effective in\npose estimation. Finally, we perform ablations over key components of the\nproblem space, with a particular focus on image subsampling and Plenoxel grid\nresolution. Project website: https://sites.google.com/view/dppe\n","authors":["Christopher Kolios","Yeganeh Bahoo","Sajad Saeedi"],"pdf_url":"https://arxiv.org/pdf/2403.10773v1.pdf","comment":"8 pages, 4 figures, conference"},{"id":"http://arxiv.org/abs/2310.03246v2","updated":"2024-03-16T02:19:58Z","published":"2023-10-05T01:31:45Z","title":"${\\tt MORALS}$: Analysis of High-Dimensional Robot Controllers via\n  Topological Tools in a Latent Space","summary":"  Estimating the region of attraction (${\\tt RoA}$) for a robot controller is\nessential for safe application and controller composition. Many existing\nmethods require a closed-form expression that limit applicability to\ndata-driven controllers. Methods that operate only over trajectory rollouts\ntend to be data-hungry. In prior work, we have demonstrated that topological\ntools based on ${\\it Morse Graphs}$ (directed acyclic graphs that\ncombinatorially represent the underlying nonlinear dynamics) offer\ndata-efficient ${\\tt RoA}$ estimation without needing an analytical model. They\nstruggle, however, with high-dimensional systems as they operate over a\nstate-space discretization. This paper presents ${\\it Mo}$rse Graph-aided\ndiscovery of ${\\it R}$egions of ${\\it A}$ttraction in a learned ${\\it L}$atent\n${\\it S}$pace (${\\tt MORALS}$). The approach combines auto-encoding neural\nnetworks with Morse Graphs. ${\\tt MORALS}$ shows promising predictive\ncapabilities in estimating attractors and their ${\\tt RoA}$s for data-driven\ncontrollers operating over high-dimensional systems, including a 67-dim\nhumanoid robot and a 96-dim 3-fingered manipulator. It first projects the\ndynamics of the controlled system into a learned latent space. Then, it\nconstructs a reduced form of Morse Graphs representing the bistability of the\nunderlying dynamics, i.e., detecting when the controller results in a desired\nversus an undesired behavior. The evaluation on high-dimensional robotic\ndatasets indicates data efficiency in ${\\tt RoA}$ estimation.\n","authors":["Ewerton R. Vieira","Aravind Sivaramakrishnan","Sumanth Tangirala","Edgar Granados","Konstantin Mischaikow","Kostas E. Bekris"],"pdf_url":"https://arxiv.org/pdf/2310.03246v2.pdf","comment":"The first two authors contributed equally to this paper"},{"id":"http://arxiv.org/abs/2403.10768v1","updated":"2024-03-16T02:14:07Z","published":"2024-03-16T02:14:07Z","title":"Task-Driven Manipulation with Reconfigurable Parallel Robots","summary":"  ReachBot, a proposed robotic platform, employs extendable booms as limbs for\nmobility in challenging environments, such as martian caves. When attached to\nthe environment, ReachBot acts as a parallel robot, with reconfiguration driven\nby the ability to detach and re-place the booms. This ability enables\nmanipulation-focused scientific objectives: for instance, through operating\ntools, or handling and transporting samples. To achieve these capabilities, we\ndevelop a two-part solution, optimizing for robustness against task uncertainty\nand stochastic failure modes. First, we present a mixed-integer stance planner\nto determine the positioning of ReachBot's booms to maximize the task wrench\nspace about the nominal point(s). Second, we present a convex tension planner\nto determine boom tensions for the desired task wrenches, accounting for the\nprobabilistic nature of microspine grasping. We demonstrate improvements in key\nrobustness metrics from the field of dexterous manipulation, and show a large\nincrease in the volume of the manipulation workspace. Finally, we employ\nMonte-Carlo simulation to validate the robustness of these methods,\ndemonstrating good performance across a range of randomized tasks and\nenvironments, and generalization to cable-driven morphologies. We make our code\navailable at our project webpage,\nhttps://stanfordasl.github.io/reachbot_manipulation/\n","authors":["Daniel Morton","Mark Cutkosky","Marco Pavone"],"pdf_url":"https://arxiv.org/pdf/2403.10768v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10762v1","updated":"2024-03-16T01:57:35Z","published":"2024-03-16T01:57:35Z","title":"NARRATE: Versatile Language Architecture for Optimal Control in Robotics","summary":"  The impressive capabilities of Large Language Models (LLMs) have led to\nvarious efforts to enable robots to be controlled through natural language\ninstructions, opening exciting possibilities for human-robot interaction The\ngoal is for the motor-control task to be performed accurately, efficiently and\nsafely while also enjoying the flexibility imparted by LLMs to specify and\nadjust the task through natural language. In this work, we demonstrate how a\ncareful layering of an LLM in combination with a Model Predictive Control (MPC)\nformulation allows for accurate and flexible robotic control via natural\nlanguage while taking into consideration safety constraints. In particular, we\nrely on the LLM to effectively frame constraints and objective functions as\nmathematical expressions, which are later used in the motor-control module via\nMPC. The transparency of the optimization formulation allows for\ninterpretability of the task and enables adjustments through human feedback. We\ndemonstrate the validity of our method through extensive experiments on\nlong-horizon reasoning, contact-rich, and multi-object interaction tasks. Our\nevaluations show that NARRATE outperforms current existing methods on these\nbenchmarks and effectively transfers to the real world on two different\nembodiments. Videos, Code and Prompts at narrate-mpc.github.io\n","authors":["Seif Ismail","Antonio Arbues","Ryan Cotterell","René Zurbrügg","Carmen Amo Alonso"],"pdf_url":"https://arxiv.org/pdf/2403.10762v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10761v1","updated":"2024-03-16T01:51:42Z","published":"2024-03-16T01:51:42Z","title":"Scheduling Drone and Mobile Charger via Hybrid-Action Deep Reinforcement\n  Learning","summary":"  Recently there has been a growing interest in industry and academia,\nregarding the use of wireless chargers to prolong the operational longevity of\nunmanned aerial vehicles (commonly knowns as drones). In this paper we consider\na charger-assisted drone application: a drone is deployed to observe a set\npoints of interest, while a charger can move to recharge the drone's battery.\nWe focus on the route and charging schedule of the drone and the mobile\ncharger, to obtain high observation utility with the shortest possible time,\nwhile ensuring the drone remains operational during task execution.\nEssentially, this proposed drone-charger scheduling problem is a multi-stage\ndecision-making process, in which the drone and the mobile charger act as two\nagents who cooperate to finish a task. The discrete-continuous hybrid action\nspace of the two agents poses a significant challenge in our problem. To\naddress this issue, we present a hybrid-action deep reinforcement learning\nframework, called HaDMC, which uses a standard policy learning algorithm to\ngenerate latent continuous actions. Motivated by representation learning, we\nspecifically design and train an action decoder. It involves two pipelines to\nconvert the latent continuous actions into original discrete and continuous\nactions, by which the drone and the charger can directly interact with\nenvironment. We embed a mutual learning scheme in model training, emphasizing\nthe collaborative rather than individual actions. We conduct extensive\nnumerical experiments to evaluate HaDMC and compare it with state-of-the-art\ndeep reinforcement learning approaches. The experimental results show the\neffectiveness and efficiency of our solution.\n","authors":["Jizhe Dou","Haotian Zhang","Guodong Sun"],"pdf_url":"https://arxiv.org/pdf/2403.10761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10760v1","updated":"2024-03-16T01:47:53Z","published":"2024-03-16T01:47:53Z","title":"CORN: Contact-based Object Representation for Nonprehensile Manipulation\n  of General Unseen Objects","summary":"  Nonprehensile manipulation is essential for manipulating objects that are too\nthin, large, or otherwise ungraspable in the wild. To sidestep the difficulty\nof contact modeling in conventional modeling-based approaches, reinforcement\nlearning (RL) has recently emerged as a promising alternative. However,\nprevious RL approaches either lack the ability to generalize over diverse\nobject shapes, or use simple action primitives that limit the diversity of\nrobot motions. Furthermore, using RL over diverse object geometry is\nchallenging due to the high cost of training a policy that takes in\nhigh-dimensional sensory inputs. We propose a novel contact-based object\nrepresentation and pretraining pipeline to tackle this. To enable massively\nparallel training, we leverage a lightweight patch-based transformer\narchitecture for our encoder that processes point clouds, thus scaling our\ntraining across thousands of environments. Compared to learning from scratch,\nor other shape representation baselines, our representation facilitates both\ntime- and data-efficient learning. We validate the efficacy of our overall\nsystem by zero-shot transferring the trained policy to novel real-world\nobjects. Code and videos are available at\nhttps://sites.google.com/view/contact-non-prehensile.\n","authors":["Yoonyoung Cho","Junhyek Han","Yoontae Cho","Beomjoon Kim"],"pdf_url":"https://arxiv.org/pdf/2403.10760v1.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.10759v1","updated":"2024-03-16T01:47:12Z","published":"2024-03-16T01:47:12Z","title":"Fully Distributed Cooperative Multi-agent Underwater Obstacle Avoidance\n  Under Dog Walking Paradigm","summary":"  Navigation in cluttered underwater environments is challenging, especially\nwhen there are constraints on communication and self-localisation. Part of the\nfully distributed underwater navigation problem has been resolved by\nintroducing multi-agent robot teams, however when the environment becomes\ncluttered, the problem remains unresolved. In this paper, we first studied the\nconnection between everyday activity of dog walking and the cooperative\nunderwater obstacle avoidance problem. Inspired by this analogy, we propose a\nnovel dog walking paradigm and implement it in a multi-agent underwater system.\nSimulations were conducted across various scenarios, with performance\nbenchmarked against traditional methods utilising Image-Based Visual Servoing\nin a multi-agent setup. Results indicate that our dog walking-inspired paradigm\nsignificantly enhances cooperative behavior among agents and outperforms the\nexisting approach in navigating through obstacles.\n","authors":["Kanzhong Yao","Ognjen Marjanovic","Simon Watson"],"pdf_url":"https://arxiv.org/pdf/2403.10759v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2403.10745v1","updated":"2024-03-16T00:34:00Z","published":"2024-03-16T00:34:00Z","title":"iDb-RRT: Sampling-based Kinodynamic Motion Planning with Motion\n  Primitives and Trajectory Optimization","summary":"  Rapidly-exploring Random Trees (RRT) and its variations have emerged as a\nrobust and efficient tool for finding collision-free paths in robotic systems.\nHowever, adding dynamic constraints makes the motion planning problem\nsignificantly harder, as it requires solving two-value boundary problems\n(computationally expensive) or propagating random control inputs\n(uninformative). Alternatively, Iterative Discontinuity Bounded A* (iDb-A*),\nintroduced in our previous study, combines search and optimization iteratively.\nThe search step connects short trajectories (motion primitives) while allowing\na bounded discontinuity between the motion primitives, which is later repaired\nin the trajectory optimization step.\n  Building upon these foundations, in this paper, we present iDb-RRT, a\nsampling-based kinodynamic motion planning algorithm that combines motion\nprimitives and trajectory optimization within the RRT framework. iDb-RRT is\nprobabilistically complete and can be implemented in forward or bidirectional\nmode. We have tested our algorithm across a benchmark suite comprising 30\nproblems, spanning 8 different systems, and shown that iDb-RRT can find\nsolutions up to 10x faster than previous methods, especially in complex\nscenarios that require long trajectories or involve navigating through narrow\npassages.\n","authors":["Joaquim Ortiz-Haro","Wolfgang Hönig","Valentin N. Hartmann","Marc Toussaint","Ludovic Righetti"],"pdf_url":"https://arxiv.org/pdf/2403.10745v1.pdf","comment":"Preprint, submitted to IROS 2024"}]},"2024-03-20T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2403.12245v2","updated":"2024-03-20T01:48:46Z","published":"2024-03-18T20:51:08Z","title":"Improving Out-of-Distribution Generalization of Learned Dynamics by\n  Learning Pseudometrics and Constraint Manifolds","summary":"  We propose a method for improving the prediction accuracy of learned robot\ndynamics models on out-of-distribution (OOD) states. We achieve this by\nleveraging two key sources of structure often present in robot dynamics: 1)\nsparsity, i.e., some components of the state may not affect the dynamics, and\n2) physical limits on the set of possible motions, in the form of nonholonomic\nconstraints. Crucially, we do not assume this structure is known a priori, and\ninstead learn it from data. We use contrastive learning to obtain a distance\npseudometric that uncovers the sparsity pattern in the dynamics, and use it to\nreduce the input space when learning the dynamics. We then learn the unknown\nconstraint manifold by approximating the normal space of possible motions from\nthe data, which we use to train a Gaussian process (GP) representation of the\nconstraint manifold. We evaluate our approach on a physical differential-drive\nrobot and a simulated quadrotor, showing improved prediction accuracy on OOD\ndata relative to baselines.\n","authors":["Yating Lin","Glen Chou","Dmitry Berenson"],"pdf_url":"https://arxiv.org/pdf/2403.12245v2.pdf","comment":"Accept to ICRA 2024, 6 pages + references"},{"id":"http://arxiv.org/abs/2403.13801v1","updated":"2024-03-20T17:58:12Z","published":"2024-03-20T17:58:12Z","title":"Natural Language as Polices: Reasoning for Coordinate-Level Embodied\n  Control with LLMs","summary":"  We demonstrate experimental results with LLMs that address robotics action\nplanning problems. Recently, LLMs have been applied in robotics action\nplanning, particularly using a code generation approach that converts complex\nhigh-level instructions into mid-level policy codes. In contrast, our approach\nacquires text descriptions of the task and scene objects, then formulates\naction planning through natural language reasoning, and outputs coordinate\nlevel control commands, thus reducing the necessity for intermediate\nrepresentation code as policies. Our approach is evaluated on a multi-modal\nprompt simulation benchmark, demonstrating that our prompt engineering\nexperiments with natural language reasoning significantly enhance success rates\ncompared to its absence. Furthermore, our approach illustrates the potential\nfor natural language descriptions to transfer robotics skills from known tasks\nto previously unseen tasks.\n","authors":["Yusuke Mikami","Andrew Melnik","Jun Miura","Ville Hautamäki"],"pdf_url":"https://arxiv.org/pdf/2403.13801v1.pdf","comment":"8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2403.13783v1","updated":"2024-03-20T17:44:33Z","published":"2024-03-20T17:44:33Z","title":"A Convex Formulation of Frictional Contact for the Material Point Method\n  and Rigid Bodies","summary":"  In this paper, we introduce a novel convex formulation that seamlessly\nintegrates the Material Point Method (MPM) with articulated rigid body dynamics\nin frictional contact scenarios. We extend the linear corotational hyperelastic\nmodel into the realm of elastoplasticity and include an efficient return\nmapping algorithm. This approach is particularly effective for MPM simulations\ninvolving significant deformation and topology changes, while preserving the\nconvexity of the optimization problem. Our method ensures global convergence,\nenabling the use of large simulation time steps without compromising\nrobustness. We have validated our approach through rigorous testing and\nperformance evaluations, highlighting its superior capabilities in managing\ncomplex simulations relevant to robotics. Compared to previous MPM based\nrobotic simulators, our method significantly improves the stability of contact\nresolution -- a critical factor in robot manipulation tasks. We make our method\navailable in the open-source robotics toolkit, Drake.\n","authors":["Zeshun Zong","Chenfanfu Jiang","Xuchen Han"],"pdf_url":"https://arxiv.org/pdf/2403.13783v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13778v1","updated":"2024-03-20T17:41:35Z","published":"2024-03-20T17:41:35Z","title":"Certified Human Trajectory Prediction","summary":"  Trajectory prediction plays an essential role in autonomous vehicles. While\nnumerous strategies have been developed to enhance the robustness of trajectory\nprediction models, these methods are predominantly heuristic and do not offer\nguaranteed robustness against adversarial attacks and noisy observations. In\nthis work, we propose a certification approach tailored for the task of\ntrajectory prediction. To this end, we address the inherent challenges\nassociated with trajectory prediction, including unbounded outputs, and\nmutli-modality, resulting in a model that provides guaranteed robustness.\nFurthermore, we integrate a denoiser into our method to further improve the\nperformance. Through comprehensive evaluations, we demonstrate the\neffectiveness of the proposed technique across various baselines and using\nstandard trajectory prediction datasets. The code will be made available\nonline: https://s-attack.github.io/\n","authors":["Mohammadhossein Bahari","Saeed Saadatnejad","Amirhossein Asgari Farsangi","Seyed-Mohsen Moosavi-Dezfooli","Alexandre Alahi"],"pdf_url":"https://arxiv.org/pdf/2403.13778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13777v1","updated":"2024-03-20T17:41:21Z","published":"2024-03-20T17:41:21Z","title":"Embedding Pose Graph, Enabling 3D Foundation Model Capabilities with a\n  Compact Representation","summary":"  This paper presents the Embedding Pose Graph (EPG), an innovative method that\ncombines the strengths of foundation models with a simple 3D representation\nsuitable for robotics applications. Addressing the need for efficient spatial\nunderstanding in robotics, EPG provides a compact yet powerful approach by\nattaching foundation model features to the nodes of a pose graph. Unlike\ntraditional methods that rely on bulky data formats like voxel grids or point\nclouds, EPG is lightweight and scalable. It facilitates a range of robotic\ntasks, including open-vocabulary querying, disambiguation, image-based\nquerying, language-directed navigation, and re-localization in 3D environments.\nWe showcase the effectiveness of EPG in handling these tasks, demonstrating its\ncapacity to improve how robots interact with and navigate through complex\nspaces. Through both qualitative and quantitative assessments, we illustrate\nEPG's strong performance and its ability to outperform existing methods in\nre-localization. Our work introduces a crucial step forward in enabling robots\nto efficiently understand and operate within large-scale 3D spaces.\n","authors":["Hugues Thomas","Jian Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.13777v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.06192v5","updated":"2024-03-20T17:36:07Z","published":"2023-06-09T18:45:15Z","title":"Ada-NAV: Adaptive Trajectory Length-Based Sample Efficient Policy\n  Learning for Robotic Navigation","summary":"  Trajectory length stands as a crucial hyperparameter within reinforcement\nlearning (RL) algorithms, significantly contributing to the sample inefficiency\nin robotics applications. Motivated by the pivotal role trajectory length plays\nin the training process, we introduce Ada-NAV, a novel adaptive trajectory\nlength scheme designed to enhance the training sample efficiency of RL\nalgorithms in robotic navigation tasks. Unlike traditional approaches that\ntreat trajectory length as a fixed hyperparameter, we propose to dynamically\nadjust it based on the entropy of the underlying navigation policy.\nInterestingly, Ada-NAV can be applied to both existing on-policy and off-policy\nRL methods, which we demonstrate by empirically validating its efficacy on\nthree popular RL methods: REINFORCE, Proximal Policy Optimization (PPO), and\nSoft Actor-Critic (SAC). We demonstrate through simulated and real-world\nrobotic experiments that Ada-NAV outperforms conventional methods that employ\nconstant or randomly sampled trajectory lengths. Specifically, for a fixed\nsample budget, Ada-NAV achieves an 18\\% increase in navigation success rate, a\n20-38\\% reduction in navigation path length, and a 9.32\\% decrease in elevation\ncosts. Furthermore, we showcase the versatility of Ada-NAV by integrating it\nwith the Clearpath Husky robot, illustrating its applicability in complex\noutdoor environments.\n","authors":["Bhrij Patel","Kasun Weerakoon","Wesley A. Suttle","Alec Koppel","Brian M. Sadler","Tianyi Zhou","Amrit Singh Bedi","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2306.06192v5.pdf","comment":"11 pages, 9 figures, 2 tables"},{"id":"http://arxiv.org/abs/2403.13730v1","updated":"2024-03-20T16:39:48Z","published":"2024-03-20T16:39:48Z","title":"Projection-free computation of robust controllable sets with constrained\n  zonotopes","summary":"  We study the problem of computing robust controllable sets for discrete-time\nlinear systems with additive uncertainty. We propose a tractable and scalable\napproach to inner- and outer-approximate robust controllable sets using\nconstrained zonotopes, when the additive uncertainty set is a symmetric,\nconvex, and compact set. Our least-squares-based approach uses novel\nclosed-form approximations of the Pontryagin difference between a constrained\nzonotopic minuend and a symmetric, convex, and compact subtrahend. Unlike\nexisting approaches, our approach does not rely on convex optimization solvers,\nand is projection-free for ellipsoidal and zonotopic uncertainty sets. We also\npropose a least-squares-based approach to compute a convex, polyhedral\nouter-approximation to constrained zonotopes, and characterize sufficient\nconditions under which all these approximations are exact. We demonstrate the\ncomputational efficiency and scalability of our approach in several case\nstudies, including the design of abort-safe rendezvous trajectories for a\nspacecraft in near-rectilinear halo orbit under uncertainty. Our approach can\ninner-approximate a 20-step robust controllable set for a 100-dimensional\nlinear system in under 15 seconds on a standard computer.\n","authors":["Abraham P. Vinod","Avishai Weiss","Stefano Di Cairano"],"pdf_url":"https://arxiv.org/pdf/2403.13730v1.pdf","comment":"22 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.13729v1","updated":"2024-03-20T16:39:17Z","published":"2024-03-20T16:39:17Z","title":"Reinforcement Learning for Online Testing of Autonomous Driving Systems:\n  a Replication and Extension Study","summary":"  In a recent study, Reinforcement Learning (RL) used in combination with\nmany-objective search, has been shown to outperform alternative techniques\n(random search and many-objective search) for online testing of Deep Neural\nNetwork-enabled systems. The empirical evaluation of these techniques was\nconducted on a state-of-the-art Autonomous Driving System (ADS). This work is a\nreplication and extension of that empirical study. Our replication shows that\nRL does not outperform pure random test generation in a comparison conducted\nunder the same settings of the original study, but with no confounding factor\ncoming from the way collisions are measured. Our extension aims at eliminating\nsome of the possible reasons for the poor performance of RL observed in our\nreplication: (1) the presence of reward components providing contrasting or\nuseless feedback to the RL agent; (2) the usage of an RL algorithm (Q-learning)\nwhich requires discretization of an intrinsically continuous state space.\nResults show that our new RL agent is able to converge to an effective policy\nthat outperforms random testing. Results also highlight other possible\nimprovements, which open to further investigations on how to best leverage RL\nfor online ADS testing.\n","authors":["Luca Giamattei","Matteo Biagiola","Roberto Pietrantuono","Stefano Russo","Paolo Tonella"],"pdf_url":"https://arxiv.org/pdf/2403.13729v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13714v1","updated":"2024-03-20T16:20:54Z","published":"2024-03-20T16:20:54Z","title":"DBA-Fusion: Tightly Integrating Deep Dense Visual Bundle Adjustment with\n  Multiple Sensors for Large-Scale Localization and Mapping","summary":"  Visual simultaneous localization and mapping (VSLAM) has broad applications,\nwith state-of-the-art methods leveraging deep neural networks for better\nrobustness and applicability. However, there is a lack of research in fusing\nthese learning-based methods with multi-sensor information, which could be\nindispensable to push related applications to large-scale and complex\nscenarios. In this paper, we tightly integrate the trainable deep dense bundle\nadjustment (DBA) with multi-sensor information through a factor graph. In the\nframework, recurrent optical flow and DBA are performed among sequential\nimages. The Hessian information derived from DBA is fed into a generic factor\ngraph for multi-sensor fusion, which employs a sliding window and supports\nprobabilistic marginalization. A pipeline for visual-inertial integration is\nfirstly developed, which provides the minimum ability of metric-scale\nlocalization and mapping. Furthermore, other sensors (e.g., global navigation\nsatellite system) are integrated for driftless and geo-referencing\nfunctionality. Extensive tests are conducted on both public datasets and\nself-collected datasets. The results validate the superior localization\nperformance of our approach, which enables real-time dense mapping in\nlarge-scale environments. The code has been made open-source\n(https://github.com/GREAT-WHU/DBA-Fusion).\n","authors":["Yuxuan Zhou","Xingxing Li","Shengyu Li","Xuanbin Wang","Shaoquan Feng","Yuxuan Tan"],"pdf_url":"https://arxiv.org/pdf/2403.13714v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05840v2","updated":"2024-03-20T16:18:26Z","published":"2024-02-08T17:17:06Z","title":"uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties","summary":"  The availability of a robust map-based localization system is essential for\nthe operation of many autonomously navigating vehicles. Since uncertainty is an\ninevitable part of perception, it is beneficial for the robustness of the robot\nto consider it in typical downstream tasks of navigation stacks. In particular\nlocalization and mapping methods, which in modern systems often employ\nconvolutional neural networks (CNNs) for perception tasks, require proper\nuncertainty estimates. In this work, we present uncertainty-aware Panoptic\nLocalization and Mapping (uPLAM), which employs pixel-wise uncertainty\nestimates for panoptic CNNs as a bridge to fuse modern perception with\nclassical probabilistic localization and mapping approaches. Beyond the\nperception, we introduce an uncertainty-based map aggregation technique to\ncreate accurate panoptic maps, containing surface semantics and landmark\ninstances. Moreover, we provide cell-wise map uncertainties, and present a\nparticle filter-based localization method that employs perception\nuncertainties. Extensive evaluations show that our proposed incorporation of\nuncertainties leads to more accurate maps with reliable uncertainty estimates\nand improved localization accuracy. Additionally, we present the Freiburg\nPanoptic Driving dataset for evaluating panoptic mapping and localization\nmethods. We make our code and dataset available at:\n\\url{http://uplam.cs.uni-freiburg.de}\n","authors":["Kshitij Sirohi","Daniel Büscher","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2402.05840v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13701v1","updated":"2024-03-20T16:06:01Z","published":"2024-03-20T16:06:01Z","title":"What Matters for Active Texture Recognition With Vision-Based Tactile\n  Sensors","summary":"  This paper explores active sensing strategies that employ vision-based\ntactile sensors for robotic perception and classification of fabric textures.\nWe formalize the active sampling problem in the context of tactile fabric\nrecognition and provide an implementation of information-theoretic exploration\nstrategies based on minimizing predictive entropy and variance of probabilistic\nmodels. Through ablation studies and human experiments, we investigate which\ncomponents are crucial for quick and reliable texture recognition. Along with\nthe active sampling strategies, we evaluate neural network architectures,\nrepresentations of uncertainty, influence of data augmentation, and dataset\nvariability. By evaluating our method on a previously published Active Clothing\nPerception Dataset and on a real robotic system, we establish that the choice\nof the active exploration strategy has only a minor influence on the\nrecognition accuracy, whereas data augmentation and dropout rate play a\nsignificantly larger role. In a comparison study, while humans achieve 66.9%\nrecognition accuracy, our best approach reaches 90.0% in under 5 touches,\nhighlighting that vision-based tactile sensors are highly effective for fabric\ntexture recognition.\n","authors":["Alina Böhm","Tim Schneider","Boris Belousov","Alap Kshirsagar","Lisa Lin","Katja Doerschner","Knut Drewing","Constantin A. Rothkopf","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2403.13701v1.pdf","comment":"7 pages, 9 figures, accepted at 2024 IEEE International Conference on\n  Robotics and Automation (ICRA)"},{"id":"http://arxiv.org/abs/2403.13695v1","updated":"2024-03-20T15:57:44Z","published":"2024-03-20T15:57:44Z","title":"Loss Regularizing Robotic Terrain Classification","summary":"  Locomotion mechanics of legged robots are suitable when pacing through\ndifficult terrains. Recognising terrains for such robots are important to fully\nyoke the versatility of their movements. Consequently, robotic terrain\nclassification becomes significant to classify terrains in real time with high\naccuracy. The conventional classifiers suffer from overfitting problem, low\naccuracy problem, high variance problem, and not suitable for live dataset. On\nthe other hand, classifying a growing dataset is difficult for convolution\nbased terrain classification. Supervised recurrent models are also not\npractical for this classification. Further, the existing recurrent\narchitectures are still evolving to improve accuracy of terrain classification\nbased on live variable-length sensory data collected from legged robots. This\npaper proposes a new semi-supervised method for terrain classification of\nlegged robots, avoiding preprocessing of long variable-length dataset. The\nproposed method has a stacked Long Short-Term Memory architecture, including a\nnew loss regularization. The proposed method solves the existing problems and\nimproves accuracy. Comparison with the existing architectures show the\nimprovements.\n","authors":["Shakti Deo Kumar","Sudhanshu Tripathi","Krishna Ujjwal","Sarvada Sakshi Jha","Suddhasil De"],"pdf_url":"https://arxiv.org/pdf/2403.13695v1.pdf","comment":"Preliminary draft of the work published in IEEE conference 2023"},{"id":"http://arxiv.org/abs/2403.13683v1","updated":"2024-03-20T15:41:32Z","published":"2024-03-20T15:41:32Z","title":"DVMNet: Computing Relative Pose for Unseen Objects Beyond Hypotheses","summary":"  Determining the relative pose of an object between two images is pivotal to\nthe success of generalizable object pose estimation. Existing approaches\ntypically approximate the continuous pose representation with a large number of\ndiscrete pose hypotheses, which incurs a computationally expensive process of\nscoring each hypothesis at test time. By contrast, we present a Deep Voxel\nMatching Network (DVMNet) that eliminates the need for pose hypotheses and\ncomputes the relative object pose in a single pass. To this end, we map the two\ninput RGB images, reference and query, to their respective voxelized 3D\nrepresentations. We then pass the resulting voxels through a pose estimation\nmodule, where the voxels are aligned and the pose is computed in an end-to-end\nfashion by solving a least-squares problem. To enhance robustness, we introduce\na weighted closest voxel algorithm capable of mitigating the impact of noisy\nvoxels. We conduct extensive experiments on the CO3D, LINEMOD, and Objaverse\ndatasets, demonstrating that our method delivers more accurate relative pose\nestimates for novel objects at a lower computational cost compared to\nstate-of-the-art methods. Our code is released at:\nhttps://github.com/sailor-z/DVMNet/.\n","authors":["Chen Zhao","Tong Zhang","Zheng Dang","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2403.13683v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.13674v1","updated":"2024-03-20T15:32:56Z","published":"2024-03-20T15:32:56Z","title":"Reward-Driven Automated Curriculum Learning for Interaction-Aware\n  Self-Driving at Unsignalized Intersections","summary":"  In this work, we present a reward-driven automated curriculum reinforcement\nlearning approach for interaction-aware self-driving at unsignalized\nintersections, taking into account the uncertainties associated with\nsurrounding vehicles (SVs). These uncertainties encompass the uncertainty of\nSVs' driving intention and also the quantity of SVs. To deal with this problem,\nthe curriculum set is specifically designed to accommodate a progressively\nincreasing number of SVs. By implementing an automated curriculum selection\nmechanism, the importance weights are rationally allocated across various\ncurricula, thereby facilitating improved sample efficiency and training\noutcomes. Furthermore, the reward function is meticulously designed to guide\nthe agent towards effective policy exploration. Thus the proposed framework\ncould proactively address the above uncertainties at unsignalized intersections\nby employing the automated curriculum learning technique that progressively\nincreases task difficulty, and this ensures safe self-driving through effective\ninteraction with SVs. Comparative experiments are conducted in $Highway\\_Env$,\nand the results indicate that our approach achieves the highest task success\nrate, attains strong robustness to initialization parameters of the curriculum\nselection module, and exhibits superior adaptability to diverse situational\nconfigurations at unsignalized intersections. Furthermore, the effectiveness of\nthe proposed method is validated using the high-fidelity CARLA simulator.\n","authors":["Zengqi Peng","Xiao Zhou","Lei Zheng","Yubin Wang","Jun Ma"],"pdf_url":"https://arxiv.org/pdf/2403.13674v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.13640v1","updated":"2024-03-20T14:43:51Z","published":"2024-03-20T14:43:51Z","title":"LaCE-LHMP: Airflow Modelling-Inspired Long-Term Human Motion Prediction\n  By Enhancing Laminar Characteristics in Human Flow","summary":"  Long-term human motion prediction (LHMP) is essential for safely operating\nautonomous robots and vehicles in populated environments. It is fundamental for\nvarious applications, including motion planning, tracking, human-robot\ninteraction and safety monitoring. However, accurate prediction of human\ntrajectories is challenging due to complex factors, including, for example,\nsocial norms and environmental conditions. The influence of such factors can be\ncaptured through Maps of Dynamics (MoDs), which encode spatial motion patterns\nlearned from (possibly scattered and partial) past observations of motion in\nthe environment and which can be used for data-efficient, interpretable motion\nprediction (MoD-LHMP). To address the limitations of prior work, especially\nregarding accuracy and sensitivity to anomalies in long-term prediction, we\npropose the Laminar Component Enhanced LHMP approach (LaCE-LHMP). Our approach\nis inspired by data-driven airflow modelling, which estimates laminar and\nturbulent flow components and uses predominantly the laminar components to make\nflow predictions. Based on the hypothesis that human trajectory patterns also\nmanifest laminar flow (that represents predictable motion) and turbulent flow\ncomponents (that reflect more unpredictable and arbitrary motion), LaCE-LHMP\nextracts the laminar patterns in human dynamics and uses them for human motion\nprediction. We demonstrate the superior prediction performance of LaCE-LHMP\nthrough benchmark comparisons with state-of-the-art LHMP methods, offering an\nunconventional perspective and a more intuitive understanding of human movement\npatterns.\n","authors":["Yufei Zhu","Han Fan","Andrey Rudenko","Martin Magnusson","Erik Schaffernicht","Achim J. Lilienthal"],"pdf_url":"https://arxiv.org/pdf/2403.13640v1.pdf","comment":"Accepted to the 2024 IEEE International Conference on Robotics and\n  Automation (ICRA)"},{"id":"http://arxiv.org/abs/2309.13139v3","updated":"2024-03-20T14:43:06Z","published":"2023-09-22T18:48:54Z","title":"Exposing the Unseen: Exposure Time Emulation for Offline Benchmarking of\n  Vision Algorithms","summary":"  Visual Odometry (VO) is one of the fundamental tasks in computer vision for\nrobotics. However, its performance is deeply affected by High Dynamic Range\n(HDR) scenes, omnipresent outdoor. While new Automatic-Exposure (AE) approaches\nto mitigate this have appeared, their comparison in a reproducible manner is\nproblematic. This stems from the fact that the behavior of AE depends on the\nenvironment, and it affects the image acquisition process. Consequently, AE has\ntraditionally only been benchmarked in an online manner, making the experiments\nnon-reproducible. To solve this, we propose a new methodology based on an\nemulator that can generate images at any exposure time. It leverages BorealHDR,\na unique multi-exposure stereo dataset collected over 10 km, on 55 trajectories\nwith challenging illumination conditions. Moreover, it includes\nlidar-inertial-based global maps with pose estimation for each image frame as\nwell as Global Navigation Satellite System (GNSS) data, for comparison. We show\nthat using these images acquired at different exposure times, we can emulate\nrealistic images, keeping a Root-Mean-Square Error (RMSE) below 1.78 % compared\nto ground truth images. To demonstrate the practicality of our approach for\noffline benchmarking, we compared three state-of-the-art AE algorithms on key\nelements of Visual Simultaneous Localization And Mapping (VSLAM) pipeline,\nagainst four baselines. Consequently, reproducible evaluation of AE is now\npossible, speeding up the development of future approaches. Our code and\ndataset are available online at this link:\nhttps://github.com/norlab-ulaval/BorealHDR\n","authors":["Olivier Gamache","Jean-Michel Fortin","Matěj Boxan","Maxime Vaidis","François Pomerleau","Philippe Giguère"],"pdf_url":"https://arxiv.org/pdf/2309.13139v3.pdf","comment":"8 pages, 6 figures, submitted to 2024 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS 2024)"},{"id":"http://arxiv.org/abs/2302.05855v3","updated":"2024-03-20T14:37:54Z","published":"2023-02-12T04:41:59Z","title":"Investigation of Enhanced Inertial Navigation Algorithms by Functional\n  Iteration","summary":"  The defects of the traditional strapdown inertial navigation algorithms\nbecome well acknowledged and the corresponding enhanced algorithms have been\nquite recently proposed trying to mitigate both theoretical and algorithmic\ndefects. In this paper, the analytical accuracy evaluation of both the\ntraditional algorithms and the enhanced algorithms is investigated, against the\ntrue reference for the first time enabled by the functional iteration approach\nhaving provable convergence. The analyses by the help of MATLAB Symbolic\nToolbox show that the resultant error orders of all algorithms under\ninvestigation are consistent with those in the existing literatures, and the\nenhanced attitude algorithm notably reduces error orders of the traditional\ncounterpart, while the impact of the enhanced velocity algorithm on error order\nreduction is insignificant. Simulation results agree with analyses that the\nsuperiority of the enhanced algorithm over the traditional one in the\nbody-frame attitude computation scenario diminishes significantly in the entire\ninertial navigation computation scenario, while the functional iteration\napproach possesses significant accuracy superiority even under sustained lowly\ndynamic conditions.\n","authors":["Hongyan Jiang","Maoran Zhu","Yuanxin Wu"],"pdf_url":"https://arxiv.org/pdf/2302.05855v3.pdf","comment":"12 pages, 3 figs"},{"id":"http://arxiv.org/abs/2309.08854v2","updated":"2024-03-20T13:44:36Z","published":"2023-09-16T03:11:06Z","title":"Intention-Aware Planner for Robust and Safe Aerial Tracking","summary":"  Autonomous target tracking with quadrotors has wide applications in many\nscenarios, such as cinematographic follow-up shooting or suspect chasing.\nTarget motion prediction is necessary when designing the tracking planner.\nHowever, the widely used constant velocity or constant rotation assumption can\nnot fully capture the dynamics of the target. The tracker may fail when the\ntarget happens to move aggressively, such as sudden turn or deceleration. In\nthis paper, we propose an intention-aware planner by additionally considering\nthe intention of the target to enhance safety and robustness in aerial tracking\napplications. Firstly, a designated intention prediction method is proposed,\nwhich combines a user-defined potential assessment function and a state\nobservation function. A reachable region is generated to specifically evaluate\nthe turning intentions. Then we design an intention-driven hybrid A* method to\npredict the future possible positions for the target. Finally, an\nintention-aware optimization approach is designed to generate a\nspatial-temporal optimal trajectory, allowing the tracker to perceive\nunexpected situations from the target. Benchmark comparisons and real-world\nexperiments are conducted to validate the performance of our method.\n","authors":["Qiuyu Ren","Huan Yu","Jiajun Dai","Zhi Zheng","Jun Meng","Li Xu","Chao Xu","Fei Gao","Yanjun Cao"],"pdf_url":"https://arxiv.org/pdf/2309.08854v2.pdf","comment":"8 pages, 10 figures, submitted to 2024 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS)"},{"id":"http://arxiv.org/abs/2306.11335v4","updated":"2024-03-20T13:18:18Z","published":"2023-06-20T07:06:04Z","title":"Surfer: Progressive Reasoning with World Models for Robotic Manipulation","summary":"  Considering how to make the model accurately understand and follow natural\nlanguage instructions and perform actions consistent with world knowledge is a\nkey challenge in robot manipulation. This mainly includes human fuzzy\ninstruction reasoning and the following of physical knowledge. Therefore, the\nembodied intelligence agent must have the ability to model world knowledge from\ntraining data. However, most existing vision and language robot manipulation\nmethods mainly operate in less realistic simulator and language settings and\nlack explicit modeling of world knowledge. To bridge this gap, we introduce a\nnovel and simple robot manipulation framework, called Surfer. It is based on\nthe world model, treats robot manipulation as a state transfer of the visual\nscene, and decouples it into two parts: action and scene. Then, the\ngeneralization ability of the model on new instructions and new scenes is\nenhanced by explicit modeling of the action and scene prediction in multi-modal\ninformation. In addition to the framework, we also built a robot manipulation\nsimulator that supports full physics execution based on the MuJoCo physics\nengine. It can automatically generate demonstration training data and test\ndata, effectively reducing labor costs. To conduct a comprehensive and\nsystematic evaluation of the robot manipulation model in terms of language\nunderstanding and physical execution, we also created a robotic manipulation\nbenchmark with progressive reasoning tasks, called SeaWave. It contains 4\nlevels of progressive reasoning tasks and can provide a standardized testing\nplatform for embedded AI agents in multi-modal environments. On average, Surfer\nachieved a success rate of 54.74% on the defined four levels of manipulation\ntasks, exceeding the best baseline performance of 47.64%.\n","authors":["Pengzhen Ren","Kaidong Zhang","Hetao Zheng","Zixuan Li","Yuhang Wen","Fengda Zhu","Mas Ma","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2306.11335v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11552v2","updated":"2024-03-20T13:15:39Z","published":"2024-03-18T08:03:47Z","title":"LLM3:Large Language Model-based Task and Motion Planning with Motion\n  Failure Reasoning","summary":"  Conventional Task and Motion Planning (TAMP) approaches rely on manually\ncrafted interfaces connecting symbolic task planning with continuous motion\ngeneration. These domain-specific and labor-intensive modules are limited in\naddressing emerging tasks in real-world settings. Here, we present LLM^3, a\nnovel Large Language Model (LLM)-based TAMP framework featuring a\ndomain-independent interface. Specifically, we leverage the powerful reasoning\nand planning capabilities of pre-trained LLMs to propose symbolic action\nsequences and select continuous action parameters for motion planning.\nCrucially, LLM^3 incorporates motion planning feedback through prompting,\nallowing the LLM to iteratively refine its proposals by reasoning about motion\nfailure. Consequently, LLM^3 interfaces between task planning and motion\nplanning, alleviating the intricate design process of handling domain-specific\nmessages between them. Through a series of simulations in a box-packing domain,\nwe quantitatively demonstrate the effectiveness of LLM^3 in solving TAMP\nproblems and the efficiency in selecting action parameters. Ablation studies\nunderscore the significant contribution of motion failure reasoning to the\nsuccess of LLM^3. Furthermore, we conduct qualitative experiments on a physical\nmanipulator, demonstrating the practical applicability of our approach in\nreal-world settings.\n","authors":["Shu Wang","Muzhi Han","Ziyuan Jiao","Zeyu Zhang","Ying Nian Wu","Song-Chun Zhu","Hangxin Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11552v2.pdf","comment":"Submitted to IROS 2024. Codes available:\n  https://github.com/AssassinWS/LLM-TAMP"},{"id":"http://arxiv.org/abs/2307.07975v3","updated":"2024-03-20T12:30:01Z","published":"2023-07-16T07:55:35Z","title":"Pseudo-rigid body networks: learning interpretable deformable object\n  dynamics from partial observations","summary":"  Accurate prediction of deformable linear object (DLO) dynamics is challenging\nif the task at hand requires a human-interpretable yet computationally fast\nmodel. In this work, we draw inspiration from the pseudo-rigid body method\n(PRB) and model a DLO as a serial chain of rigid bodies whose internal state is\nunrolled through time by a dynamics network. This dynamics network is trained\njointly with a physics-informed encoder which maps observed motion variables to\nthe DLO's hidden state. To encourage that the state acquires a physically\nmeaningful representation, we leverage the forward kinematics of the PRB model\nas decoder. We demonstrate in robot experiments that the proposed DLO dynamics\nmodel provides physically interpretable predictions from partial observations\nwhile being on par with black-box models regarding prediction accuracy. The\nproject code is available at: http://tinyurl.com/prb-networks\n","authors":["Shamil Mamedov","A. René Geist","Jan Swevers","Sebastian Trimpe"],"pdf_url":"https://arxiv.org/pdf/2307.07975v3.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2403.13541v1","updated":"2024-03-20T12:26:02Z","published":"2024-03-20T12:26:02Z","title":"From One to Many: How Active Robot Swarm Sizes Influence Human Cognitive\n  Processes","summary":"  In robotics, understanding human interaction with autonomous systems is\ncrucial for enhancing collaborative technologies. We focus on human-swarm\ninteraction (HSI), exploring how differently sized groups of active robots\naffect operators' cognitive and perceptual reactions over different durations.\nWe analyze the impact of different numbers of active robots within a 15-robot\nswarm on operators' time perception, emotional state, flow experience, and task\ndifficulty perception. Our findings indicate that managing multiple active\nrobots when compared to one active robot significantly alters time perception\nand flow experience, leading to a faster passage of time and increased flow.\nMore active robots and extended durations cause increased emotional arousal and\nperceived task difficulty, highlighting the interaction between robot the\nnumber of active robots and human cognitive processes. These insights inform\nthe creation of intuitive human-swarm interfaces and aid in developing swarm\nrobotic systems aligned with human cognitive structures, enhancing human-robot\ncollaboration.\n","authors":["Julian Kaduk","Müge Cavdan","Knut Drewing","Heiko Hamann"],"pdf_url":"https://arxiv.org/pdf/2403.13541v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2403.13518v1","updated":"2024-03-20T11:38:30Z","published":"2024-03-20T11:38:30Z","title":"Motion Generation from Fine-grained Textual Descriptions","summary":"  The task of text2motion is to generate motion sequences from given textual\ndescriptions, where a model should explore the interactions between natural\nlanguage instructions and human body movements. While most existing works are\nconfined to coarse-grained motion descriptions (e.g., \"A man squats.\"),\nfine-grained ones specifying movements of relevant body parts are barely\nexplored. Models trained with coarse texts may not be able to learn mappings\nfrom fine-grained motion-related words to motion primitives, resulting in the\nfailure in generating motions from unseen descriptions. In this paper, we build\na large-scale language-motion dataset with fine-grained textual descriptions,\nFineHumanML3D, by feeding GPT-3.5-turbo with delicate prompts. Accordingly, we\ndesign a new text2motion model, FineMotionDiffuse, which makes full use of\nfine-grained textual information. Our experiments show that FineMotionDiffuse\ntrained on FineHumanML3D acquires good results in quantitative evaluation. We\nalso find this model can better generate spatially/chronologically composite\nmotions by learning the implicit mappings from simple descriptions to the\ncorresponding basic motions.\n","authors":["Kunhang Li","Yansong Feng"],"pdf_url":"https://arxiv.org/pdf/2403.13518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13474v1","updated":"2024-03-20T10:22:22Z","published":"2024-03-20T10:22:22Z","title":"Iterative Active-Inactive Obstacle Classification for Time-Optimal\n  Collision Avoidance","summary":"  Time-optimal obstacle avoidance is a prevalent problem encountered in various\nfields, including robotics and autonomous vehicles, where the task involves\ndetermining a path for a moving vehicle to reach its goal while navigating\naround obstacles within its environment. This problem becomes increasingly\nchallenging as the number of obstacles in the environment rises. We propose an\niterative active-inactive obstacle approach, which involves identifying a\nsubset of the obstacles as \"active\", that considers solely the effect of the\n\"active\" obstacles on the path of the moving vehicle. The remaining obstacles\nare considered \"inactive\" and are not considered in the path planning process.\nThe obstacles are classified as 'active' on the basis of previous findings\nderived from prior iterations. This approach allows for a more efficient\ncalculation of the optimal path by reducing the number of obstacles that need\nto be considered. The effectiveness of the proposed method is demonstrated with\ntwo different dynamic models using the various number of obstacles. The results\nshow that the proposed method is able to find the optimal path in a timely\nmanner, while also being able to handle a large number of obstacles in the\nenvironment and the constraints on the motion of the object.\n","authors":["Mehmetcan Kaymaz","Nazim Kemal Ure"],"pdf_url":"https://arxiv.org/pdf/2403.13474v1.pdf","comment":"This paper is under review in IROS24"},{"id":"http://arxiv.org/abs/2403.13467v1","updated":"2024-03-20T10:17:39Z","published":"2024-03-20T10:17:39Z","title":"CLIPSwarm: Generating Drone Shows from Text Prompts with Vision-Language\n  Models","summary":"  This paper introduces CLIPSwarm, a new algorithm designed to automate the\nmodeling of swarm drone formations based on natural language. The algorithm\nbegins by enriching a provided word, to compose a text prompt that serves as\ninput to an iterative approach to find the formation that best matches the\nprovided word. The algorithm iteratively refines formations of robots to align\nwith the textual description, employing different steps for \"exploration\" and\n\"exploitation\". Our framework is currently evaluated on simple formation\ntargets, limited to contour shapes. A formation is visually represented through\nalpha-shape contours and the most representative color is automatically found\nfor the input word. To measure the similarity between the description and the\nvisual representation of the formation, we use CLIP [1], encoding text and\nimages into vectors and assessing their similarity. Subsequently, the algorithm\nrearranges the formation to visually represent the word more effectively,\nwithin the given constraints of available drones. Control actions are then\nassigned to the drones, ensuring robotic behavior and collision-free movement.\nExperimental results demonstrate the system's efficacy in accurately modeling\nrobot formations from natural language descriptions. The algorithm's\nversatility is showcased through the execution of drone shows in photorealistic\nsimulation with varying shapes. We refer the reader to the supplementary video\nfor a visual reference of the results.\n","authors":["Pablo Pueyo","Eduardo Montijano","Ana C. Murillo","Mac Schwager"],"pdf_url":"https://arxiv.org/pdf/2403.13467v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13455v1","updated":"2024-03-20T10:01:52Z","published":"2024-03-20T10:01:52Z","title":"FACT: Fast and Active Coordinate Initialization for Vision-based Drone\n  Swarms","summary":"  Swarm robots have sparked remarkable developments across a range of fields.\nWhile it is necessary for various applications in swarm robots, a fast and\nrobust coordinate initialization in vision-based drone swarms remains elusive.\nTo this end, our paper proposes a complete system to recover a swarm's initial\nrelative pose on platforms with size, weight, and power (SWaP) constraints. To\novercome limited coverage of field-of-view (FoV), the drones rotate in place to\nobtain observations. To tackle the anonymous measurements, we formulate a\nnon-convex rotation estimation problem and transform it into a semi-definite\nprogramming (SDP) problem, which can steadily obtain global optimal values.\nThen we utilize the Hungarian algorithm to recover relative translation and\ncorrespondences between observations and drone identities. To safely acquire\ncomplete observations, we actively search for positions and generate feasible\ntrajectories to avoid collisions. To validate the practicability of our system,\nwe conduct experiments on a vision-based drone swarm with only stereo cameras\nand inertial measurement units (IMUs) as sensors. The results demonstrate that\nthe system can robustly get accurate relative poses in real time with limited\nonboard computation resources. The source code is released.\n","authors":["Yuan Li","Anke Zhao","Yingjian Wang","Ziyi Xu","Xin Zhou","Jinni Zhou","Chao Xu","Fei Gao"],"pdf_url":"https://arxiv.org/pdf/2403.13455v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13452v1","updated":"2024-03-20T09:53:31Z","published":"2024-03-20T09:53:31Z","title":"Mobile Robot Localization: a Modular, Odometry-Improving Approach","summary":"  Despite the number of works published in recent years, vehicle localization\nremains an open, challenging problem. While map-based localization and SLAM\nalgorithms are getting better and better, they remain a single point of failure\nin typical localization pipelines. This paper proposes a modular localization\narchitecture that fuses sensor measurements with the outputs of off-the-shelf\nlocalization algorithms. The fusion filter estimates model uncertainties to\nimprove odometry in case absolute pose measurements are lost entirely. The\narchitecture is validated experimentally on a real robot navigating\nautonomously proving a reduction of the position error of more than 90% with\nrespect to the odometrical estimate without uncertainty estimation in a\ntwo-minute navigation period without position measurements.\n","authors":["Luca Mozzarelli","Luca Cattaneo","Matteo Corno","Sergio Matteo Savaresi"],"pdf_url":"https://arxiv.org/pdf/2403.13452v1.pdf","comment":"Accepted at IEEE European Control Conference 2024"},{"id":"http://arxiv.org/abs/2403.13443v1","updated":"2024-03-20T09:39:39Z","published":"2024-03-20T09:39:39Z","title":"Fast-Poly: A Fast Polyhedral Framework For 3D Multi-Object Tracking","summary":"  3D Multi-Object Tracking (MOT) captures stable and comprehensive motion\nstates of surrounding obstacles, essential for robotic perception. However,\ncurrent 3D trackers face issues with accuracy and latency consistency. In this\npaper, we propose Fast-Poly, a fast and effective filter-based method for 3D\nMOT. Building upon our previous work Poly-MOT, Fast-Poly addresses object\nrotational anisotropy in 3D space, enhances local computation densification,\nand leverages parallelization technique, improving inference speed and\nprecision. Fast-Poly is extensively tested on two large-scale tracking\nbenchmarks with Python implementation. On the nuScenes dataset, Fast-Poly\nachieves new state-of-the-art performance with 75.8% AMOTA among all methods\nand can run at 34.2 FPS on a personal CPU. On the Waymo dataset, Fast-Poly\nexhibits competitive accuracy with 63.6% MOTA and impressive inference speed\n(35.5 FPS). The source code is publicly available at\nhttps://github.com/lixiaoyu2000/FastPoly.\n","authors":["Xiaoyu Li","Dedong Liu","Lijun Zhao","Yitao Wu","Xian Wu","Jinghan Gao"],"pdf_url":"https://arxiv.org/pdf/2403.13443v1.pdf","comment":"1st on the NuScenes Tracking benchmark with 75.8 AMOTA and 34.2 FPS"},{"id":"http://arxiv.org/abs/2403.13431v1","updated":"2024-03-20T09:18:19Z","published":"2024-03-20T09:18:19Z","title":"Automatic Navigation Map Generation for Mobile Robots in Urban\n  Environments","summary":"  A fundamental prerequisite for safe and efficient navigation of mobile robots\nis the availability of reliable navigation maps upon which trajectories can be\nplanned. With the increasing industrial interest in mobile robotics, especially\nin urban environments, the process of generating navigation maps has become of\nparticular interest, being a labor intensive step of the deployment process.\nAutomating this step is challenging and becomes even more arduous when the\nperception capabilities are limited by cost considerations. This paper proposes\nan algorithm to automatically generate navigation maps using a typical\nnavigation-oriented sensor setup: a single top-mounted 3D LiDAR sensor. The\nproposed method is designed and validated with the urban environment as the\nmain use case: it is shown to be able to produce accurate maps featuring\ndifferent terrain types, positive obstacles of different heights as well as\nnegative obstacles. The algorithm is applied to data collected in a typical\nurban environment with a wheeled inverted pendulum robot, showing its\nrobustness against localization, perception and dynamic uncertainties. The\ngenerated map is validated against a human-made map.\n","authors":["Luca Mozzarelli","Simone Specchia","Matteo Corno","Sergio Matteo Savaresi"],"pdf_url":"https://arxiv.org/pdf/2403.13431v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04530v2","updated":"2024-03-20T09:12:21Z","published":"2023-12-07T18:50:01Z","title":"Camera Height Doesn't Change: Unsupervised Training for Metric Monocular\n  Road-Scene Depth Estimation","summary":"  In this paper, we introduce a novel training method for making any monocular\ndepth network learn absolute scale and estimate metric road-scene depth just\nfrom regular training data, i.e., driving videos. We refer to this training\nframework as StableCamH. The key idea is to leverage cars found on the road as\nsources of scale supervision but to incorporate them in the training robustly.\nStableCamH detects and estimates the sizes of cars in the frame and aggregates\nscale information extracted from them into a camera height estimate whose\nconsistency across the entire video sequence is enforced as scale supervision.\nThis realizes robust unsupervised training of any, otherwise scale-oblivious,\nmonocular depth network to become not only scale-aware but also metric-accurate\nwithout the need for auxiliary sensors and extra supervision. Extensive\nexperiments on the KITTI and Cityscapes datasets show the effectiveness of\nStableCamH and its state-of-the-art accuracy compared with related methods. We\nalso show that StableCamH enables training on mixed datasets of different\ncamera heights, which leads to larger-scale training and thus higher\ngeneralization. Metric depth reconstruction is essential in any road-scene\nvisual modeling, and StableCamH democratizes its deployment by establishing the\nmeans to train any model as a metric depth estimator.\n","authors":["Genki Kinoshita","Ko Nishino"],"pdf_url":"https://arxiv.org/pdf/2312.04530v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13421v1","updated":"2024-03-20T09:07:23Z","published":"2024-03-20T09:07:23Z","title":"Caching-Augmented Lifelong Multi-Agent Path Finding","summary":"  Multi-Agent Path Finding (MAPF), which involves finding collision-free paths\nfor multiple robots, is crucial in various applications. Lifelong MAPF, where\ntargets are reassigned to agents as soon as they complete their initial\nobjectives, offers a more accurate approximation of real-world warehouse\nplanning. In this paper, we present a novel mechanism named Caching-Augmented\nLifelong MAPF (CAL-MAPF), designed to improve the performance of Lifelong MAPF.\nWe have developed a new map grid type called cache for temporary item storage\nand replacement and designed a lock mechanism for it to improve the stability\nof the planning solution. This cache mechanism was evaluated using various\ncache replacement policies and a spectrum of input task distributions. We\nidentified three main factors significantly impacting CAL-MAPF performance\nthrough experimentation: suitable input task distribution, high cache hit rate,\nand smooth traffic. Overall, CAL-MAPF has demonstrated potential for\nperformance improvements in certain task distributions, maps and agent\nconfigurations.\n","authors":["Yimin Tang","Zhenghong Yu","Yi Zheng","T. K. Satish Kumar","Jiaoyang Li","Sven Koenig"],"pdf_url":"https://arxiv.org/pdf/2403.13421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.08221v2","updated":"2024-03-20T08:52:04Z","published":"2023-07-17T03:45:47Z","title":"NDT-Map-Code: A 3D global descriptor for real-time loop closure\n  detection in lidar SLAM","summary":"  Loop-closure detection, also known as place recognition, aiming to identify\npreviously visited locations, is an essential component of a SLAM system.\nExisting research on lidar-based loop closure heavily relies on dense point\ncloud and 360 FOV lidars. This paper proposes an out-of-the-box NDT (Normal\nDistribution Transform) based global descriptor, NDT-Map-Code, designed for\nboth on-road driving and underground valet parking scenarios. NDT-Map-Code can\nbe directly extracted from the NDT map without the need for a dense point\ncloud, resulting in excellent scalability and low maintenance cost. The NDT\nrepresentation is leveraged to identify representative patterns, which are\nfurther encoded according to their spatial location (bearing, range, and\nheight). Experimental results on the NIO underground parking lot dataset and\nthe KITTI dataset demonstrate that our method achieves significantly better\nperformance compared to the state-of-the-art.\n","authors":["Lizhou Liao","Wenlei Yan","Li Sun","Xinhui Bai","Zhenxing You","Hongyuan Yuan","Chunyun Fu"],"pdf_url":"https://arxiv.org/pdf/2307.08221v2.pdf","comment":"8 pages, 6 figures, 4 tables"},{"id":"http://arxiv.org/abs/2206.06112v3","updated":"2024-03-20T08:41:49Z","published":"2022-06-13T12:53:54Z","title":"Vision-State Fusion: Improving Deep Neural Networks for Autonomous\n  Robotics","summary":"  Vision-based deep learning perception fulfills a paramount role in robotics,\nfacilitating solutions to many challenging scenarios, such as acrobatic\nmaneuvers of autonomous unmanned aerial vehicles (UAVs) and robot-assisted\nhigh-precision surgery. Control-oriented end-to-end perception approaches,\nwhich directly output control variables for the robot, commonly take advantage\nof the robot's state estimation as an auxiliary input. When intermediate\noutputs are estimated and fed to a lower-level controller, i.e. mediated\napproaches, the robot's state is commonly used as an input only for egocentric\ntasks, which estimate physical properties of the robot itself. In this work, we\npropose to apply a similar approach for the first time -- to the best of our\nknowledge -- to non-egocentric mediated tasks, where the estimated outputs\nrefer to an external subject. We prove how our general methodology improves the\nregression performance of deep convolutional neural networks (CNNs) on a broad\nclass of non-egocentric 3D pose estimation problems, with minimal computational\ncost. By analyzing three highly-different use cases, spanning from grasping\nwith a robotic arm to following a human subject with a pocket-sized UAV, our\nresults consistently improve the R\\textsuperscript{2} regression metric, up to\n+0.51, compared to their stateless baselines. Finally, we validate the in-field\nperformance of a closed-loop autonomous cm-scale UAV on the human pose\nestimation task. Our results show a significant reduction, i.e., 24\\% on\naverage, on the mean absolute error of our stateful CNN, compared to a\nState-of-the-Art stateless counterpart.\n","authors":["Elia Cereda","Stefano Bonato","Mirko Nava","Alessandro Giusti","Daniele Palossi"],"pdf_url":"https://arxiv.org/pdf/2206.06112v3.pdf","comment":"This paper has been accepted for publication in the Journal of\n  Intelligent & Robotic Systems. \\copyright 2024 Springer"},{"id":"http://arxiv.org/abs/2403.13395v1","updated":"2024-03-20T08:35:57Z","published":"2024-03-20T08:35:57Z","title":"Unifying Local and Global Multimodal Features for Place Recognition in\n  Aliased and Low-Texture Environments","summary":"  Perceptual aliasing and weak textures pose significant challenges to the task\nof place recognition, hindering the performance of Simultaneous Localization\nand Mapping (SLAM) systems. This paper presents a novel model, called UMF\n(standing for Unifying Local and Global Multimodal Features) that 1) leverages\nmulti-modality by cross-attention blocks between vision and LiDAR features, and\n2) includes a re-ranking stage that re-orders based on local feature matching\nthe top-k candidates retrieved using a global representation. Our experiments,\nparticularly on sequences captured on a planetary-analogous environment, show\nthat UMF outperforms significantly previous baselines in those challenging\naliased environments. Since our work aims to enhance the reliability of SLAM in\nall situations, we also explore its performance on the widely used RobotCar\ndataset, for broader applicability. Code and models are available at\nhttps://github.com/DLR-RM/UMF\n","authors":["Alberto García-Hernández","Riccardo Giubilato","Klaus H. Strobl","Javier Civera","Rudolph Triebel"],"pdf_url":"https://arxiv.org/pdf/2403.13395v1.pdf","comment":"Accepted submission to International Conference on Robotics and\n  Automation (ICRA), 2024"},{"id":"http://arxiv.org/abs/2401.01657v3","updated":"2024-03-20T08:15:37Z","published":"2024-01-03T10:31:12Z","title":"Distributed Pose-graph Optimization with Multi-level Partitioning for\n  Collaborative SLAM","summary":"  The back-end module of Distributed Collaborative Simultaneous Localization\nand Mapping (DCSLAM) requires solving a nonlinear Pose Graph Optimization (PGO)\nunder a distributed setting, also known as SE(d)-synchronization. Most existing\ndistributed graph optimization algorithms employ a simple sequential\npartitioning scheme, which may result in unbalanced subgraph dimensions due to\nthe different geographic locations of each robot, and hence imposes extra\ncommunication load. Moreover, the performance of current Riemannian\noptimization algorithms can be further accelerated. In this letter, we propose\na novel distributed pose graph optimization algorithm combining multi-level\npartitioning with an accelerated Riemannian optimization method. Firstly, we\nemploy the multi-level graph partitioning algorithm to preprocess the naive\npose graph to formulate a balanced optimization problem. In addition, inspired\nby the accelerated coordinate descent method, we devise an Improved Riemannian\nBlock Coordinate Descent (IRBCD) algorithm and the critical point obtained is\nglobally optimal. Finally, we evaluate the effects of four common graph\npartitioning approaches on the correlation of the inter-subgraphs, and discover\nthat the Highest scheme has the best partitioning performance. Also, we\nimplement simulations to quantitatively demonstrate that our proposed algorithm\noutperforms the state-of-the-art distributed pose graph optimization protocols.\n","authors":["Cunhao Li","Peng Yi","Guanghui Guo","Yiguang Hong"],"pdf_url":"https://arxiv.org/pdf/2401.01657v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01233v2","updated":"2024-03-20T07:52:35Z","published":"2024-03-02T15:32:15Z","title":"Results and Lessons Learned from Autonomous Driving Transportation\n  Services in Airfield, Crowded Indoor, and Urban Environments","summary":"  Autonomous vehicles have been actively investigated over the past few\ndecades. Several recent works show the potential of autonomous vehicles in\nurban environments with impressive experimental results. However, these works\nnote that autonomous vehicles are still occasionally inferior to expert drivers\nin complex scenarios. Furthermore, they do not focus on the possibilities of\nautonomous driving transportation services in other areas beyond urban\nenvironments. This paper presents the research results and lessons learned from\nautonomous driving transportation services in airfield, crowded indoor, and\nurban environments. We discuss how we address several unique challenges in\nthese diverse environments. We also offer an overview of remaining challenges\nthat have not received much attention but must be addressed. This paper aims to\nshare our unique experience to support researchers who are interested in\nexploring autonomous driving transportation services in various real-world\nenvironments.\n","authors":["Doosan Baek","Sanghyun Kim","Seung-Woo Seo","Sang-Hyun Lee"],"pdf_url":"https://arxiv.org/pdf/2403.01233v2.pdf","comment":"8 pages, 7 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.13366v1","updated":"2024-03-20T07:51:53Z","published":"2024-03-20T07:51:53Z","title":"Centroidal State Estimation based on the Koopman Embedding for Dynamic\n  Legged Locomotion","summary":"  In this paper, we introduce a novel approach to centroidal state estimation,\nwhich plays a crucial role in predictive model-based control strategies for\ndynamic legged locomotion. Our approach uses the Koopman operator theory to\ntransform the robot's complex nonlinear dynamics into a linear system, by\nemploying dynamic mode decomposition and deep learning for model construction.\nWe evaluate both models on their linearization accuracy and capability to\ncapture both fast and slow dynamic system responses. We then select the most\nsuitable model for estimation purposes, and integrate it within a moving\nhorizon estimator. This estimator is formulated as a convex quadratic program,\nto facilitate robust, real-time centroidal state estimation. Through extensive\nsimulation experiments on a quadruped robot executing various dynamic gaits,\nour data-driven framework outperforms conventional filtering techniques based\non nonlinear dynamics. Our estimator addresses challenges posed by force/torque\nmeasurement noise in highly dynamic motions and accurately recovers the\ncentroidal states, demonstrating the adaptability and effectiveness of the\nKoopman-based linear representation for complex locomotive behaviors.\nImportantly, our model based on dynamic mode decomposition, trained with two\nlocomotion patterns (trot and jump), successfully estimates the centroidal\nstates for a different motion (bound) without retraining.\n","authors":["Shahram Khorshidi","Murad Dawood","Maren Bennewitz"],"pdf_url":"https://arxiv.org/pdf/2403.13366v1.pdf","comment":"Submitted to IROS 2024"},{"id":"http://arxiv.org/abs/2403.13365v1","updated":"2024-03-20T07:48:32Z","published":"2024-03-20T07:48:32Z","title":"ManiPose: A Comprehensive Benchmark for Pose-aware Object Manipulation\n  in Robotics","summary":"  Robotic manipulation in everyday scenarios, especially in unstructured\nenvironments, requires skills in pose-aware object manipulation (POM), which\nadapts robots' grasping and handling according to an object's 6D pose.\nRecognizing an object's position and orientation is crucial for effective\nmanipulation. For example, if a mug is lying on its side, it's more effective\nto grasp it by the rim rather than the handle. Despite its importance, research\nin POM skills remains limited, because learning manipulation skills requires\npose-varying simulation environments and datasets. This paper introduces\nManiPose, a pioneering benchmark designed to advance the study of pose-varying\nmanipulation tasks. ManiPose encompasses: 1) Simulation environments for POM\nfeature tasks ranging from 6D pose-specific pick-and-place of single objects to\ncluttered scenes, further including interactions with articulated objects. 2) A\ncomprehensive dataset featuring geometrically consistent and\nmanipulation-oriented 6D pose labels for 2936 real-world scanned rigid objects\nand 100 articulated objects across 59 categories. 3) A baseline for POM,\nleveraging the inferencing abilities of LLM (e.g., ChatGPT) to analyze the\nrelationship between 6D pose and task-specific requirements, offers enhanced\npose-aware grasp prediction and motion planning capabilities. Our benchmark\ndemonstrates notable advancements in pose estimation, pose-aware manipulation,\nand real-robot skill transfer, setting new standards for POM research. We will\nopen-source the ManiPose benchmark with the final version paper, inviting the\ncommunity to engage with our resources, available at our\nwebsite:https://sites.google.com/view/manipose.\n","authors":["Qiaojun Yu","Ce Hao","Junbo Wang","Wenhai Liu","Liu Liu","Yao Mu","Yang You","Hengxu Yan","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2403.13365v1.pdf","comment":"8 pages, 7 figures, submitted to 2024 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS 2024)"},{"id":"http://arxiv.org/abs/2403.13358v1","updated":"2024-03-20T07:36:43Z","published":"2024-03-20T07:36:43Z","title":"GeRM: A Generalist Robotic Model with Mixture-of-experts for Quadruped\n  Robot","summary":"  Multi-task robot learning holds significant importance in tackling diverse\nand complex scenarios. However, current approaches are hindered by performance\nissues and difficulties in collecting training datasets. In this paper, we\npropose GeRM (Generalist Robotic Model). We utilize offline reinforcement\nlearning to optimize data utilization strategies to learn from both\ndemonstrations and sub-optimal data, thus surpassing the limitations of human\ndemonstrations. Thereafter, we employ a transformer-based VLA network to\nprocess multi-modal inputs and output actions. By introducing the\nMixture-of-Experts structure, GeRM allows faster inference speed with higher\nwhole model capacity, and thus resolves the issue of limited RL parameters,\nenhancing model performance in multi-task learning while controlling\ncomputational costs. Through a series of experiments, we demonstrate that GeRM\noutperforms other methods across all tasks, while also validating its\nefficiency in both training and inference processes. Additionally, we uncover\nits potential to acquire emergent skills. Additionally, we contribute the\nQUARD-Auto dataset, collected automatically to support our training approach\nand foster advancements in multi-task quadruped robot learning. This work\npresents a new paradigm for reducing the cost of collecting robot data and\ndriving progress in the multi-task learning community.\n","authors":["Wenxuan Song","Han Zhao","Pengxiang Ding","Can Cui","Shangke Lyu","Yaning Fan","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.13358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13348v1","updated":"2024-03-20T07:19:53Z","published":"2024-03-20T07:19:53Z","title":"MULAN-WC: Multi-Robot Localization Uncertainty-aware Active NeRF with\n  Wireless Coordination","summary":"  This paper presents MULAN-WC, a novel multi-robot 3D reconstruction framework\nthat leverages wireless signal-based coordination between robots and Neural\nRadiance Fields (NeRF). Our approach addresses key challenges in multi-robot 3D\nreconstruction, including inter-robot pose estimation, localization uncertainty\nquantification, and active best-next-view selection. We introduce a method for\nusing wireless Angle-of-Arrival (AoA) and ranging measurements to estimate\nrelative poses between robots, as well as quantifying and incorporating the\nuncertainty embedded in the wireless localization of these pose estimates into\nthe NeRF training loss to mitigate the impact of inaccurate camera poses.\nFurthermore, we propose an active view selection approach that accounts for\nrobot pose uncertainty when determining the next-best views to improve the 3D\nreconstruction, enabling faster convergence through intelligent view selection.\nExtensive experiments on both synthetic and real-world datasets demonstrate the\neffectiveness of our framework in theory and in practice. Leveraging wireless\ncoordination and localization uncertainty-aware training, MULAN-WC can achieve\nhigh-quality 3d reconstruction which is close to applying the ground truth\ncamera poses. Furthermore, the quantification of the information gain from a\nnovel view enables consistent rendering quality improvement with incrementally\ncaptured images by commending the robot the novel view position. Our hardware\nexperiments showcase the practicality of deploying MULAN-WC to real robotic\nsystems.\n","authors":["Weiying Wang","Victor Cai","Stephanie Gil"],"pdf_url":"https://arxiv.org/pdf/2403.13348v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13336v1","updated":"2024-03-20T06:42:12Z","published":"2024-03-20T06:42:12Z","title":"Discretizing SO(2)-Equivariant Features for Robotic Kitting","summary":"  Robotic kitting has attracted considerable attention in logistics and\nindustrial settings. However, existing kitting methods encounter challenges\nsuch as low precision and poor efficiency, limiting their widespread\napplications. To address these issues, we present a novel kitting framework\nthat improves both the precision and computational efficiency of complex\nkitting tasks. Firstly, our approach introduces a fine-grained orientation\nestimation technique in the picking module, significantly enhancing orientation\nprecision while effectively decoupling computational load from orientation\ngranularity. This approach combines an SO(2)-equivariant network with a group\ndiscretization operation to preciously predict discrete orientation\ndistributions. Secondly, we develop the Hand-tool Kitting Dataset (HKD) to\nevaluate the performance of different solutions in handling\norientation-sensitive kitting tasks. This dataset comprises a diverse\ncollection of hand tools and synthetically created kits, which reflects the\ncomplexities encountered in real-world kitting scenarios. Finally, a series of\nexperiments are conducted to evaluate the performance of the proposed method.\nThe results demonstrate that our approach offers remarkable precision and\nenhanced computational efficiency in robotic kitting tasks.\n","authors":["Jiadong Zhou","Yadan Zeng","Huixu Dong","I-Ming Chen"],"pdf_url":"https://arxiv.org/pdf/2403.13336v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.13321v1","updated":"2024-03-20T05:57:20Z","published":"2024-03-20T05:57:20Z","title":"Robotics meets Fluid Dynamics: A Characterization of the Induced Airflow\n  around a Quadrotor","summary":"  The widespread adoption of quadrotors for diverse applications, from\nagriculture to public safety, necessitates an understanding of the aerodynamic\ndisturbances they create. This paper introduces a computationally lightweight\nmodel for estimating the time-averaged magnitude of the induced flow below\nquadrotors in hover. Unlike related approaches that rely on expensive\ncomputational fluid dynamics (CFD) simulations or time-consuming empirical\nmeasurements, our method leverages classical theory from turbulent flows. By\nanalyzing over 9 hours of flight data from drones of varying sizes within a\nlarge motion capture system, we show that the combined flow from all propellers\nof the drone is well-approximated by a turbulent jet. Through the use of a\nnovel normalization and scaling, we have developed and experimentally validated\na unified model that describes the mean velocity field of the induced flow for\ndifferent drone sizes. The model accurately describes the far-field airflow in\na very large volume below the drone which is difficult to simulate in CFD. Our\nmodel, which requires only the drone's mass, propeller size, and drone size for\ncalculations, offers a practical tool for dynamic planning in multi-agent\nscenarios, ensuring safer operations near humans and optimizing sensor\nplacements.\n","authors":["Leonard Bauersfeld","Koen Muller","Dominic Ziegler","Filippo Coletti","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2403.13321v1.pdf","comment":"7+1 pages"},{"id":"http://arxiv.org/abs/2403.13318v1","updated":"2024-03-20T05:46:56Z","published":"2024-03-20T05:46:56Z","title":"Workload Estimation for Unknown Tasks: A Survey of Machine Learning\n  Under Distribution Shift","summary":"  Human-robot teams involve humans and robots collaborating to achieve tasks\nunder various environmental conditions. Successful teaming will require robots\nto adapt autonomously to a human teammate's internal state. An important\nelement of such adaptation is the ability to estimate the human teammates'\nworkload in unknown situations. Existing workload models use machine learning\nto model the relationships between physiological metrics and workload; however,\nthese methods are susceptible to individual differences and are heavily\ninfluenced by other factors. These methods cannot generalize to unknown tasks,\nas they rely on standard machine learning approaches that assume data consists\nof independent and identically distributed (IID) samples. This assumption does\nnot necessarily hold for estimating workload for new tasks. A survey of non-IID\nmachine learning techniques is presented, where commonly used techniques are\nevaluated using three criteria: portability, model complexity, and\nadaptability. These criteria are used to argue which techniques are most\napplicable for estimating workload for unknown tasks in dynamic, real-time\nenvironments.\n","authors":["Josh Bhagat Smith","Julie A. Adams"],"pdf_url":"https://arxiv.org/pdf/2403.13318v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13311v1","updated":"2024-03-20T05:23:24Z","published":"2024-03-20T05:23:24Z","title":"Multi-Robot Connected Fermat Spiral Coverage","summary":"  We introduce the Multi-Robot Connected Fermat Spiral (MCFS), a novel\nalgorithmic framework for Multi-Robot Coverage Path Planning (MCPP) that adapts\nConnected Fermat Spiral (CFS) from the computer graphics community to\nmulti-robot coordination for the first time. MCFS uniquely enables the\norchestration of multiple robots to generate coverage paths that contour around\narbitrarily shaped obstacles, a feature that is notably lacking in traditional\nmethods. Our framework not only enhances area coverage and optimizes task\nperformance, particularly in terms of makespan, for workspaces rich in\nirregular obstacles but also addresses the challenges of path continuity and\ncurvature critical for non-holonomic robots by generating smooth paths without\ndecomposing the workspace. MCFS solves MCPP by constructing a graph of isolines\nand transforming MCPP into a combinatorial optimization problem, aiming to\nminimize the makespan while covering all vertices. Our contributions include\ndeveloping a unified CFS version for scalable and adaptable MCPP, extending it\nto MCPP with novel optimization techniques for cost reduction and path\ncontinuity and smoothness, and demonstrating through extensive experiments that\nMCFS outperforms existing MCPP methods in makespan, path curvature, coverage\nratio, and overlapping ratio. Our research marks a significant step in MCPP,\nshowcasing the fusion of computer graphics and automated planning principles to\nadvance the capabilities of multi-robot systems in complex environments. Our\ncode is available at https://github.com/reso1/MCFS.\n","authors":["Jingtao Tang","Hang Ma"],"pdf_url":"https://arxiv.org/pdf/2403.13311v1.pdf","comment":"accepted to ICAPS24"},{"id":"http://arxiv.org/abs/2403.13297v1","updated":"2024-03-20T04:39:15Z","published":"2024-03-20T04:39:15Z","title":"POLICEd RL: Learning Closed-Loop Robot Control Policies with Provable\n  Satisfaction of Hard Constraints","summary":"  In this paper, we seek to learn a robot policy guaranteed to satisfy state\nconstraints. To encourage constraint satisfaction, existing RL algorithms\ntypically rely on Constrained Markov Decision Processes and discourage\nconstraint violations through reward shaping. However, such soft constraints\ncannot offer verifiable safety guarantees. To address this gap, we propose\nPOLICEd RL, a novel RL algorithm explicitly designed to enforce affine hard\nconstraints in closed-loop with a black-box environment. Our key insight is to\nforce the learned policy to be affine around the unsafe set and use this affine\nregion as a repulsive buffer to prevent trajectories from violating the\nconstraint. We prove that such policies exist and guarantee constraint\nsatisfaction. Our proposed framework is applicable to both systems with\ncontinuous and discrete state and action spaces and is agnostic to the choice\nof the RL training algorithm. Our results demonstrate the capacity of POLICEd\nRL to enforce hard constraints in robotic tasks while significantly\noutperforming existing methods.\n","authors":["Jean-Baptiste Bouvier","Kartik Nagpal","Negar Mehr"],"pdf_url":"https://arxiv.org/pdf/2403.13297v1.pdf","comment":"26 pages, 11 figures"},{"id":"http://arxiv.org/abs/2403.13294v1","updated":"2024-03-20T04:25:09Z","published":"2024-03-20T04:25:09Z","title":"Map-Aware Human Pose Prediction for Robot Follow-Ahead","summary":"  In the robot follow-ahead task, a mobile robot is tasked to maintain its\nrelative position in front of a moving human actor while keeping the actor in\nsight. To accomplish this task, it is important that the robot understand the\nfull 3D pose of the human (since the head orientation can be different than the\ntorso) and predict future human poses so as to plan accordingly. This\nprediction task is especially tricky in a complex environment with junctions\nand multiple corridors. In this work, we address the problem of forecasting the\nfull 3D trajectory of a human in such environments. Our main insight is to show\nthat one can first predict the 2D trajectory and then estimate the full 3D\ntrajectory by conditioning the estimator on the predicted 2D trajectory. With\nthis approach, we achieve results comparable or better than the\nstate-of-the-art methods three times faster. As part of our contribution, we\npresent a new dataset where, in contrast to existing datasets, the human motion\nis in a much larger area than a single room. We also present a complete robot\nsystem that integrates our human pose forecasting network on the mobile robot\nto enable real-time robot follow-ahead and present results from real-world\nexperiments in multiple buildings on campus. Our project page, including\nsupplementary material and videos, can be found at:\nhttps://qingyuan-jiang.github.io/iros2024_poseForecasting/\n","authors":["Qingyuan Jiang","Burak Susam","Jun-Jee Chao","Volkan Isler"],"pdf_url":"https://arxiv.org/pdf/2403.13294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13284v1","updated":"2024-03-20T03:53:20Z","published":"2024-03-20T03:53:20Z","title":"Look Before You Leap: Socially Acceptable High-Speed Ground Robot\n  Navigation in Crowded Hallways","summary":"  To operate safely and efficiently, autonomous warehouse/delivery robots must\nbe able to accomplish tasks while navigating in dynamic environments and\nhandling the large uncertainties associated with the motions/behaviors of other\nrobots and/or humans. A key scenario in such environments is the hallway\nproblem, where robots must operate in the same narrow corridor as human traffic\ngoing in one or both directions. Traditionally, robot planners have tended to\nfocus on socially acceptable behavior in the hallway scenario at the expense of\nperformance. This paper proposes a planner that aims to address the consequent\n\"robot freezing problem\" in hallways by allowing for \"peek-and-pass\" maneuvers.\nWe then go on to demonstrate in simulation how this planner improves robot time\nto goal without violating social norms. Finally, we show initial hardware\ndemonstrations of this planner in the real world.\n","authors":["Lakshay Sharma","Jonathan P. How"],"pdf_url":"https://arxiv.org/pdf/2403.13284v1.pdf","comment":"Submitted to IROS 2024"},{"id":"http://arxiv.org/abs/2403.13281v1","updated":"2024-03-20T03:42:03Z","published":"2024-03-20T03:42:03Z","title":"Waypoint-Based Reinforcement Learning for Robot Manipulation Tasks","summary":"  Robot arms should be able to learn new tasks. One framework here is\nreinforcement learning, where the robot is given a reward function that encodes\nthe task, and the robot autonomously learns actions to maximize its reward.\nExisting approaches to reinforcement learning often frame this problem as a\nMarkov decision process, and learn a policy (or a hierarchy of policies) to\ncomplete the task. These policies reason over hundreds of fine-grained actions\nthat the robot arm needs to take: e.g., moving slightly to the right or\nrotating the end-effector a few degrees. But the manipulation tasks that we\nwant robots to perform can often be broken down into a small number of\nhigh-level motions: e.g., reaching an object or turning a handle. In this paper\nwe therefore propose a waypoint-based approach for model-free reinforcement\nlearning. Instead of learning a low-level policy, the robot now learns a\ntrajectory of waypoints, and then interpolates between those waypoints using\nexisting controllers. Our key novelty is framing this waypoint-based setting as\na sequence of multi-armed bandits: each bandit problem corresponds to one\nwaypoint along the robot's motion. We theoretically show that an ideal solution\nto this reformulation has lower regret bounds than standard frameworks. We also\nintroduce an approximate posterior sampling solution that builds the robot's\nmotion one waypoint at a time. Results across benchmark simulations and two\nreal-world experiments suggest that this proposed approach learns new tasks\nmore quickly than state-of-the-art baselines. See videos here:\nhttps://youtu.be/MMEd-lYfq4Y\n","authors":["Shaunak A. Mehta","Soheil Habibian","Dylan P. Losey"],"pdf_url":"https://arxiv.org/pdf/2403.13281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13274v1","updated":"2024-03-20T03:17:07Z","published":"2024-03-20T03:17:07Z","title":"UNO Push: Unified Nonprehensile Object Pushing via Non-Parametric\n  Estimation and Model Predictive Control","summary":"  Nonprehensile manipulation through precise pushing is an essential skill that\nhas been commonly challenged by perception and physical uncertainties, such as\nthose associated with contacts, object geometries, and physical properties. For\nthis, we propose a unified framework that jointly addresses system modeling,\naction generation, and control. While most existing approaches either heavily\nrely on a priori system information for analytic modeling, or leverage a large\ndataset to learn dynamic models, our framework approximates a system transition\nfunction via non-parametric learning only using a small number of exploratory\nactions (ca. 10). The approximated function is then integrated with model\npredictive control to provide precise pushing manipulation. Furthermore, we\nshow that the approximated system transition functions can be robustly\ntransferred across novel objects while being online updated to continuously\nimprove the manipulation accuracy. Through extensive experiments on a real\nrobot platform with a set of novel objects and comparing against a\nstate-of-the-art baseline, we show that the proposed unified framework is a\nlight-weight and highly effective approach to enable precise pushing\nmanipulation all by itself. Our evaluation results illustrate that the system\ncan robustly ensure millimeter-level precision and can straightforwardly work\non any novel object.\n","authors":["Gaotian Wang","Kejia Ren","Kaiyu Hang"],"pdf_url":"https://arxiv.org/pdf/2403.13274v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13266v1","updated":"2024-03-20T03:03:22Z","published":"2024-03-20T03:03:22Z","title":"Enhancing Security in Multi-Robot Systems through Co-Observation\n  Planning, Reachability Analysis, and Network Flow","summary":"  This paper addresses security challenges in multi-robot systems (MRS) where\nadversaries may compromise robot control, risking unauthorized access to\nforbidden areas. We propose a novel multi-robot optimal planning algorithm that\nintegrates mutual observations and introduces reachability constraints for\nenhanced security. This ensures that, even with adversarial movements,\ncompromised robots cannot breach forbidden regions without missing scheduled\nco-observations. The reachability constraint uses ellipsoidal\nover-approximation for efficient intersection checking and gradient\ncomputation. To enhance system resilience and tackle feasibility challenges, we\nalso introduce sub-teams. These cohesive units replace individual robot\nassignments along each route, enabling redundant robots to deviate for\nco-observations across different trajectories, securing multiple sub-teams\nwithout requiring modifications. We formulate the cross-trajectory\nco-observation plan by solving a network flow coverage problem on the\ncheckpoint graph generated from the original unsecured MRS trajectories,\nproviding the same security guarantees against plan-deviation attacks. We\ndemonstrate the effectiveness and robustness of our proposed algorithm, which\nsignificantly strengthens the security of multi-robot systems in the face of\nadversarial threats.\n","authors":["Ziqi Yang","Roberto Tron"],"pdf_url":"https://arxiv.org/pdf/2403.13266v1.pdf","comment":"12 pages, 6 figures, submitted to IEEE Transactions on Control of\n  Network Systems"},{"id":"http://arxiv.org/abs/2403.13251v1","updated":"2024-03-20T02:31:23Z","published":"2024-03-20T02:31:23Z","title":"A Rule-Compliance Path Planner for Lane-Merge Scenarios Based on\n  Responsibility-Sensitive Safety","summary":"  Lane merging is one of the critical tasks for self-driving cars, and how to\nperform lane-merge maneuvers effectively and safely has become one of the\nimportant standards in measuring the capability of autonomous driving systems.\nHowever, due to the ambiguity in driving intentions and right-of-way issues,\nthe lane merging process in autonomous driving remains deficient in terms of\nmaintaining or ceding the right-of-way and attributing liability, which could\nresult in protracted durations for merging and problems such as trajectory\noscillation. Hence, we present a rule-compliance path planner (RCPP) for\nlane-merge scenarios, which initially employs the extended\nresponsibility-sensitive safety (RSS) to elucidate the right-of-way, followed\nby the potential field-based sigmoid planner for path generation. In the\nsimulation, we have validated the efficacy of the proposed algorithm. The\nalgorithm demonstrated superior performance over previous approaches in aspects\nsuch as merging time (Saved 72.3%), path length (reduced 53.4%), and\neliminating the trajectory oscillation.\n","authors":["Pengfei Lin","Ehsan Javanmardi","Yuze Jiang","Manabu Tsukada"],"pdf_url":"https://arxiv.org/pdf/2403.13251v1.pdf","comment":"Submitted to IEEE IROS 2024"},{"id":"http://arxiv.org/abs/2403.13245v1","updated":"2024-03-20T02:16:54Z","published":"2024-03-20T02:16:54Z","title":"Federated reinforcement learning for robot motion planning with\n  zero-shot generalization","summary":"  This paper considers the problem of learning a control policy for robot\nmotion planning with zero-shot generalization, i.e., no data collection and\npolicy adaptation is needed when the learned policy is deployed in new\nenvironments. We develop a federated reinforcement learning framework that\nenables collaborative learning of multiple learners and a central server, i.e.,\nthe Cloud, without sharing their raw data. In each iteration, each learner\nuploads its local control policy and the corresponding estimated normalized\narrival time to the Cloud, which then computes the global optimum among the\nlearners and broadcasts the optimal policy to the learners. Each learner then\nselects between its local control policy and that from the Cloud for next\niteration. The proposed framework leverages on the derived zero-shot\ngeneralization guarantees on arrival time and safety. Theoretical guarantees on\nalmost-sure convergence, almost consensus, Pareto improvement and optimality\ngap are also provided. Monte Carlo simulation is conducted to evaluate the\nproposed framework.\n","authors":["Zhenyuan Yuan","Siyuan Xu","Minghui Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.13245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13235v1","updated":"2024-03-20T01:57:24Z","published":"2024-03-20T01:57:24Z","title":"AMCO: Adaptive Multimodal Coupling of Vision and Proprioception for\n  Quadruped Robot Navigation in Outdoor Environments","summary":"  We present AMCO, a novel navigation method for quadruped robots that\nadaptively combines vision-based and proprioception-based perception\ncapabilities. Our approach uses three cost maps: general knowledge map;\ntraversability history map; and current proprioception map; which are derived\nfrom a robot's vision and proprioception data, and couples them to obtain a\ncoupled traversability cost map for navigation. The general knowledge map\nencodes terrains semantically segmented from visual sensing, and represents a\nterrain's typically expected traversability. The traversability history map\nencodes the robot's recent proprioceptive measurements on a terrain and its\nsemantic segmentation as a cost map. Further, the robot's present\nproprioceptive measurement is encoded as a cost map in the current\nproprioception map. As the general knowledge map and traversability history map\nrely on semantic segmentation, we evaluate the reliability of the visual\nsensory data by estimating the brightness and motion blur of input RGB images\nand accordingly combine the three cost maps to obtain the coupled\ntraversability cost map used for navigation. Leveraging this adaptive coupling,\nthe robot can depend on the most reliable input modality available. Finally, we\npresent a novel planner that selects appropriate gaits and velocities for\ntraversing challenging outdoor environments using the coupled traversability\ncost map. We demonstrate AMCO's navigation performance in different real-world\noutdoor environments and observe 10.8%-34.9% reduction w.r.t. two stability\nmetrics, and up to 50% improvement in terms of success rate compared to current\nnavigation methods.\n","authors":["Mohamed Elnoor","Kasun Weerakoon","Adarsh Jagan Sathyamoorthy","Tianrui Guan","Vignesh Rajagopal","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2403.13235v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2403.13221v1","updated":"2024-03-20T00:56:27Z","published":"2024-03-20T00:56:27Z","title":"A Contact Model based on Denoising Diffusion to Learn Variable Impedance\n  Control for Contact-rich Manipulation","summary":"  In this paper, a novel approach is proposed for learning robot control in\ncontact-rich tasks such as wiping, by developing Diffusion Contact Model (DCM).\nPrevious methods of learning such tasks relied on impedance control with\ntime-varying stiffness tuning by performing Bayesian optimization by\ntrial-and-error with robots. The proposed approach aims to reduce the cost of\nrobot operation by predicting the robot contact trajectories from the variable\nstiffness inputs and using neural models. However, contact dynamics are\ninherently highly nonlinear, and their simulation requires iterative\ncomputations such as convex optimization. Moreover, approximating such\ncomputations by using finite-layer neural models is difficult. To overcome\nthese limitations, the proposed DCM used the denoising diffusion models that\ncould simulate the complex dynamics via iterative computations of multi-step\ndenoising, thus improving the prediction accuracy. Stiffness tuning experiments\nconducted in simulated and real environments showed that the DCM achieved\ncomparable performance to a conventional robot-based optimization method while\nreducing the number of robot trials.\n","authors":["Masashi Okada","Mayumi Komatsu","Tadahiro Taniguchi"],"pdf_url":"https://arxiv.org/pdf/2403.13221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14115v2","updated":"2024-03-20T00:23:39Z","published":"2023-12-21T18:40:34Z","title":"LingoQA: Video Question Answering for Autonomous Driving","summary":"  Autonomous driving has long faced a challenge with public acceptance due to\nthe lack of explainability in the decision-making process. Video\nquestion-answering (QA) in natural language provides the opportunity for\nbridging this gap. Nonetheless, evaluating the performance of Video QA models\nhas proved particularly tough due to the absence of comprehensive benchmarks.\nTo fill this gap, we introduce LingoQA, a benchmark specifically for autonomous\ndriving Video QA. The LingoQA trainable metric demonstrates a 0.95 Spearman\ncorrelation coefficient with human evaluations. We introduce a Video QA dataset\nof central London consisting of 419k samples that we release with the paper. We\nestablish a baseline vision-language model and run extensive ablation studies\nto understand its performance.\n","authors":["Ana-Maria Marcu","Long Chen","Jan Hünermann","Alice Karnsund","Benoit Hanotte","Prajwal Chidananda","Saurabh Nair","Vijay Badrinarayanan","Alex Kendall","Jamie Shotton","Elahe Arani","Oleg Sinavski"],"pdf_url":"https://arxiv.org/pdf/2312.14115v2.pdf","comment":"Benchmark and dataset are available at\n  https://github.com/wayveai/LingoQA/"},{"id":"http://arxiv.org/abs/2403.14041v1","updated":"2024-03-20T23:36:30Z","published":"2024-03-20T23:36:30Z","title":"\"It's Not a Replacement:\" Enabling Parent-Robot Collaboration to Support\n  In-Home Learning Experiences of Young Children","summary":"  Learning companion robots for young children are increasingly adopted in\ninformal learning environments. Although parents play a pivotal role in their\nchildren's learning, very little is known about how parents prefer to\nincorporate robots into their children's learning activities. We developed\nprototype capabilities for a learning companion robot to deliver educational\nprompts and responses to parent-child pairs during reading sessions and\nconducted in-home user studies involving 10 families with children aged 3-5.\nOur data indicates that parents want to work with robots as collaborators to\naugment parental activities to foster children's learning, introducing the\nnotion of parent-robot collaboration. Our findings offer an empirical\nunderstanding of the needs and challenges of parent-child interaction in\ninformal learning scenarios and design opportunities for integrating a\ncompanion robot into these interactions. We offer insights into how robots\nmight be designed to facilitate parent-robot collaboration, including parenting\npolicies, collaboration patterns, and interaction paradigms.\n","authors":["Hui-Ru Ho","Edward Hubbard","Bilge Mutlu"],"pdf_url":"https://arxiv.org/pdf/2403.14041v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14029v1","updated":"2024-03-20T22:57:34Z","published":"2024-03-20T22:57:34Z","title":"Quadcopter Team Configurable Motion Guided by a Quadruped","summary":"  The paper focuses on modeling and experimental evaluation of a quadcopter\nteam configurable coordination guided by a single quadruped robot. We consider\nthe quadcopter team as particles of a two-dimensional deformable body and\npropose a two-dimensional affine transformation model for safe and\ncollision-free configurable coordination of this heterogeneous robotic system.\nThe proposed affine transformation is decomposed into translation, that is\nspecified by the quadruped global position, and configurable motion of the\nquadcopters, which is determined by a nonsingular Jacobian matrix so that the\nquadcopter team can safely navigate a constrained environment while avoiding\ncollision. We propose two methods to experimentally evaluate the proposed\nheterogeneous robot coordination model. The first method measures real\npositions of quadcopters, quadruped, and environmental objects all with respect\nto the global coordinate system. On the other hand, the second method measures\nposition with respect to the local coordinate system fixed on the dog robot\nwhich in turn enables safe planning the Jacobian matrix of the quadcopter team\nwhile the world is virtually approached the robotic system.\n","authors":["Mohammad Ghufran","Sourish Tetakayala","Jack Hughes","Aron Wilson","Hossein Rastgoftar"],"pdf_url":"https://arxiv.org/pdf/2403.14029v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14025v1","updated":"2024-03-20T22:51:29Z","published":"2024-03-20T22:51:29Z","title":"HRI Curriculum for a Liberal Arts Education","summary":"  In this paper, we discuss the opportunities and challenges of teaching a\nhuman-robot interaction course at an undergraduate liberal arts college. We\nprovide a sample syllabus adapted from a previous version of a course.\n","authors":["Jason R. Wilson","Emily Jensen"],"pdf_url":"https://arxiv.org/pdf/2403.14025v1.pdf","comment":"Presented at the Designing an Intro to HRI Course Workshop at HRI\n  2024 (arXiv:2403.05588)"},{"id":"http://arxiv.org/abs/2403.14014v1","updated":"2024-03-20T22:24:52Z","published":"2024-03-20T22:24:52Z","title":"Crowdsourcing Task Traces for Service Robotics","summary":"  Demonstration is an effective end-user development paradigm for teaching\nrobots how to perform new tasks. In this paper, we posit that demonstration is\nuseful not only as a teaching tool, but also as a way to understand and assist\nend-user developers in thinking about a task at hand. As a first step toward\ngaining this understanding, we constructed a lightweight web interface to\ncrowdsource step-by-step instructions of common household tasks, leveraging the\nimaginations and past experiences of potential end-user developers. As evidence\nof the utility of our interface, we deployed the interface on Amazon Mechanical\nTurk and collected 207 task traces that span 18 different task categories. We\ndescribe our vision for how these task traces can be operationalized as task\nmodels within end-user development tools and provide a roadmap for future work.\n","authors":["David Porfirio","Allison Sauppé","Maya Cakmak","Aws Albarghouthi","Bilge Mutlu"],"pdf_url":"https://arxiv.org/pdf/2403.14014v1.pdf","comment":"Published in the companion proceedings of the 2023 ACM/IEEE\n  International Conference on Human-Robot Interaction"},{"id":"http://arxiv.org/abs/2403.02316v2","updated":"2024-03-20T22:05:06Z","published":"2024-03-04T18:52:15Z","title":"Designing Library of Skill-Agents for Hardware-Level Reusability","summary":"  To use new robot hardware in a new environment, it is necessary to develop a\ncontrol program tailored to that specific robot in that environment.\nConsidering the reusability of software among robots is crucial to minimize the\neffort involved in this process and maximize software reuse across different\nrobots in different environments. This paper proposes a method to remedy this\nprocess by considering hardware-level reusability, using\nLearning-from-observation (LfO) paradigm with a pre-designed skill-agent\nlibrary. The LfO framework represents the required actions in\nhardware-independent representations, referred to as task models, from\nobserving human demonstrations, capturing the necessary parameters for the\ninteraction between the environment and the robot. When executing the desired\nactions from the task models, a set of skill agents is employed to convert the\nrepresentations into robot commands. This paper focuses on the latter part of\nthe LfO framework, utilizing the set to generate robot actions from the task\nmodels, and explores a hardware-independent design approach for these skill\nagents. These skill agents are described in a hardware-independent manner,\nconsidering the relative relationship between the robot's hand position and the\nenvironment. As a result, it is possible to execute these actions on robots\nwith different hardware configurations by simply swapping the inverse\nkinematics solver. This paper, first, defines a necessary and sufficient\nskill-agent set corresponding to cover all possible actions, and considers the\ndesign principles for these skill agents in the library. We provide concrete\nexamples of such skill agents and demonstrate the practicality of using these\nskill agents by showing that the same representations can be executed on two\ndifferent robots, Nextage and Fetch, using the proposed skill-agents set.\n","authors":["Jun Takamatsu","Daichi Saito","Katsushi Ikeuchi","Atsushi Kanehira","Kazuhiro Sasabuchi","Naoki Wake"],"pdf_url":"https://arxiv.org/pdf/2403.02316v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14000v1","updated":"2024-03-20T21:57:14Z","published":"2024-03-20T21:57:14Z","title":"Visual Imitation Learning of Task-Oriented Object Grasping and\n  Rearrangement","summary":"  Task-oriented object grasping and rearrangement are critical skills for\nrobots to accomplish different real-world manipulation tasks. However, they\nremain challenging due to partial observations of the objects and shape\nvariations in categorical objects. In this paper, we propose the Multi-feature\nImplicit Model (MIMO), a novel object representation that encodes multiple\nspatial features between a point and an object in an implicit neural field.\nTraining such a model on multiple features ensures that it embeds the object\nshapes consistently in different aspects, thus improving its performance in\nobject shape reconstruction from partial observation, shape similarity measure,\nand modeling spatial relations between objects. Based on MIMO, we propose a\nframework to learn task-oriented object grasping and rearrangement from single\nor multiple human demonstration videos. The evaluations in simulation show that\nour approach outperforms the state-of-the-art methods for multi- and\nsingle-view observations. Real-world experiments demonstrate the efficacy of\nour approach in one- and few-shot imitation learning of manipulation tasks.\n","authors":["Yichen Cai","Jianfeng Gao","Christoph Pohl","Tamim Asfour"],"pdf_url":"https://arxiv.org/pdf/2403.14000v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13988v1","updated":"2024-03-20T21:41:44Z","published":"2024-03-20T21:41:44Z","title":"Goal-Oriented End-User Programming of Robots","summary":"  End-user programming (EUP) tools must balance user control with the robot's\nability to plan and act autonomously. Many existing task-oriented EUP tools\nenforce a specific level of control, e.g., by requiring that users hand-craft\ndetailed sequences of actions, rather than offering users the flexibility to\nchoose the level of task detail they wish to express. We thereby created a\nnovel EUP system, Polaris, that in contrast to most existing EUP tools, uses\ngoal predicates as the fundamental building block of programs. Users can\nthereby express high-level robot objectives or lower-level checkpoints at their\nchoosing, while an off-the-shelf task planner fills in any remaining program\ndetail. To ensure that goal-specified programs adhere to user expectations of\nrobot behavior, Polaris is equipped with a Plan Visualizer that exposes the\nplanner's output to the user before runtime. In what follows, we describe our\ndesign of Polaris and its evaluation with 32 human participants. Our results\nsupport the Plan Visualizer's ability to help users craft higher-quality\nprograms. Furthermore, there are strong associations between user perception of\nthe robot and Plan Visualizer usage, and evidence that robot familiarity has a\nkey role in shaping user experience.\n","authors":["David Porfirio","Mark Roberts","Laura M. Hiatt"],"pdf_url":"https://arxiv.org/pdf/2403.13988v1.pdf","comment":"Published in the proceedings of the 2024 ACM/IEEE International\n  Conference on Human-Robot Interaction"},{"id":"http://arxiv.org/abs/2403.13960v1","updated":"2024-03-20T20:13:39Z","published":"2024-03-20T20:13:39Z","title":"Open Access NAO (OAN): a ROS2-based software framework for HRI\n  applications with the NAO robot","summary":"  This paper presents a new software framework for HRI experimentation with the\nsixth version of the common NAO robot produced by the United Robotics Group.\nEmbracing the common demand of researchers for better performance and new\nfeatures for NAO, the authors took advantage of the ability to run ROS2 onboard\non the NAO to develop a framework independent of the APIs provided by the\nmanufacturer. Such a system provides NAO with not only the basic skills of a\nhumanoid robot such as walking and reproducing movements of interest but also\nfeatures often used in HRI such as: speech recognition/synthesis, face and\nobject detention, and the use of Generative Pre-trained Transformer (GPT)\nmodels for conversation. The developed code is therefore configured as a\nready-to-use but also highly expandable and improvable tool thanks to the\npossibilities provided by the ROS community.\n","authors":["Antonio Bono","Kenji Brameld","Luigi D'Alfonso","Giuseppe Fedele"],"pdf_url":"https://arxiv.org/pdf/2403.13960v1.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2312.02352v2","updated":"2024-03-20T19:57:24Z","published":"2023-12-04T21:32:00Z","title":"Working Backwards: Learning to Place by Picking","summary":"  We present placing via picking (PvP), a method to autonomously collect\nreal-world demonstrations for a family of placing tasks in which objects must\nbe manipulated to specific contact-constrained locations. With PvP, we approach\nthe collection of robotic object placement demonstrations by reversing the\ngrasping process and exploiting the inherent symmetry of the pick and place\nproblems. Specifically, we obtain placing demonstrations from a set of grasp\nsequences of objects initially located at their target placement locations. Our\nsystem can collect hundreds of demonstrations in contact-constrained\nenvironments without human intervention by combining two modules: tactile\nregrasping and compliant control for grasps. We train a policy directly from\nvisual observations through behavioral cloning, using the\nautonomously-collected demonstrations. By doing so, the policy can generalize\nto object placement scenarios outside of the training environment without\nprivileged information (e.g., placing a plate picked up from a table). We\nvalidate our approach in home robotic scenarios that include dishwasher loading\nand table setting. Our approach yields robotic placing policies that outperform\npolicies trained with kinesthetic teaching, both in terms of performance and\ndata efficiency, while requiring no human supervision.\n","authors":["Oliver Limoyo","Abhisek Konar","Trevor Ablett","Jonathan Kelly","Francois R. Hogan","Gregory Dudek"],"pdf_url":"https://arxiv.org/pdf/2312.02352v2.pdf","comment":"Submitted to the IEEE/RSJ International Conference on Intelligent\n  Robotics and Systems (IROS'24), Abu Dhabi, UAE, Oct 14-18, 2024"},{"id":"http://arxiv.org/abs/2401.11061v2","updated":"2024-03-20T19:44:07Z","published":"2024-01-19T23:34:48Z","title":"PhotoBot: Reference-Guided Interactive Photography via Natural Language","summary":"  We introduce PhotoBot, a framework for fully automated photo acquisition\nbased on an interplay between high-level human language guidance and a robot\nphotographer. We propose to communicate photography suggestions to the user via\nreference images that are selected from a curated gallery. We leverage a visual\nlanguage model (VLM) and an object detector to characterize the reference\nimages via textual descriptions and then use a large language model (LLM) to\nretrieve relevant reference images based on a user's language query through\ntext-based reasoning. To correspond the reference image and the observed scene,\nwe exploit pre-trained features from a vision transformer capable of capturing\nsemantic similarity across marked appearance variations. Using these features,\nwe compute pose adjustments for an RGB-D camera by solving a\nperspective-n-point (PnP) problem. We demonstrate our approach using a\nmanipulator equipped with a wrist camera. Our user studies show that photos\ntaken by PhotoBot are often more aesthetically pleasing than those taken by\nusers themselves, as measured by human feedback. We also show that PhotoBot can\ngeneralize to other reference sources such as paintings.\n","authors":["Oliver Limoyo","Jimmy Li","Dmitriy Rivkin","Jonathan Kelly","Gregory Dudek"],"pdf_url":"https://arxiv.org/pdf/2401.11061v2.pdf","comment":"Submitted to the IEEE/RSJ International Conference on Intelligent\n  Robotics and Systems (IROS'24), Abu Dhabi, UAE, Oct 14-18, 2024"},{"id":"http://arxiv.org/abs/2403.04169v2","updated":"2024-03-20T19:28:56Z","published":"2024-03-07T02:58:15Z","title":"Social Robots for Sleep Health: A Scoping Review","summary":"  Poor sleep health is an increasingly concerning public healthcare crisis,\nespecially when coupled with a dwindling number of health professionals\nqualified to combat it. However, there is a growing body of scientific\nliterature on the use of digital technologies in supporting and sustaining\nindividuals' healthy sleep habits. Social robots are a relatively recent\ntechnology that has been used to facilitate health care interventions and may\nhave potential in improving sleep health outcomes, as well. Social robots'\nunique characteristics -- such as anthropomorphic physical embodiment or\neffective communication methods -- help to engage users and motivate them to\ncomply with specific interventions, thus improving the interventions' outcomes.\nThis scoping review aims to evaluate current scientific evidence for employing\nsocial robots in sleep health interventions, identify critical research gaps,\nand suggest future directions for developing and using social robots to improve\npeople's sleep health. Our analysis of the reviewed studies found them limited\ndue to a singular focus on the older adult population, use of small sample\nsizes, limited intervention durations, and other compounding factors.\nNevertheless, the reviewed studies reported several positive outcomes,\nhighlighting the potential social robots hold in this field. Although our\nreview found limited clinical evidence for the efficacy of social robots as\npurveyors of sleep health interventions, it did elucidate the potential for a\nsuccessful future in this domain if current limitations are addressed and more\nresearch is conducted.\n","authors":["Victor Nikhil Antony","Mengchi Li","Shu-Han Lin","Junxin Li","Chien-Ming Huang"],"pdf_url":"https://arxiv.org/pdf/2403.04169v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13941v1","updated":"2024-03-20T19:26:27Z","published":"2024-03-20T19:26:27Z","title":"Sensory Glove-Based Surgical Robot User Interface","summary":"  Robotic surgery has reached a high level of maturity and has become an\nintegral part of standard surgical care. However, existing surgeon consoles are\nbulky and take up valuable space in the operating room, present challenges for\nsurgical team coordination, and their proprietary nature makes it difficult to\ntake advantage of recent technological advances, especially in virtual and\naugmented reality. One potential area for further improvement is the\nintegration of modern sensory gloves into robotic platforms, allowing surgeons\nto control robotic arms directly with their hand movements intuitively. We\npropose one such system that combines an HTC Vive tracker, a Manus Meta Prime 3\nXR sensory glove, and God Vision wireless smart glasses. The system controls\none arm of a da Vinci surgical robot. In addition to moving the arm, the\nsurgeon can use fingers to control the end-effector of the surgical instrument.\nHand gestures are used to implement clutching and similar functions. In\nparticular, we introduce clutching of the instrument orientation, a\nfunctionality not available in the da Vinci system. The vibrotactile elements\nof the glove are used to provide feedback to the user when gesture commands are\ninvoked. A preliminary evaluation of the system shows that it has excellent\ntracking accuracy and allows surgeons to efficiently perform common surgical\ntraining tasks with minimal practice with the new interface; this suggests that\nthe interface is highly intuitive. The proposed system is inexpensive, allows\nrapid prototyping, and opens opportunities for further innovations in the\ndesign of surgical robot interfaces.\n","authors":["Leonardo Borgioli","Ki-Hwan Oh","Alberto Mangano","Alvaro Ducas","Luciano Ambrosini","Federico Pinto","Paula A Lopez","Jessica Cassiani","Milos Zefran","Liaohai Chen","Pier Cristoforo Giulianotti"],"pdf_url":"https://arxiv.org/pdf/2403.13941v1.pdf","comment":"6 pages, 5 figures, 7 tables, submitted to International Conference\n  on Intelligent Robots and Systems (IROS)2024"},{"id":"http://arxiv.org/abs/2403.13929v1","updated":"2024-03-20T19:03:26Z","published":"2024-03-20T19:03:26Z","title":"Safety-Aware Perception for Autonomous Collision Avoidance in Dynamic\n  Environments","summary":"  Autonomous collision avoidance requires accurate environmental perception;\nhowever, flight systems often possess limited sensing capabilities with\nfield-of-view (FOV) restrictions. To navigate this challenge, we present a\nsafety-aware approach for online determination of the optimal sensor-pointing\ndirection $\\psi_\\text{d}$ which utilizes control barrier functions (CBFs).\nFirst, we generate a spatial density function $\\Phi$ which leverages CBF\nconstraints to map the collision risk of all local coordinates. Then, we\nconvolve $\\Phi$ with an attitude-dependent sensor FOV quality function to\nproduce the objective function $\\Gamma$ which quantifies the total observed\nrisk for a given pointing direction. Finally, by finding the global optimizer\nfor $\\Gamma$, we identify the value of $\\psi_\\text{d}$ which maximizes the\nperception of risk within the FOV. We incorporate $\\psi_\\text{d}$ into a\nsafety-critical flight architecture and conduct a numerical analysis using\nmultiple simulated mission profiles. Our algorithm achieves a success rate of\n$88-96\\%$, constituting a $16-29\\%$ improvement compared to the best heuristic\nmethods. We demonstrate the functionality of our approach via a flight\ndemonstration using the Crazyflie 2.1 micro-quadrotor. Without a priori\nobstacle knowledge, the quadrotor follows a dynamic flight path while\nsimultaneously calculating and tracking $\\psi_\\text{d}$ to perceive and avoid\ntwo static obstacles with an average computation time of 371 $\\mu$s.\n","authors":["Ryan M. Bena","Chongbo Zhao","Quan Nguyen"],"pdf_url":"https://arxiv.org/pdf/2403.13929v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.15821v2","updated":"2024-03-20T18:45:54Z","published":"2023-09-27T17:45:49Z","title":"LGMCTS: Language-Guided Monte-Carlo Tree Search for Executable Semantic\n  Object Rearrangement","summary":"  We introduce a novel approach to the executable semantic object rearrangement\nproblem. In this challenge, a robot seeks to create an actionable plan that\nrearranges objects within a scene according to a pattern dictated by a natural\nlanguage description. Unlike existing methods such as StructFormer and\nStructDiffusion, which tackle the issue in two steps by first generating poses\nand then leveraging a task planner for action plan formulation, our method\nconcurrently addresses pose generation and action planning. We achieve this\nintegration using a Language-Guided Monte-Carlo Tree Search (LGMCTS).\nQuantitative evaluations are provided on two simulation datasets, and\ncomplemented by qualitative tests with a real robot.\n","authors":["Haonan Chang","Kai Gao","Kowndinya Boyalakuntla","Alex Lee","Baichuan Huang","Harish Udhaya Kumar","Jinjin Yu","Abdeslam Boularias"],"pdf_url":"https://arxiv.org/pdf/2309.15821v2.pdf","comment":"Our code and supplementary materials are accessible at\n  https://github.com/changhaonan/LG-MCTS"},{"id":"http://arxiv.org/abs/2403.13910v1","updated":"2024-03-20T18:30:12Z","published":"2024-03-20T18:30:12Z","title":"Augmented Reality Demonstrations for Scalable Robot Imitation Learning","summary":"  Robot Imitation Learning (IL) is a widely used method for training robots to\nperform manipulation tasks that involve mimicking human demonstrations to\nacquire skills. However, its practicality has been limited due to its\nrequirement that users be trained in operating real robot arms to provide\ndemonstrations. This paper presents an innovative solution: an Augmented\nReality (AR)-assisted framework for demonstration collection, empowering\nnon-roboticist users to produce demonstrations for robot IL using devices like\nthe HoloLens 2. Our framework facilitates scalable and diverse demonstration\ncollection for real-world tasks. We validate our approach with experiments on\nthree classical robotics tasks: reach, push, and pick-and-place. The real robot\nperforms each task successfully while replaying demonstrations collected via\nAR.\n","authors":["Yue Yang","Bryce Ikeda","Gedas Bertasius","Daniel Szafir"],"pdf_url":"https://arxiv.org/pdf/2403.13910v1.pdf","comment":null}]},"2024-03-21T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2403.13331v2","updated":"2024-03-21T04:01:10Z","published":"2024-03-20T06:22:37Z","title":"AMP: Autoregressive Motion Prediction Revisited with Next Token\n  Prediction for Autonomous Driving","summary":"  As an essential task in autonomous driving (AD), motion prediction aims to\npredict the future states of surround objects for navigation. One natural\nsolution is to estimate the position of other agents in a step-by-step manner\nwhere each predicted time-step is conditioned on both observed time-steps and\npreviously predicted time-steps, i.e., autoregressive prediction. Pioneering\nworks like SocialLSTM and MFP design their decoders based on this intuition.\nHowever, almost all state-of-the-art works assume that all predicted time-steps\nare independent conditioned on observed time-steps, where they use a single\nlinear layer to generate positions of all time-steps simultaneously. They\ndominate most motion prediction leaderboards due to the simplicity of training\nMLPs compared to autoregressive networks.\n  In this paper, we introduce the GPT style next token prediction into motion\nforecasting. In this way, the input and output could be represented in a\nunified space and thus the autoregressive prediction becomes more feasible.\nHowever, different from language data which is composed of homogeneous units\n-words, the elements in the driving scene could have complex spatial-temporal\nand semantic relations. To this end, we propose to adopt three factorized\nattention modules with different neighbors for information aggregation and\ndifferent position encoding styles to capture their relations, e.g., encoding\nthe transformation between coordinate systems for spatial relativity while\nadopting RoPE for temporal relativity. Empirically, by equipping with the\naforementioned tailored designs, the proposed method achieves state-of-the-art\nperformance in the Waymo Open Motion and Waymo Interaction datasets. Notably,\nAMP outperforms other recent autoregressive motion prediction methods: MotionLM\nand StateTransformer, which demonstrates the effectiveness of the proposed\ndesigns.\n","authors":["Xiaosong Jia","Shaoshuai Shi","Zijun Chen","Li Jiang","Wenlong Liao","Tao He","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2403.13331v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14626v1","updated":"2024-03-21T17:59:55Z","published":"2024-03-21T17:59:55Z","title":"ODTFormer: Efficient Obstacle Detection and Tracking with Stereo Cameras\n  Based on Transformer","summary":"  Obstacle detection and tracking represent a critical component in robot\nautonomous navigation. In this paper, we propose ODTFormer, a Transformer-based\nmodel to address both obstacle detection and tracking problems. For the\ndetection task, our approach leverages deformable attention to construct a 3D\ncost volume, which is decoded progressively in the form of voxel occupancy\ngrids. We further track the obstacles by matching the voxels between\nconsecutive frames. The entire model can be optimized in an end-to-end manner.\nThrough extensive experiments on DrivingStereo and KITTI benchmarks, our model\nachieves state-of-the-art performance in the obstacle detection task. We also\nreport comparable accuracy to state-of-the-art obstacle tracking models while\nrequiring only a fraction of their computation cost, typically ten-fold to\ntwenty-fold less. The code and model weights will be publicly released.\n","authors":["Tianye Ding","Hongyu Li","Huaizu Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.14626v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2310.16828v2","updated":"2024-03-21T17:56:19Z","published":"2023-10-25T17:57:07Z","title":"TD-MPC2: Scalable, Robust World Models for Continuous Control","summary":"  TD-MPC is a model-based reinforcement learning (RL) algorithm that performs\nlocal trajectory optimization in the latent space of a learned implicit\n(decoder-free) world model. In this work, we present TD-MPC2: a series of\nimprovements upon the TD-MPC algorithm. We demonstrate that TD-MPC2 improves\nsignificantly over baselines across 104 online RL tasks spanning 4 diverse task\ndomains, achieving consistently strong results with a single set of\nhyperparameters. We further show that agent capabilities increase with model\nand data size, and successfully train a single 317M parameter agent to perform\n80 tasks across multiple task domains, embodiments, and action spaces. We\nconclude with an account of lessons, opportunities, and risks associated with\nlarge TD-MPC2 agents. Explore videos, models, data, code, and more at\nhttps://tdmpc2.com\n","authors":["Nicklas Hansen","Hao Su","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2310.16828v2.pdf","comment":"ICLR 2024. Explore videos, models, data, code, and more at\n  https://tdmpc2.com"},{"id":"http://arxiv.org/abs/2403.14605v1","updated":"2024-03-21T17:54:56Z","published":"2024-03-21T17:54:56Z","title":"SDP Synthesis of Maximum Coverage Trees for Probabilistic Planning under\n  Control Constraints","summary":"  The paper presents Maximal Covariance Backward Reachable Trees (MAXCOVAR\nBRT), which is a multi-query algorithm for planning of dynamic systems under\nstochastic motion uncertainty and constraints on the control input with\nexplicit coverage guarantees. In contrast to existing roadmap-based\nprobabilistic planning methods that sample belief nodes randomly and draw edges\nbetween them \\cite{csbrm_tro2024}, under control constraints, the reachability\nof belief nodes needs to be explicitly established and is determined by\nchecking the feasibility of a non-convex program. Moreover, there is no\nexplicit consideration of coverage of the roadmap while adding nodes and edges\nduring the construction procedure for the existing methods. Our contribution is\na novel optimization formulation to add nodes and construct the corresponding\nedge controllers such that the generated roadmap results in provably maximal\ncoverage under control constraints as compared to any other method of adding\nnodes and edges. We characterize formally the notion of coverage of a roadmap\nin this stochastic domain via introduction of the h-$\\operatorname{BRS}$\n(Backward Reachable Set of Distributions) of a tree of distributions under\ncontrol constraints, and also support our method with extensive simulations on\na 6 DoF model.\n","authors":["Naman Aggarwal","Jonathan P. How"],"pdf_url":"https://arxiv.org/pdf/2403.14605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14597v1","updated":"2024-03-21T17:50:22Z","published":"2024-03-21T17:50:22Z","title":"Extended Reality for Enhanced Human-Robot Collaboration: a\n  Human-in-the-Loop Approach","summary":"  The rise of automation has provided an opportunity to achieve higher\nefficiency in manufacturing processes, yet it often compromises the flexibility\nrequired to promptly respond to evolving market needs and meet the demand for\ncustomization. Human-robot collaboration attempts to tackle these challenges by\ncombining the strength and precision of machines with human ingenuity and\nperceptual understanding. In this paper, we conceptualize and propose an\nimplementation framework for an autonomous, machine learning-based manipulator\nthat incorporates human-in-the-loop principles and leverages Extended Reality\n(XR) to facilitate intuitive communication and programming between humans and\nrobots. Furthermore, the conceptual framework foresees human involvement\ndirectly in the robot learning process, resulting in higher adaptability and\ntask generalization. The paper highlights key technologies enabling the\nproposed framework, emphasizing the importance of developing the digital\necosystem as a whole. Additionally, we review the existent implementation\napproaches of XR in human-robot collaboration, showcasing diverse perspectives\nand methodologies. The challenges and future outlooks are discussed, delving\ninto the major obstacles and potential research avenues of XR for more natural\nhuman-robot interaction and integration in the industrial landscape.\n","authors":["Yehor Karpichev","Todd Charter","Homayoun Najjaran"],"pdf_url":"https://arxiv.org/pdf/2403.14597v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14594v1","updated":"2024-03-21T17:49:26Z","published":"2024-03-21T17:49:26Z","title":"VXP: Voxel-Cross-Pixel Large-scale Image-LiDAR Place Recognition","summary":"  Recent works on the global place recognition treat the task as a retrieval\nproblem, where an off-the-shelf global descriptor is commonly designed in\nimage-based and LiDAR-based modalities. However, it is non-trivial to perform\naccurate image-LiDAR global place recognition since extracting consistent and\nrobust global descriptors from different domains (2D images and 3D point\nclouds) is challenging. To address this issue, we propose a novel\nVoxel-Cross-Pixel (VXP) approach, which establishes voxel and pixel\ncorrespondences in a self-supervised manner and brings them into a shared\nfeature space. Specifically, VXP is trained in a two-stage manner that first\nexplicitly exploits local feature correspondences and enforces similarity of\nglobal descriptors. Extensive experiments on the three benchmarks (Oxford\nRobotCar, ViViD++ and KITTI) demonstrate our method surpasses the\nstate-of-the-art cross-modal retrieval by a large margin.\n","authors":["Yun-Jin Li","Mariia Gladkova","Yan Xia","Rui Wang","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2403.14594v1.pdf","comment":"Project page https://yunjinli.github.io/projects-vxp/"},{"id":"http://arxiv.org/abs/2403.14583v1","updated":"2024-03-21T17:37:43Z","published":"2024-03-21T17:37:43Z","title":"Co-Optimization of Environment and Policies for Decentralized\n  Multi-Agent Navigation","summary":"  This work views the multi-agent system and its surrounding environment as a\nco-evolving system, where the behavior of one affects the other. The goal is to\ntake both agent actions and environment configurations as decision variables,\nand optimize these two components in a coordinated manner to improve some\nmeasure of interest. Towards this end, we consider the problem of decentralized\nmulti-agent navigation in cluttered environments. By introducing two\nsub-objectives of multi-agent navigation and environment optimization, we\npropose an $\\textit{agent-environment co-optimization}$ problem and develop a\n$\\textit{coordinated algorithm}$ that alternates between these sub-objectives\nto search for an optimal synthesis of agent actions and obstacle configurations\nin the environment; ultimately, improving the navigation performance. Due to\nthe challenge of explicitly modeling the relation between agents, environment\nand performance, we leverage policy gradient to formulate a model-free learning\nmechanism within the coordinated framework. A formal convergence analysis shows\nthat our coordinated algorithm tracks the local minimum trajectory of an\nassociated time-varying non-convex optimization problem. Extensive numerical\nresults corroborate theoretical findings and show the benefits of\nco-optimization over baselines. Interestingly, the results also indicate that\noptimized environment configurations are able to offer structural guidance that\nis key to de-conflicting agents in motion.\n","authors":["Zhan Gao","Guang Yang","Amanda Prorok"],"pdf_url":"https://arxiv.org/pdf/2403.14583v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.01477v2","updated":"2024-03-21T17:31:00Z","published":"2024-02-02T15:06:00Z","title":"A Modular Aerial System Based on Homogeneous Quadrotors with\n  Fault-Tolerant Control","summary":"  The standard quadrotor is one of the most popular and widely used aerial\nvehicle of recent decades, offering great maneuverability with mechanical\nsimplicity. However, the under-actuation characteristic limits its\napplications, especially when it comes to generating desired wrench with six\ndegrees of freedom (DOF). Therefore, existing work often compromises between\nmechanical complexity and the controllable DOF of the aerial system. To take\nadvantage of the mechanical simplicity of a standard quadrotor, we propose a\nmodular aerial system, IdentiQuad, that combines only homogeneous\nquadrotor-based modules. Each IdentiQuad can be operated alone like a standard\nquadrotor, but at the same time allows task-specific assembly, increasing the\ncontrollable DOF of the system. Each module is interchangeable within its\nassembly. We also propose a general controller for different configurations of\nassemblies, capable of tolerating rotor failures and balancing the energy\nconsumption of each module. The functionality and robustness of the system and\nits controller are validated using physics-based simulations for different\nassembly configurations.\n","authors":["Mengguang Li","Kai Cui","Heinz Koeppl"],"pdf_url":"https://arxiv.org/pdf/2402.01477v2.pdf","comment":"ICRA2024"},{"id":"http://arxiv.org/abs/2403.14545v1","updated":"2024-03-21T16:44:49Z","published":"2024-03-21T16:44:49Z","title":"Learning Hierarchical Control For Constrained Dynamic Task Assignment","summary":"  This paper introduces a novel data-driven hierarchical control scheme for\nmanaging a fleet of nonlinear, capacity-constrained autonomous agents in an\niterative environment. We propose a control framework consisting of a\nhigh-level dynamic task assignment and routing layer and low-level motion\nplanning and tracking layer. Each layer of the control hierarchy uses a\ndata-driven MPC policy, maintaining bounded computational complexity at each\ncalculation of a new task assignment or actuation input. We utilize collected\ndata to iteratively refine estimates of agent capacity usage, and update MPC\npolicy parameters accordingly. Our approach leverages tools from iterative\nlearning control to integrate learning at both levels of the hierarchy, and\ncoordinates learning between levels in order to maintain closed-loop\nfeasibility and performance improvement of the connected architecture.\n","authors":["Charlott Vallon","Alessandro Pinto","Bartolomeo Stellato","Francesco Borrelli"],"pdf_url":"https://arxiv.org/pdf/2403.14545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17587v2","updated":"2024-03-21T16:40:43Z","published":"2024-02-25T07:59:10Z","title":"Instance-aware Exploration-Verification-Exploitation for Instance\n  ImageGoal Navigation","summary":"  As a new embodied vision task, Instance ImageGoal Navigation (IIN) aims to\nnavigate to a specified object depicted by a goal image in an unexplored\nenvironment.\n  The main challenge of this task lies in identifying the target object from\ndifferent viewpoints while rejecting similar distractors.\n  Existing ImageGoal Navigation methods usually adopt the simple\nExploration-Exploitation framework and ignore the identification of specific\ninstance during navigation.\n  In this work, we propose to imitate the human behaviour of ``getting closer\nto confirm\" when distinguishing objects from a distance.\n  Specifically, we design a new modular navigation framework named\nInstance-aware Exploration-Verification-Exploitation (IEVE) for instance-level\nimage goal navigation.\n  Our method allows for active switching among the exploration, verification,\nand exploitation actions, thereby facilitating the agent in making reasonable\ndecisions under different situations.\n  On the challenging HabitatMatterport 3D semantic (HM3D-SEM) dataset, our\nmethod surpasses previous state-of-the-art work, with a classical segmentation\nmodel (0.684 vs. 0.561 success) or a robust model (0.702 vs. 0.561 success).\nOur code will be made publicly available at https://github.com/XiaohanLei/IEVE.\n","authors":["Xiaohan Lei","Min Wang","Wengang Zhou","Li Li","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2402.17587v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14526v1","updated":"2024-03-21T16:26:19Z","published":"2024-03-21T16:26:19Z","title":"Click to Grasp: Zero-Shot Precise Manipulation via Visual Diffusion\n  Descriptors","summary":"  Precise manipulation that is generalizable across scenes and objects remains\na persistent challenge in robotics. Current approaches for this task heavily\ndepend on having a significant number of training instances to handle objects\nwith pronounced visual and/or geometric part ambiguities. Our work explores the\ngrounding of fine-grained part descriptors for precise manipulation in a\nzero-shot setting by utilizing web-trained text-to-image diffusion-based\ngenerative models. We tackle the problem by framing it as a dense semantic part\ncorrespondence task. Our model returns a gripper pose for manipulating a\nspecific part, using as reference a user-defined click from a source image of a\nvisually different instance of the same object. We require no manual grasping\ndemonstrations as we leverage the intrinsic object geometry and features.\nPractical experiments in a real-world tabletop scenario validate the efficacy\nof our approach, demonstrating its potential for advancing semantic-aware\nrobotics manipulation. Web page: https://tsagkas.github.io/click2grasp\n","authors":["Nikolaos Tsagkas","Jack Rome","Subramanian Ramamoorthy","Oisin Mac Aodha","Chris Xiaoxuan Lu"],"pdf_url":"https://arxiv.org/pdf/2403.14526v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.12157v2","updated":"2024-03-21T16:09:57Z","published":"2023-03-21T19:34:20Z","title":"Learning a Depth Covariance Function","summary":"  We propose learning a depth covariance function with applications to\ngeometric vision tasks. Given RGB images as input, the covariance function can\nbe flexibly used to define priors over depth functions, predictive\ndistributions given observations, and methods for active point selection. We\nleverage these techniques for a selection of downstream tasks: depth\ncompletion, bundle adjustment, and monocular dense visual odometry.\n","authors":["Eric Dexheimer","Andrew J. Davison"],"pdf_url":"https://arxiv.org/pdf/2303.12157v2.pdf","comment":"CVPR 2023. Project page: https://edexheim.github.io/DepthCov/"},{"id":"http://arxiv.org/abs/2403.14488v1","updated":"2024-03-21T15:36:26Z","published":"2024-03-21T15:36:26Z","title":"Physics-Based Causal Reasoning for Safe & Robust Next-Best Action\n  Selection in Robot Manipulation Tasks","summary":"  Safe and efficient object manipulation is a key enabler of many real-world\nrobot applications. However, this is challenging because robot operation must\nbe robust to a range of sensor and actuator uncertainties. In this paper, we\npresent a physics-informed causal-inference-based framework for a robot to\nprobabilistically reason about candidate actions in a block stacking task in a\npartially observable setting. We integrate a physics-based simulation of the\nrigid-body system dynamics with a causal Bayesian network (CBN) formulation to\ndefine a causal generative probabilistic model of the robot decision-making\nprocess. Using simulation-based Monte Carlo experiments, we demonstrate our\nframework's ability to successfully: (1) predict block tower stability with\nhigh accuracy (Pred Acc: 88.6%); and, (2) select an approximate next-best\naction for the block stacking task, for execution by an integrated robot\nsystem, achieving 94.2% task success rate. We also demonstrate our framework's\nsuitability for real-world robot systems by demonstrating successful task\nexecutions with a domestic support robot, with perception and manipulation\nsub-system integration. Hence, we show that by embedding physics-based causal\nreasoning into robots' decision-making processes, we can make robot task\nexecution safer, more reliable, and more robust to various types of\nuncertainty.\n","authors":["Ricardo Cannizzaro","Michael Groom","Jonathan Routley","Robert Osazuwa Ness","Lars Kunze"],"pdf_url":"https://arxiv.org/pdf/2403.14488v1.pdf","comment":"8 pages, 9 figures, submitted to 2024 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS)"},{"id":"http://arxiv.org/abs/2304.03133v2","updated":"2024-03-21T15:18:20Z","published":"2023-04-06T15:06:28Z","title":"Deep learning reduces sensor requirements for gust rejection on a small\n  uncrewed aerial vehicle morphing wing","summary":"  There is a growing need for uncrewed aerial vehicles (UAVs) to operate in\ncities. However, the uneven urban landscape and complex street systems cause\nlarge-scale wind gusts that challenge the safe and effective operation of UAVs.\nCurrent gust alleviation methods rely on traditional control surfaces and\ncomputationally expensive modeling to select a control action, leading to a\nslower response. Here, we used deep reinforcement learning to create an\nautonomous gust alleviation controller for a camber-morphing wing. This method\nreduced gust impact by 84%, directly from real-time, on-board pressure signals.\nNotably, we found that gust alleviation using signals from only three pressure\ntaps was statistically indistinguishable from using six signals. This\nreduced-sensor fly-by-feel control opens the door to UAV missions in previously\ninoperable locations.\n","authors":["Kevin PT. Haughn","Christina Harvey","Daniel J. Inman"],"pdf_url":"https://arxiv.org/pdf/2304.03133v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.12001v3","updated":"2024-03-21T15:07:30Z","published":"2023-09-21T12:16:32Z","title":"Exploring Human's Gender Perception and Bias toward Non-Humanoid Robots","summary":"  As non-humanoid robots increasingly permeate various sectors, understanding\ntheir design implications for human acceptance becomes paramount. Despite their\nubiquity, studies on how to improve human interaction are sparse. Our\ninvestigation, conducted through two surveys, addresses this gap. The first\nsurvey emphasizes non-humanoid robots and human perceptions about gender\nattributions, suggesting that both design and perceived gender influence\nacceptance. Survey 2 investigates the effects of varying gender cues on robot\ndesigns and their consequent impacts on human-robot interactions. Our findings\nhighlighted that distinct gender cues can bolster or impede interaction\ncomfort.\n","authors":["Mahya Ramezani","Jose Luis Sanchez-Lopez"],"pdf_url":"https://arxiv.org/pdf/2309.12001v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14449v1","updated":"2024-03-21T14:56:46Z","published":"2024-03-21T14:56:46Z","title":"Bringing Robots Home: The Rise of AI Robots in Consumer Electronics","summary":"  On March 18, 2024, NVIDIA unveiled Project GR00T, a general-purpose\nmultimodal generative AI model designed specifically for training humanoid\nrobots. Preceding this event, Tesla's unveiling of the Optimus Gen 2 humanoid\nrobot on December 12, 2023, underscored the profound impact robotics is poised\nto have on reshaping various facets of our daily lives. While robots have long\ndominated industrial settings, their presence within our homes is a burgeoning\nphenomenon. This can be attributed, in part, to the complexities of domestic\nenvironments and the challenges of creating robots that can seamlessly\nintegrate into our daily routines.\n","authors":["Haiwei Dong","Yang Liu","Ted Chu","Abdulmotaleb El Saddik"],"pdf_url":"https://arxiv.org/pdf/2403.14449v1.pdf","comment":"Accepted by IEEE Consumer Electronics Magazine"},{"id":"http://arxiv.org/abs/2403.14447v1","updated":"2024-03-21T14:53:50Z","published":"2024-03-21T14:53:50Z","title":"Exploring 3D Human Pose Estimation and Forecasting from the Robot's\n  Perspective: The HARPER Dataset","summary":"  We introduce HARPER, a novel dataset for 3D body pose estimation and forecast\nin dyadic interactions between users and \\spot, the quadruped robot\nmanufactured by Boston Dynamics. The key-novelty is the focus on the robot's\nperspective, i.e., on the data captured by the robot's sensors. These make 3D\nbody pose analysis challenging because being close to the ground captures\nhumans only partially. The scenario underlying HARPER includes 15 actions, of\nwhich 10 involve physical contact between the robot and users. The Corpus\ncontains not only the recordings of the built-in stereo cameras of Spot, but\nalso those of a 6-camera OptiTrack system (all recordings are synchronized).\nThis leads to ground-truth skeletal representations with a precision lower than\na millimeter. In addition, the Corpus includes reproducible benchmarks on 3D\nHuman Pose Estimation, Human Pose Forecasting, and Collision Prediction, all\nbased on publicly available baseline approaches. This enables future HARPER\nusers to rigorously compare their results with those we provide in this work.\n","authors":["Andrea Avogaro. Andrea Toaiari","Federico Cunico","Xiangmin Xu","Haralambos Dafas","Alessandro Vinciarelli","Emma Li","Marco Cristani"],"pdf_url":"https://arxiv.org/pdf/2403.14447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14414v1","updated":"2024-03-21T13:59:32Z","published":"2024-03-21T13:59:32Z","title":"Efficient Model Learning and Adaptive Tracking Control of Magnetic\n  Micro-Robots for Non-Contact Manipulation","summary":"  Magnetic microrobots can be navigated by an external magnetic field to\nautonomously move within living organisms with complex and unstructured\nenvironments. Potential applications include drug delivery, diagnostics, and\ntherapeutic interventions. Existing techniques commonly impart magnetic\nproperties to the target object,or drive the robot to contact and then\nmanipulate the object, both probably inducing physical damage. This paper\nconsiders a non-contact formulation, where the robot spins to generate a\nrepulsive field to push the object without physical contact. Under such a\nformulation, the main challenge is that the motion model between the input of\nthe magnetic field and the output velocity of the target object is commonly\nunknown and difficult to analyze. To deal with it, this paper proposes a\ndata-driven-based solution. A neural network is constructed to efficiently\nestimate the motion model. Then, an approximate model-based optimal control\nscheme is developed to push the object to track a time-varying trajectory,\nmaintaining the non-contact with distance constraints. Furthermore, a\nstraightforward planner is introduced to assess the adaptability of non-contact\nmanipulation in a cluttered unstructured environment. Experimental results are\npresented to show the tracking and navigation performance of the proposed\nscheme.\n","authors":["Yongyi Jia","Shu Miao","Junjian Zhou","Niandong Jiao","Lianqing Liu","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2403.14414v1.pdf","comment":"7 pages, 6 figures, received by 2024 IEEE International Conference on\n  Robotics and Automation"},{"id":"http://arxiv.org/abs/2402.16348v2","updated":"2024-03-21T13:58:30Z","published":"2024-02-26T07:02:05Z","title":"Star-Searcher: A Complete and Efficient Aerial System for Autonomous\n  Target Search in Complex Unknown Environments","summary":"  This paper tackles the challenge of autonomous target search using unmanned\naerial vehicles (UAVs) in complex unknown environments. To fill the gap in\nsystematic approaches for this task, we introduce Star-Searcher, an aerial\nsystem featuring specialized sensor suites, mapping, and planning modules to\noptimize searching. Path planning challenges due to increased inspection\nrequirements are addressed through a hierarchical planner with a\nvisibility-based viewpoint clustering method. This simplifies planning by\nbreaking it into global and local sub-problems, ensuring efficient global and\nlocal path coverage in real-time. Furthermore, our global path planning employs\na history-aware mechanism to reduce motion inconsistency from frequent map\nchanges, significantly enhancing search efficiency. We conduct comparisons with\nstate-of-the-art methods in both simulation and the real world, demonstrating\nshorter flight paths, reduced time, and higher target search completeness. Our\napproach will be open-sourced for community benefit at\nhttps://github.com/SYSU-STAR/STAR-Searcher.\n","authors":["Yiming Luo","Zixuan Zhuang","Neng Pan","Chen Feng","Shaojie Shen","Fei Gao","Hui Cheng","Boyu Zhou"],"pdf_url":"https://arxiv.org/pdf/2402.16348v2.pdf","comment":"Aceepted to IEEE RA-L. Code:\n  https://github.com/SYSU-STAR/STAR-Searcher. Video:\n  https://www.youtube.com/watch?v=08ll_oo_DtU"},{"id":"http://arxiv.org/abs/2401.15174v2","updated":"2024-03-21T13:16:13Z","published":"2024-01-26T19:39:33Z","title":"Large Language Models for Multi-Modal Human-Robot Interaction","summary":"  This paper presents an innovative large language model (LLM)-based robotic\nsystem for enhancing multi-modal human-robot interaction (HRI). Traditional HRI\nsystems relied on complex designs for intent estimation, reasoning, and\nbehavior generation, which were resource-intensive. In contrast, our system\nempowers researchers and practitioners to regulate robot behavior through three\nkey aspects: providing high-level linguistic guidance, creating \"atomics\" for\nactions and expressions the robot can use, and offering a set of examples.\nImplemented on a physical robot, it demonstrates proficiency in adapting to\nmulti-modal inputs and determining the appropriate manner of action to assist\nhumans with its arms, following researchers' defined guidelines.\nSimultaneously, it coordinates the robot's lid, neck, and ear movements with\nspeech output to produce dynamic, multi-modal expressions. This showcases the\nsystem's potential to revolutionize HRI by shifting from conventional, manual\nstate-and-flow design methods to an intuitive, guidance-based, and\nexample-driven approach.\n","authors":["Chao Wang","Stephan Hasler","Daniel Tanneberg","Felix Ocker","Frank Joublin","Antonello Ceravola","Joerg Deigmoeller","Michael Gienger"],"pdf_url":"https://arxiv.org/pdf/2401.15174v2.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2401.16899v2","updated":"2024-03-21T12:56:24Z","published":"2024-01-30T11:05:43Z","title":"MAkEable: Memory-centered and Affordance-based Task Execution Framework\n  for Transferable Mobile Manipulation Skills","summary":"  To perform versatile mobile manipulation tasks in human-centered\nenvironments, the ability to efficiently transfer learned tasks and experiences\nfrom one robot to another or across different environments is key. In this\npaper, we present MAkEable, a versatile uni- and multi-manual mobile\nmanipulation framework that facilitates the transfer of capabilities and\nknowledge across different tasks, environments, and robots. Our framework\nintegrates an affordance-based task description into the memory-centric\ncognitive architecture of the ARMAR humanoid robot family, which supports the\nsharing of experiences and demonstrations for transfer learning. By\nrepresenting mobile manipulation actions through affordances, i.e., interaction\npossibilities of the robot with its environment, we provide a unifying\nframework for the autonomous uni- and multi-manual manipulation of known and\nunknown objects in various environments. We demonstrate the applicability of\nthe framework in real-world experiments for multiple robots, tasks, and\nenvironments. This includes grasping known and unknown objects, object placing,\nbimanual object grasping, memory-enabled skill transfer in a drawer opening\nscenario across two different humanoid robots, and a pouring task learned from\nhuman demonstration.\n","authors":["Christoph Pohl","Fabian Reister","Fabian Peller-Konrad","Tamim Asfour"],"pdf_url":"https://arxiv.org/pdf/2401.16899v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14353v1","updated":"2024-03-21T12:28:44Z","published":"2024-03-21T12:28:44Z","title":"DaCapo: Accelerating Continuous Learning in Autonomous Systems for Video\n  Analytics","summary":"  Deep neural network (DNN) video analytics is crucial for autonomous systems\nsuch as self-driving vehicles, unmanned aerial vehicles (UAVs), and security\nrobots. However, real-world deployment faces challenges due to their limited\ncomputational resources and battery power. To tackle these challenges,\ncontinuous learning exploits a lightweight \"student\" model at deployment\n(inference), leverages a larger \"teacher\" model for labeling sampled data\n(labeling), and continuously retrains the student model to adapt to changing\nscenarios (retraining). This paper highlights the limitations in\nstate-of-the-art continuous learning systems: (1) they focus on computations\nfor retraining, while overlooking the compute needs for inference and labeling,\n(2) they rely on power-hungry GPUs, unsuitable for battery-operated autonomous\nsystems, and (3) they are located on a remote centralized server, intended for\nmulti-tenant scenarios, again unsuitable for autonomous systems due to privacy,\nnetwork availability, and latency concerns. We propose a hardware-algorithm\nco-designed solution for continuous learning, DaCapo, that enables autonomous\nsystems to perform concurrent executions of inference, labeling, and training\nin a performant and energy-efficient manner. DaCapo comprises (1) a\nspatially-partitionable and precision-flexible accelerator enabling parallel\nexecution of kernels on sub-accelerators at their respective precisions, and\n(2) a spatiotemporal resource allocation algorithm that strategically navigates\nthe resource-accuracy tradeoff space, facilitating optimal decisions for\nresource allocation to achieve maximal accuracy. Our evaluation shows that\nDaCapo achieves 6.5% and 5.5% higher accuracy than a state-of-the-art GPU-based\ncontinuous learning systems, Ekya and EOMU, respectively, while consuming 254x\nless power.\n","authors":["Yoonsung Kim","Changhun Oh","Jinwoo Hwang","Wonung Kim","Seongryong Oh","Yubin Lee","Hardik Sharma","Amir Yazdanbakhsh","Jongse Park"],"pdf_url":"https://arxiv.org/pdf/2403.14353v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14347v1","updated":"2024-03-21T12:24:01Z","published":"2024-03-21T12:24:01Z","title":"A Comparative Study of Real-Time Implementable Cooperative Aerial\n  Manipulation Systems","summary":"  This survey paper focuses on quadrotor- and multirotor- based cooperative\naerial manipulation. Emphasis is first given on comparing and evaluating\nprototype systems that have been implemented and tested in real-time in diverse\napplication environments. Underlying modeling and control approaches are also\ndiscussed and compared. The outcome of the survey allows for understanding the\nmotivation and rationale to develop such systems, their applicability and\nimplementability in diverse applications and also challenges that need to be\naddressed and overcome. Moreover, the survey provides a guide to develop the\nnext generation of prototype systems based on preferred characteristics,\nfunctionality, operability and application domain.\n","authors":["Stamatina C. Barakou","Costas S. Tzafestas","Kimon P. Valavanis"],"pdf_url":"https://arxiv.org/pdf/2403.14347v1.pdf","comment":"Submitted to MDPI Drones"},{"id":"http://arxiv.org/abs/2403.14344v1","updated":"2024-03-21T12:22:47Z","published":"2024-03-21T12:22:47Z","title":"Tell Me What You Want (What You Really, Really Want): Addressing the\n  Expectation Gap for Goal Conveyance from Humans to Robots","summary":"  Conveying human goals to autonomous systems (AS) occurs both when the system\nis being designed and when it is being operated. The design-step conveyance is\ntypically mediated by robotics and AI engineers, who must appropriately capture\nend-user requirements and concepts of operations, while the operation-step\nconveyance is mediated by the design, interfaces, and behavior of the AI.\nHowever, communication can be difficult during both these periods because of\nmismatches in the expectations and expertise of the end-user and the\nroboticist, necessitating more design cycles to resolve. We examine some of the\nbarriers in communicating system design requirements, and develop an\naugmentation for applied cognitive task analysis (ACTA) methods, that we call\nrobot task analysis (RTA), pertaining specifically to the development of\nautonomous systems. Further, we introduce a top-down view of an underexplored\narea of friction between requirements communication -- implied human\nexpectations -- utilizing a collection of work primarily from experimental\npsychology and social sciences. We show how such expectations can be used in\nconjunction with task-specific expectations and the system design process for\nAS to improve design team communication, alleviate barriers to user rejection,\nand reduce the number of design cycles.\n","authors":["Kevin Leahy","Ho Chit Siu"],"pdf_url":"https://arxiv.org/pdf/2403.14344v1.pdf","comment":"Presented at the End-User Development for Human-Robot Interaction\n  (EUD4HRI) workshop at HRI 2024"},{"id":"http://arxiv.org/abs/2403.12670v2","updated":"2024-03-21T12:14:14Z","published":"2024-03-19T12:11:57Z","title":"Driving Animatronic Robot Facial Expression From Speech","summary":"  Animatronic robots aim to enable natural human-robot interaction through\nlifelike facial expressions. However, generating realistic, speech-synchronized\nrobot expressions is challenging due to the complexities of facial biomechanics\nand responsive motion synthesis. This paper presents a principled,\nskinning-centric approach to drive animatronic robot facial expressions from\nspeech. The proposed approach employs linear blend skinning (LBS) as the core\nrepresentation to guide tightly integrated innovations in embodiment design and\nmotion synthesis. LBS informs the actuation topology, enables human expression\nretargeting, and allows speech-driven facial motion generation. The proposed\napproach is capable of generating highly realistic, real-time facial\nexpressions from speech on an animatronic face, significantly advancing robots'\nability to replicate nuanced human expressions for natural interaction.\n","authors":["Boren Li","Hang Li","Hangxin Liu"],"pdf_url":"https://arxiv.org/pdf/2403.12670v2.pdf","comment":"Under review. For associated project page, see\n  https://library87.github.io/animatronic-face-iros24"},{"id":"http://arxiv.org/abs/2402.16068v3","updated":"2024-03-21T11:58:49Z","published":"2024-02-25T11:37:23Z","title":"ROS-Causal: A ROS-based Causal Analysis Framework for Human-Robot\n  Interaction Applications","summary":"  Deploying robots in human-shared spaces requires understanding interactions\namong nearby agents and objects. Modelling cause-and-effect relations through\ncausal inference aids in predicting human behaviours and anticipating robot\ninterventions. However, a critical challenge arises as existing causal\ndiscovery methods currently lack an implementation inside the ROS ecosystem,\nthe standard de facto in robotics, hindering effective utilisation in robotics.\nTo address this gap, this paper introduces ROS-Causal, a ROS-based framework\nfor onboard data collection and causal discovery in human-robot spatial\ninteractions. An ad-hoc simulator, integrated with ROS, illustrates the\napproach's effectiveness, showcasing the robot onboard generation of causal\nmodels during data collection. ROS-Causal is available on GitHub:\nhttps://github.com/lcastri/roscausal.git.\n","authors":["Luca Castri","Gloria Beraldo","Sariah Mghames","Marc Hanheide","Nicola Bellotto"],"pdf_url":"https://arxiv.org/pdf/2402.16068v3.pdf","comment":"Accepted by the \"Causal-HRI: Causal Learning for Human-Robot\n  Interaction\" workshop at the 2024 ACM/IEEE International Conference on\n  Human-Robot Interaction (HRI)"},{"id":"http://arxiv.org/abs/2403.14328v1","updated":"2024-03-21T11:54:45Z","published":"2024-03-21T11:54:45Z","title":"Distilling Reinforcement Learning Policies for Interpretable Robot\n  Locomotion: Gradient Boosting Machines and Symbolic Regression","summary":"  Recent advancements in reinforcement learning (RL) have led to remarkable\nachievements in robot locomotion capabilities. However, the complexity and\n``black-box'' nature of neural network-based RL policies hinder their\ninterpretability and broader acceptance, particularly in applications demanding\nhigh levels of safety and reliability. This paper introduces a novel approach\nto distill neural RL policies into more interpretable forms using Gradient\nBoosting Machines (GBMs), Explainable Boosting Machines (EBMs) and Symbolic\nRegression. By leveraging the inherent interpretability of generalized additive\nmodels, decision trees, and analytical expressions, we transform opaque neural\nnetwork policies into more transparent ``glass-box'' models. We train expert\nneural network policies using RL and subsequently distill them into (i) GBMs,\n(ii) EBMs, and (iii) symbolic policies. To address the inherent distribution\nshift challenge of behavioral cloning, we propose to use the Dataset\nAggregation (DAgger) algorithm with a curriculum of episode-dependent\nalternation of actions between expert and distilled policies, to enable\nefficient distillation of feedback control policies. We evaluate our approach\non various robot locomotion gaits -- walking, trotting, bounding, and pacing --\nand study the importance of different observations in joint actions for\ndistilled policies using various methods. We train neural expert policies for\n205 hours of simulated experience and distill interpretable policies with only\n10 minutes of simulated interaction for each gait using the proposed method.\n","authors":["Fernando Acero","Zhibin Li"],"pdf_url":"https://arxiv.org/pdf/2403.14328v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14326v1","updated":"2024-03-21T11:50:00Z","published":"2024-03-21T11:50:00Z","title":"Evaluation and Deployment of LiDAR-based Place Recognition in Dense\n  Forests","summary":"  Many LiDAR place recognition systems have been developed and tested\nspecifically for urban driving scenarios. Their performance in natural\nenvironments such as forests and woodlands have been studied less closely. In\nthis paper, we analyzed the capabilities of four different LiDAR place\nrecognition systems, both handcrafted and learning-based methods, using LiDAR\ndata collected with a handheld device and legged robot within dense forest\nenvironments. In particular, we focused on evaluating localization where there\nis significant translational and orientation difference between corresponding\nLiDAR scan pairs. This is particularly important for forest survey systems\nwhere the sensor or robot does not follow a defined road or path. Extending our\nanalysis we then incorporated the best performing approach, Logg3dNet, into a\nfull 6-DoF pose estimation system -- introducing several verification layers\nfor precise registration. We demonstrated the performance of our methods in\nthree operational modes: online SLAM, offline multi-mission SLAM map merging,\nand relocalization into a prior map. We evaluated these modes using data\ncaptured in forests from three different countries, achieving 80% of correct\nloop closures candidates with baseline distances up to 5m, and 60% up to 10m.\n","authors":["Haedam Oh","Nived Chebrolu","Matias Mattamala","Leonard Freißmuth","Maurice Fallon"],"pdf_url":"https://arxiv.org/pdf/2403.14326v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14320v1","updated":"2024-03-21T11:41:39Z","published":"2024-03-21T11:41:39Z","title":"Exosense: A Vision-Centric Scene Understanding System For Safe\n  Exoskeleton Navigation","summary":"  Exoskeletons for daily use by those with mobility impairments are being\ndeveloped. They will require accurate and robust scene understanding systems.\nCurrent research has used vision to identify immediate terrain and geometric\nobstacles, however these approaches are constrained to detections directly in\nfront of the user and are limited to classifying a finite range of terrain\ntypes (e.g., stairs, ramps and level-ground). This paper presents Exosense, a\nvision-centric scene understanding system which is capable of generating rich,\nglobally-consistent elevation maps, incorporating both semantic and terrain\ntraversability information. It features an elastic Atlas mapping framework\nassociated with a visual SLAM pose graph, embedded with open-vocabulary room\nlabels from a Vision-Language Model (VLM). The device's design includes a wide\nfield-of-view (FoV) fisheye multi-camera system to mitigate the challenges\nintroduced by the exoskeleton walking pattern. We demonstrate the system's\nrobustness to the challenges of typical periodic walking gaits, and its ability\nto construct accurate semantically-rich maps in indoor settings. Additionally,\nwe showcase its potential for motion planning -- providing a step towards safe\nnavigation for exoskeletons.\n","authors":["Jianeng Wang","Matias Mattamala","Christina Kassab","Lintong Zhang","Maurice Fallon"],"pdf_url":"https://arxiv.org/pdf/2403.14320v1.pdf","comment":"8 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.14305v1","updated":"2024-03-21T11:21:17Z","published":"2024-03-21T11:21:17Z","title":"Bayesian Optimization for Sample-Efficient Policy Improvement in Robotic\n  Manipulation","summary":"  Sample efficient learning of manipulation skills poses a major challenge in\nrobotics. While recent approaches demonstrate impressive advances in the type\nof task that can be addressed and the sensing modalities that can be\nincorporated, they still require large amounts of training data. Especially\nwith regard to learning actions on robots in the real world, this poses a major\nproblem due to the high costs associated with both demonstrations and\nreal-world robot interactions. To address this challenge, we introduce\nBOpt-GMM, a hybrid approach that combines imitation learning with own\nexperience collection. We first learn a skill model as a dynamical system\nencoded in a Gaussian Mixture Model from a few demonstrations. We then improve\nthis model with Bayesian optimization building on a small number of autonomous\nskill executions in a sparse reward setting. We demonstrate the sample\nefficiency of our approach on multiple complex manipulation skills in both\nsimulations and real-world experiments. Furthermore, we make the code and\npre-trained models publicly available at http://bopt-gmm. cs.uni-freiburg.de.\n","authors":["Adrian Röfer","Iman Nematollahi","Tim Welschehold","Wolfram Burgard","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2403.14305v1.pdf","comment":"7 pages, 5 figures, 2 tables, submitted to IROS2024"},{"id":"http://arxiv.org/abs/2403.14300v1","updated":"2024-03-21T11:16:28Z","published":"2024-03-21T11:16:28Z","title":"DexDribbler: Learning Dexterous Soccer Manipulation via Dynamic\n  Supervision","summary":"  Learning dexterous locomotion policy for legged robots is becoming\nincreasingly popular due to its ability to handle diverse terrains and resemble\nintelligent behaviors. However, joint manipulation of moving objects and\nlocomotion with legs, such as playing soccer, receive scant attention in the\nlearning community, although it is natural for humans and smart animals. A key\nchallenge to solve this multitask problem is to infer the objectives of\nlocomotion from the states and targets of the manipulated objects. The implicit\nrelation between the object states and robot locomotion can be hard to capture\ndirectly from the training experience. We propose adding a feedback control\nblock to compute the necessary body-level movement accurately and using the\noutputs as dynamic joint-level locomotion supervision explicitly. We further\nutilize an improved ball dynamic model, an extended context-aided estimator,\nand a comprehensive ball observer to facilitate transferring policy learned in\nsimulation to the real world. We observe that our learning scheme can not only\nmake the policy network converge faster but also enable soccer robots to\nperform sophisticated maneuvers like sharp cuts and turns on flat surfaces, a\ncapability that was lacking in previous methods. Video and code are available\nat https://github.com/SysCV/soccer-player\n","authors":["Yutong Hu","Kehan Wen","Fisher Yu"],"pdf_url":"https://arxiv.org/pdf/2403.14300v1.pdf","comment":"8 pages, 7 figures, submitted to IROS 2024"},{"id":"http://arxiv.org/abs/2312.07214v3","updated":"2024-03-21T11:12:31Z","published":"2023-12-12T12:26:48Z","title":"Exploring Large Language Models to Facilitate Variable Autonomy for\n  Human-Robot Teaming","summary":"  In a rapidly evolving digital landscape autonomous tools and robots are\nbecoming commonplace. Recognizing the significance of this development, this\npaper explores the integration of Large Language Models (LLMs) like Generative\npre-trained transformer (GPT) into human-robot teaming environments to\nfacilitate variable autonomy through the means of verbal human-robot\ncommunication. In this paper, we introduce a novel framework for such a\nGPT-powered multi-robot testbed environment, based on a Unity Virtual Reality\n(VR) setting. This system allows users to interact with robot agents through\nnatural language, each powered by individual GPT cores. By means of OpenAI's\nfunction calling, we bridge the gap between unstructured natural language input\nand structure robot actions. A user study with 12 participants explores the\neffectiveness of GPT-4 and, more importantly, user strategies when being given\nthe opportunity to converse in natural language within a multi-robot\nenvironment. Our findings suggest that users may have preconceived expectations\non how to converse with robots and seldom try to explore the actual language\nand cognitive capabilities of their robot collaborators. Still, those users who\ndid explore where able to benefit from a much more natural flow of\ncommunication and human-like back-and-forth. We provide a set of lessons\nlearned for future research and technical implementations of similar systems.\n","authors":["Younes Lakhnati","Max Pascher","Jens Gerken"],"pdf_url":"https://arxiv.org/pdf/2312.07214v3.pdf","comment":"Frontiers in Robotics and AI, Variable Autonomy for Human-Robot\n  Teaming"},{"id":"http://arxiv.org/abs/2403.14293v1","updated":"2024-03-21T11:00:11Z","published":"2024-03-21T11:00:11Z","title":"Human Reactions to Incorrect Answers from Robots","summary":"  As robots grow more and more integrated into numerous industries, it is\ncritical to comprehend how humans respond to their failures. This paper\nsystematically studies how trust dynamics and system design are affected by\nhuman responses to robot failures. The three-stage survey used in the study\nprovides a thorough understanding of human-robot interactions. While the second\nstage concentrates on interaction details, such as robot precision and error\nacknowledgment, the first stage collects demographic data and initial levels of\ntrust. In the last phase, participants' perceptions are examined after the\nencounter, and trust dynamics, forgiveness, and propensity to suggest robotic\ntechnologies are evaluated. Results show that participants' trust in robotic\ntechnologies increased significantly when robots acknowledged their errors or\nlimitations to participants and their willingness to suggest robots for\nactivities in the future points to a favorable change in perception,\nemphasizing the role that direct engagement has in influencing trust dynamics.\nBy providing useful advice for creating more sympathetic, responsive, and\nreliable robotic systems, the study advances the science of human-robot\ninteraction and promotes a wider adoption of robotic technologies.\n","authors":["Ponkoj Chandra Shill","Md. Azizul Hakim","Muhammad Jahanzeb Khan","Bashira Akter Anima"],"pdf_url":"https://arxiv.org/pdf/2403.14293v1.pdf","comment":"6 pages, 6 figures, 1 table, Ro-Man 2024"},{"id":"http://arxiv.org/abs/2403.14281v1","updated":"2024-03-21T10:41:31Z","published":"2024-03-21T10:41:31Z","title":"UAV-Assisted Maritime Search and Rescue: A Holistic Approach","summary":"  In this paper, we explore the application of Unmanned Aerial Vehicles (UAVs)\nin maritime search and rescue (mSAR) missions, focusing on medium-sized\nfixed-wing drones and quadcopters. We address the challenges and limitations\ninherent in operating some of the different classes of UAVs, particularly in\nsearch operations. Our research includes the development of a comprehensive\nsoftware framework designed to enhance the efficiency and efficacy of SAR\noperations. This framework combines preliminary detection onboard UAVs with\nadvanced object detection at ground stations, aiming to reduce visual strain\nand improve decision-making for operators. It will be made publicly available\nupon publication. We conduct experiments to evaluate various Region of Interest\n(RoI) proposal methods, especially by imposing simulated limited bandwidth on\nthem, an important consideration when flying remote or offshore operations.\nThis forces the algorithm to prioritize some predictions over others.\n","authors":["Martin Messmer","Benjamin Kiefer","Leon Amadeus Varga","Andreas Zell"],"pdf_url":"https://arxiv.org/pdf/2403.14281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.00823v2","updated":"2024-03-21T10:21:37Z","published":"2024-02-01T18:07:33Z","title":"SLIM: Skill Learning with Multiple Critics","summary":"  Self-supervised skill learning aims to acquire useful behaviors that leverage\nthe underlying dynamics of the environment. Latent variable models, based on\nmutual information maximization, have been successful in this task but still\nstruggle in the context of robotic manipulation. As it requires impacting a\npossibly large set of degrees of freedom composing the environment, mutual\ninformation maximization fails alone in producing useful and safe manipulation\nbehaviors. Furthermore, tackling this by augmenting skill discovery rewards\nwith additional rewards through a naive combination might fail to produce\ndesired behaviors. To address this limitation, we introduce SLIM, a\nmulti-critic learning approach for skill discovery with a particular focus on\nrobotic manipulation. Our main insight is that utilizing multiple critics in an\nactor-critic framework to gracefully combine multiple reward functions leads to\na significant improvement in latent-variable skill discovery for robotic\nmanipulation while overcoming possible interference occurring among rewards\nwhich hinders convergence to useful skills. Furthermore, in the context of\ntabletop manipulation, we demonstrate the applicability of our novel skill\ndiscovery approach to acquire safe and efficient motor primitives in a\nhierarchical reinforcement learning fashion and leverage them through planning,\nsignificantly surpassing baseline approaches for skill discovery.\n","authors":["David Emukpere","Bingbing Wu","Julien Perez","Jean-Michel Renders"],"pdf_url":"https://arxiv.org/pdf/2402.00823v2.pdf","comment":"Accepted at IEEE ICRA 2024"},{"id":"http://arxiv.org/abs/2403.14270v1","updated":"2024-03-21T10:15:57Z","published":"2024-03-21T10:15:57Z","title":"Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship\n  Detection","summary":"  Visual relationship detection aims to identify objects and their\nrelationships in images. Prior methods approach this task by adding separate\nrelationship modules or decoders to existing object detection architectures.\nThis separation increases complexity and hinders end-to-end training, which\nlimits performance. We propose a simple and highly efficient decoder-free\narchitecture for open-vocabulary visual relationship detection. Our model\nconsists of a Transformer-based image encoder that represents objects as tokens\nand models their relationships implicitly. To extract relationship information,\nwe introduce an attention mechanism that selects object pairs likely to form a\nrelationship. We provide a single-stage recipe to train this model on a mixture\nof object and relationship detection data. Our approach achieves\nstate-of-the-art relationship detection performance on Visual Genome and on the\nlarge-vocabulary GQA benchmark at real-time inference speeds. We provide\nanalyses of zero-shot performance, ablations, and real-world qualitative\nexamples.\n","authors":["Tim Salzmann","Markus Ryll","Alex Bewley","Matthias Minderer"],"pdf_url":"https://arxiv.org/pdf/2403.14270v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11567v2","updated":"2024-03-21T10:04:26Z","published":"2024-03-18T08:41:36Z","title":"R2SNet: Scalable Domain Adaptation for Object Detection in Cloud-Based\n  Robots Ecosystems via Proposal Refinement","summary":"  We introduce a novel approach for scalable domain adaptation in cloud\nrobotics scenarios where robots rely on third-party AI inference services\npowered by large pre-trained deep neural networks. Our method is based on a\ndownstream proposal-refinement stage running locally on the robots, exploiting\na new lightweight DNN architecture, R2SNet. This architecture aims to mitigate\nperformance degradation from domain shifts by adapting the object detection\nprocess to the target environment, focusing on relabeling, rescoring, and\nsuppression of bounding-box proposals. Our method allows for local execution on\nrobots, addressing the scalability challenges of domain adaptation without\nincurring significant computational costs. Real-world results on mobile service\nrobots performing door detection show the effectiveness of the proposed method\nin achieving scalable domain adaptation.\n","authors":["Michele Antonazzi","Matteo Luperto","N. Alberto Borghese","Nicola Basilico"],"pdf_url":"https://arxiv.org/pdf/2403.11567v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09337v4","updated":"2024-03-21T09:34:49Z","published":"2022-03-17T14:13:19Z","title":"CoBRA: A Composable Benchmark for Robotics Applications","summary":"  Selecting an optimal robot, its base pose, and trajectory for a given task is\ncurrently mainly done by human expertise or trial and error. To evaluate\nautomatic approaches to this combined optimization problem, we introduce a\nbenchmark suite encompassing a unified format for robots, environments, and\ntask descriptions. Our benchmark suite is especially useful for modular robots,\nwhere the multitude of robots that can be assembled creates a host of\nadditional parameters to optimize. We include tasks such as machine tending and\nwelding in synthetic environments and 3D scans of real-world machine shops. All\nbenchmarks are accessible through https://cobra.cps.cit.tum.de, a platform to\nconveniently share, reference, and compare tasks, robot models, and solutions.\n","authors":["Matthias Mayer","Jonathan Külz","Matthias Althoff"],"pdf_url":"https://arxiv.org/pdf/2203.09337v4.pdf","comment":"7 pages, 5 Figures, 5 Tables Final version for IEEE ICRA'24"},{"id":"http://arxiv.org/abs/2308.03574v2","updated":"2024-03-21T09:13:17Z","published":"2023-08-07T13:25:48Z","title":"Generalized Early Stopping in Evolutionary Direct Policy Search","summary":"  Lengthy evaluation times are common in many optimization problems such as\ndirect policy search tasks, especially when they involve conducting evaluations\nin the physical world, e.g. in robotics applications. Often when evaluating\nsolution over a fixed time period it becomes clear that the objective value\nwill not increase with additional computation time (for example when a two\nwheeled robot continuously spins on the spot). In such cases, it makes sense to\nstop the evaluation early to save computation time. However, most approaches to\nstop the evaluation are problem specific and need to be specifically designed\nfor the task at hand. Therefore, we propose an early stopping method for direct\npolicy search. The proposed method only looks at the objective value at each\ntime step and requires no problem specific knowledge. We test the introduced\nstopping criterion in five direct policy search environments drawn from games,\nrobotics and classic control domains, and show that it can save up to 75% of\nthe computation time. We also compare it with problem specific stopping\ncriteria and show that it performs comparably, while being more generally\napplicable.\n","authors":["Etor Arza","Leni K. Le Goff","Emma Hart"],"pdf_url":"https://arxiv.org/pdf/2308.03574v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08244v2","updated":"2024-03-21T08:02:59Z","published":"2023-11-14T15:29:52Z","title":"Language and Sketching: An LLM-driven Interactive Multimodal Multitask\n  Robot Navigation Framework","summary":"  The socially-aware navigation system has evolved to adeptly avoid various\nobstacles while performing multiple tasks, such as point-to-point navigation,\nhuman-following, and -guiding. However, a prominent gap persists: in\nHuman-Robot Interaction (HRI), the procedure of communicating commands to\nrobots demands intricate mathematical formulations. Furthermore, the transition\nbetween tasks does not quite possess the intuitive control and user-centric\ninteractivity that one would desire. In this work, we propose an LLM-driven\ninteractive multimodal multitask robot navigation framework, termed LIM2N, to\nsolve the above new challenge in the navigation field. We achieve this by first\nintroducing a multimodal interaction framework where language and hand-drawn\ninputs can serve as navigation constraints and control objectives. Next, a\nreinforcement learning agent is built to handle multiple tasks with the\nreceived information. Crucially, LIM2N creates smooth cooperation among the\nreasoning of multimodal input, multitask planning, and adaptation and\nprocessing of the intelligent sensing modules in the complicated system.\nExtensive experiments are conducted in both simulation and the real world\ndemonstrating that LIM2N has superior user needs understanding, alongside an\nenhanced interactive experience.\n","authors":["Weiqin Zu","Wenbin Song","Ruiqing Chen","Ze Guo","Fanglei Sun","Zheng Tian","Wei Pan","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2311.08244v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14176v1","updated":"2024-03-21T06:57:28Z","published":"2024-03-21T06:57:28Z","title":"ReFeree: Radar-based efficient global descriptor using a Feature and\n  Free space for Place Recognition","summary":"  Radar is highlighted for robust sensing capabilities in adverse weather\nconditions (e.g. dense fog, heavy rain, or snowfall). In addition, Radar can\ncover wide areas and penetrate small particles. Despite these advantages,\nRadar-based place recognition remains in the early stages compared to other\nsensors due to its unique characteristics such as low resolution, and\nsignificant noise. In this paper, we propose a Radarbased place recognition\nutilizing a descriptor called ReFeree using a feature and free space. Unlike\ntraditional methods, we overwhelmingly summarize the Radar image. Despite being\nlightweight, it contains semi-metric information and is also outstanding from\nthe perspective of place recognition performance. For concrete validation, we\ntest a single session from the MulRan dataset and a multi-session from the\nOxford Radar RobotCar and the Boreas dataset.\n","authors":["Byunghee Choi","Hogyun Kim","Younggun Cho"],"pdf_url":"https://arxiv.org/pdf/2403.14176v1.pdf","comment":"5 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.14173v1","updated":"2024-03-21T06:53:20Z","published":"2024-03-21T06:53:20Z","title":"HCTO: Optimality-Aware LiDAR Inertial Odometry with Hybrid Continuous\n  Time Optimization for Compact Wearable Mapping System","summary":"  Compact wearable mapping system (WMS) has gained significant attention due to\ntheir convenience in various applications. Specifically, it provides an\nefficient way to collect prior maps for 3D structure inspection and robot-based\n\"last-mile delivery\" in complex environments. However, vibrations in human\nmotion and the uneven distribution of point cloud features in complex\nenvironments often lead to rapid drift, which is a prevalent issue when\napplying existing LiDAR Inertial Odometry (LIO) methods on low-cost WMS. To\naddress these limitations, we propose a novel LIO for WMSs based on Hybrid\nContinuous Time Optimization (HCTO) considering the optimality of Lidar\ncorrespondences. First, HCTO recognizes patterns in human motion\n(high-frequency part, low-frequency part, and constant velocity part) by\nanalyzing raw IMU measurements. Second, HCTO constructs hybrid IMU factors\naccording to different motion states, which enables robust and accurate\nestimation against vibration-induced noise in the IMU measurements. Third, the\nbest point correspondences are selected using optimal design to achieve\nreal-time performance and better odometry accuracy. We conduct experiments on\nhead-mounted WMS datasets to evaluate the performance of our system,\ndemonstrating significant advantages over state-of-the-art methods. Video\nrecordings of experiments can be found on the project page of HCTO:\n\\href{https://github.com/kafeiyin00/HCTO}{https://github.com/kafeiyin00/HCTO}.\n","authors":["Jianping Li","Shenghai Yuan","Muqing Cao","Thien-Minh Nguyen","Kun Cao","Lihua Xie"],"pdf_url":"https://arxiv.org/pdf/2403.14173v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14163v1","updated":"2024-03-21T06:32:36Z","published":"2024-03-21T06:32:36Z","title":"Leveraging Large Language Model-based Room-Object Relationships\n  Knowledge for Enhancing Multimodal-Input Object Goal Navigation","summary":"  Object-goal navigation is a crucial engineering task for the community of\nembodied navigation; it involves navigating to an instance of a specified\nobject category within unseen environments. Although extensive investigations\nhave been conducted on both end-to-end and modular-based, data-driven\napproaches, fully enabling an agent to comprehend the environment through\nperceptual knowledge and perform object-goal navigation as efficiently as\nhumans remains a significant challenge. Recently, large language models have\nshown potential in this task, thanks to their powerful capabilities for\nknowledge extraction and integration. In this study, we propose a data-driven,\nmodular-based approach, trained on a dataset that incorporates common-sense\nknowledge of object-to-room relationships extracted from a large language\nmodel. We utilize the multi-channel Swin-Unet architecture to conduct\nmulti-task learning incorporating with multimodal inputs. The results in the\nHabitat simulator demonstrate that our framework outperforms the baseline by an\naverage of 10.6% in the efficiency metric, Success weighted by Path Length\n(SPL). The real-world demonstration shows that the proposed approach can\nefficiently conduct this task by traversing several rooms. For more details and\nreal-world demonstrations, please check our project webpage\n(https://sunleyuan.github.io/ObjectNav).\n","authors":["Leyuan Sun","Asako Kanezaki","Guillaume Caron","Yusuke Yoshiyasu"],"pdf_url":"https://arxiv.org/pdf/2403.14163v1.pdf","comment":"will soon submit to the Elsevier journal, Advanced Engineering\n  Informatics"},{"id":"http://arxiv.org/abs/2310.11792v2","updated":"2024-03-21T06:26:50Z","published":"2023-10-18T08:33:08Z","title":"Real-time Perceptive Motion Control using Control Barrier Functions with\n  Analytical Smoothing for Six-Wheeled-Telescopic-Legged Robot Tachyon 3","summary":"  To achieve safe legged locomotion, it is important to generate motion in\nreal-time considering various constraints in robots and environments. In this\nstudy, we propose a lightweight real-time perspective motion control system for\nthe newly developed six-wheeled-telescopic-legged robot, Tachyon 3. In the\nproposed method, analytically smoothed constraints including Smooth Separating\nAxis Theorem (Smooth SAT) as a novel higher order differentiable collision\ndetection for 3D shapes is applied to the Control Barrier Function (CBF). The\nproposed system integrating the CBF achieves online motion generation in a\nshort control cycle of 1 ms that satisfies joint limitations, environmental\ncollision avoidance and safe convex foothold constraints. The efficiency of\nSmooth SAT is shown from the collision detection time of 1 us or less and the\nCBF constraint computation time for Tachyon3 of several us. Furthermore, the\neffectiveness of the proposed system is verified through the stair-climbing\nmotion, integrating online recognition in a simulation and a real machine.\n","authors":["Noriaki Takasugi","Masaya Kinoshita","Yasuhisa Kamikawa","Ryoichi Tsuzaki","Atsushi Sakamoto","Toshimitsu Kai","Yasunori Kawanami"],"pdf_url":"https://arxiv.org/pdf/2310.11792v2.pdf","comment":"8 pages, 8 figures, This work has been submitted to the IEEE for\n  possible publication. Copyright may be transferred without notice, after\n  which this version may no longer be accessible"},{"id":"http://arxiv.org/abs/2403.14161v1","updated":"2024-03-21T06:24:01Z","published":"2024-03-21T06:24:01Z","title":"Extrinsic Calibration of Multiple LiDARs for a Mobile Robot based on\n  Floor Plane And Object Segmentation","summary":"  Mobile robots equipped with multiple light detection and ranging (LiDARs) and\ncapable of recognizing their surroundings are increasing due to the\nminitualization and cost reduction of LiDAR. This paper proposes a target-less\nextrinsic calibration method of multiple LiDARs with non-overlapping field of\nview (FoV). The proposed method uses accumulated point clouds of floor plane\nand objects while in motion. It enables accurate calibration with challenging\nconfiguration of LiDARs that directed towards the floor plane, caused by biased\nfeature values. Additionally, the method includes a noise removal module that\nconsiders the scanning pattern to address bleeding points, which are noises of\nsignificant source of error in point cloud alignment using high-density LiDARs.\nEvaluations through simulation demonstrate that the proposed method achieved\nhigher accuracy extrinsic calibration with two and four LiDARs than\nconventional methods, regardless type of objects. Furthermore, the experiments\nusing a real mobile robot has shown that our proposed noise removal module can\neliminate noise more precisely than conventional methods, and the estimated\nextrinsic parameters have successfully created consistent 3D maps.\n","authors":["Shun Niijima","Atsushi Suzuki","Ryoichi Tsuzaki","Masaya Kinoshita"],"pdf_url":"https://arxiv.org/pdf/2403.14161v1.pdf","comment":"8pages, 10figures"},{"id":"http://arxiv.org/abs/2403.14160v1","updated":"2024-03-21T06:23:57Z","published":"2024-03-21T06:23:57Z","title":"Development of a Compact Robust Passive Transformable Omni-Ball for\n  Enhanced Step-Climbing and Vibration Reduction","summary":"  This paper introduces the Passive Transformable Omni-Ball (PTOB), an advanced\nomnidirectional wheel engineered to enhance step-climbing performance,\nincorporate built-in actuators, diminish vibrations, and fortify structural\nintegrity. By modifying the omni-ball's structure from two to three segments,\nwe have achieved improved in-wheel actuation and a reduction in vibrational\nfeedback. Additionally, we have implemented a sliding mechanism in the follower\nwheels to boost the wheel's step-climbing abilities. A prototype with a 127 mm\ndiameter PTOB was constructed, which confirmed its functionality for\nomnidirectional movement and internal actuation. Compared to a traditional\nomni-wheel, the PTOB demonstrated a comparable level of vibration while\noffering superior capabilities. Extensive testing in varied settings showed\nthat the PTOB can adeptly handle step obstacles up to 45 mm, equivalent to 35\n$\\%$ of the wheel's diameter, in both the forward and lateral directions. The\nPTOB showcased robust construction and proved to be versatile in navigating\nthrough environments with diverse obstacles.\n","authors":["Kazuo Hongo","Takashi Kito","Yasuhisa Kamikawa","Masaya Kinoshita","Yasunori Kawanami"],"pdf_url":"https://arxiv.org/pdf/2403.14160v1.pdf","comment":"8 pages, 16 figures"},{"id":"http://arxiv.org/abs/2403.14159v1","updated":"2024-03-21T06:23:38Z","published":"2024-03-21T06:23:38Z","title":"Robust Locomotion via Zero-order Stochastic Nonlinear Model Predictive\n  Control with Guard Saltation Matrix","summary":"  This paper presents a stochastic/robust nonlinear model predictive control\n(NMPC) to enhance the robustness of legged locomotion against contact\nuncertainties. We integrate the contact uncertainties into the covariance\npropagation of stochastic/robust NMPC framework by leveraging the guard\nsaltation matrix and an extended Kalman filter-like covariance update. We\nachieve fast stochastic/robust NMPC computation by utilizing the zero-order\nstochastic/robust NMPC algorithm with additional improvements in computational\nefficiency concerning the feedback gains. We conducted numerical experiments\nand demonstrate that the proposed method can accurately forecast future state\ncovariance and generate trajectories that satisfies constraints even in the\npresence of the contact uncertainties. Hardware experiments on the perceptive\nlocomotion of a wheeled-legged robot were also carried out, validating the\nfeasibility of the proposed method in a real-world system with limited on-board\ncomputation.\n","authors":["Sotaro Katayama","Noriaki Takasugi","Mitsuhisa Kaneko","Norio Nagatsuka","and Masaya Kinoshita"],"pdf_url":"https://arxiv.org/pdf/2403.14159v1.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2311.10678v2","updated":"2024-03-21T05:47:22Z","published":"2023-11-17T18:00:20Z","title":"Distilling and Retrieving Generalizable Knowledge for Robot Manipulation\n  via Language Corrections","summary":"  Today's robot policies exhibit subpar performance when faced with the\nchallenge of generalizing to novel environments. Human corrective feedback is a\ncrucial form of guidance to enable such generalization. However, adapting to\nand learning from online human corrections is a non-trivial endeavor: not only\ndo robots need to remember human feedback over time to retrieve the right\ninformation in new settings and reduce the intervention rate, but also they\nwould need to be able to respond to feedback that can be arbitrary corrections\nabout high-level human preferences to low-level adjustments to skill\nparameters. In this work, we present Distillation and Retrieval of Online\nCorrections (DROC), a large language model (LLM)-based system that can respond\nto arbitrary forms of language feedback, distill generalizable knowledge from\ncorrections, and retrieve relevant past experiences based on textual and visual\nsimilarity for improving performance in novel settings. DROC is able to respond\nto a sequence of online language corrections that address failures in both\nhigh-level task plans and low-level skill primitives. We demonstrate that DROC\neffectively distills the relevant information from the sequence of online\ncorrections in a knowledge base and retrieves that knowledge in settings with\nnew task or object instances. DROC outperforms other techniques that directly\ngenerate robot code via LLMs by using only half of the total number of\ncorrections needed in the first round and requires little to no corrections\nafter two iterations. We show further results, videos, prompts and code on\nhttps://sites.google.com/stanford.edu/droc .\n","authors":["Lihan Zha","Yuchen Cui","Li-Heng Lin","Minae Kwon","Montserrat Gonzalez Arenas","Andy Zeng","Fei Xia","Dorsa Sadigh"],"pdf_url":"https://arxiv.org/pdf/2311.10678v2.pdf","comment":"8 pages, 4 figures, videos and code links on website\n  https://sites.google.com/stanford.edu/droc"},{"id":"http://arxiv.org/abs/2403.14138v1","updated":"2024-03-21T05:13:34Z","published":"2024-03-21T05:13:34Z","title":"Evidential Semantic Mapping in Off-road Environments with\n  Uncertainty-aware Bayesian Kernel Inference","summary":"  Robotic mapping with Bayesian Kernel Inference (BKI) has shown promise in\ncreating semantic maps by effectively leveraging local spatial information.\nHowever, existing semantic mapping methods face challenges in constructing\nreliable maps in unstructured outdoor scenarios due to unreliable semantic\npredictions. To address this issue, we propose an evidential semantic mapping,\nwhich can enhance reliability in perceptually challenging off-road\nenvironments. We integrate Evidential Deep Learning into the semantic\nsegmentation network to obtain the uncertainty estimate of semantic prediction.\nSubsequently, this semantic uncertainty is incorporated into an\nuncertainty-aware BKI, tailored to prioritize more confident semantic\npredictions when accumulating semantic information. By adaptively handling\nsemantic uncertainties, the proposed framework constructs robust\nrepresentations of the surroundings even in previously unseen environments.\nComprehensive experiments across various off-road datasets demonstrate that our\nframework enhances accuracy and robustness, consistently outperforming existing\nmethods in scenes with high perceptual uncertainties.\n","authors":["Junyoung Kim","Junwon Seo","Jihong Min"],"pdf_url":"https://arxiv.org/pdf/2403.14138v1.pdf","comment":"Our project website can be found at\n  https://kjyoung.github.io/Homepage/#/Projects/Evidential-Semantic-Mapping"},{"id":"http://arxiv.org/abs/2303.03757v3","updated":"2024-03-21T05:11:08Z","published":"2023-03-07T09:33:49Z","title":"Deep Learning for Inertial Positioning: A Survey","summary":"  Inertial sensors are widely utilized in smartphones, drones, robots, and IoT\ndevices, playing a crucial role in enabling ubiquitous and reliable\nlocalization. Inertial sensor-based positioning is essential in various\napplications, including personal navigation, location-based security, and\nhuman-device interaction. However, low-cost MEMS inertial sensors' measurements\nare inevitably corrupted by various error sources, leading to unbounded drifts\nwhen integrated doubly in traditional inertial navigation algorithms,\nsubjecting inertial positioning to the problem of error drifts. In recent\nyears, with the rapid increase in sensor data and computational power, deep\nlearning techniques have been developed, sparking significant research into\naddressing the problem of inertial positioning. Relevant literature in this\nfield spans across mobile computing, robotics, and machine learning. In this\narticle, we provide a comprehensive review of deep learning-based inertial\npositioning and its applications in tracking pedestrians, drones, vehicles, and\nrobots. We connect efforts from different fields and discuss how deep learning\ncan be applied to address issues such as sensor calibration, positioning error\ndrift reduction, and multi-sensor fusion. This article aims to attract readers\nfrom various backgrounds, including researchers and practitioners interested in\nthe potential of deep learning-based techniques to solve inertial positioning\nproblems. Our review demonstrates the exciting possibilities that deep learning\nbrings to the table and provides a roadmap for future research in this field.\n","authors":["Changhao Chen","Xianfei Pan"],"pdf_url":"https://arxiv.org/pdf/2303.03757v3.pdf","comment":"Accepted by IEEE Transactions on Intelligent Transportation Systems"},{"id":"http://arxiv.org/abs/2307.13122v2","updated":"2024-03-21T01:15:02Z","published":"2023-07-24T20:40:18Z","title":"Redundancy parameterization and inverse kinematics of 7-DOF revolute\n  manipulators","summary":"  Seven degree-of-freedom (DOF) robot arms have one redundant DOF which does\nnot change the motion of the end effector. The redundant DOF offers greater\nmanipulability of the arm configuration to avoid obstacles and singularities,\nbut it must be parameterized to fully specify the joint angles for a given end\neffector pose. For 7-DOF revolute (7R) manipulators, we introduce a new concept\nof generalized shoulder-elbow-wrist (SEW) angle, a generalization of the\nconventional SEW angle but with an arbitrary choice of the reference direction\nfunction. The SEW angle is widely used and easy for human operators to\nvisualize as a rotation of the elbow about the shoulder-wrist line. Since other\nredundancy parameterizations including the conventional SEW angle encounter an\nalgorithmic singularity along a line in the workspace, we introduce a special\nchoice of the reference direction function called the stereographic SEW angle\nwhich has a singularity only along a half-line, which can be placed out of\nreach. We prove that such a singularity is unavoidable for any\nparameterization. We also include expressions for the SEW angle Jacobian along\nwith singularity analysis. Finally, we provide efficient and singularity-robust\ninverse kinematics solutions for most known 7R manipulators using the general\nSEW angle and the subproblem decomposition method. These solutions are often\nclosed-form but may sometimes involve a 1D or 2D search in the general case.\nSearch-based solutions may be converted to finding zeros of a high-order\npolynomial. Inverse kinematics solutions, examples, and evaluations are\navailable in a publicly accessible repository.\n","authors":["Alexander J. Elias","John T. Wen"],"pdf_url":"https://arxiv.org/pdf/2307.13122v2.pdf","comment":"22 pages, 14 figures. Update: Sawyer IK using polynomial method, two\n  video extensions, expanded related literature"},{"id":"http://arxiv.org/abs/2403.14056v1","updated":"2024-03-21T00:59:35Z","published":"2024-03-21T00:59:35Z","title":"Semantics from Space: Satellite-Guided Thermal Semantic Segmentation\n  Annotation for Aerial Field Robots","summary":"  We present a new method to automatically generate semantic segmentation\nannotations for thermal imagery captured from an aerial vehicle by utilizing\nsatellite-derived data products alongside onboard global positioning and\nattitude estimates. This new capability overcomes the challenge of developing\nthermal semantic perception algorithms for field robots due to the lack of\nannotated thermal field datasets and the time and costs of manual annotation,\nenabling precise and rapid annotation of thermal data from field collection\nefforts at a massively-parallelizable scale. By incorporating a\nthermal-conditioned refinement step with visual foundation models, our approach\ncan produce highly-precise semantic segmentation labels using low-resolution\nsatellite land cover data for little-to-no cost. It achieves 98.5% of the\nperformance from using costly high-resolution options and demonstrates between\n70-160% improvement over popular zero-shot semantic segmentation methods based\non large vision-language models currently used for generating annotations for\nRGB imagery. Code will be available at:\nhttps://github.com/connorlee77/aerial-auto-segment.\n","authors":["Connor Lee","Saraswati Soedarmadji","Matthew Anderson","Anthony J. Clark","Soon-Jo Chung"],"pdf_url":"https://arxiv.org/pdf/2403.14056v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08864v5","updated":"2024-03-21T00:48:52Z","published":"2023-10-13T05:20:40Z","title":"Open X-Embodiment: Robotic Learning Datasets and RT-X Models","summary":"  Large, high-capacity models trained on diverse datasets have shown remarkable\nsuccesses on efficiently tackling downstream applications. In domains from NLP\nto Computer Vision, this has led to a consolidation of pretrained models, with\ngeneral pretrained backbones serving as a starting point for many applications.\nCan such a consolidation happen in robotics? Conventionally, robotic learning\nmethods train a separate model for every application, every robot, and even\nevery environment. Can we instead train generalist X-robot policy that can be\nadapted efficiently to new robots, tasks, and environments? In this paper, we\nprovide datasets in standardized data formats and models to make it possible to\nexplore this possibility in the context of robotic manipulation, alongside\nexperimental results that provide an example of effective X-robot policies. We\nassemble a dataset from 22 different robots collected through a collaboration\nbetween 21 institutions, demonstrating 527 skills (160266 tasks). We show that\na high-capacity model trained on this data, which we call RT-X, exhibits\npositive transfer and improves the capabilities of multiple robots by\nleveraging experience from other platforms. More details can be found on the\nproject website https://robotics-transformer-x.github.io.\n","authors":["Open X-Embodiment Collaboration","Abby O'Neill","Abdul Rehman","Abhiram Maddukuri","Abhishek Gupta","Abhishek Padalkar","Abraham Lee","Acorn Pooley","Agrim Gupta","Ajay Mandlekar","Ajinkya Jain","Albert Tung","Alex Bewley","Alex Herzog","Alex Irpan","Alexander Khazatsky","Anant Rai","Anchit Gupta","Andrew Wang","Anikait Singh","Animesh Garg","Aniruddha Kembhavi","Annie Xie","Anthony Brohan","Antonin Raffin","Archit Sharma","Arefeh Yavary","Arhan Jain","Ashwin Balakrishna","Ayzaan Wahid","Ben Burgess-Limerick","Beomjoon Kim","Bernhard Schölkopf","Blake Wulfe","Brian Ichter","Cewu Lu","Charles Xu","Charlotte Le","Chelsea Finn","Chen Wang","Chenfeng Xu","Cheng Chi","Chenguang Huang","Christine Chan","Christopher Agia","Chuer Pan","Chuyuan Fu","Coline Devin","Danfei Xu","Daniel Morton","Danny Driess","Daphne Chen","Deepak Pathak","Dhruv Shah","Dieter Büchler","Dinesh Jayaraman","Dmitry Kalashnikov","Dorsa Sadigh","Edward Johns","Ethan Foster","Fangchen Liu","Federico Ceola","Fei Xia","Feiyu Zhao","Freek Stulp","Gaoyue Zhou","Gaurav S. Sukhatme","Gautam Salhotra","Ge Yan","Gilbert Feng","Giulio Schiavi","Glen Berseth","Gregory Kahn","Guanzhi Wang","Hao Su","Hao-Shu Fang","Haochen Shi","Henghui Bao","Heni Ben Amor","Henrik I Christensen","Hiroki Furuta","Homer Walke","Hongjie Fang","Huy Ha","Igor Mordatch","Ilija Radosavovic","Isabel Leal","Jacky Liang","Jad Abou-Chakra","Jaehyung Kim","Jaimyn Drake","Jan Peters","Jan Schneider","Jasmine Hsu","Jeannette Bohg","Jeffrey Bingham","Jeffrey Wu","Jensen Gao","Jiaheng Hu","Jiajun Wu","Jialin Wu","Jiankai Sun","Jianlan Luo","Jiayuan Gu","Jie Tan","Jihoon Oh","Jimmy Wu","Jingpei Lu","Jingyun Yang","Jitendra Malik","João Silvério","Joey Hejna","Jonathan Booher","Jonathan Tompson","Jonathan Yang","Jordi Salvador","Joseph J. Lim","Junhyek Han","Kaiyuan Wang","Kanishka Rao","Karl Pertsch","Karol Hausman","Keegan Go","Keerthana Gopalakrishnan","Ken Goldberg","Kendra Byrne","Kenneth Oslund","Kento Kawaharazuka","Kevin Black","Kevin Lin","Kevin Zhang","Kiana Ehsani","Kiran Lekkala","Kirsty Ellis","Krishan Rana","Krishnan Srinivasan","Kuan Fang","Kunal Pratap Singh","Kuo-Hao Zeng","Kyle Hatch","Kyle Hsu","Laurent Itti","Lawrence Yunliang Chen","Lerrel Pinto","Li Fei-Fei","Liam Tan","Linxi \"Jim\" Fan","Lionel Ott","Lisa Lee","Luca Weihs","Magnum Chen","Marion Lepert","Marius Memmel","Masayoshi Tomizuka","Masha Itkina","Mateo Guaman Castro","Max Spero","Maximilian Du","Michael Ahn","Michael C. Yip","Mingtong Zhang","Mingyu Ding","Minho Heo","Mohan Kumar Srirama","Mohit Sharma","Moo Jin Kim","Naoaki Kanazawa","Nicklas Hansen","Nicolas Heess","Nikhil J Joshi","Niko Suenderhauf","Ning Liu","Norman Di Palo","Nur Muhammad Mahi Shafiullah","Oier Mees","Oliver Kroemer","Osbert Bastani","Pannag R Sanketi","Patrick \"Tree\" Miller","Patrick Yin","Paul Wohlhart","Peng Xu","Peter David Fagan","Peter Mitrano","Pierre Sermanet","Pieter Abbeel","Priya Sundaresan","Qiuyu Chen","Quan Vuong","Rafael Rafailov","Ran Tian","Ria Doshi","Roberto Martín-Martín","Rohan Baijal","Rosario Scalise","Rose Hendrix","Roy Lin","Runjia Qian","Ruohan Zhang","Russell Mendonca","Rutav Shah","Ryan Hoque","Ryan Julian","Samuel Bustamante","Sean Kirmani","Sergey Levine","Shan Lin","Sherry Moore","Shikhar Bahl","Shivin Dass","Shubham Sonawani","Shuran Song","Sichun Xu","Siddhant Haldar","Siddharth Karamcheti","Simeon Adebola","Simon Guist","Soroush Nasiriany","Stefan Schaal","Stefan Welker","Stephen Tian","Subramanian Ramamoorthy","Sudeep Dasari","Suneel Belkhale","Sungjae Park","Suraj Nair","Suvir Mirchandani","Takayuki Osa","Tanmay Gupta","Tatsuya Harada","Tatsuya Matsushima","Ted Xiao","Thomas Kollar","Tianhe Yu","Tianli Ding","Todor Davchev","Tony Z. Zhao","Travis Armstrong","Trevor Darrell","Trinity Chung","Vidhi Jain","Vincent Vanhoucke","Wei Zhan","Wenxuan Zhou","Wolfram Burgard","Xi Chen","Xiaolong Wang","Xinghao Zhu","Xinyang Geng","Xiyuan Liu","Xu Liangwei","Xuanlin Li","Yao Lu","Yecheng Jason Ma","Yejin Kim","Yevgen Chebotar","Yifan Zhou","Yifeng Zhu","Yilin Wu","Ying Xu","Yixuan Wang","Yonatan Bisk","Yoonyoung Cho","Youngwoon Lee","Yuchen Cui","Yue Cao","Yueh-Hua Wu","Yujin Tang","Yuke Zhu","Yunchu Zhang","Yunfan Jiang","Yunshuang Li","Yunzhu Li","Yusuke Iwasawa","Yutaka Matsuo","Zehan Ma","Zhuo Xu","Zichen Jeff Cui","Zichen Zhang","Zipeng Lin"],"pdf_url":"https://arxiv.org/pdf/2310.08864v5.pdf","comment":"Project website: https://robotics-transformer-x.github.io"},{"id":"http://arxiv.org/abs/2403.14049v1","updated":"2024-03-21T00:14:53Z","published":"2024-03-21T00:14:53Z","title":"A Roadmap Towards Automated and Regulated Robotic Systems","summary":"  The rapid development of generative technology opens up possibility for\nhigher level of automation, and artificial intelligence (AI) embodiment in\nrobotic systems is imminent. However, due to the blackbox nature of the\ngenerative technology, the generation of the knowledge and workflow scheme is\nuncontrolled, especially in a dynamic environment and a complex scene. This\nposes challenges to regulations in safety-demanding applications such as\nmedical scenes. We argue that the unregulated generative processes from AI is\nfitted for low level end tasks, but intervention in the form of manual or\nautomated regulation should happen post-workflow-generation and\npre-robotic-execution. To address this, we propose a roadmap that can lead to\nfully automated and regulated robotic systems. In this paradigm, the high level\npolicies are generated as structured graph data, enabling regulatory oversight\nand reusability, while the code base for lower level tasks is generated by\ngenerative models. Our approach aims the transitioning from expert knowledge to\nregulated action, akin to the iterative processes of study, practice, scrutiny,\nand execution in human tasks. We identify the generative and deterministic\nprocesses in a design cycle, where generative processes serve as a text-based\nworld simulator and the deterministic processes generate the executable system.\nWe propose State Machine Seralization Language (SMSL) to be the conversion\npoint between text simulator and executable workflow control. From there, we\nanalyze the modules involved based on the current literature, and discuss human\nin the loop. As a roadmap, this work identifies the current possible\nimplementation and future work. This work does not provide an implemented\nsystem but envisions to inspire the researchers working on the direction in the\nroadmap. We implement the SMSL and D-SFO paradigm that serve as the starting\npoint of the roadmap.\n","authors":["Yihao Liu","Mehran Armand"],"pdf_url":"https://arxiv.org/pdf/2403.14049v1.pdf","comment":"17 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.14887v1","updated":"2024-03-21T23:44:42Z","published":"2024-03-21T23:44:42Z","title":"GelLink: A Compact Multi-phalanx Finger with Vision-based Tactile\n  Sensing and Proprioception","summary":"  Compared to fully-actuated robotic end-effectors, underactuated ones are\ngenerally more adaptive, robust, and cost-effective. However, state estimation\nfor underactuated hands is usually more challenging. Vision-based tactile\nsensors, like Gelsight, can mitigate this issue by providing high-resolution\ntactile sensing and accurate proprioceptive sensing. As such, we present\nGelLink, a compact, underactuated, linkage-driven robotic finger with low-cost,\nhigh-resolution vision-based tactile sensing and proprioceptive sensing\ncapabilities. In order to reduce the amount of embedded hardware, i.e. the\ncameras and motors, we optimize the linkage transmission with a planar linkage\nmechanism simulator and develop a planar reflection simulator to simplify the\ntactile sensing hardware. As a result, GelLink only requires one motor to\nactuate the three phalanges, and one camera to capture tactile signals along\nthe entire finger. Overall, GelLink is a compact robotic finger that shows\nadaptability and robustness when performing grasping tasks. The integration of\nvision-based tactile sensors can significantly enhance the capabilities of\nunderactuated fingers and potentially broaden their future usage.\n","authors":["Yuxiang Ma"," Jialiang"," Zhao","Edward Adelson"],"pdf_url":"https://arxiv.org/pdf/2403.14887v1.pdf","comment":"Supplement video: https://www.youtube.com/watch?v=hZwUpAig5C0 . 7\n  pages, 9 figures. ICRA 2024 (IEEE International Conference on Robotics and\n  Automation)"},{"id":"http://arxiv.org/abs/2403.14879v1","updated":"2024-03-21T23:00:10Z","published":"2024-03-21T23:00:10Z","title":"Learning to Change: Choreographing Mixed Traffic Through Lateral Control\n  and Hierarchical Reinforcement Learning","summary":"  The management of mixed traffic that consists of robot vehicles (RVs) and\nhuman-driven vehicles (HVs) at complex intersections presents a multifaceted\nchallenge. Traditional signal controls often struggle to adapt to dynamic\ntraffic conditions and heterogeneous vehicle types. Recent advancements have\nturned to strategies based on reinforcement learning (RL), leveraging its\nmodel-free nature, real-time operation, and generalizability over different\nscenarios. We introduce a hierarchical RL framework to manage mixed traffic\nthrough precise longitudinal and lateral control of RVs. Our proposed\nhierarchical framework combines the state-of-the-art mixed traffic control\nalgorithm as a high level decision maker to improve the performance and\nrobustness of the whole system. Our experiments demonstrate that the framework\ncan reduce the average waiting time by up to 54% compared to the\nstate-of-the-art mixed traffic control method. When the RV penetration rate\nexceeds 60%, our technique consistently outperforms conventional traffic signal\ncontrol programs in terms of the average waiting time for all vehicles at the\nintersection.\n","authors":["Dawei Wang","Weizi Li","Lei Zhu","Jia Pan"],"pdf_url":"https://arxiv.org/pdf/2403.14879v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14877v1","updated":"2024-03-21T22:54:08Z","published":"2024-03-21T22:54:08Z","title":"TEeVTOL: Balancing Energy and Time Efficiency in eVTOL Aircraft Path\n  Planning Across City-Scale Wind Fields","summary":"  Electric vertical-takeoff and landing (eVTOL) aircraft, recognized for their\nmaneuverability and flexibility, offer a promising alternative to our\ntransportation system. However, the operational effectiveness of these aircraft\nfaces many challenges, such as the delicate balance between energy and time\nefficiency, stemming from unpredictable environmental factors, including wind\nfields. Mathematical modeling-based approaches have been adopted to plan\naircraft flight path in urban wind fields with the goal to save energy and time\ncosts. While effective, they are limited in adapting to dynamic and complex\nenvironments. To optimize energy and time efficiency in eVTOL's flight through\ndynamic wind fields, we introduce a novel path planning method leveraging deep\nreinforcement learning. We assess our method with extensive experiments,\ncomparing it to Dijkstra's algorithm -- the theoretically optimal approach for\ndetermining shortest paths in a weighted graph, where weights represent either\nenergy or time cost. The results show that our method achieves a graceful\nbalance between energy and time efficiency, closely resembling the\ntheoretically optimal values for both objectives.\n","authors":["Songyang Liu","Shuai Li","Haochen Li","Weizi Li","Jindong Tan"],"pdf_url":"https://arxiv.org/pdf/2403.14877v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14864v1","updated":"2024-03-21T22:18:59Z","published":"2024-03-21T22:18:59Z","title":"Learning Quadruped Locomotion Using Differentiable Simulation","summary":"  While most recent advancements in legged robot control have been driven by\nmodel-free reinforcement learning, we explore the potential of differentiable\nsimulation. Differentiable simulation promises faster convergence and more\nstable training by computing low-variant first-order gradients using the robot\nmodel, but so far, its use for legged robot control has remained limited to\nsimulation. The main challenge with differentiable simulation lies in the\ncomplex optimization landscape of robotic tasks due to discontinuities in\ncontact-rich environments, e.g., quadruped locomotion. This work proposes a\nnew, differentiable simulation framework to overcome these challenges. The key\nidea involves decoupling the complex whole-body simulation, which may exhibit\ndiscontinuities due to contact, into two separate continuous domains.\nSubsequently, we align the robot state resulting from the simplified model with\na more precise, non-differentiable simulator to maintain sufficient simulation\naccuracy. Our framework enables learning quadruped walking in minutes using a\nsingle simulated robot without any parallelization. When augmented with GPU\nparallelization, our approach allows the quadruped robot to master diverse\nlocomotion skills, including trot, pace, bound, and gallop, on challenging\nterrains in minutes. Additionally, our policy achieves robust locomotion\nperformance in the real world zero-shot. To the best of our knowledge, this\nwork represents the first demonstration of using differentiable simulation for\ncontrolling a real quadruped robot. This work provides several important\ninsights into using differentiable simulations for legged locomotion in the\nreal world.\n","authors":["Yunlong Song","Sangbae Kim","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2403.14864v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07869v2","updated":"2024-03-21T19:57:46Z","published":"2024-03-12T17:58:01Z","title":"TeleMoMa: A Modular and Versatile Teleoperation System for Mobile\n  Manipulation","summary":"  A critical bottleneck limiting imitation learning in robotics is the lack of\ndata. This problem is more severe in mobile manipulation, where collecting\ndemonstrations is harder than in stationary manipulation due to the lack of\navailable and easy-to-use teleoperation interfaces. In this work, we\ndemonstrate TeleMoMa, a general and modular interface for whole-body\nteleoperation of mobile manipulators. TeleMoMa unifies multiple human\ninterfaces including RGB and depth cameras, virtual reality controllers,\nkeyboard, joysticks, etc., and any combination thereof. In its more accessible\nversion, TeleMoMa works using simply vision (e.g., an RGB-D camera), lowering\nthe entry bar for humans to provide mobile manipulation demonstrations. We\ndemonstrate the versatility of TeleMoMa by teleoperating several existing\nmobile manipulators - PAL Tiago++, Toyota HSR, and Fetch - in simulation and\nthe real world. We demonstrate the quality of the demonstrations collected with\nTeleMoMa by training imitation learning policies for mobile manipulation tasks\ninvolving synchronized whole-body motion. Finally, we also show that TeleMoMa's\nteleoperation channel enables teleoperation on site, looking at the robot, or\nremote, sending commands and observations through a computer network, and\nperform user studies to evaluate how easy it is for novice users to learn to\ncollect demonstrations with different combinations of human interfaces enabled\nby our system. We hope TeleMoMa becomes a helpful tool for the community\nenabling researchers to collect whole-body mobile manipulation demonstrations.\nFor more information and video results,\nhttps://robin-lab.cs.utexas.edu/telemoma-web.\n","authors":["Shivin Dass","Wensi Ai","Yuqian Jiang","Samik Singh","Jiaheng Hu","Ruohan Zhang","Peter Stone","Ben Abbatematteo","Roberto Martín-Martín"],"pdf_url":"https://arxiv.org/pdf/2403.07869v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14780v1","updated":"2024-03-21T18:50:33Z","published":"2024-03-21T18:50:33Z","title":"Multi-agent Task-Driven Exploration via Intelligent Map Compression and\n  Sharing","summary":"  This paper investigates the task-driven exploration of unknown environments\nwith mobile sensors communicating compressed measurements. The sensors explore\nthe area and transmit their compressed data to another robot, assisting it in\nreaching a goal location. We propose a novel communication framework and a\ntractable multi-agent exploration algorithm to select the sensors' actions. The\nalgorithm uses a task-driven measure of uncertainty, resulting from map\ncompression, as a reward function. We validate the efficacy of our algorithm\nthrough numerical simulations conducted on a realistic map and compare it with\ntwo alternative approaches. The results indicate that the proposed algorithm\neffectively decreases the time required for the robot to reach its target\nwithout causing excessive load on the communication network.\n","authors":["Evangelos Psomiadis","Dipankar Maity","Panagiotis Tsiotras"],"pdf_url":"https://arxiv.org/pdf/2403.14780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10698v2","updated":"2024-03-21T18:20:14Z","published":"2023-09-19T15:37:24Z","title":"OASIS: Optimal Arrangements for Sensing in SLAM","summary":"  The number and arrangement of sensors on mobile robot dramatically influence\nits perception capabilities. Ensuring that sensors are mounted in a manner that\nenables accurate detection, localization, and mapping is essential for the\nsuccess of downstream control tasks. However, when designing a new robotic\nplatform, researchers and practitioners alike usually mimic standard\nconfigurations or maximize simple heuristics like field-of-view (FOV) coverage\nto decide where to place exteroceptive sensors. In this work, we conduct an\ninformation-theoretic investigation of this overlooked element of robotic\nperception in the context of simultaneous localization and mapping (SLAM). We\nshow how to formalize the sensor arrangement problem as a form of subset\nselection under the E-optimality performance criterion. While this formulation\nis NP-hard in general, we show that a combination of greedy sensor selection\nand fast convex relaxation-based post-hoc verification enables the efficient\nrecovery of certifiably optimal sensor designs in practice. Results from\nsynthetic experiments reveal that sensors placed with OASIS outperform\nbenchmarks in terms of mean squared error of visual SLAM estimates.\n","authors":["Pushyami Kaveti","Matthew Giamou","Hanumant Singh","David M. Rosen"],"pdf_url":"https://arxiv.org/pdf/2309.10698v2.pdf","comment":null}]},"2024-03-22T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2403.14545v2","updated":"2024-03-22T03:19:50Z","published":"2024-03-21T16:44:49Z","title":"Learning Hierarchical Control For Multi-Agent Capacity-Constrained\n  Systems","summary":"  This paper introduces a novel data-driven hierarchical control scheme for\nmanaging a fleet of nonlinear, capacity-constrained autonomous agents in an\niterative environment. We propose a control framework consisting of a\nhigh-level dynamic task assignment and routing layer and low-level motion\nplanning and tracking layer. Each layer of the control hierarchy uses a\ndata-driven Model Predictive Control (MPC) policy, maintaining bounded\ncomputational complexity at each calculation of a new task assignment or\nactuation input. We utilize collected data to iteratively refine estimates of\nagent capacity usage, and update MPC policy parameters accordingly. Our\napproach leverages tools from iterative learning control to integrate learning\nat both levels of the hierarchy, and coordinates learning between levels in\norder to maintain closed-loop feasibility and performance improvement of the\nconnected architecture.\n","authors":["Charlott Vallon","Alessandro Pinto","Bartolomeo Stellato","Francesco Borrelli"],"pdf_url":"https://arxiv.org/pdf/2403.14545v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17587v3","updated":"2024-03-22T07:23:51Z","published":"2024-02-25T07:59:10Z","title":"Instance-aware Exploration-Verification-Exploitation for Instance\n  ImageGoal Navigation","summary":"  As a new embodied vision task, Instance ImageGoal Navigation (IIN) aims to\nnavigate to a specified object depicted by a goal image in an unexplored\nenvironment.\n  The main challenge of this task lies in identifying the target object from\ndifferent viewpoints while rejecting similar distractors.\n  Existing ImageGoal Navigation methods usually adopt the simple\nExploration-Exploitation framework and ignore the identification of specific\ninstance during navigation.\n  In this work, we propose to imitate the human behaviour of ``getting closer\nto confirm\" when distinguishing objects from a distance.\n  Specifically, we design a new modular navigation framework named\nInstance-aware Exploration-Verification-Exploitation (IEVE) for instance-level\nimage goal navigation.\n  Our method allows for active switching among the exploration, verification,\nand exploitation actions, thereby facilitating the agent in making reasonable\ndecisions under different situations.\n  On the challenging HabitatMatterport 3D semantic (HM3D-SEM) dataset, our\nmethod surpasses previous state-of-the-art work, with a classical segmentation\nmodel (0.684 vs. 0.561 success) or a robust model (0.702 vs. 0.561 success)\n","authors":["Xiaohan Lei","Min Wang","Wengang Zhou","Li Li","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2402.17587v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10070v2","updated":"2024-03-22T17:59:09Z","published":"2023-12-06T10:47:53Z","title":"Gaussian-SLAM: Photo-realistic Dense SLAM with Gaussian Splatting","summary":"  We present a dense simultaneous localization and mapping (SLAM) method that\nuses 3D Gaussians as a scene representation. Our approach enables\ninteractive-time reconstruction and photo-realistic rendering from real-world\nsingle-camera RGBD videos. To this end, we propose a novel effective strategy\nfor seeding new Gaussians for newly explored areas and their effective online\noptimization that is independent of the scene size and thus scalable to larger\nscenes. This is achieved by organizing the scene into sub-maps which are\nindependently optimized and do not need to be kept in memory. We further\naccomplish frame-to-model camera tracking by minimizing photometric and\ngeometric losses between the input and rendered frames. The Gaussian\nrepresentation allows for high-quality photo-realistic real-time rendering of\nreal-world scenes. Evaluation on synthetic and real-world datasets demonstrates\ncompetitive or superior performance in mapping, tracking, and rendering\ncompared to existing neural dense SLAM methods.\n","authors":["Vladimir Yugay","Yue Li","Theo Gevers","Martin R. Oswald"],"pdf_url":"https://arxiv.org/pdf/2312.10070v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15370v1","updated":"2024-03-22T17:49:11Z","published":"2024-03-22T17:49:11Z","title":"Augmented Reality based Simulated Data (ARSim) with multi-view\n  consistency for AV perception networks","summary":"  Detecting a diverse range of objects under various driving scenarios is\nessential for the effectiveness of autonomous driving systems. However, the\nreal-world data collected often lacks the necessary diversity presenting a\nlong-tail distribution. Although synthetic data has been utilized to overcome\nthis issue by generating virtual scenes, it faces hurdles such as a significant\ndomain gap and the substantial efforts required from 3D artists to create\nrealistic environments. To overcome these challenges, we present ARSim, a fully\nautomated, comprehensive, modular framework designed to enhance real multi-view\nimage data with 3D synthetic objects of interest. The proposed method\nintegrates domain adaptation and randomization strategies to address covariate\nshift between real and simulated data by inferring essential domain attributes\nfrom real data and employing simulation-based randomization for other\nattributes. We construct a simplified virtual scene using real data and\nstrategically place 3D synthetic assets within it. Illumination is achieved by\nestimating light distribution from multiple images capturing the surroundings\nof the vehicle. Camera parameters from real data are employed to render\nsynthetic assets in each frame. The resulting augmented multi-view consistent\ndataset is used to train a multi-camera perception network for autonomous\nvehicles. Experimental results on various AV perception tasks demonstrate the\nsuperior performance of networks trained on the augmented dataset.\n","authors":["Aqeel Anwar","Tae Eun Choe","Zian Wang","Sanja Fidler","Minwoo Park"],"pdf_url":"https://arxiv.org/pdf/2403.15370v1.pdf","comment":"17 pages, 15 figures, 7 tables"},{"id":"http://arxiv.org/abs/2403.15369v1","updated":"2024-03-22T17:48:13Z","published":"2024-03-22T17:48:13Z","title":"OceanPlan: Hierarchical Planning and Replanning for Natural Language AUV\n  Piloting in Large-scale Unexplored Ocean Environments","summary":"  We develop a hierarchical LLM-task-motion planning and replanning framework\nto efficiently ground an abstracted human command into tangible Autonomous\nUnderwater Vehicle (AUV) control through enhanced representations of the world.\nWe also incorporate a holistic replanner to provide real-world feedback with\nall planners for robust AUV operation. While there has been extensive research\nin bridging the gap between LLMs and robotic missions, they are unable to\nguarantee success of AUV applications in the vast and unknown ocean\nenvironment. To tackle specific challenges in marine robotics, we design a\nhierarchical planner to compose executable motion plans, which achieves\nplanning efficiency and solution quality by decomposing long-horizon missions\ninto sub-tasks. At the same time, real-time data stream is obtained by a\nreplanner to address environmental uncertainties during plan execution.\nExperiments validate that our proposed framework delivers successful AUV\nperformance of long-duration missions through natural language piloting.\n","authors":["Ruochu Yang","Fumin Zhang","Mengxue Hou"],"pdf_url":"https://arxiv.org/pdf/2403.15369v1.pdf","comment":"submitted to IROS 2024"},{"id":"http://arxiv.org/abs/2403.13783v2","updated":"2024-03-22T17:17:14Z","published":"2024-03-20T17:44:33Z","title":"A Convex Formulation of Frictional Contact for the Material Point Method\n  and Rigid Bodies","summary":"  In this paper, we introduce a novel convex formulation that seamlessly\nintegrates the Material Point Method (MPM) with articulated rigid body dynamics\nin frictional contact scenarios. We extend the linear corotational hyperelastic\nmodel into the realm of elastoplasticity and include an efficient return\nmapping algorithm. This approach is particularly effective for MPM simulations\ninvolving significant deformation and topology changes, while preserving the\nconvexity of the optimization problem. Our method ensures global convergence,\nenabling the use of large simulation time steps without compromising\nrobustness. We have validated our approach through rigorous testing and\nperformance evaluations, highlighting its superior capabilities in managing\ncomplex simulations relevant to robotics. Compared to previous MPM based\nrobotic simulators, our method significantly improves the stability of contact\nresolution -- a critical factor in robot manipulation tasks. We make our method\navailable in the open-source robotics toolkit, Drake.\n","authors":["Zeshun Zong","Chenfanfu Jiang","Xuchen Han"],"pdf_url":"https://arxiv.org/pdf/2403.13783v2.pdf","comment":"The supplemental video is available at https://youtu.be/5jrQtF5D0DA"},{"id":"http://arxiv.org/abs/2403.15335v1","updated":"2024-03-22T16:40:48Z","published":"2024-03-22T16:40:48Z","title":"Safe and Stable Teleoperation of Quadrotor UAVs under Haptic Shared\n  Autonomy","summary":"  We present a novel approach that aims to address both safety and stability of\na haptic teleoperation system within a framework of Haptic Shared Autonomy\n(HSA). We use Control Barrier Functions (CBFs) to generate the control input\nthat follows the user's input as closely as possible while guaranteeing safety.\nIn the context of stability of the human-in-the-loop system, we limit the force\nfeedback perceived by the user via a small $L_2$-gain, which is achieved by\nlimiting the control and the force feedback via a differential constraint.\nSpecifically, with the property of HSA, we propose two pathways to design the\ncontrol and the force feedback: Sequential Control Force (SCF) and Joint\nControl Force (JCF). Both designs can achieve safety and stability but with\ndifferent responses to the user's commands. We conducted experimental\nsimulations to evaluate and investigate the properties of the designed methods.\nWe also tested the proposed method on a physical quadrotor UAV and a haptic\ninterface.\n","authors":["Dawei Zhang","Roberto Tron"],"pdf_url":"https://arxiv.org/pdf/2403.15335v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15333v1","updated":"2024-03-22T16:39:13Z","published":"2024-03-22T16:39:13Z","title":"Gesture-Controlled Aerial Robot Formation for Human-Swarm Interaction in\n  Safety Monitoring Applications","summary":"  This paper presents a formation control approach for contactless\ngesture-based Human-Swarm Interaction (HSI) between a team of multi-rotor\nUnmanned Aerial Vehicles (UAVs) and a human worker. The approach is intended\nfor monitoring the safety of human workers, especially those working at\nheights. In the proposed dynamic formation scheme, one UAV acts as the leader\nof the formation and is equipped with sensors for human worker detection and\ngesture recognition. The follower UAVs maintain a predetermined formation\nrelative to the worker's position, thereby providing additional perspectives of\nthe monitored scene. Hand gestures allow the human worker to specify movements\nand action commands for the UAV team and initiate other mission-related\ncommands without the need for an additional communication channel or specific\nmarkers. Together with a novel unified human detection and tracking algorithm,\nhuman pose estimation approach and gesture detection pipeline, the proposed\napproach forms a first instance of an HSI system incorporating all these\nmodules onboard real-world UAVs. Simulations and field experiments with three\nUAVs and a human worker in a mock-up scenario showcase the effectiveness and\nresponsiveness of the proposed approach.\n","authors":["Vít Krátký","Giuseppe Silano","Matouš Vrba","Christos Papaioannidis","Ioannis Mademlis","Robert Pěnička","Ioannis Pitas","Martin Saska"],"pdf_url":"https://arxiv.org/pdf/2403.15333v1.pdf","comment":"8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2310.00401v2","updated":"2024-03-22T16:32:24Z","published":"2023-09-30T14:54:31Z","title":"Learning High-level Semantic-Relational Concepts for SLAM","summary":"  Recent works on SLAM extend their pose graphs with higher-level semantic\nconcepts like Rooms exploiting relationships between them, to provide, not only\na richer representation of the situation/environment but also to improve the\naccuracy of its estimation. Concretely, our previous work, Situational Graphs\n(S-Graphs+), a pioneer in jointly leveraging semantic relationships in the\nfactor optimization process, relies on semantic entities such as Planes and\nRooms, whose relationship is mathematically defined. Nevertheless, there is no\nunique approach to finding all the hidden patterns in lower-level factor-graphs\nthat correspond to high-level concepts of different natures. It is currently\ntackled with ad-hoc algorithms, which limits its graph expressiveness.\n  To overcome this limitation, in this work, we propose an algorithm based on\nGraph Neural Networks for learning high-level semantic-relational concepts that\ncan be inferred from the low-level factor graph. Given a set of mapped Planes\nour algorithm is capable of inferring Room entities relating to the Planes.\nAdditionally, to demonstrate the versatility of our method, our algorithm can\ninfer an additional semantic-relational concept, i.e. Wall, and its\nrelationship with its Planes. We validate our method in both simulated and real\ndatasets demonstrating improved performance over two baseline approaches.\nFurthermore, we integrate our method into the S-Graphs+ algorithm providing\nimproved pose and map accuracy compared to the baseline while further enhancing\nthe scene representation.\n","authors":["Jose Andres Millan-Romera","Hriday Bavle","Muhammad Shaheer","Martin R. Oswald","Holger Voos","Jose Luis Sanchez-Lopez"],"pdf_url":"https://arxiv.org/pdf/2310.00401v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15323v1","updated":"2024-03-22T16:19:56Z","published":"2024-03-22T16:19:56Z","title":"Introduction to Human-Robot Interaction: A Multi-Perspective\n  Introductory Course","summary":"  In this paper I describe the design of an introductory course in Human-Robot\nInteraction. This project-driven course is designed to introduce undergraduate\nand graduate engineering students, especially those enrolled in Computer\nScience, Mechanical Engineering, and Robotics degree programs, to key theories\nand methods used in the field of Human-Robot Interaction that they would\notherwise be unlikely to see in those degree programs. To achieve this aim, the\ncourse takes students all the way from stakeholder analysis to empirical\nevaluation, covering and integrating key Qualitative, Design, Computational,\nand Quantitative methods along the way. I detail the goals, audience, and\nformat of the course, and provide a detailed walkthrough of the course\nsyllabus.\n","authors":["Tom Williams"],"pdf_url":"https://arxiv.org/pdf/2403.15323v1.pdf","comment":"Presented at the Designing an Intro to HRI Course Workshop at HRI\n  2024 (arXiv:2403.05588)"},{"id":"http://arxiv.org/abs/2403.15306v1","updated":"2024-03-22T15:58:34Z","published":"2024-03-22T15:58:34Z","title":"HortiBot: An Adaptive Multi-Arm System for Robotic Horticulture of Sweet\n  Peppers","summary":"  Horticultural tasks such as pruning and selective harvesting are labor\nintensive and horticultural staff are hard to find. Automating these tasks is\nchallenging due to the semi-structured greenhouse workspaces, changing\nenvironmental conditions such as lighting, dense plant growth with many\nocclusions, and the need for gentle manipulation of non-rigid plant organs. In\nthis work, we present the three-armed system HortiBot, with two arms for\nmanipulation and a third arm as an articulated head for active perception using\nstereo cameras. Its perception system detects not only peppers, but also\npeduncles and stems in real time, and performs online data association to build\na world model of pepper plants. Collision-aware online trajectory generation\nallows all three arms to safely track their respective targets for observation,\ngrasping, and cutting. We integrated perception and manipulation to perform\nselective harvesting of peppers and evaluated the system in lab experiments.\nUsing active perception coupled with end-effector force torque sensing for\ncompliant manipulation, HortiBot achieves high success rates.\n","authors":["Christian Lenz","Rohit Menon","Michael Schreiber","Melvin Paul Jacob","Sven Behnke","Maren Bennewitz"],"pdf_url":"https://arxiv.org/pdf/2403.15306v1.pdf","comment":"Submitted to International Conference on Intelligent Robots and\n  Systems (IROS) 2024. C. Lenz and R. Menon contributed equally"},{"id":"http://arxiv.org/abs/2401.15174v3","updated":"2024-03-22T15:02:37Z","published":"2024-01-26T19:39:33Z","title":"LaMI: Large Language Models for Multi-Modal Human-Robot Interaction","summary":"  This paper presents an innovative large language model (LLM)-based robotic\nsystem for enhancing multi-modal human-robot interaction (HRI). Traditional HRI\nsystems relied on complex designs for intent estimation, reasoning, and\nbehavior generation, which were resource-intensive. In contrast, our system\nempowers researchers and practitioners to regulate robot behavior through three\nkey aspects: providing high-level linguistic guidance, creating \"atomic\nactions\" and expressions the robot can use, and offering a set of examples.\nImplemented on a physical robot, it demonstrates proficiency in adapting to\nmulti-modal inputs and determining the appropriate manner of action to assist\nhumans with its arms, following researchers' defined guidelines.\nSimultaneously, it coordinates the robot's lid, neck, and ear movements with\nspeech output to produce dynamic, multi-modal expressions. This showcases the\nsystem's potential to revolutionize HRI by shifting from conventional, manual\nstate-and-flow design methods to an intuitive, guidance-based, and\nexample-driven approach. Supplementary material can be found at\nhttps://hri-eu.github.io/Lami/\n","authors":["Chao Wang","Stephan Hasler","Daniel Tanneberg","Felix Ocker","Frank Joublin","Antonello Ceravola","Joerg Deigmoeller","Michael Gienger"],"pdf_url":"https://arxiv.org/pdf/2401.15174v3.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.15239v1","updated":"2024-03-22T14:32:27Z","published":"2024-03-22T14:32:27Z","title":"Guided Decoding for Robot Motion Generation and Adaption","summary":"  We address motion generation for high-DoF robot arms in complex settings with\nobstacles, via points, etc. A significant advancement in this domain is\nachieved by integrating Learning from Demonstration (LfD) into the motion\ngeneration process. This integration facilitates rapid adaptation to new tasks\nand optimizes the utilization of accumulated expertise by allowing robots to\nlearn and generalize from demonstrated trajectories.\n  We train a transformer architecture on a large dataset of simulated\ntrajectories. This architecture, based on a conditional variational autoencoder\ntransformer, learns essential motion generation skills and adapts these to meet\nauxiliary tasks and constraints. Our auto-regressive approach enables real-time\nintegration of feedback from the physical system, enhancing the adaptability\nand efficiency of motion generation. We show that our model can generate motion\nfrom initial and target points, but also that it can adapt trajectories in\nnavigating complex tasks, including obstacle avoidance, via points, and meeting\nvelocity and acceleration constraints, across platforms.\n","authors":["Nutan Chen","Elie Aljalbout","Botond Cseke","Patrick van der Smagt"],"pdf_url":"https://arxiv.org/pdf/2403.15239v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2403.15223v1","updated":"2024-03-22T14:15:27Z","published":"2024-03-22T14:15:27Z","title":"TriHelper: Zero-Shot Object Navigation with Dynamic Assistance","summary":"  Navigating toward specific objects in unknown environments without additional\ntraining, known as Zero-Shot object navigation, poses a significant challenge\nin the field of robotics, which demands high levels of auxiliary information\nand strategic planning. Traditional works have focused on holistic solutions,\noverlooking the specific challenges agents encounter during navigation such as\ncollision, low exploration efficiency, and misidentification of targets. To\naddress these challenges, our work proposes TriHelper, a novel framework\ndesigned to assist agents dynamically through three primary navigation\nchallenges: collision, exploration, and detection. Specifically, our framework\nconsists of three innovative components: (i) Collision Helper, (ii) Exploration\nHelper, and (iii) Detection Helper. These components work collaboratively to\nsolve these challenges throughout the navigation process. Experiments on the\nHabitat-Matterport 3D (HM3D) and Gibson datasets demonstrate that TriHelper\nsignificantly outperforms all existing baseline methods in Zero-Shot object\nnavigation, showcasing superior success rates and exploration efficiency. Our\nablation studies further underscore the effectiveness of each helper in\naddressing their respective challenges, notably enhancing the agent's\nnavigation capabilities. By proposing TriHelper, we offer a fresh perspective\non advancing the object navigation task, paving the way for future research in\nthe domain of Embodied AI and visual-based navigation.\n","authors":["Lingfeng Zhang","Qiang Zhang","Hao Wang","Erjia Xiao","Zixuan Jiang","Honglei Chen","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2403.15223v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.15203v1","updated":"2024-03-22T13:46:51Z","published":"2024-03-22T13:46:51Z","title":"DITTO: Demonstration Imitation by Trajectory Transformation","summary":"  Teaching robots new skills quickly and conveniently is crucial for the\nbroader adoption of robotic systems. In this work, we address the problem of\none-shot imitation from a single human demonstration, given by an RGB-D video\nrecording through a two-stage process. In the first stage which is offline, we\nextract the trajectory of the demonstration. This entails segmenting\nmanipulated objects and determining their relative motion in relation to\nsecondary objects such as containers. Subsequently, in the live online\ntrajectory generation stage, we first \\mbox{re-detect} all objects, then we\nwarp the demonstration trajectory to the current scene, and finally, we trace\nthe trajectory with the robot. To complete these steps, our method makes\nleverages several ancillary models, including those for segmentation, relative\nobject pose estimation, and grasp prediction. We systematically evaluate\ndifferent combinations of correspondence and re-detection methods to validate\nour design decision across a diverse range of tasks. Specifically, we collect\ndemonstrations of ten different tasks including pick-and-place tasks as well as\narticulated object manipulation. Finally, we perform extensive evaluations on a\nreal robot system to demonstrate the effectiveness and utility of our approach\nin real-world scenarios. We make the code publicly available at\nhttp://ditto.cs.uni-freiburg.de.\n","authors":["Nick Heppert","Max Argus","Tim Welschehold","Thomas Brox","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2403.15203v1.pdf","comment":"8 pages, 4 figures, 3 tables, submitted to IROS 2024"},{"id":"http://arxiv.org/abs/2403.15183v1","updated":"2024-03-22T13:12:30Z","published":"2024-03-22T13:12:30Z","title":"CRPlace: Camera-Radar Fusion with BEV Representation for Place\n  Recognition","summary":"  The integration of complementary characteristics from camera and radar data\nhas emerged as an effective approach in 3D object detection. However, such\nfusion-based methods remain unexplored for place recognition, an equally\nimportant task for autonomous systems. Given that place recognition relies on\nthe similarity between a query scene and the corresponding candidate scene, the\nstationary background of a scene is expected to play a crucial role in the\ntask. As such, current well-designed camera-radar fusion methods for 3D object\ndetection can hardly take effect in place recognition because they mainly focus\non dynamic foreground objects. In this paper, a background-attentive\ncamera-radar fusion-based method, named CRPlace, is proposed to generate\nbackground-attentive global descriptors from multi-view images and radar point\nclouds for accurate place recognition. To extract stationary background\nfeatures effectively, we design an adaptive module that generates the\nbackground-attentive mask by utilizing the camera BEV feature and radar dynamic\npoints. With the guidance of a background mask, we devise a bidirectional\ncross-attention-based spatial fusion strategy to facilitate comprehensive\nspatial interaction between the background information of the camera BEV\nfeature and the radar BEV feature. As the first camera-radar fusion-based place\nrecognition network, CRPlace has been evaluated thoroughly on the nuScenes\ndataset. The results show that our algorithm outperforms a variety of baseline\nmethods across a comprehensive set of metrics (recall@1 reaches 91.2%).\n","authors":["Shaowei Fu","Yifan Duan","Yao Li","Chengzhen Meng","Yingjie Wang","Jianmin Ji","Yanyong Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.15183v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15171v1","updated":"2024-03-22T12:48:00Z","published":"2024-03-22T12:48:00Z","title":"AV-Occupant Perceived Risk Model for Cut-In Scenarios with Empirical\n  Evaluation","summary":"  Advancements in autonomous vehicle (AV) technologies necessitate precise\nestimation of perceived risk to enhance user comfort, acceptance and trust.\nThis paper introduces a novel AV-Occupant Risk (AVOR) model designed for\nperceived risk estimation during AV cut-in scenarios. An empirical study is\nconducted with 18 participants with realistic cut-in scenarios. Two factors\nwere investigated: scenario risk and scene population. 76% of subjective risk\nresponses indicate an increase in perceived risk at cut-in initiation. The\nexisting perceived risk model did not capture this critical phenomenon. Our\nAVOR model demonstrated a significant improvement in estimating perceived risk\nduring the early stages of cut-ins, especially for the high-risk scenario,\nenhancing modelling accuracy by up to 54%. The concept of the AVOR model can\nquantify perceived risk in other diverse driving contexts characterized by\ndynamic uncertainties, enhancing the reliability and human-centred focus of AV\nsystems.\n","authors":["Sarah Barendswaard","Tong Duy Son"],"pdf_url":"https://arxiv.org/pdf/2403.15171v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03270v2","updated":"2024-03-22T12:40:23Z","published":"2024-03-05T19:11:17Z","title":"Bi-KVIL: Keypoints-based Visual Imitation Learning of Bimanual\n  Manipulation Tasks","summary":"  Visual imitation learning has achieved impressive progress in learning\nunimanual manipulation tasks from a small set of visual observations, thanks to\nthe latest advances in computer vision. However, learning bimanual coordination\nstrategies and complex object relations from bimanual visual demonstrations, as\nwell as generalizing them to categorical objects in novel cluttered scenes\nremain unsolved challenges. In this paper, we extend our previous work on\nkeypoints-based visual imitation learning (\\mbox{K-VIL})~\\cite{gao_kvil_2023}\nto bimanual manipulation tasks. The proposed Bi-KVIL jointly extracts so-called\n\\emph{Hybrid Master-Slave Relationships} (HMSR) among objects and hands,\nbimanual coordination strategies, and sub-symbolic task representations. Our\nbimanual task representation is object-centric, embodiment-independent, and\nviewpoint-invariant, thus generalizing well to categorical objects in novel\nscenes. We evaluate our approach in various real-world applications, showcasing\nits ability to learn fine-grained bimanual manipulation tasks from a small\nnumber of human demonstration videos. Videos and source code are available at\nhttps://sites.google.com/view/bi-kvil.\n","authors":["Jianfeng Gao","Xiaoshu Jin","Franziska Krebs","Noémie Jaquier","Tamim Asfour"],"pdf_url":"https://arxiv.org/pdf/2403.03270v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15156v1","updated":"2024-03-22T12:11:06Z","published":"2024-03-22T12:11:06Z","title":"Infrastructure-Assisted Collaborative Perception in Automated Valet\n  Parking: A Safety Perspective","summary":"  Environmental perception in Automated Valet Parking (AVP) has been a\nchallenging task due to severe occlusions in parking garages. Although\nCollaborative Perception (CP) can be applied to broaden the field of view of\nconnected vehicles, the limited bandwidth of vehicular communications restricts\nits application. In this work, we propose a BEV feature-based CP network\narchitecture for infrastructure-assisted AVP systems. The model takes the\nroadside camera and LiDAR as optional inputs and adaptively fuses them with\nonboard sensors in a unified BEV representation. Autoencoder and downsampling\nare applied for channel-wise and spatial-wise dimension reduction, while\nsparsification and quantization further compress the feature map with little\nloss in data precision. Combining these techniques, the size of a BEV feature\nmap is effectively compressed to fit in the feasible data rate of the NR-V2X\nnetwork. With the synthetic AVP dataset, we observe that CP can effectively\nincrease perception performance, especially for pedestrians. Moreover, the\nadvantage of infrastructure-assisted CP is demonstrated in two typical\nsafety-critical scenarios in the AVP setting, increasing the maximum safe\ncruising speed by up to 3m/s in both scenarios.\n","authors":["Yukuan Jia","Jiawen Zhang","Shimeng Lu","Baokang Fan","Ruiqing Mao","Sheng Zhou","Zhisheng Niu"],"pdf_url":"https://arxiv.org/pdf/2403.15156v1.pdf","comment":"7 pages, 7 figures, 4 tables, accepted by IEEE VTC2024-Spring"},{"id":"http://arxiv.org/abs/2403.15151v1","updated":"2024-03-22T12:07:03Z","published":"2024-03-22T12:07:03Z","title":"RHINO-VR Experience: Teaching Mobile Robotics Concepts in an Interactive\n  Museum Exhibit","summary":"  In 1997, the very first tour guide robot RHINO was deployed in a museum in\nGermany. With the ability to navigate autonomously through the environment, the\nrobot gave tours to over 2,000 visitors. Today, RHINO itself has become an\nexhibit and is no longer operational. In this paper, we present RHINO-VR, an\ninteractive museum exhibit using virtual reality (VR) that allows museum\nvisitors to experience the historical robot RHINO in operation in a virtual\nmuseum. RHINO-VR, unlike static exhibits, enables users to familiarize\nthemselves with basic mobile robotics concepts without the fear of damaging the\nexhibit. In the virtual environment, the user is able to interact with RHINO in\nVR by pointing to a location to which the robot should navigate and observing\nthe corresponding actions of the robot. To include other visitors who cannot\nuse the VR, we provide an external observation view to make RHINO visible to\nthem. We evaluated our system by measuring the frame rate of the VR simulation,\ncomparing the generated virtual 3D models with the originals, and conducting a\nuser study. The user-study showed that RHINO-VR improved the visitors'\nunderstanding of the robot's functionality and that they would recommend\nexperiencing the VR exhibit to others.\n","authors":["Erik Schlachhoff","Nils Dengler","Leif Van Holland","Patrick Stotko","Jorge de Heuvel","Reinhard Klein","Maren Bennewitz"],"pdf_url":"https://arxiv.org/pdf/2403.15151v1.pdf","comment":"Submitted to IEEE International Symposium on Robot and Human\n  Interactive Communication (RO-MAN)"},{"id":"http://arxiv.org/abs/2403.15142v1","updated":"2024-03-22T11:52:31Z","published":"2024-03-22T11:52:31Z","title":"ALPINE: a climbing robot for operations in mountain environments","summary":"  Mountain slopes are perfect examples of harsh environments in which humans\nare required to perform difficult and dangerous operations such as removing\nunstable boulders, dangerous vegetation or deploying safety nets. A good\nreplacement for human intervention can be offered by climbing robots. The\ndifferent solutions existing in the literature are not up to the task for the\ndifficulty of the requirements (navigation, heavy payloads, flexibility in the\nexecution of the tasks). In this paper, we propose a robotic platform that can\nfill this gap. Our solution is based on a robot that hangs on ropes, and uses a\nretractable leg to jump away from the mountain walls. Our package of mechanical\nsolutions, along with the algorithms developed for motion planning and control,\ndelivers swift navigation on irregular and steep slopes, the possibility to\novercome or travel around significant natural barriers, and the ability to\ncarry heavy payloads and execute complex tasks. In the paper, we give a full\naccount of our main design and algorithmic choices and show the feasibility of\nthe solution through a large number of physically simulated scenarios.\n","authors":["Michele Focchi","Andrea Del Prete","Daniele Fontanelli","Marco Frego","Angelika Peer","Luigi Palopoli"],"pdf_url":"https://arxiv.org/pdf/2403.15142v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15116v1","updated":"2024-03-22T11:20:30Z","published":"2024-03-22T11:20:30Z","title":"Collision Avoidance Safety Filter for an Autonomous E-Scooter using\n  Ultrasonic Sensors","summary":"  In this paper, we propose a collision avoidance safety filter for autonomous\nelectric scooters to enable safe operation of such vehicles in pedestrian\nareas. In particular, we employ multiple low-cost ultrasonic sensors to detect\na wide range of possible obstacles in front of the e-scooter. Based on possibly\nfaulty distance measurements, we design a filter to mitigate measurement noise\nand missing values as well as a gain-scheduled controller to limit the velocity\ncommanded to the e-scooter when required due to imminent collisions. The\nproposed controller structure is able to prevent collisions with unknown\nobstacles by deploying a reduced safe velocity ensuring a sufficiently large\nsafety distance. The collision avoidance approach is designed such that it may\nbe easily deployed in similar applications of general micromobility vehicles.\nThe effectiveness of our proposed safety filter is demonstrated in real-world\nexperiments.\n","authors":["Robin Strässer","Marc Seidel","Felix Brändle","David Meister","Raffaele Soloperto","David Hambach Ferrer","Frank Allgöwer"],"pdf_url":"https://arxiv.org/pdf/2403.15116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15113v1","updated":"2024-03-22T11:08:56Z","published":"2024-03-22T11:08:56Z","title":"Set-membership target search and tracking within an unknown cluttered\n  area using cooperating UAVs equipped with vision systems","summary":"  This paper addresses the problem of target search and tracking using a fleet\nof cooperating UAVs evolving in some unknown region of interest containing an a\npriori unknown number of moving ground targets. Each drone is equipped with an\nembedded Computer Vision System (CVS), providing an image with labeled pixels\nand a depth map of the observed part of its environment. Moreover, a box\ncontaining the corresponding pixels in the image frame is available when a UAV\nidentifies a target. Hypotheses regarding information provided by the pixel\nclassification, depth map construction, and target identification algorithms\nare proposed to allow its exploitation by set-membership approaches. A\nset-membership target location estimator is developed using the information\nprovided by the CVS. Each UAV evaluates sets guaranteed to contain the location\nof the identified targets and a set possibly containing the locations of\ntargets still to be identified. Then, each UAV uses these sets to search and\ntrack targets cooperatively.\n","authors":["Maxime Zagar","Luc Meyer","Michel Kieffer","Hélène Piet-Lahanier"],"pdf_url":"https://arxiv.org/pdf/2403.15113v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15107v1","updated":"2024-03-22T10:51:31Z","published":"2024-03-22T10:51:31Z","title":"PseudoTouch: Efficiently Imaging the Surface Feel of Objects for Robotic\n  Manipulation","summary":"  Humans seemingly incorporate potential touch signals in their perception. Our\ngoal is to equip robots with a similar capability, which we term \\ourmodel.\n\\ourmodel aims to predict the expected touch signal based on a visual patch\nrepresenting the touched area. We frame this problem as the task of learning a\nlow-dimensional visual-tactile embedding, wherein we encode a depth patch from\nwhich we decode the tactile signal. To accomplish this task, we employ ReSkin,\nan inexpensive and replaceable magnetic-based tactile sensor. Using ReSkin, we\ncollect and train PseudoTouch on a dataset comprising aligned tactile and\nvisual data pairs obtained through random touching of eight basic geometric\nshapes. We demonstrate the efficacy of PseudoTouch through its application to\ntwo downstream tasks: object recognition and grasp stability prediction. In the\nobject recognition task, we evaluate the learned embedding's performance on a\nset of five basic geometric shapes and five household objects. Using\nPseudoTouch, we achieve an object recognition accuracy 84% after just ten\ntouches, surpassing a proprioception baseline. For the grasp stability task, we\nuse ACRONYM labels to train and evaluate a grasp success predictor using\nPseudoTouch's predictions derived from virtual depth information. Our approach\nyields an impressive 32% absolute improvement in accuracy compared to the\nbaseline relying on partial point cloud data. We make the data, code, and\ntrained models publicly available at http://pseudotouch.cs.uni-freiburg.de.\n","authors":["Adrian Röfer","Nick Heppert","Abdallah Ayman","Eugenio Chisari","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2403.15107v1.pdf","comment":"8 pages, 7 figures, 2 tables, submitted to IROS2024"},{"id":"http://arxiv.org/abs/2403.15102v1","updated":"2024-03-22T10:41:25Z","published":"2024-03-22T10:41:25Z","title":"Learning from Visual Demonstrations through Differentiable Nonlinear MPC\n  for Personalized Autonomous Driving","summary":"  Human-like autonomous driving controllers have the potential to enhance\npassenger perception of autonomous vehicles. This paper proposes DriViDOC: a\nmodel for Driving from Vision through Differentiable Optimal Control, and its\napplication to learn personalized autonomous driving controllers from human\ndemonstrations. DriViDOC combines the automatic inference of relevant features\nfrom camera frames with the properties of nonlinear model predictive control\n(NMPC), such as constraint satisfaction. Our approach leverages the\ndifferentiability of parametric NMPC, allowing for end-to-end learning of the\ndriving model from images to control. The model is trained on an offline\ndataset comprising various driving styles collected on a motion-base driving\nsimulator. During online testing, the model demonstrates successful imitation\nof different driving styles, and the interpreted NMPC parameters provide\ninsights into the achievement of specific driving behaviors. Our experimental\nresults show that DriViDOC outperforms other methods involving NMPC and neural\nnetworks, exhibiting an average improvement of 20% in imitation scores.\n","authors":["Flavia Sofia Acerbo","Jan Swevers","Tinne Tuytelaars","Tong Duy Son"],"pdf_url":"https://arxiv.org/pdf/2403.15102v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible. Accompanying video available at:\n  https://youtu.be/WxWPuAtJ08E"},{"id":"http://arxiv.org/abs/2403.15100v1","updated":"2024-03-22T10:39:22Z","published":"2024-03-22T10:39:22Z","title":"Subequivariant Reinforcement Learning Framework for Coordinated Motion\n  Control","summary":"  Effective coordination is crucial for motion control with reinforcement\nlearning, especially as the complexity of agents and their motions increases.\nHowever, many existing methods struggle to account for the intricate\ndependencies between joints. We introduce CoordiGraph, a novel architecture\nthat leverages subequivariant principles from physics to enhance coordination\nof motion control with reinforcement learning. This method embeds the\nprinciples of equivariance as inherent patterns in the learning process under\ngravity influence, which aids in modeling the nuanced relationships between\njoints vital for motion control. Through extensive experimentation with\nsophisticated agents in diverse environments, we highlight the merits of our\napproach. Compared to current leading methods, CoordiGraph notably enhances\ngeneralization and sample efficiency.\n","authors":["Haoyu Wang","Xiaoyu Tan","Xihe Qiu","Chao Qu"],"pdf_url":"https://arxiv.org/pdf/2403.15100v1.pdf","comment":"7 pages, 7 figures, 2024 IEEE International Conference on Robotics\n  and Automation"},{"id":"http://arxiv.org/abs/2304.09793v2","updated":"2024-03-22T10:36:32Z","published":"2023-04-19T16:21:14Z","title":"Event-based Simultaneous Localization and Mapping: A Comprehensive\n  Survey","summary":"  In recent decades, visual simultaneous localization and mapping (vSLAM) has\ngained significant interest in both academia and industry. It estimates camera\nmotion and reconstructs the environment concurrently using visual sensors on a\nmoving robot. However, conventional cameras are limited by hardware, including\nmotion blur and low dynamic range, which can negatively impact performance in\nchallenging scenarios like high-speed motion and high dynamic range\nillumination. Recent studies have demonstrated that event cameras, a new type\nof bio-inspired visual sensor, offer advantages such as high temporal\nresolution, dynamic range, low power consumption, and low latency. This paper\npresents a timely and comprehensive review of event-based vSLAM algorithms that\nexploit the benefits of asynchronous and irregular event streams for\nlocalization and mapping tasks. The review covers the working principle of\nevent cameras and various event representations for preprocessing event data.\nIt also categorizes event-based vSLAM methods into four main categories:\nfeature-based, direct, motion-compensation, and deep learning methods, with\ndetailed discussions and practical guidance for each approach. Furthermore, the\npaper evaluates the state-of-the-art methods on various benchmarks,\nhighlighting current challenges and future opportunities in this emerging\nresearch area. A public repository will be maintained to keep track of the\nrapid developments in this field at\n{\\url{https://github.com/kun150kun/ESLAM-survey}}.\n","authors":["Kunping Huang","Sen Zhang","Jing Zhang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2304.09793v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.16973v2","updated":"2024-03-22T10:27:53Z","published":"2023-06-29T14:28:22Z","title":"Robust Direct Data-Driven Control for Probabilistic Systems","summary":"  We propose a data-driven control method for systems with aleatoric\nuncertainty, for example, robot fleets with variations between agents. Our\nmethod leverages shared trajectory data to increase the robustness of the\ndesigned controller and thus facilitate transfer to new variations without the\nneed for prior parameter and uncertainty estimations. In contrast to existing\nwork on experience transfer for performance, our approach focuses on robustness\nand uses data collected from multiple realizations to guarantee generalization\nto unseen ones. Our method is based on scenario optimization combined with\nrecent formulations for direct data-driven control. We derive lower bounds on\nthe amount of data required to achieve quadratic stability for probabilistic\nsystems with aleatoric uncertainty and demonstrate the benefits of our\ndata-driven method through a numerical example. We find that the learned\ncontrollers generalize well to high variations in the dynamics even when based\non only a few short open-loop trajectories. Robust experience transfer enables\nthe design of safe and robust controllers that work out of the box without any\nadditional learning during deployment.\n","authors":["Alexander von Rohr","Dmitrii Likhachev","Sebastian Trimpe"],"pdf_url":"https://arxiv.org/pdf/2306.16973v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15079v1","updated":"2024-03-22T10:05:21Z","published":"2024-03-22T10:05:21Z","title":"Automated Feature Selection for Inverse Reinforcement Learning","summary":"  Inverse reinforcement learning (IRL) is an imitation learning approach to\nlearning reward functions from expert demonstrations. Its use avoids the\ndifficult and tedious procedure of manual reward specification while retaining\nthe generalization power of reinforcement learning. In IRL, the reward is\nusually represented as a linear combination of features. In continuous state\nspaces, the state variables alone are not sufficiently rich to be used as\nfeatures, but which features are good is not known in general. To address this\nissue, we propose a method that employs polynomial basis functions to form a\ncandidate set of features, which are shown to allow the matching of statistical\nmoments of state distributions. Feature selection is then performed for the\ncandidates by leveraging the correlation between trajectory probabilities and\nfeature expectations. We demonstrate the approach's effectiveness by recovering\nreward functions that capture expert policies across non-linear control tasks\nof increasing complexity. Code, data, and videos are available at\nhttps://sites.google.com/view/feature4irl.\n","authors":["Daulet Baimukashev","Gokhan Alcan","Ville Kyrki"],"pdf_url":"https://arxiv.org/pdf/2403.15079v1.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.15067v1","updated":"2024-03-22T09:48:40Z","published":"2024-03-22T09:48:40Z","title":"A Twin Delayed Deep Deterministic Policy Gradient Algorithm for\n  Autonomous Ground Vehicle Navigation via Digital Twin Perception Awareness","summary":"  Autonomous ground vehicle (UGV) navigation has the potential to revolutionize\nthe transportation system by increasing accessibility to disabled people,\nensure safety and convenience of use. However, UGV requires extensive and\nefficient testing and evaluation to ensure its acceptance for public use. This\ntesting are mostly done in a simulator which result to sim2real transfer gap.\nIn this paper, we propose a digital twin perception awareness approach for the\ncontrol of robot navigation without prior creation of the virtual environment\n(VT) environment state. To achieve this, we develop a twin delayed deep\ndeterministic policy gradient (TD3) algorithm that ensures collision avoidance\nand goal-based path planning. We demonstrate the performance of our approach on\ndifferent environment dynamics. We show that our approach is capable of\nefficiently avoiding collision with obstacles and navigating to its desired\ndestination, while at the same time safely avoids obstacles using the\ninformation received from the LIDAR sensor mounted on the robot. Our approach\nbridges the gap between sim-to-real transfer and contributes to the adoption of\nUGVs in real world. We validate our approach in simulation and a real-world\napplication in an office space.\n","authors":["Kabirat Olayemi","Mien Van","Sean McLoone","Yuzhu Sun","Jack Close","Nguyen Minh Nhat","Stephen McIlvanna"],"pdf_url":"https://arxiv.org/pdf/2403.15067v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.15054v1","updated":"2024-03-22T09:26:52Z","published":"2024-03-22T09:26:52Z","title":"Rethinking 6-Dof Grasp Detection: A Flexible Framework for High-Quality\n  Grasping","summary":"  Robotic grasping is a primitive skill for complex tasks and is fundamental to\nintelligence. For general 6-Dof grasping, most previous methods directly\nextract scene-level semantic or geometric information, while few of them\nconsider the suitability for various downstream applications, such as\ntarget-oriented grasping. Addressing this issue, we rethink 6-Dof grasp\ndetection from a grasp-centric view and propose a versatile grasp framework\ncapable of handling both scene-level and target-oriented grasping. Our\nframework, FlexLoG, is composed of a Flexible Guidance Module and a Local Grasp\nModel. Specifically, the Flexible Guidance Module is compatible with both\nglobal (e.g., grasp heatmap) and local (e.g., visual grounding) guidance,\nenabling the generation of high-quality grasps across various tasks. The Local\nGrasp Model focuses on object-agnostic regional points and predicts grasps\nlocally and intently. Experiment results reveal that our framework achieves\nover 18% and 23% improvement on unseen splits of the GraspNet-1Billion Dataset.\nFurthermore, real-world robotic tests in three distinct settings yield a 95%\nsuccess rate.\n","authors":["Wei Tang","Siang Chen","Pengwei Xie","Dingchang Hu","Wenming Yang","Guijin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.15054v1.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2401.10519v2","updated":"2024-03-22T09:09:45Z","published":"2024-01-19T06:32:54Z","title":"A Wind-Aware Path Planning Method for UAV-Asisted Bridge Inspection","summary":"  In response to the gap in considering wind conditions in the bridge\ninspection using unmanned aerial vehicle (UAV) , this paper proposes a path\nplanning method for UAVs that takes into account the influence of wind, based\non the simulated annealing algorithm. The algorithm considers the wind factors,\nincluding the influence of different wind speeds and directions at the same\ntime on the path planning of the UAV. Firstly, An environment model is\nconstructed specifically for UAV bridge inspection, taking into account the\nvarious objective functions and constraint conditions of UAVs. A more\nsophisticated and precise mathematical model is then developed based on this\nenvironmental model to enable efficient and effective UAV path planning.\nSecondly, the bridge separation planning model is applied in a novel way, and a\nseries of parameters are simulated, including the adjustment of the initial\ntemperature value. The experimental results demonstrate that, compared with\ntraditional local search algorithms, the proposed method achieves a cost\nreduction of 30.05\\% and significantly improves effectiveness. Compared to path\nplanning methods that do not consider wind factors, the proposed approach\nyields more realistic and practical results for UAV applications, as\ndemonstrated by its improved effectiveness in simulations. These findings\nhighlight the value of our method in facilitating more accurate and efficient\nUAV path planning in wind-prone environments.\n","authors":["Jian Xu","Hua Dai"],"pdf_url":"https://arxiv.org/pdf/2401.10519v2.pdf","comment":"After carefully analysis, there is a bit design flaws in Algorithm 1.\n  The experimental work of the paper is not comprehensive,which lacks an\n  evaluation of the algorithm's running time"},{"id":"http://arxiv.org/abs/2403.14997v1","updated":"2024-03-22T07:17:56Z","published":"2024-03-22T07:17:56Z","title":"Linear Quadratic Guidance Law for Joint Motion Planning of a\n  Pursuer-Turret Assembly","summary":"  This paper presents joint motion planning of a vehicle with an attached\nrotating turret. The turret has a limited range as well as the field of view.\nThe objective is capture a maneuvering target such that at the terminal time it\nis withing the field-of-view and range limits. Catering to it, we present a\nminimum effort guidance law that commensurate for the turn rate abilities of\nthe vehicle and the turret. The guidance law is obtained using linearization\nabout the collision triangle and admits an analytical solution. Simulation\nresults are presented to exemplify the cooperation between the turret and the\nvehicle.\n","authors":["Bhargav Jha","Shaunak Bopardikar","Alexander Von Moll","David Casbeer"],"pdf_url":"https://arxiv.org/pdf/2403.14997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.07433v5","updated":"2024-03-22T06:42:03Z","published":"2023-02-15T02:32:26Z","title":"A Survey on Global LiDAR Localization: Challenges, Advances and Open\n  Problems","summary":"  Knowledge about the own pose is key for all mobile robot applications. Thus\npose estimation is part of the core functionalities of mobile robots. Over the\nlast two decades, LiDAR scanners have become the standard sensor for robot\nlocalization and mapping. This article aims to provide an overview of recent\nprogress and advancements in LiDAR-based global localization. We begin by\nformulating the problem and exploring the application scope. We then present a\nreview of the methodology, including recent advancements in several topics,\nsuch as maps, descriptor extraction, and cross-robot localization. The contents\nof the article are organized under three themes. The first theme concerns the\ncombination of global place retrieval and local pose estimation. The second\ntheme is upgrading single-shot measurements to sequential ones for sequential\nglobal localization. Finally, the third theme focuses on extending single-robot\nglobal localization to cross-robot localization in multi-robot systems. We\nconclude the survey with a discussion of open challenges and promising\ndirections in global LiDAR localization. To our best knowledge, this is the\nfirst comprehensive survey on global LiDAR localization for mobile robots.\n","authors":["Huan Yin","Xuecheng Xu","Sha Lu","Xieyuanli Chen","Rong Xiong","Shaojie Shen","Cyrill Stachniss","Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2302.07433v5.pdf","comment":"Publishe on International Journal of Computer Vision (IJCV)"},{"id":"http://arxiv.org/abs/2403.14956v1","updated":"2024-03-22T05:21:05Z","published":"2024-03-22T05:21:05Z","title":"Boundary-Aware Value Function Generation for Safe Stochastic Motion\n  Planning","summary":"  Navigation safety is critical for many autonomous systems such as\nself-driving vehicles in an urban environment. It requires an explicit\nconsideration of boundary constraints that describe the borders of any\ninfeasible, non-navigable, or unsafe regions. We propose a principled\nboundary-aware safe stochastic planning framework with promising results. Our\nmethod generates a value function that can strictly distinguish the state\nvalues between free (safe) and non-navigable (boundary) spaces in the\ncontinuous state, naturally leading to a safe boundary-aware policy. At the\ncore of our solution lies a seamless integration of finite elements and\nkernel-based functions, where the finite elements allow us to characterize\nsafety-critical states' borders accurately, and the kernel-based function\nspeeds up computation for the non-safety-critical states. The proposed method\nwas evaluated through extensive simulations and demonstrated safe navigation\nbehaviors in mobile navigation tasks. Additionally, we demonstrate that our\napproach can maneuver safely and efficiently in cluttered real-world\nenvironments using a ground vehicle with strong external disturbances, such as\nnavigating on a slippery floor and against external human intervention.\n","authors":["Junhong Xu","Kai Yin","Jason M. Gregory","Kris Hauser","Lantao Liu"],"pdf_url":"https://arxiv.org/pdf/2403.14956v1.pdf","comment":"Accepted by International Journal of Robotics Research"},{"id":"http://arxiv.org/abs/2309.15271v2","updated":"2024-03-22T03:32:09Z","published":"2023-09-26T21:10:12Z","title":"Kinematic Modularity of Elementary Dynamic Actions","summary":"  In this paper, a kinematically modular approach to robot control is\npresented. The method involves structures called Elementary Dynamic Actions and\na network model combining these elements. With this control framework, a rich\nrepertoire of movements can be generated by combination of basic modules. The\nproblems of solving inverse kinematics, managing kinematic singularity and\nkinematic redundancy are avoided. The modular approach is robust against\ncontact and physical interaction, which makes it particularly effective for\ncontact-rich manipulation. Each kinematic module can be learned by Imitation\nLearning, thereby resulting in a modular learning strategy for robot control.\nThe theoretical foundations and their real robot implementation are presented.\nUsing a KUKA LBR iiwa14 robot, three tasks were considered: (1) generating a\nsequence of discrete movements, (2) generating a combination of discrete and\nrhythmic movements, and (3) a drawing and erasing task. The results obtained\nindicate that this modular approach has the potential to simplify the\ngeneration of a diverse range of robot actions.\n","authors":["Moses C. Nah","Johannes Lachner","Federico Tessari","Neville Hogan"],"pdf_url":"https://arxiv.org/pdf/2309.15271v2.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2306.07894v5","updated":"2024-03-22T02:10:49Z","published":"2023-06-13T16:39:39Z","title":"iSLAM: Imperative SLAM","summary":"  Simultaneous Localization and Mapping (SLAM) stands as one of the critical\nchallenges in robot navigation. A SLAM system often consists of a front-end\ncomponent for motion estimation and a back-end system for eliminating\nestimation drifts. Recent advancements suggest that data-driven methods are\nhighly effective for front-end tasks, while geometry-based methods continue to\nbe essential in the back-end processes. However, such a decoupled paradigm\nbetween the data-driven front-end and geometry-based back-end can lead to\nsub-optimal performance, consequently reducing the system's capabilities and\ngeneralization potential. To solve this problem, we proposed a novel\nself-supervised imperative learning framework, named imperative SLAM (iSLAM),\nwhich fosters reciprocal correction between the front-end and back-end, thus\nenhancing performance without necessitating any external supervision.\nSpecifically, we formulate the SLAM problem as a bilevel optimization so that\nthe front-end and back-end are bidirectionally connected. As a result, the\nfront-end model can learn global geometric knowledge obtained through pose\ngraph optimization by back-propagating the residuals from the back-end\ncomponent. We showcase the effectiveness of this new framework through an\napplication of stereo-inertial SLAM. The experiments show that the iSLAM\ntraining strategy achieves an accuracy improvement of 22% on average over a\nbaseline model. To the best of our knowledge, iSLAM is the first SLAM system\nshowing that the front-end and back-end components can mutually correct each\nother in a self-supervised manner.\n","authors":["Taimeng Fu","Shaoshu Su","Yiren Lu","Chen Wang"],"pdf_url":"https://arxiv.org/pdf/2306.07894v5.pdf","comment":"The paper has been accepted by IEEE Robotics and Automation Letters\n  (RA-L)"},{"id":"http://arxiv.org/abs/2306.06531v3","updated":"2024-03-22T00:21:04Z","published":"2023-06-10T21:58:29Z","title":"AutoTAMP: Autoregressive Task and Motion Planning with LLMs as\n  Translators and Checkers","summary":"  For effective human-robot interaction, robots need to understand, plan, and\nexecute complex, long-horizon tasks described by natural language. Recent\nadvances in large language models (LLMs) have shown promise for translating\nnatural language into robot action sequences for complex tasks. However,\nexisting approaches either translate the natural language directly into robot\ntrajectories or factor the inference process by decomposing language into task\nsub-goals and relying on a motion planner to execute each sub-goal. When\ncomplex environmental and temporal constraints are involved, inference over\nplanning tasks must be performed jointly with motion plans using traditional\ntask-and-motion planning (TAMP) algorithms, making factorization into subgoals\nuntenable. Rather than using LLMs to directly plan task sub-goals, we instead\nperform few-shot translation from natural language task descriptions to an\nintermediate task representation that can then be consumed by a TAMP algorithm\nto jointly solve the task and motion plan. To improve translation, we\nautomatically detect and correct both syntactic and semantic errors via\nautoregressive re-prompting, resulting in significant improvements in task\ncompletion. We show that our approach outperforms several methods using LLMs as\nplanners in complex task domains. See our project website\nhttps://yongchao98.github.io/MIT-REALM-AutoTAMP/ for prompts, videos, and code.\n","authors":["Yongchao Chen","Jacob Arkin","Charles Dawson","Yang Zhang","Nicholas Roy","Chuchu Fan"],"pdf_url":"https://arxiv.org/pdf/2306.06531v3.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2309.15943v2","updated":"2024-03-22T00:11:21Z","published":"2023-09-27T18:40:36Z","title":"Scalable Multi-Robot Collaboration with Large Language Models:\n  Centralized or Decentralized Systems?","summary":"  A flurry of recent work has demonstrated that pre-trained large language\nmodels (LLMs) can be effective task planners for a variety of single-robot\ntasks. The planning performance of LLMs is significantly improved via prompting\ntechniques, such as in-context learning or re-prompting with state feedback,\nplacing new importance on the token budget for the context window. An\nunder-explored but natural next direction is to investigate LLMs as multi-robot\ntask planners. However, long-horizon, heterogeneous multi-robot planning\nintroduces new challenges of coordination while also pushing up against the\nlimits of context window length. It is therefore critical to find\ntoken-efficient LLM planning frameworks that are also able to reason about the\ncomplexities of multi-robot coordination. In this work, we compare the task\nsuccess rate and token efficiency of four multi-agent communication frameworks\n(centralized, decentralized, and two hybrid) as applied to four\ncoordination-dependent multi-agent 2D task scenarios for increasing numbers of\nagents. We find that a hybrid framework achieves better task success rates\nacross all four tasks and scales better to more agents. We further demonstrate\nthe hybrid frameworks in 3D simulations where the vision-to-text problem and\ndynamical errors are considered. See our project website\nhttps://yongchao98.github.io/MIT-REALM-Multi-Robot/ for prompts, videos, and\ncode.\n","authors":["Yongchao Chen","Jacob Arkin","Yang Zhang","Nicholas Roy","Chuchu Fan"],"pdf_url":"https://arxiv.org/pdf/2309.15943v2.pdf","comment":"7 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.15648v1","updated":"2024-03-22T23:12:28Z","published":"2024-03-22T23:12:28Z","title":"SRLM: Human-in-Loop Interactive Social Robot Navigation with Large\n  Language Model and Deep Reinforcement Learning","summary":"  An interactive social robotic assistant must provide services in complex and\ncrowded spaces while adapting its behavior based on real-time human language\ncommands or feedback. In this paper, we propose a novel hybrid approach called\nSocial Robot Planner (SRLM), which integrates Large Language Models (LLM) and\nDeep Reinforcement Learning (DRL) to navigate through human-filled public\nspaces and provide multiple social services. SRLM infers global planning from\nhuman-in-loop commands in real-time, and encodes social information into a\nLLM-based large navigation model (LNM) for low-level motion execution.\nMoreover, a DRL-based planner is designed to maintain benchmarking performance,\nwhich is blended with LNM by a large feedback model (LFM) to address the\ninstability of current text and LLM-driven LNM. Finally, SRLM demonstrates\noutstanding performance in extensive experiments. More details about this work\nare available at: https://sites.google.com/view/navi-srlm\n","authors":["Weizheng Wang","Le Mao","Ruiqi Wang","Byung-Cheol Min"],"pdf_url":"https://arxiv.org/pdf/2403.15648v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15637v1","updated":"2024-03-22T22:27:42Z","published":"2024-03-22T22:27:42Z","title":"CoNVOI: Context-aware Navigation using Vision Language Models in Outdoor\n  and Indoor Environments","summary":"  We present ConVOI, a novel method for autonomous robot navigation in\nreal-world indoor and outdoor environments using Vision Language Models (VLMs).\nWe employ VLMs in two ways: first, we leverage their zero-shot image\nclassification capability to identify the context or scenario (e.g., indoor\ncorridor, outdoor terrain, crosswalk, etc) of the robot's surroundings, and\nformulate context-based navigation behaviors as simple text prompts (e.g.\n``stay on the pavement\"). Second, we utilize their state-of-the-art semantic\nunderstanding and logical reasoning capabilities to compute a suitable\ntrajectory given the identified context. To this end, we propose a novel\nmulti-modal visual marking approach to annotate the obstacle-free regions in\nthe RGB image used as input to the VLM with numbers, by correlating it with a\nlocal occupancy map of the environment. The marked numbers ground image\nlocations in the real-world, direct the VLM's attention solely to navigable\nlocations, and elucidate the spatial relationships between them and terrains\ndepicted in the image to the VLM. Next, we query the VLM to select numbers on\nthe marked image that satisfy the context-based behavior text prompt, and\nconstruct a reference path using the selected numbers. Finally, we propose a\nmethod to extrapolate the reference trajectory when the robot's environmental\ncontext has not changed to prevent unnecessary VLM queries. We use the\nreference trajectory to guide a motion planner, and demonstrate that it leads\nto human-like behaviors (e.g. not cutting through a group of people, using\ncrosswalks, etc.) in various real-world indoor and outdoor scenarios.\n","authors":["Adarsh Jagan Sathyamoorthy","Kasun Weerakoon","Mohamed Elnoor","Anuj Zore","Brian Ichter","Fei Xia","Jie Tan","Wenhao Yu","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2403.15637v1.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2308.00186v3","updated":"2024-03-22T22:06:29Z","published":"2023-07-31T22:50:14Z","title":"Learning Complex Motion Plans using Neural ODEs with Safety and\n  Stability Guarantees","summary":"  We propose a Dynamical System (DS) approach to learn complex, possibly\nperiodic motion plans from kinesthetic demonstrations using Neural Ordinary\nDifferential Equations (NODE). To ensure reactivity and robustness to\ndisturbances, we propose a novel approach that selects a target point at each\ntime step for the robot to follow, by combining tools from control theory and\nthe target trajectory generated by the learned NODE. A correction term to the\nNODE model is computed online by solving a quadratic program that guarantees\nstability and safety using control Lyapunov functions and control barrier\nfunctions, respectively. Our approach outperforms baseline DS learning\ntechniques on the LASA handwriting dataset and complex periodic trajectories.\nIt is also validated on the Franka Emika robot arm to produce stable motions\nfor wiping and stirring tasks that do not have a single attractor, while being\nrobust to perturbations and safe around humans and obstacles.\n","authors":["Farhad Nawaz","Tianyu Li","Nikolai Matni","Nadia Figueroa"],"pdf_url":"https://arxiv.org/pdf/2308.00186v3.pdf","comment":"accepted to ICRA 2024"},{"id":"http://arxiv.org/abs/2403.15621v1","updated":"2024-03-22T21:16:25Z","published":"2024-03-22T21:16:25Z","title":"Global Games with Negative Feedback for Autonomous Colony Maintenance\n  using Robot Teams","summary":"  In this article we address the colony maintenance problem, where a team of\nrobots are tasked with continuously maintaining the energy supply of an\nautonomous colony. We model this as a global game, where robots measure the\nenergy level of a central nest to determine whether or not to forage for energy\nsources. We design a mechanism that avoids the trivial equilibrium where all\nrobots always forage. Furthermore, we demonstrate that when the game is played\niteratively a negative feedback term stabilizes the number of foraging robots\nat a non-trivial Nash equilibrium. We compare our approach qualitatively to\nexisting global games, where a positive positive feedback term admits\nthreshold-based decision making, and encourages many robots to forage\nsimultaneously. We discuss how positive feedback can lead to a cascading\nfailure in the presence of a human who recruits robots for external tasks, and\nwe demonstrate the performance of our approach in simulation.\n","authors":["Logan E. Beaver"],"pdf_url":"https://arxiv.org/pdf/2403.15621v1.pdf","comment":"6 pages, 5 figures"},{"id":"http://arxiv.org/abs/2309.07139v2","updated":"2024-03-22T20:49:54Z","published":"2023-09-01T16:19:27Z","title":"A Traffic Management Framework for On-Demand Urban Air Mobility Systems","summary":"  Urban Air Mobility (UAM) offers a solution to current traffic congestion by\nproviding on-demand air mobility in urban areas. Effective traffic management\nis crucial for efficient operation of UAM systems, especially for high-demand\nscenarios. In this paper, we present a centralized traffic management framework\nfor on-demand UAM systems. Specifically, we provide a scheduling policy, called\nVertiSync, which schedules the aircraft for either servicing trip requests or\nrebalancing in the system subject to aircraft safety margins and energy\nrequirements. We characterize the system-level throughput of VertiSync, which\ndetermines the demand threshold at which passenger waiting times transition\nfrom being stabilized to being increasing over time. We show that the proposed\npolicy is able to maximize throughput for sufficiently large fleet sizes. We\ndemonstrate the performance of VertiSync through a case study for the city of\nLos Angeles, and show that it significantly reduces passenger waiting times\ncompared to a first-come first-serve scheduling policy.\n","authors":["Milad Pooladsanj","Ketan Savla","Petros A. Ioannou"],"pdf_url":"https://arxiv.org/pdf/2309.07139v2.pdf","comment":"9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.15577v1","updated":"2024-03-22T19:04:58Z","published":"2024-03-22T19:04:58Z","title":"Autonomous Driving With Perception Uncertainties: Deep-Ensemble Based\n  Adaptive Cruise Control","summary":"  Autonomous driving depends on perception systems to understand the\nenvironment and to inform downstream decision-making. While advanced perception\nsystems utilizing black-box Deep Neural Networks (DNNs) demonstrate human-like\ncomprehension, their unpredictable behavior and lack of interpretability may\nhinder their deployment in safety critical scenarios. In this paper, we develop\nan Ensemble of DNN regressors (Deep Ensemble) that generates predictions with\nquantification of prediction uncertainties. In the scenario of Adaptive Cruise\nControl (ACC), we employ the Deep Ensemble to estimate distance headway to the\nlead vehicle from RGB images and enable the downstream controller to account\nfor the estimation uncertainty. We develop an adaptive cruise controller that\nutilizes Stochastic Model Predictive Control (MPC) with chance constraints to\nprovide a probabilistic safety guarantee. We evaluate our ACC algorithm using a\nhigh-fidelity traffic simulator and a real-world traffic dataset and\ndemonstrate the ability of the proposed approach to effect speed tracking and\ncar following while maintaining a safe distance headway. The\nout-of-distribution scenarios are also examined.\n","authors":["Xiao Li","H. Eric Tseng","Anouck Girard","Ilya Kolmanovsky"],"pdf_url":"https://arxiv.org/pdf/2403.15577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.09317v2","updated":"2024-03-22T18:59:15Z","published":"2023-09-17T16:06:38Z","title":"Kinematics-aware Trajectory Generation and Prediction with Latent\n  Stochastic Differential Modeling","summary":"  Trajectory generation and trajectory prediction are two critical tasks in\nautonomous driving, which generate various trajectories for testing during\ndevelopment and predict the trajectories of surrounding vehicles during\noperation, respectively. In recent years, emerging data-driven deep\nlearning-based methods have shown great promise for these two tasks in learning\nvarious traffic scenarios and improving average performance without assuming\nphysical models. However, it remains a challenging problem for these methods to\nensure that the generated/predicted trajectories are physically realistic. This\nchallenge arises because learning-based approaches often function as opaque\nblack boxes and do not adhere to physical laws. Conversely, existing\nmodel-based methods provide physically feasible results but are constrained by\npredefined model structures, limiting their capabilities to address complex\nscenarios. To address the limitations of these two types of approaches, we\npropose a new method that integrates kinematic knowledge into neural stochastic\ndifferential equations (SDE) and designs a variational autoencoder based on\nthis latent kinematics-aware SDE (LK-SDE) to generate vehicle motions.\nExperimental results demonstrate that our method significantly outperforms both\nmodel-based and learning-based baselines in producing physically realistic and\nprecisely controllable vehicle trajectories. Additionally, it performs well in\npredicting unobservable physical variables in the latent space.\n","authors":["Ruochen Jiao","Yixuan Wang","Xiangguo Liu","Chao Huang","Qi Zhu"],"pdf_url":"https://arxiv.org/pdf/2309.09317v2.pdf","comment":"8 pages, conference paper in motion generation"},{"id":"http://arxiv.org/abs/2403.15569v1","updated":"2024-03-22T18:47:54Z","published":"2024-03-22T18:47:54Z","title":"Music to Dance as Language Translation using Sequence Models","summary":"  Synthesising appropriate choreographies from music remains an open problem.\nWe introduce MDLT, a novel approach that frames the choreography generation\nproblem as a translation task. Our method leverages an existing data set to\nlearn to translate sequences of audio into corresponding dance poses. We\npresent two variants of MDLT: one utilising the Transformer architecture and\nthe other employing the Mamba architecture. We train our method on AIST++ and\nPhantomDance data sets to teach a robotic arm to dance, but our method can be\napplied to a full humanoid robot. Evaluation metrics, including Average Joint\nError and Frechet Inception Distance, consistently demonstrate that, when given\na piece of music, MDLT excels at producing realistic and high-quality\nchoreography. The code can be found at github.com/meowatthemoon/MDLT.\n","authors":["André Correia","Luís A. Alexandre"],"pdf_url":"https://arxiv.org/pdf/2403.15569v1.pdf","comment":null}]},"2024-03-23T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2403.14447v2","updated":"2024-03-23T11:26:38Z","published":"2024-03-21T14:53:50Z","title":"Exploring 3D Human Pose Estimation and Forecasting from the Robot's\n  Perspective: The HARPER Dataset","summary":"  We introduce HARPER, a novel dataset for 3D body pose estimation and forecast\nin dyadic interactions between users and Spot, the quadruped robot manufactured\nby Boston Dynamics. The key-novelty is the focus on the robot's perspective,\ni.e., on the data captured by the robot's sensors. These make 3D body pose\nanalysis challenging because being close to the ground captures humans only\npartially. The scenario underlying HARPER includes 15 actions, of which 10\ninvolve physical contact between the robot and users. The Corpus contains not\nonly the recordings of the built-in stereo cameras of Spot, but also those of a\n6-camera OptiTrack system (all recordings are synchronized). This leads to\nground-truth skeletal representations with a precision lower than a millimeter.\nIn addition, the Corpus includes reproducible benchmarks on 3D Human Pose\nEstimation, Human Pose Forecasting, and Collision Prediction, all based on\npublicly available baseline approaches. This enables future HARPER users to\nrigorously compare their results with those we provide in this work.\n","authors":["Andrea Avogaro","Andrea Toaiari","Federico Cunico","Xiangmin Xu","Haralambos Dafas","Alessandro Vinciarelli","Emma Li","Marco Cristani"],"pdf_url":"https://arxiv.org/pdf/2403.14447v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15959v1","updated":"2024-03-23T23:36:26Z","published":"2024-03-23T23:36:26Z","title":"Risk-Calibrated Human-Robot Interaction via Set-Valued Intent Prediction","summary":"  Tasks where robots must cooperate with humans, such as navigating around a\ncluttered home or sorting everyday items, are challenging because they exhibit\na wide range of valid actions that lead to similar outcomes. Moreover,\nzero-shot cooperation between human-robot partners is an especially challenging\nproblem because it requires the robot to infer and adapt on the fly to a latent\nhuman intent, which could vary significantly from human to human. Recently,\ndeep learned motion prediction models have shown promising results in\npredicting human intent but are prone to being confidently incorrect. In this\nwork, we present Risk-Calibrated Interactive Planning (RCIP), which is a\nframework for measuring and calibrating risk associated with uncertain action\nselection in human-robot cooperation, with the fundamental idea that the robot\nshould ask for human clarification when the risk associated with the\nuncertainty in the human's intent cannot be controlled. RCIP builds on the\ntheory of set-valued risk calibration to provide a finite-sample statistical\nguarantee on the cumulative loss incurred by the robot while minimizing the\ncost of human clarification in complex multi-step settings. Our main insight is\nto frame the risk control problem as a sequence-level multi-hypothesis testing\nproblem, allowing efficient calibration using a low-dimensional parameter that\ncontrols a pre-trained risk-aware policy. Experiments across a variety of\nsimulated and real-world environments demonstrate RCIP's ability to predict and\nadapt to a diverse set of dynamic human intents.\n","authors":["Justin Lidard","Hang Pham","Ariel Bachman","Bryan Boateng","Anirudha Majumdar"],"pdf_url":"https://arxiv.org/pdf/2403.15959v1.pdf","comment":"Website with additional information, videos, and code:\n  https://risk-calibrated-planning.github.io/"},{"id":"http://arxiv.org/abs/2403.15941v1","updated":"2024-03-23T22:04:03Z","published":"2024-03-23T22:04:03Z","title":"Explore until Confident: Efficient Exploration for Embodied Question\n  Answering","summary":"  We consider the problem of Embodied Question Answering (EQA), which refers to\nsettings where an embodied agent such as a robot needs to actively explore an\nenvironment to gather information until it is confident about the answer to a\nquestion. In this work, we leverage the strong semantic reasoning capabilities\nof large vision-language models (VLMs) to efficiently explore and answer such\nquestions. However, there are two main challenges when using VLMs in EQA: they\ndo not have an internal memory for mapping the scene to be able to plan how to\nexplore over time, and their confidence can be miscalibrated and can cause the\nrobot to prematurely stop exploration or over-explore. We propose a method that\nfirst builds a semantic map of the scene based on depth information and via\nvisual prompting of a VLM - leveraging its vast knowledge of relevant regions\nof the scene for exploration. Next, we use conformal prediction to calibrate\nthe VLM's question answering confidence, allowing the robot to know when to\nstop exploration - leading to a more calibrated and efficient exploration\nstrategy. To test our framework in simulation, we also contribute a new EQA\ndataset with diverse, realistic human-robot scenarios and scenes built upon the\nHabitat-Matterport 3D Research Dataset (HM3D). Both simulated and real robot\nexperiments show our proposed approach improves the performance and efficiency\nover baselines that do no leverage VLM for exploration or do not calibrate its\nconfidence. Webpage with experiment videos and code:\nhttps://explore-eqa.github.io/\n","authors":["Allen Z. Ren","Jaden Clark","Anushri Dixit","Masha Itkina","Anirudha Majumdar","Dorsa Sadigh"],"pdf_url":"https://arxiv.org/pdf/2403.15941v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2309.04937v3","updated":"2024-03-23T21:09:19Z","published":"2023-09-10T05:45:36Z","title":"LONER: LiDAR Only Neural Representations for Real-Time SLAM","summary":"  This paper proposes LONER, the first real-time LiDAR SLAM algorithm that uses\na neural implicit scene representation. Existing implicit mapping methods for\nLiDAR show promising results in large-scale reconstruction, but either require\ngroundtruth poses or run slower than real-time. In contrast, LONER uses LiDAR\ndata to train an MLP to estimate a dense map in real-time, while simultaneously\nestimating the trajectory of the sensor. To achieve real-time performance, this\npaper proposes a novel information-theoretic loss function that accounts for\nthe fact that different regions of the map may be learned to varying degrees\nthroughout online training. The proposed method is evaluated qualitatively and\nquantitatively on two open-source datasets. This evaluation illustrates that\nthe proposed loss function converges faster and leads to more accurate geometry\nreconstruction than other loss functions used in depth-supervised neural\nimplicit frameworks. Finally, this paper shows that LONER estimates\ntrajectories competitively with state-of-the-art LiDAR SLAM methods, while also\nproducing dense maps competitive with existing real-time implicit mapping\nmethods that use groundtruth poses.\n","authors":["Seth Isaacson","Pou-Chun Kung","Mani Ramanagopal","Ram Vasudevan","Katherine A. Skinner"],"pdf_url":"https://arxiv.org/pdf/2309.04937v3.pdf","comment":"First two authors equally contributed. Webpage:\n  https://umautobots.github.io/loner"},{"id":"http://arxiv.org/abs/2309.08865v3","updated":"2024-03-23T20:33:45Z","published":"2023-09-16T04:01:34Z","title":"ARTEMIS: AI-driven Robotic Triage Labeling and Emergency Medical\n  Information System","summary":"  Mass casualty incidents (MCIs) pose a significant challenge to emergency\nmedical services by overwhelming available resources and personnel. Effective\nvictim assessment is the key to minimizing casualties during such a crisis. We\nintroduce ARTEMIS, an AI-driven Robotic Triage Labeling and Emergency Medical\nInformation System, to aid first responders in MCI events. It leverages speech\nprocessing, natural language processing, and deep learning to help with acuity\nclassification. This is deployed on a quadruped that performs victim\nlocalization and preliminary injury severity assessment. First responders\naccess victim information through a Graphical User Interface that is updated in\nreal-time. To validate our proposed algorithmic triage protocol, we used the\nUnitree Go1 quadruped. The robot identifies humans, interacts with them, gets\nvitals and information, and assigns an acuity label. Simulations of an MCI in\nsoftware and a controlled environment outdoors were conducted. The system\nachieved a triage-level classification precision of over 74% on average and 99%\nfor the most critical victims, i.e. level 1 acuity, outperforming\nstate-of-the-art deep learning-based triage labeling systems. In this paper, we\nshowcase the potential of human-robot interaction in assisting medical\npersonnel in MCI events.\n","authors":["Revanth Krishna Senthilkumaran","Mridu Prashanth","Hrishikesh Viswanath","Sathvika Kotha","Kshitij Tiwari","Aniket Bera"],"pdf_url":"https://arxiv.org/pdf/2309.08865v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15852v4","updated":"2024-03-23T16:54:01Z","published":"2024-02-24T16:39:16Z","title":"NaVid: Video-based VLM Plans the Next Step for Vision-and-Language\n  Navigation","summary":"  Vision-and-Language Navigation (VLN) stands as a key research problem of\nEmbodied AI, aiming at enabling agents to navigate in unseen environments\nfollowing linguistic instructions. In this field, generalization is a\nlong-standing challenge, either to out-of-distribution scenes or from Sim to\nReal. In this paper, we propose NaVid, a video-based large vision language\nmodel (VLM), to mitigate such a generalization gap. NaVid makes the first\nendeavour to showcase the capability of VLMs to achieve state-of-the-art level\nnavigation performance without any maps, odometer and depth inputs. Following\nhuman instruction, NaVid only requires an on-the-fly video stream from a\nmonocular RGB camera equipped on the robot to output the next-step action. Our\nformulation mimics how humans navigate and naturally gets rid of the problems\nintroduced by odometer noises, and the Sim2Real gaps from map or depth inputs.\nMoreover, our video-based approach can effectively encode the historical\nobservations of robots as spatio-temporal contexts for decision-making and\ninstruction following. We train NaVid with 550k navigation samples collected\nfrom VLN-CE trajectories, including action-planning and instruction-reasoning\nsamples, along with 665k large-scale web data. Extensive experiments show that\nNaVid achieves SOTA performance in simulation environments and the real world,\ndemonstrating superior cross-dataset and Sim2Real transfer. We thus believe our\nproposed VLM approach plans the next step for not only the navigation agents\nbut also this research field.\n","authors":["Jiazhao Zhang","Kunyu Wang","Rongtao Xu","Gengze Zhou","Yicong Hong","Xiaomeng Fang","Qi Wu","Zhizheng Zhang","He Wang"],"pdf_url":"https://arxiv.org/pdf/2402.15852v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15870v1","updated":"2024-03-23T15:35:54Z","published":"2024-03-23T15:35:54Z","title":"iA$^*$: Imperative Learning-based A$^*$ Search for Pathfinding","summary":"  The pathfinding problem, which aims to identify a collision-free path between\ntwo points, is crucial for many applications, such as robot navigation and\nautonomous driving. Classic methods, such as A$^*$ search, perform well on\nsmall-scale maps but face difficulties scaling up. Conversely, data-driven\napproaches can improve pathfinding efficiency but require extensive data\nlabeling and lack theoretical guarantees, making it challenging for practical\napplications. To combine the strengths of the two methods, we utilize the\nimperative learning (IL) strategy and propose a novel self-supervised\npathfinding framework, termed imperative learning-based A$^*$ (iA$^*$).\nSpecifically, iA$^*$ is a bilevel optimization process where the lower-level\noptimization is dedicated to finding the optimal path by a differentiable A$^*$\nsearch module, and the upper-level optimization narrows down the search space\nto improve efficiency via setting suitable initial values from a data-driven\nmodel. Besides, the model within the upper-level optimization is a fully\nconvolutional network, trained by the calculated loss in the lower-level\noptimization. Thus, the framework avoids extensive data labeling and can be\napplied in diverse environments. Our comprehensive experiments demonstrate that\niA$^*$ surpasses both classical and data-driven methods in pathfinding\nefficiency and shows superior robustness among different tasks, validated with\npublic datasets and simulation environments.\n","authors":["Xiangyu Chen","Fan Yang","Chen Wang"],"pdf_url":"https://arxiv.org/pdf/2403.15870v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15857v1","updated":"2024-03-23T14:47:26Z","published":"2024-03-23T14:47:26Z","title":"Automated System-level Testing of Unmanned Aerial Systems","summary":"  Unmanned aerial systems (UAS) rely on various avionics systems that are\nsafety-critical and mission-critical. A major requirement of international\nsafety standards is to perform rigorous system-level testing of avionics\nsoftware systems. The current industrial practice is to manually create test\nscenarios, manually/automatically execute these scenarios using simulators, and\nmanually evaluate outcomes. The test scenarios typically consist of setting\ncertain flight or environment conditions and testing the system under test in\nthese settings. The state-of-the-art approaches for this purpose also require\nmanual test scenario development and evaluation. In this paper, we propose a\nnovel approach to automate the system-level testing of the UAS. The proposed\napproach (AITester) utilizes model-based testing and artificial intelligence\n(AI) techniques to automatically generate, execute, and evaluate various test\nscenarios. The test scenarios are generated on the fly, i.e., during test\nexecution based on the environmental context at runtime. The approach is\nsupported by a toolset. We empirically evaluate the proposed approach on two\ncore components of UAS, an autopilot system of an unmanned aerial vehicle (UAV)\nand cockpit display systems (CDS) of the ground control station (GCS). The\nresults show that the AITester effectively generates test scenarios causing\ndeviations from the expected behavior of the UAV autopilot and reveals\npotential flaws in the GCS-CDS.\n","authors":["Hassan Sartaj","Asmar Muqeet","Muhammad Zohaib Iqbal","Muhammad Uzair Khan"],"pdf_url":"https://arxiv.org/pdf/2403.15857v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14552v2","updated":"2024-03-23T14:15:02Z","published":"2023-09-25T21:51:48Z","title":"Tactile Estimation of Extrinsic Contact Patch for Stable Placement","summary":"  Precise perception of contact interactions is essential for fine-grained\nmanipulation skills for robots. In this paper, we present the design of\nfeedback skills for robots that must learn to stack complex-shaped objects on\ntop of each other (see Fig.1). To design such a system, a robot should be able\nto reason about the stability of placement from very gentle contact\ninteractions. Our results demonstrate that it is possible to infer the\nstability of object placement based on tactile readings during contact\nformation between the object and its environment. In particular, we estimate\nthe contact patch between a grasped object and its environment using force and\ntactile observations to estimate the stability of the object during a contact\nformation. The contact patch could be used to estimate the stability of the\nobject upon release of the grasp. The proposed method is demonstrated in\nvarious pairs of objects that are used in a very popular board game.\n","authors":["Kei Ota","Devesh K. Jha","Krishna Murthy Jatavallabhula","Asako Kanezaki","Joshua B. Tenenbaum"],"pdf_url":"https://arxiv.org/pdf/2309.14552v2.pdf","comment":"Accepted at ICRA2024"},{"id":"http://arxiv.org/abs/2403.15834v1","updated":"2024-03-23T13:21:09Z","published":"2024-03-23T13:21:09Z","title":"ARO: Large Language Model Supervised Robotics Text2Skill Autonomous\n  Learning","summary":"  Robotics learning highly relies on human expertise and efforts, such as\ndemonstrations, design of reward functions in reinforcement learning,\nperformance evaluation using human feedback, etc. However, reliance on human\nassistance can lead to expensive learning costs and make skill learning\ndifficult to scale. In this work, we introduce the Large Language Model\nSupervised Robotics Text2Skill Autonomous Learning (ARO) framework, which aims\nto replace human participation in the robot skill learning process with\nlarge-scale language models that incorporate reward function design and\nperformance evaluation. We provide evidence that our approach enables fully\nautonomous robot skill learning, capable of completing partial tasks without\nhuman intervention. Furthermore, we also analyze the limitations of this\napproach in task understanding and optimization stability.\n","authors":["Yiwen Chen","Yuyao Ye","Ziyi Chen","Chuheng Zhang","Marcelo H. Ang"],"pdf_url":"https://arxiv.org/pdf/2403.15834v1.pdf","comment":"6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2403.15826v1","updated":"2024-03-23T12:53:51Z","published":"2024-03-23T12:53:51Z","title":"Scaling Learning based Policy Optimization for Temporal Tasks via\n  Dropout","summary":"  This paper introduces a model-based approach for training feedback\ncontrollers for an autonomous agent operating in a highly nonlinear\nenvironment. We desire the trained policy to ensure that the agent satisfies\nspecific task objectives, expressed in discrete-time Signal Temporal Logic\n(DT-STL). One advantage for reformulation of a task via formal frameworks, like\nDT-STL, is that it permits quantitative satisfaction semantics. In other words,\ngiven a trajectory and a DT-STL formula, we can compute the robustness, which\ncan be interpreted as an approximate signed distance between the trajectory and\nthe set of trajectories satisfying the formula. We utilize feedback\ncontrollers, and we assume a feed forward neural network for learning these\nfeedback controllers. We show how this learning problem is similar to training\nrecurrent neural networks (RNNs), where the number of recurrent units is\nproportional to the temporal horizon of the agent's task objectives. This poses\na challenge: RNNs are susceptible to vanishing and exploding gradients, and\nna\\\"{i}ve gradient descent-based strategies to solve long-horizon task\nobjectives thus suffer from the same problems. To tackle this challenge, we\nintroduce a novel gradient approximation algorithm based on the idea of dropout\nor gradient sampling. We show that, the existing smooth semantics for\nrobustness are inefficient regarding gradient computation when the\nspecification becomes complex. To address this challenge, we propose a new\nsmooth semantics for DT-STL that under-approximates the robustness value and\nscales well for backpropagation over a complex specification. We show that our\ncontrol synthesis methodology, can be quite helpful for stochastic gradient\ndescent to converge with less numerical issues, enabling scalable\nbackpropagation over long time horizons and trajectories over high dimensional\nstate spaces.\n","authors":["Navid Hashemi","Bardh Hoxha","Danil Prokhorov","Georgios Fainekos","Jyotirmoy Deshmukh"],"pdf_url":"https://arxiv.org/pdf/2403.15826v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14945v2","updated":"2024-03-23T12:39:14Z","published":"2023-09-26T14:00:25Z","title":"Integration of Large Language Models within Cognitive Architectures for\n  Autonomous Robots","summary":"  Symbolic reasoning systems have been used in cognitive architectures to\nprovide inference and planning capabilities. However, defining domains and\nproblems has proven difficult and prone to errors. Moreover, Large Language\nModels (LLMs) have emerged as tools to process natural language for different\ntasks. In this paper, we propose the use of LLMs to tackle these problems. This\nway, this paper proposes the integration of LLMs in the ROS 2-integrated\ncognitive architecture MERLIN2 for autonomous robots. Specifically, we present\nthe design, development and deployment of how to leverage the reasoning\ncapabilities of LLMs inside the deliberative processes of MERLIN2. As a result,\nthe deliberative system is updated from a PDDL-based planner system to a\nnatural language planning system. This proposal is evaluated quantitatively and\nqualitatively, measuring the impact of incorporating the LLMs in the cognitive\narchitecture. Results show that a classical approach achieves better\nperformance but the proposed solution provides an enhanced interaction through\nnatural language.\n","authors":["Miguel Á. González-Santamarta","Francisco J. Rodríguez-Lera","Ángel Manuel Guerrero-Higueras","Vicente Matellán-Olivera"],"pdf_url":"https://arxiv.org/pdf/2309.14945v2.pdf","comment":"8 pages, 6 figures, 2 tables, Submitted to IROS 2024"},{"id":"http://arxiv.org/abs/2403.15813v1","updated":"2024-03-23T12:00:00Z","published":"2024-03-23T12:00:00Z","title":"Learning Early Social Maneuvers for Enhanced Social Navigation","summary":"  Socially compliant navigation is an integral part of safety features in\nHuman-Robot Interaction. Traditional approaches to mobile navigation prioritize\nphysical aspects, such as efficiency, but social behaviors gain traction as\nrobots appear more in daily life. Recent techniques to improve the social\ncompliance of navigation often rely on predefined features or reward functions,\nintroducing assumptions about social human behavior. To address this\nlimitation, we propose a novel Learning from Demonstration (LfD) framework for\nsocial navigation that exclusively utilizes raw sensory data. Additionally, the\nproposed system contains mechanisms to consider the future paths of the\nsurrounding pedestrians, acknowledging the temporal aspect of the problem. The\nfinal product is expected to reduce the anxiety of people sharing their\nenvironment with a mobile robot, helping them trust that the robot is aware of\ntheir presence and will not harm them. As the framework is currently being\ndeveloped, we outline its components, present experimental results, and discuss\nfuture work towards realizing this framework.\n","authors":["Yigit Yıldırim","Mehmet Suzer","Emre Ugur"],"pdf_url":"https://arxiv.org/pdf/2403.15813v1.pdf","comment":"Submitted to the workshop of Robot Trust for Symbiotic Societies\n  (RTSS) at ICRA 2024 on March 23, 2024"},{"id":"http://arxiv.org/abs/2403.15812v1","updated":"2024-03-23T11:50:20Z","published":"2024-03-23T11:50:20Z","title":"The Impact of Evolutionary Computation on Robotic Design: A Case Study\n  with an Underactuated Hand Exoskeleton","summary":"  Robotic exoskeletons can enhance human strength and aid people with physical\ndisabilities. However, designing them to ensure safety and optimal performance\npresents significant challenges. Developing exoskeletons should incorporate\nspecific optimization algorithms to find the best design. This study\ninvestigates the potential of Evolutionary Computation (EC) methods in robotic\ndesign optimization, with an underactuated hand exoskeleton (U-HEx) used as a\ncase study. We propose improving the performance and usability of the U-HEx\ndesign, which was initially optimized using a naive brute-force approach, by\nintegrating EC techniques such as Genetic Algorithm and Big Bang-Big Crunch\nAlgorithm. Comparative analysis revealed that EC methods consistently yield\nmore precise and optimal solutions than brute force in a significantly shorter\ntime. This allowed us to improve the optimization by increasing the number of\nvariables in the design, which was impossible with naive methods. The results\nshow significant improvements in terms of the torque magnitude the device\ntransfers to the user, enhancing its efficiency. These findings underline the\nimportance of performing proper optimization while designing exoskeletons, as\nwell as providing a significant improvement to this specific robotic design.\n","authors":["Baris Akbas","Huseyin Taner Yuksel","Aleyna Soylemez","Mazhar Eid Zyada","Mine Sarac","Fabio Stroppa"],"pdf_url":"https://arxiv.org/pdf/2403.15812v1.pdf","comment":"6 pages (+ref), 4 figures, IEEE International Conference on Robotics\n  and Automation (ICRA) 2024"},{"id":"http://arxiv.org/abs/2403.15805v1","updated":"2024-03-23T11:30:44Z","published":"2024-03-23T11:30:44Z","title":"AirCrab: A Hybrid Aerial-Ground Manipulator with An Active Wheel","summary":"  Inspired by the behavior of birds, we present AirCrab, a hybrid aerial ground\nmanipulator (HAGM) with a single active wheel and a 3-degree of freedom (3-DoF)\nmanipulator. AirCrab leverages a single point of contact with the ground to\nreduce position drift and improve manipulation accuracy. The single active\nwheel enables locomotion on narrow surfaces without adding significant weight\nto the robot. To realize accurate attitude maintenance using propellers on the\nground, we design a control allocation method for AirCrab that prioritizes\nattitude control and dynamically adjusts the thrust input to reduce energy\nconsumption. Experiments verify the effectiveness of the proposed control\nmethod and the gain in manipulation accuracy with ground contact. A series of\noperations to complete the letters 'NTU' demonstrates the capability of the\nrobot to perform challenging hybrid aerial-ground manipulation missions.\n","authors":["Muqing Cao","Jiayan Zhao","Xinhang Xu","Lihua Xie"],"pdf_url":"https://arxiv.org/pdf/2403.15805v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15798v1","updated":"2024-03-23T11:09:41Z","published":"2024-03-23T11:09:41Z","title":"Vid2Real HRI: Align video-based HRI study designs with real-world\n  settings","summary":"  HRI research using autonomous robots in real-world settings can produce\nresults with the highest ecological validity of any study modality, but many\ndifficulties limit such studies' feasibility and effectiveness. We propose\nVid2Real HRI, a research framework to maximize real-world insights offered by\nvideo-based studies. The Vid2Real HRI framework was used to design an online\nstudy using first-person videos of robots as real-world encounter surrogates.\nThe online study ($n = 385$) distinguished the within-subjects effects of four\nrobot behavioral conditions on perceived social intelligence and human\nwillingness to help the robot enter an exterior door. A real-world,\nbetween-subjects replication ($n = 26$) using two conditions confirmed the\nvalidity of the online study's findings and the sufficiency of the participant\nrecruitment target ($22$) based on a power analysis of online study results.\nThe Vid2Real HRI framework offers HRI researchers a principled way to take\nadvantage of the efficiency of video-based study modalities while generating\ndirectly transferable knowledge of real-world HRI. Code and data from the study\nare provided at https://vid2real.github.io/vid2realHRI\n","authors":["Elliott Hauser","Yao-Cheng Chan","Sadanand Modak","Joydeep Biswas","Justin Hart"],"pdf_url":"https://arxiv.org/pdf/2403.15798v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15791v1","updated":"2024-03-23T10:38:59Z","published":"2024-03-23T10:38:59Z","title":"DriveEnv-NeRF: Exploration of A NeRF-Based Autonomous Driving\n  Environment for Real-World Performance Validation","summary":"  In this study, we introduce the DriveEnv-NeRF framework, which leverages\nNeural Radiance Fields (NeRF) to enable the validation and faithful forecasting\nof the efficacy of autonomous driving agents in a targeted real-world scene.\nStandard simulator-based rendering often fails to accurately reflect real-world\nperformance due to the sim-to-real gap, which represents the disparity between\nvirtual simulations and real-world conditions. To mitigate this gap, we propose\na workflow for building a high-fidelity simulation environment of the targeted\nreal-world scene using NeRF. This approach is capable of rendering realistic\nimages from novel viewpoints and constructing 3D meshes for emulating\ncollisions. The validation of these capabilities through the comparison of\nsuccess rates in both simulated and real environments demonstrates the benefits\nof using DriveEnv-NeRF as a real-world performance indicator. Furthermore, the\nDriveEnv-NeRF framework can serve as a training environment for autonomous\ndriving agents under various lighting conditions. This approach enhances the\nrobustness of the agents and reduces performance degradation when deployed to\nthe target real scene, compared to agents fully trained using the standard\nsimulator rendering pipeline.\n","authors":["Mu-Yi Shen","Chia-Chi Hsu","Hao-Yu Hou","Yu-Chen Huang","Wei-Fang Sun","Chia-Che Chang","Yu-Lun Liu","Chun-Yi Lee"],"pdf_url":"https://arxiv.org/pdf/2403.15791v1.pdf","comment":"Project page: https://github.com/muyishen2040/DriveEnvNeRF"},{"id":"http://arxiv.org/abs/2403.15762v1","updated":"2024-03-23T08:26:20Z","published":"2024-03-23T08:26:20Z","title":"RicMonk: A Three-Link Brachiation Robot with Passive Grippers for\n  Energy-Efficient Brachiation","summary":"  This paper presents the design, analysis, and performance evaluation of\nRicMonk, a novel three-link brachiation robot equipped with passive hook-shaped\ngrippers. Brachiation, an agile and energy-efficient mode of locomotion\nobserved in primates, has inspired the development of RicMonk to explore\nversatile locomotion and maneuvers on ladder-like structures. The robot's\nanatomical resemblance to gibbons and the integration of a tail mechanism for\nenergy injection contribute to its unique capabilities. The paper discusses the\nuse of the Direct Collocation methodology for optimizing trajectories for the\nrobot's dynamic behaviors and stabilization of these trajectories using a\nTime-varying Linear Quadratic Regulator. With RicMonk we demonstrate\nbidirectional brachiation, and provide comparative analysis with its\npredecessor, AcroMonk - a two-link brachiation robot, to demonstrate that the\npresence of a passive tail helps improve energy efficiency. The system design,\ncontrollers, and software implementation are publicly available on GitHub and\nthe video demonstration of the experiments can be viewed YouTube.\n","authors":["Shourie S. Grama","Mahdi Javadi","Shivesh Kumar","Hossein Zamani Boroujeni","Frank Kirchner"],"pdf_url":"https://arxiv.org/pdf/2403.15762v1.pdf","comment":"Open sourced system design, controllers, software implementation can\n  be found at https://github.com/dfki-ric-underactuated-lab/ricmonk and a video\n  demonstrating the experiments performed with RicMonk can be found at\n  https://www.youtube.com/watch?v=hOuDQI7CD8w"},{"id":"http://arxiv.org/abs/2209.01426v2","updated":"2024-03-23T07:50:26Z","published":"2022-09-03T14:00:43Z","title":"Space Filling Curves for Coverage Path Planning with Online Obstacle\n  Avoidance","summary":"  The paper presents a strategy for robotic exploration problem using\nSpace-Filling curves (SFC). The strategy plans a path that avoids unknown\nobstacles while ensuring complete coverage of the free space in region of\ninterest. The region of interest is first tessellated, and the tiles/cells are\nconnected using a SFC pattern. A robot follows the SFC to explore the entire\narea. However, obstacles can block the systematic movement of the robot. We\novercome this problem by determining an alternate path online that avoids the\nblocked cells while ensuring all the accessible cells are visited at least\nonce. The proposed strategy chooses next waypoint based on the graph\nconnectivity of the cells and the obstacle encountered so far. It is online,\nexhaustive and works in situations demanding non-uniform coverage. The\ncompleteness of the strategy is proved and its desirable properties are\ndiscussed with examples.\n","authors":["Ashay Wakode","Arpita Sinha"],"pdf_url":"https://arxiv.org/pdf/2209.01426v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05022v2","updated":"2024-03-23T06:58:58Z","published":"2023-10-08T05:48:30Z","title":"Fully Spiking Neural Network for Legged Robots","summary":"  In recent years, legged robots based on deep reinforcement learning have made\nremarkable progress. Quadruped robots have demonstrated the ability to complete\nchallenging tasks in complex environments and have been deployed in real-world\nscenarios to assist humans. Simultaneously, bipedal and humanoid robots have\nachieved breakthroughs in various demanding tasks. Current reinforcement\nlearning methods can utilize diverse robot bodies and historical information to\nperform actions. However, prior research has not emphasized the speed and\nenergy consumption of network inference, as well as the biological significance\nof the neural networks themselves. Most of the networks employed are\ntraditional artificial neural networks that utilize multilayer perceptrons\n(MLP). In this paper, we successfully apply a novel Spiking Neural Network\n(SNN) to process legged robots, achieving outstanding results across a range of\nsimulated terrains. SNN holds a natural advantage over traditional neural\nnetworks in terms of inference speed and energy consumption, and their\npulse-form processing of body perception signals offers improved biological\ninterpretability. Applying more biomimetic neural networks to legged robots can\nfurther reduce the heat dissipation and structural burden caused by the high\npower consumption of neural networks. To the best of our knowledge, this is the\nfirst work to implement SNN in legged robots.\n","authors":["Xiaoyang Jiang","Qiang Zhang","Jingkai Sun","Jiahang Cao","Jingtong Ma","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2310.05022v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15716v1","updated":"2024-03-23T04:36:12Z","published":"2024-03-23T04:36:12Z","title":"Distributed Robust Learning based Formation Control of Mobile Robots\n  based on Bioinspired Neural Dynamics","summary":"  This paper addresses the challenges of distributed formation control in\nmultiple mobile robots, introducing a novel approach that enhances real-world\npracticability. We first introduce a distributed estimator using a variable\nstructure and cascaded design technique, eliminating the need for derivative\ninformation to improve the real time performance. Then, a kinematic tracking\ncontrol method is developed utilizing a bioinspired neural dynamic-based\napproach aimed at providing smooth control inputs and effectively resolving the\nspeed jump issue. Furthermore, to address the challenges for robots operating\nwith completely unknown dynamics and disturbances, a learning-based robust\ndynamic controller is developed. This controller provides real time parameter\nestimates while maintaining its robustness against disturbances. The overall\nstability of the proposed method is proved with rigorous mathematical analysis.\nAt last, multiple comprehensive simulation studies have shown the advantages\nand effectiveness of the proposed method.\n","authors":["Zhe Xu","Tao Yan","Simon X. Yang","S. Andrew Gadsden","Mohammad Biglarbegian"],"pdf_url":"https://arxiv.org/pdf/2403.15716v1.pdf","comment":"This paper is accepted by IEEE Transactions on Intelligent Vehicles"},{"id":"http://arxiv.org/abs/2403.15712v1","updated":"2024-03-23T04:18:49Z","published":"2024-03-23T04:18:49Z","title":"PNAS-MOT: Multi-Modal Object Tracking with Pareto Neural Architecture\n  Search","summary":"  Multiple object tracking is a critical task in autonomous driving. Existing\nworks primarily focus on the heuristic design of neural networks to obtain high\naccuracy. As tracking accuracy improves, however, neural networks become\nincreasingly complex, posing challenges for their practical application in real\ndriving scenarios due to the high level of latency. In this paper, we explore\nthe use of the neural architecture search (NAS) methods to search for efficient\narchitectures for tracking, aiming for low real-time latency while maintaining\nrelatively high accuracy. Another challenge for object tracking is the\nunreliability of a single sensor, therefore, we propose a multi-modal framework\nto improve the robustness. Experiments demonstrate that our algorithm can run\non edge devices within lower latency constraints, thus greatly reducing the\ncomputational requirements for multi-modal object tracking while keeping lower\nlatency.\n","authors":["Chensheng Peng","Zhaoyu Zeng","Jinling Gao","Jundong Zhou","Masayoshi Tomizuka","Xinbing Wang","Chenghu Zhou","Nanyang Ye"],"pdf_url":"https://arxiv.org/pdf/2403.15712v1.pdf","comment":"IEEE Robotics and Automation Letters 2024. Code is available at\n  https://github.com/PholyPeng/PNAS-MOT"},{"id":"http://arxiv.org/abs/2309.10062v2","updated":"2024-03-23T03:50:18Z","published":"2023-09-18T18:17:56Z","title":"SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language\n  Models","summary":"  In this work, we introduce SMART-LLM, an innovative framework designed for\nembodied multi-robot task planning. SMART-LLM: Smart Multi-Agent Robot Task\nPlanning using Large Language Models (LLMs), harnesses the power of LLMs to\nconvert high-level task instructions provided as input into a multi-robot task\nplan. It accomplishes this by executing a series of stages, including task\ndecomposition, coalition formation, and task allocation, all guided by\nprogrammatic LLM prompts within the few-shot prompting paradigm. We create a\nbenchmark dataset designed for validating the multi-robot task planning\nproblem, encompassing four distinct categories of high-level instructions that\nvary in task complexity. Our evaluation experiments span both simulation and\nreal-world scenarios, demonstrating that the proposed model can achieve\npromising results for generating multi-robot task plans. The experimental\nvideos, code, and datasets from the work can be found at\nhttps://sites.google.com/view/smart-llm/.\n","authors":["Shyam Sundar Kannan","Vishnunandan L. N. Venkatesh","Byung-Cheol Min"],"pdf_url":"https://arxiv.org/pdf/2309.10062v2.pdf","comment":"Submitted to IROS 2024"},{"id":"http://arxiv.org/abs/2312.14481v2","updated":"2024-03-23T03:13:35Z","published":"2023-12-22T07:17:51Z","title":"SurgicalPart-SAM: Part-to-Whole Collaborative Prompting for Surgical\n  Instrument Segmentation","summary":"  The Segment Anything Model (SAM) exhibits promise in generic object\nsegmentation and offers potential for various applications. Existing methods\nhave applied SAM to surgical instrument segmentation (SIS) by tuning SAM-based\nframeworks with surgical data. However, they fall short in two crucial aspects:\n(1) Straightforward model tuning with instrument masks treats each instrument\nas a single entity, neglecting their complex structures and fine-grained\ndetails; and (2) Instrument category-based prompts are not flexible and\ninformative enough to describe instrument structures. To address these\nproblems, in this paper, we investigate text promptable SIS and propose\nSurgicalPart-SAM (SP-SAM), a novel SAM efficient-tuning approach that\nexplicitly integrates instrument structure knowledge with SAM's generic\nknowledge, guided by expert knowledge on instrument part compositions.\nSpecifically, we achieve this by proposing (1) Collaborative Prompts that\ndescribe instrument structures via collaborating category-level and part-level\ntexts; (2) Cross-Modal Prompt Encoder that encodes text prompts jointly with\nvisual embeddings into discriminative part-level representations; and (3)\nPart-to-Whole Adaptive Fusion and Hierarchical Decoding that adaptively fuse\nthe part-level representations into a whole for accurate instrument\nsegmentation in surgical scenarios. Built upon them, SP-SAM acquires a better\ncapability to comprehend surgical instruments in terms of both overall\nstructure and part-level details. Extensive experiments on both the EndoVis2018\nand EndoVis2017 datasets demonstrate SP-SAM's state-of-the-art performance with\nminimal tunable parameters. The code will be available at\nhttps://github.com/wenxi-yue/SurgicalPart-SAM.\n","authors":["Wenxi Yue","Jing Zhang","Kun Hu","Qiuxia Wu","Zongyuan Ge","Yong Xia","Jiebo Luo","Zhiyong Wang"],"pdf_url":"https://arxiv.org/pdf/2312.14481v2.pdf","comment":"Technical Report. The source code will be released at\n  https://github.com/wenxi-yue/SurgicalPart-SAM"},{"id":"http://arxiv.org/abs/2403.15658v1","updated":"2024-03-23T00:39:05Z","published":"2024-03-23T00:39:05Z","title":"Data-Driven Predictive Control for Robust Exoskeleton Locomotion","summary":"  Exoskeleton locomotion must be robust while being adaptive to different users\nwith and without payloads. To address these challenges, this work introduces a\ndata-driven predictive control (DDPC) framework to synthesize walking gaits for\nlower-body exoskeletons, employing Hankel matrices and a state transition\nmatrix for its data-driven model. The proposed approach leverages DDPC through\na multi-layer architecture. At the top layer, DDPC serves as a planner\nemploying Hankel matrices and a state transition matrix to generate a\ndata-driven model that can learn and adapt to varying users and payloads. At\nthe lower layer, our method incorporates inverse kinematics and passivity-based\ncontrol to map the planned trajectory from DDPC into the full-order states of\nthe lower-body exoskeleton. We validate the effectiveness of this approach\nthrough numerical simulations and hardware experiments conducted on the\nAtalante lower-body exoskeleton with different payloads. Moreover, we conducted\na comparative analysis against the model predictive control (MPC) framework\nbased on the reduced-order linear inverted pendulum (LIP) model. Through this\ncomparison, the paper demonstrates that DDPC enables robust bipedal walking at\nvarious velocities while accounting for model uncertainties and unknown\nperturbations.\n","authors":["Kejun Li","Jeeseop Kim","Xiaobin Xiong","Kaveh Akbari Hamed","Yisong Yue","Aaron D. Ames"],"pdf_url":"https://arxiv.org/pdf/2403.15658v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18847v2","updated":"2024-03-23T00:28:21Z","published":"2023-10-28T23:25:19Z","title":"Bird's Eye View Based Pretrained World model for Visual Navigation","summary":"  Sim2Real transfer has gained popularity because it helps transfer from\ninexpensive simulators to real world. This paper presents a novel system that\nfuses components in a traditional World Model into a robust system, trained\nentirely within a simulator, that Zero-Shot transfers to the real world. To\nfacilitate transfer, we use an intermediary representation that is based on\n\\textit{Bird's Eye View (BEV)} images. Thus, our robot learns to navigate in a\nsimulator by first learning to translate from complex \\textit{First-Person View\n(FPV)} based RGB images to BEV representations, then learning to navigate using\nthose representations. Later, when tested in the real world, the robot uses the\nperception model that translates FPV-based RGB images to embeddings that were\nlearned by the FPV to BEV translator and that can be used by the downstream\npolicy. The incorporation of state-checking modules using \\textit{Anchor\nimages} and Mixture Density LSTM not only interpolates uncertain and missing\nobservations but also enhances the robustness of the model in the real-world.\nWe trained the model using data from a Differential drive robot in the CARLA\nsimulator. Our methodology's effectiveness is shown through the deployment of\ntrained models onto a real-world Differential drive robot. Lastly we release a\ncomprehensive codebase, dataset and models for training and deployment\n(\\url{https://sites.google.com/view/value-explicit-pretraining}).\n","authors":["Kiran Lekkala","Chen Liu","Laurent Itti"],"pdf_url":"https://arxiv.org/pdf/2310.18847v2.pdf","comment":"Under Review at the IROS 2024; Accepted at NeurIPS 2023, Robot\n  Learning Workshop"}]},"2024-03-25T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2403.16803v1","updated":"2024-03-25T14:21:49Z","published":"2024-03-25T14:21:49Z","title":"Exploiting Priors from 3D Diffusion Models for RGB-Based One-Shot View\n  Planning","summary":"  Object reconstruction is relevant for many autonomous robotic tasks that\nrequire interaction with the environment. A key challenge in such scenarios is\nplanning view configurations to collect informative measurements for\nreconstructing an initially unknown object. One-shot view planning enables\nefficient data collection by predicting view configurations and planning the\nglobally shortest path connecting all views at once. However, geometric priors\nabout the object are required to conduct one-shot view planning. In this work,\nwe propose a novel one-shot view planning approach that utilizes the powerful\n3D generation capabilities of diffusion models as priors. By incorporating such\ngeometric priors into our pipeline, we achieve effective one-shot view planning\nstarting with only a single RGB image of the object to be reconstructed. Our\nplanning experiments in simulation and real-world setups indicate that our\napproach balances well between object reconstruction quality and movement cost.\n","authors":["Sicong Pan","Liren Jin","Xuying Huang","Cyrill Stachniss","Marija Popović","Maren Bennewitz"],"pdf_url":"https://arxiv.org/pdf/2403.16803v1.pdf","comment":"Sicong Pan and Liren Jin have equal contribution. Submitted to IROS\n  2024"},{"id":"http://arxiv.org/abs/2403.16794v1","updated":"2024-03-25T14:13:09Z","published":"2024-03-25T14:13:09Z","title":"CurbNet: Curb Detection Framework Based on LiDAR Point Cloud\n  Segmentation","summary":"  Curb detection is an important function in intelligent driving and can be\nused to determine drivable areas of the road. However, curbs are difficult to\ndetect due to the complex road environment. This paper introduces CurbNet, a\nnovel framework for curb detection, leveraging point cloud segmentation.\nAddressing the dearth of comprehensive curb datasets and the absence of 3D\nannotations, we have developed the 3D-Curb dataset, encompassing 7,100 frames,\nwhich represents the largest and most categorically diverse collection of curb\npoint clouds currently available. Recognizing that curbs are primarily\ncharacterized by height variations, our approach harnesses spatially-rich 3D\npoint clouds for training. To tackle the challenges presented by the uneven\ndistribution of curb features on the xy-plane and their reliance on z-axis\nhigh-frequency features, we introduce the multi-scale and channel attention\n(MSCA) module, a bespoke solution designed to optimize detection performance.\nMoreover, we propose an adaptive weighted loss function group, specifically\nformulated to counteract the imbalance in the distribution of curb point clouds\nrelative to other categories. Our extensive experimentation on 2 major datasets\nhas yielded results that surpass existing benchmarks set by leading curb\ndetection and point cloud segmentation models. By integrating multi-clustering\nand curve fitting techniques in our post-processing stage, we have\nsubstantially reduced noise in curb detection, thereby enhancing precision to\n0.8744. Notably, CurbNet has achieved an exceptional average metrics of over\n0.95 at a tolerance of just 0.15m, thereby establishing a new benchmark.\nFurthermore, corroborative real-world experiments and dataset analyzes mutually\nvalidate each other, solidifying CurbNet's superior detection proficiency and\nits robust generalizability.\n","authors":["Guoyang Zhao","Fulong Ma","Yuxuan Liu","Weiqing Qi","Ming Liu"],"pdf_url":"https://arxiv.org/pdf/2403.16794v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16786v1","updated":"2024-03-25T14:01:58Z","published":"2024-03-25T14:01:58Z","title":"DBPF: A Framework for Efficient and Robust Dynamic Bin-Picking","summary":"  Efficiency and reliability are critical in robotic bin-picking as they\ndirectly impact the productivity of automated industrial processes. However,\ntraditional approaches, demanding static objects and fixed collisions, lead to\ndeployment limitations, operational inefficiencies, and process unreliability.\nThis paper introduces a Dynamic Bin-Picking Framework (DBPF) that challenges\ntraditional static assumptions. The DBPF endows the robot with the reactivity\nto pick multiple moving arbitrary objects while avoiding dynamic obstacles,\nsuch as the moving bin. Combined with scene-level pose generation, the proposed\npose selection metric leverages the Tendency-Aware Manipulability Network\noptimizing suction pose determination. Heuristic task-specific designs like\nvelocity-matching, dynamic obstacle avoidance, and the resight policy, enhance\nthe picking success rate and reliability. Empirical experiments demonstrate the\nimportance of these components. Our method achieves an average 84% success\nrate, surpassing the 60% of the most comparable baseline, crucially, with zero\ncollisions. Further evaluations under diverse dynamic scenarios showcase DBPF's\nrobust performance in dynamic bin-picking. Results suggest that our framework\noffers a promising solution for efficient and reliable robotic bin-picking\nunder dynamics.\n","authors":["Yichuan Li","Junkai Zhao","Yixiao Li","Zheng Wu","Rui Cao","Masayoshi Tomizuka","Yunhui Liu"],"pdf_url":"https://arxiv.org/pdf/2403.16786v1.pdf","comment":"8 pages, 5 figures. This paper has been accepted by IEEE RA-L on\n  2024-03-24. See the supplementary video at youtube:\n  https://youtu.be/n5af2VsKhkg"},{"id":"http://arxiv.org/abs/2403.16781v1","updated":"2024-03-25T13:55:49Z","published":"2024-03-25T13:55:49Z","title":"Visual Action Planning with Multiple Heterogeneous Agents","summary":"  Visual planning methods are promising to handle complex settings where\nextracting the system state is challenging. However, none of the existing works\ntackles the case of multiple heterogeneous agents which are characterized by\ndifferent capabilities and/or embodiment. In this work, we propose a method to\nrealize visual action planning in multi-agent settings by exploiting a roadmap\nbuilt in a low-dimensional structured latent space and used for planning. To\nenable multi-agent settings, we infer possible parallel actions from a dataset\ncomposed of tuples associated with individual actions. Next, we evaluate\nfeasibility and cost of them based on the capabilities of the multi-agent\nsystem and endow the roadmap with this information, building a capability\nlatent space roadmap (C-LSR). Additionally, a capability suggestion strategy is\ndesigned to inform the human operator about possible missing capabilities when\nno paths are found. The approach is validated in a simulated burger cooking\ntask and a real-world box packing task.\n","authors":["Martina Lippi","Michael C. Welle","Marco Moletta","Alessandro Marino","Andrea Gasparri","Danica Kragic"],"pdf_url":"https://arxiv.org/pdf/2403.16781v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11742v2","updated":"2024-03-25T13:47:06Z","published":"2024-03-18T12:54:33Z","title":"Accelerating Model Predictive Control for Legged Robots through\n  Distributed Optimization","summary":"  This paper presents a novel approach to enhance Model Predictive Control\n(MPC) for legged robots through Distributed Optimization. Our method focuses on\ndecomposing the robot dynamics into smaller, parallelizable subsystems, and\nutilizing the Alternating Direction Method of Multipliers (ADMM) to ensure\nconsensus among them. Each subsystem is managed by its own Optimal Control\nProblem, with ADMM facilitating consistency between their optimizations. This\napproach not only decreases the computational time but also allows for\neffective scaling with more complex robot configurations, facilitating the\nintegration of additional subsystems such as articulated arms on a quadruped\nrobot. We demonstrate, through numerical evaluations, the convergence of our\napproach on two systems with increasing complexity. In addition, we showcase\nthat our approach converges towards the same solution when compared to a\nstate-of-the-art centralized whole-body MPC implementation. Moreover, we\nquantitatively compare the computational efficiency of our method to the\ncentralized approach, revealing up to a 75\\% reduction in computational time.\nOverall, our approach offers a promising avenue for accelerating MPC solutions\nfor legged robots, paving the way for more effective utilization of the\ncomputational performance of modern hardware.\n","authors":["Lorenzo Amatucci","Giulio Turrisi","Angelo Bratta","Victor Barasuol","Claudio Semini"],"pdf_url":"https://arxiv.org/pdf/2403.11742v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16764v1","updated":"2024-03-25T13:41:57Z","published":"2024-03-25T13:41:57Z","title":"Low-Cost Teleoperation with Haptic Feedback through Vision-based Tactile\n  Sensors for Rigid and Soft Object Manipulation","summary":"  Haptic feedback is essential for humans to successfully perform complex and\ndelicate manipulation tasks. A recent rise in tactile sensors has enabled\nrobots to leverage the sense of touch and expand their capability drastically.\nHowever, many tasks still need human intervention/guidance. For this reason, we\npresent a teleoperation framework designed to provide haptic feedback to human\noperators based on the data from camera-based tactile sensors mounted on the\nrobot gripper. Partial autonomy is introduced to prevent slippage of grasped\nobjects during task execution. Notably, we rely exclusively on low-cost\noff-the-shelf hardware to realize an affordable solution. We demonstrate the\nversatility of the framework on nine different objects ranging from rigid to\nsoft and fragile ones, using three different operators on real hardware.\n","authors":["Martina Lippi","Michael C. Welle","Maciej K. Wozniak","Andrea Gasparri","Danica Kragic"],"pdf_url":"https://arxiv.org/pdf/2403.16764v1.pdf","comment":"https://vision-tactile-manip.github.io/teleop/"},{"id":"http://arxiv.org/abs/2401.16700v2","updated":"2024-03-25T13:33:51Z","published":"2024-01-30T03:00:25Z","title":"Towards Precise 3D Human Pose Estimation with Multi-Perspective\n  Spatial-Temporal Relational Transformers","summary":"  3D human pose estimation captures the human joint points in three-dimensional\nspace while keeping the depth information and physical structure. That is\nessential for applications that require precise pose information, such as\nhuman-computer interaction, scene understanding, and rehabilitation training.\nDue to the challenges in data collection, mainstream datasets of 3D human pose\nestimation are primarily composed of multi-view video data collected in\nlaboratory environments, which contains rich spatial-temporal correlation\ninformation besides the image frame content. Given the remarkable\nself-attention mechanism of transformers, capable of capturing the\nspatial-temporal correlation from multi-view video datasets, we propose a\nmulti-stage framework for 3D sequence-to-sequence (seq2seq) human pose\ndetection. Firstly, the spatial module represents the human pose feature by\nintra-image content, while the frame-image relation module extracts temporal\nrelationships and 3D spatial positional relationship features between the\nmulti-perspective images. Secondly, the self-attention mechanism is adopted to\neliminate the interference from non-human body parts and reduce computing\nresources. Our method is evaluated on Human3.6M, a popular 3D human pose\ndetection dataset. Experimental results demonstrate that our approach achieves\nstate-of-the-art performance on this dataset. The source code will be available\nat https://github.com/WUJINHUAN/3D-human-pose.\n","authors":["Jianbin Jiao","Xina Cheng","Weijie Chen","Xiaoting Yin","Hao Shi","Kailun Yang"],"pdf_url":"https://arxiv.org/pdf/2401.16700v2.pdf","comment":"Accepted to IJCNN 2024. The source code will be available at\n  https://github.com/WUJINHUAN/3D-human-pose"},{"id":"http://arxiv.org/abs/2402.02423v2","updated":"2024-03-25T13:20:46Z","published":"2024-02-04T09:40:22Z","title":"Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement\n  Learning with Diverse Human Feedback","summary":"  Reinforcement Learning with Human Feedback (RLHF) has received significant\nattention for performing tasks without the need for costly manual reward design\nby aligning human preferences. It is crucial to consider diverse human feedback\ntypes and various learning methods in different environments. However,\nquantifying progress in RLHF with diverse feedback is challenging due to the\nlack of standardized annotation platforms and widely used unified benchmarks.\nTo bridge this gap, we introduce Uni-RLHF, a comprehensive system\nimplementation tailored for RLHF. It aims to provide a complete workflow from\nreal human feedback, fostering progress in the development of practical\nproblems. Uni-RLHF contains three packages: 1) a universal multi-feedback\nannotation platform, 2) large-scale crowdsourced feedback datasets, and 3)\nmodular offline RLHF baseline implementations. Uni-RLHF develops a\nuser-friendly annotation interface tailored to various feedback types,\ncompatible with a wide range of mainstream RL environments. We then establish a\nsystematic pipeline of crowdsourced annotations, resulting in large-scale\nannotated datasets comprising more than 15 million steps across 30+ popular\ntasks. Through extensive experiments, the results in the collected datasets\ndemonstrate competitive performance compared to those from well-designed manual\nrewards. We evaluate various design choices and offer insights into their\nstrengths and potential areas of improvement. We wish to build valuable\nopen-source platforms, datasets, and baselines to facilitate the development of\nmore robust and reliable RLHF solutions based on realistic human feedback. The\nwebsite is available at https://uni-rlhf.github.io/.\n","authors":["Yifu Yuan","Jianye Hao","Yi Ma","Zibin Dong","Hebin Liang","Jinyi Liu","Zhixin Feng","Kai Zhao","Yan Zheng"],"pdf_url":"https://arxiv.org/pdf/2402.02423v2.pdf","comment":"Published as a conference paper at ICLR 2024. The website is\n  available at https://uni-rlhf.github.io/"},{"id":"http://arxiv.org/abs/2403.16730v1","updated":"2024-03-25T13:04:20Z","published":"2024-03-25T13:04:20Z","title":"A Robotic Skill Learning System Built Upon Diffusion Policies and\n  Foundation Models","summary":"  In this paper, we build upon two major recent developments in the field,\nDiffusion Policies for visuomotor manipulation and large pre-trained multimodal\nfoundational models to obtain a robotic skill learning system. The system can\nobtain new skills via the behavioral cloning approach of visuomotor diffusion\npolicies given teleoperated demonstrations. Foundational models are being used\nto perform skill selection given the user's prompt in natural language. Before\nexecuting a skill the foundational model performs a precondition check given an\nobservation of the workspace. We compare the performance of different\nfoundational models to this end as well as give a detailed experimental\nevaluation of the skills taught by the user in simulation and the real world.\nFinally, we showcase the combined system on a challenging food serving scenario\nin the real world. Videos of all experimental executions, as well as the\nprocess of teaching new skills in simulation and the real world, are available\non the project's website.\n","authors":["Nils Ingelhag","Jesper Munkeby","Jonne van Haastregt","Anastasia Varava","Michael C. Welle","Danica Kragic"],"pdf_url":"https://arxiv.org/pdf/2403.16730v1.pdf","comment":"https://roboskillframework.github.io"},{"id":"http://arxiv.org/abs/2403.16696v1","updated":"2024-03-25T12:27:24Z","published":"2024-03-25T12:27:24Z","title":"BatDeck: Advancing Nano-drone Navigation with Low-power Ultrasound-based\n  Obstacle Avoidance","summary":"  Nano-drones, distinguished by their agility, minimal weight, and\ncost-effectiveness, are particularly well-suited for exploration in confined,\ncluttered and narrow spaces. Recognizing transparent, highly reflective or\nabsorbing materials, such as glass and metallic surfaces is challenging, as\nclassical sensors, such as cameras or laser rangers, often do not detect them.\nInspired by bats, which can fly at high speeds in complete darkness with the\nhelp of ultrasound, this paper introduces \\textit{BatDeck}, a pioneering\nsensor-deck employing a lightweight and low-power ultrasonic sensor for\nnano-drone autonomous navigation. This paper first provides insights about\nsensor characteristics, highlighting the influence of motor noise on the\nultrasound readings, then it introduces the results of extensive experimental\ntests for obstacle avoidance (OA) in a diverse environment. Results show that\n\\textit{BatDeck} allows exploration for a flight time of 8 minutes while\ncovering 136m on average before crash in a challenging environment with\ntransparent and reflective obstacles, proving the effectiveness of ultrasonic\nsensors for OA on nano-drones.\n","authors":["Hanna Müller","Victor Kartsch","Michele Magno","Luca Benini"],"pdf_url":"https://arxiv.org/pdf/2403.16696v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2403.16689v1","updated":"2024-03-25T12:23:39Z","published":"2024-03-25T12:23:39Z","title":"Synapse: Learning Preferential Concepts from Visual Demonstrations","summary":"  This paper addresses the problem of preference learning, which aims to learn\nuser-specific preferences (e.g., \"good parking spot\", \"convenient drop-off\nlocation\") from visual input. Despite its similarity to learning factual\nconcepts (e.g., \"red cube\"), preference learning is a fundamentally harder\nproblem due to its subjective nature and the paucity of person-specific\ntraining data. We address this problem using a new framework called Synapse,\nwhich is a neuro-symbolic approach designed to efficiently learn preferential\nconcepts from limited demonstrations. Synapse represents preferences as\nneuro-symbolic programs in a domain-specific language (DSL) that operates over\nimages, and leverages a novel combination of visual parsing, large language\nmodels, and program synthesis to learn programs representing individual\npreferences. We evaluate Synapse through extensive experimentation including a\nuser case study focusing on mobility-related concepts in mobile robotics and\nautonomous driving. Our evaluation demonstrates that Synapse significantly\noutperforms existing baselines as well as its own ablations. The code and other\ndetails can be found on the project website https://amrl.cs.utexas.edu/synapse .\n","authors":["Sadanand Modak","Noah Patton","Isil Dillig","Joydeep Biswas"],"pdf_url":"https://arxiv.org/pdf/2403.16689v1.pdf","comment":"23 pages, 7 figures; Preprint"},{"id":"http://arxiv.org/abs/2403.16669v1","updated":"2024-03-25T12:07:24Z","published":"2024-03-25T12:07:24Z","title":"Domain Adaptive Detection of MAVs: A Benchmark and Noise Suppression\n  Network","summary":"  Visual detection of Micro Air Vehicles (MAVs) has attracted increasing\nattention in recent years due to its important application in various tasks.\nThe existing methods for MAV detection assume that the training set and testing\nset have the same distribution. As a result, when deployed in new domains, the\ndetectors would have a significant performance degradation due to domain\ndiscrepancy. In this paper, we study the problem of cross-domain MAV detection.\nThe contributions of this paper are threefold. 1) We propose a\nMulti-MAV-Multi-Domain (M3D) dataset consisting of both simulation and\nrealistic images. Compared to other existing datasets, the proposed one is more\ncomprehensive in the sense that it covers rich scenes, diverse MAV types, and\nvarious viewing angles. A new benchmark for cross-domain MAV detection is\nproposed based on the proposed dataset. 2) We propose a Noise Suppression\nNetwork (NSN) based on the framework of pseudo-labeling and a large-to-small\ntraining procedure. To reduce the challenging pseudo-label noises, two novel\nmodules are designed in this network. The first is a prior-based curriculum\nlearning module for allocating adaptive thresholds for pseudo labels with\ndifferent difficulties. The second is a masked copy-paste augmentation module\nfor pasting truly-labeled MAVs on unlabeled target images and thus decreasing\npseudo-label noises. 3) Extensive experimental results verify the superior\nperformance of the proposed method compared to the state-of-the-art ones. In\nparticular, it achieves mAP of 46.9%(+5.8%), 50.5%(+3.7%), and 61.5%(+11.3%) on\nthe tasks of simulation-to-real adaptation, cross-scene adaptation, and\ncross-camera adaptation, respectively.\n","authors":["Yin Zhang","Jinhong Deng","Peidong Liu","Wen Li","Shiyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.16669v1.pdf","comment":"17 pages, 11 figures. Accepted by IEEE Transactions on Automation\n  Science and Engineering"},{"id":"http://arxiv.org/abs/2311.00390v3","updated":"2024-03-25T12:02:27Z","published":"2023-11-01T09:33:11Z","title":"A Modular Pneumatic Soft Gripper Design for Aerial Grasping and Landing","summary":"  Aerial robots have garnered significant attention due to their potential\napplications in various industries, such as inspection, search and rescue, and\ndrone delivery. Successful missions often depend on the ability of these robots\nto grasp and land effectively. This paper presents a novel modular soft gripper\ndesign tailored explicitly for aerial grasping and landing operations. The\nproposed modular pneumatic soft gripper incorporates a feed-forward\nproportional controller to regulate pressure, enabling compliant gripping\ncapabilities. The modular connectors of the soft fingers offer two\nconfigurations for the 4-tip soft gripper, H-base (cylindrical) and X-base\n(spherical), allowing adaptability to different target objects. Additionally,\nthe gripper can serve as a soft landing gear when deflated, eliminating the\nneed for an extra landing gear. This design reduces weight, simplifies aerial\nmanipulation control, and enhances flight efficiency. We demonstrate the\nefficacy of indoor aerial grasping and achieve a maximum payload of 217 g using\nthe proposed soft aerial vehicle and its H-base pneumatic soft gripper (808 g).\n","authors":["Hiu Ching Cheung","Ching-Wei Chang","Bailun Jiang","Chih-Yung Wen","Henry K. Chu"],"pdf_url":"https://arxiv.org/pdf/2311.00390v3.pdf","comment":"7 pages, 13 figures, accepted by IEEE RoboSoft 2024"},{"id":"http://arxiv.org/abs/2403.16664v1","updated":"2024-03-25T11:57:30Z","published":"2024-03-25T11:57:30Z","title":"Skill Q-Network: Learning Adaptive Skill Ensemble for Mapless Navigation\n  in Unknown Environments","summary":"  This paper focuses on the acquisition of mapless navigation skills within\nunknown environments. We introduce the Skill Q-Network (SQN), a novel\nreinforcement learning method featuring an adaptive skill ensemble mechanism.\nUnlike existing methods, our model concurrently learns a high-level skill\ndecision process alongside multiple low-level navigation skills, all without\nthe need for prior knowledge. Leveraging a tailored reward function for mapless\nnavigation, the SQN is capable of learning adaptive maneuvers that incorporate\nboth exploration and goal-directed skills, enabling effective navigation in new\nenvironments. Our experiments demonstrate that our SQN can effectively navigate\ncomplex environments, exhibiting a 40% higher performance compared to baseline\nmodels. Without explicit guidance, SQN discovers how to combine low-level skill\npolicies, showcasing both goal-directed navigations to reach destinations and\nexploration maneuvers to escape from local minimum regions in challenging\nscenarios. Remarkably, our adaptive skill ensemble method enables zero-shot\ntransfer to out-of-distribution domains, characterized by unseen observations\nfrom non-convex obstacles or uneven, subterranean-like environments.\n","authors":["Hyunki Seong","David Hyunchul Shim"],"pdf_url":"https://arxiv.org/pdf/2403.16664v1.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.16652v1","updated":"2024-03-25T11:40:32Z","published":"2024-03-25T11:40:32Z","title":"Trajectory Planning of Robotic Manipulator in Dynamic Environment\n  Exploiting DRL","summary":"  This study is about the implementation of a reinforcement learning algorithm\nin the trajectory planning of manipulators. We have a 7-DOF robotic arm to pick\nand place the randomly placed block at a random target point in an unknown\nenvironment. The obstacle is randomly moving which creates a hurdle in picking\nthe object. The objective of the robot is to avoid the obstacle and pick the\nblock with constraints to a fixed timestamp. In this literature, we have\napplied a deep deterministic policy gradient (DDPG) algorithm and compared the\nmodel's efficiency with dense and sparse rewards.\n","authors":["Osama Ahmad","Zawar Hussain","Hammad Naeem"],"pdf_url":"https://arxiv.org/pdf/2403.16652v1.pdf","comment":"Accepted in ICIESTR-2024"},{"id":"http://arxiv.org/abs/2403.16644v1","updated":"2024-03-25T11:29:32Z","published":"2024-03-25T11:29:32Z","title":"Bridging the Sim-to-Real Gap with Bayesian Inference","summary":"  We present SIM-FSVGD for learning robot dynamics from data. As opposed to\ntraditional methods, SIM-FSVGD leverages low-fidelity physical priors, e.g., in\nthe form of simulators, to regularize the training of neural network models.\nWhile learning accurate dynamics already in the low data regime, SIM-FSVGD\nscales and excels also when more data is available. We empirically show that\nlearning with implicit physical priors results in accurate mean model\nestimation as well as precise uncertainty quantification. We demonstrate the\neffectiveness of SIM-FSVGD in bridging the sim-to-real gap on a\nhigh-performance RC racecar system. Using model-based RL, we demonstrate a\nhighly dynamic parking maneuver with drifting, using less than half the data\ncompared to the state of the art.\n","authors":["Jonas Rothfuss","Bhavya Sukhija","Lenart Treven","Florian Dörfler","Stelian Coros","Andreas Krause"],"pdf_url":"https://arxiv.org/pdf/2403.16644v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16634v1","updated":"2024-03-25T11:22:38Z","published":"2024-03-25T11:22:38Z","title":"Symbolic and User-friendly Geometric Algebra Routines (SUGAR) for\n  Computations in Matlab","summary":"  Geometric algebra (GA) is a mathematical tool for geometric computing,\nproviding a framework that allows a unified and compact approach to geometric\nrelations which in other mathematical systems are typically described using\ndifferent more complicated elements. This fact has led to an increasing\nadoption of GA in applied mathematics and engineering problems. However, the\nscarcity of symbolic implementations of GA and its inherent complexity,\nrequiring a specific mathematical background, make it challenging and less\nintuitive for engineers to work with. This prevents wider adoption among more\napplied professionals. To address this challenge, this paper introduces SUGAR\n(Symbolic and User-friendly Geometric Algebra Routines), an open-source toolbox\ndesigned for Matlab and licensed under the MIT License. SUGAR facilitates the\ntranslation of GA concepts into Matlab and provides a collection of\nuser-friendly functions tailored for GA computations, including support for\nsymbolic operations. It supports both numeric and symbolic computations in\nhigh-dimensional GAs. Specifically tailored for applied mathematics and\nengineering applications, SUGAR has been meticulously engineered to represent\ngeometric elements and transformations within two and three-dimensional\nprojective and conformal geometric algebras, aligning with established\ncomputational methodologies in the literature. Furthermore, SUGAR efficiently\nhandles functions of multivectors, such as exponential, logarithmic,\nsinusoidal, and cosine functions, enhancing its applicability across various\nengineering domains, including robotics, control systems, and power\nelectronics. Finally, this work includes four distinct validation examples,\ndemonstrating SUGAR's capabilities across the above-mentioned fields and its\npractical utility in addressing real-world applied mathematics and engineering\nproblems.\n","authors":["Manel Velasco","Isiah Zaplana","Arnau Dória-Cerezo","Pau Martí"],"pdf_url":"https://arxiv.org/pdf/2403.16634v1.pdf","comment":"33 pages, 6 figures, journal paper submitted to ACM TOMS"},{"id":"http://arxiv.org/abs/2310.00262v2","updated":"2024-03-25T10:45:50Z","published":"2023-09-30T05:26:42Z","title":"Robust Integral Consensus Control of Multi-Agent Networks Perturbed by\n  Matched and Unmatched Disturbances: The Case of Directed Graphs","summary":"  This work presents a new method to design consensus controllers for perturbed\ndouble integrator systems whose interconnection is described by a directed\ngraph containing a rooted spanning tree. We propose new robust controllers to\nsolve the consensus and synchronization problems when the systems are under the\neffects of matched and unmatched disturbances. In both problems, we present\nsimple continuous controllers, whose integral actions allow us to handle the\ndisturbances. A rigorous stability analysis based on Lyapunov's direct method\nfor unperturbed networked systems is presented. To assess the performance of\nour result, a representative simulation study is presented.\n","authors":["Jose Guadalupe Romero","David Navarro-Alarcon"],"pdf_url":"https://arxiv.org/pdf/2310.00262v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16613v1","updated":"2024-03-25T10:43:47Z","published":"2024-03-25T10:43:47Z","title":"Technical Development of a Semi-Autonomous Robotic Partition","summary":"  This technical description details the design and engineering process of a\nsemi-autonomous robotic partition. This robotic partition prototype was\nsubsequently employed in a longer-term evaluation in-the-wild study conducted\nby the authors in a real-world office setting.\n","authors":["Binh Vinh Duc Nguyen","Andrew Vande Moere"],"pdf_url":"https://arxiv.org/pdf/2403.16613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16606v1","updated":"2024-03-25T10:33:20Z","published":"2024-03-25T10:33:20Z","title":"ROXIE: Defining a Robotic eXplanation and Interpretability Engine","summary":"  In an era where autonomous robots increasingly inhabit public spaces, the\nimperative for transparency and interpretability in their decision-making\nprocesses becomes paramount. This paper presents the overview of a Robotic\neXplanation and Interpretability Engine (ROXIE), which addresses this critical\nneed, aiming to demystify the opaque nature of complex robotic behaviors. This\npaper elucidates the key features and requirements needed for providing\ninformation and explanations about robot decision-making processes. It also\noverviews the suite of software components and libraries available for\ndeployment with ROS 2, empowering users to provide comprehensive explanations\nand interpretations of robot processes and behaviors, thereby fostering trust\nand collaboration in human-robot interactions.\n","authors":["Francisco J. Rodríguez-Lera","Miguel A. González-Santamarta","Alejandro González-Cantón","Laura Fernández-Becerra","David Sobrín-Hidalgo","Angel Manuel Guerrero-Higueras"],"pdf_url":"https://arxiv.org/pdf/2403.16606v1.pdf","comment":"7 pages, 3 figures, 1 tables, Submitted to IROS 2024"},{"id":"http://arxiv.org/abs/2403.16600v1","updated":"2024-03-25T10:20:50Z","published":"2024-03-25T10:20:50Z","title":"Research Challenges for Adaptive Architecture: Empowering Occupants of\n  Multi-Occupancy Buildings","summary":"  This positional paper outlines our vision of 'adaptive architecture', which\ninvolves the integration of robotic technology to physically change an\narchitectural space in supporting the changing needs of its occupants, in\nresponse to the CHI'24 workshop \"HabiTech - Inhabiting Buildings, Data &\nTechnology\" call on \"How do new technologies enable and empower the inhabitants\nof multi-occupancy buildings?\". Specifically, while adaptive architecture holds\npromise for enhancing occupant satisfaction, comfort, and overall health and\nwell-being, there remains a range of research challenges of (1) how it can\neffectively support individual occupants, while (2) mediating the conflicting\nneeds of collocated others, and (3) integrating meaningfully into the\nsociocultural characteristics of their building community.\n","authors":["Binh Vinh Duc Nguyen","Andrew Vande Moere"],"pdf_url":"https://arxiv.org/pdf/2403.16600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16595v1","updated":"2024-03-25T10:16:51Z","published":"2024-03-25T10:16:51Z","title":"The Adaptive Workplace: Orchestrating Architectural Services around the\n  Wellbeing of Individual Occupants","summary":"  As the academic consortia members of the EU Horizon project SONATA\n(\"Situation-aware OrchestratioN of AdapTive Architecture\"), we respond to the\nworkshop call for \"Office Wellbeing by Design: Don't Stand for Anything Less\"\nby proposing the \"Adaptive Workplace\" concept. In essence, our vision aims to\nadapt a workplace to the ever-changing needs of individual occupants, instead\nof that occupants are expected to adapt to their workplace.\n","authors":["Andrew Vande Moere","Sara Arko","Alena Safrova Drasilova","Tomáš Ondráček","Ilaria Pigliautile","Benedetta Pioppi","Anna Laura Pisello","Jakub Prochazka","Paula Acuna Roncancio","Davide Schaumann","Marcel Schweiker","Binh Vinh Duc Nguyen"],"pdf_url":"https://arxiv.org/pdf/2403.16595v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16593v1","updated":"2024-03-25T10:09:42Z","published":"2024-03-25T10:09:42Z","title":"Counter-example guided Imitation Learning of Feedback Controllers from\n  Temporal Logic Specifications","summary":"  We present a novel method for imitation learning for control requirements\nexpressed using Signal Temporal Logic (STL). More concretely we focus on the\nproblem of training a neural network to imitate a complex controller. The\nlearning process is guided by efficient data aggregation based on\ncounter-examples and a coverage measure. Moreover, we introduce a method to\nevaluate the performance of the learned controller via parameterization and\nparameter estimation of the STL requirements. We demonstrate our approach with\na flying robot case study.\n","authors":["Thao Dang","Alexandre Donzé","Inzemamul Haque","Nikolaos Kekatos","Indranil Saha"],"pdf_url":"https://arxiv.org/pdf/2403.16593v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16560v1","updated":"2024-03-25T09:18:48Z","published":"2024-03-25T09:18:48Z","title":"Active Admittance Control with Iterative Learning for General-Purpose\n  Contact-Rich Manipulation","summary":"  Force interaction is inevitable when robots face multiple operation\nscenarios. How to make the robot competent in force control for generalized\noperations such as multi-tasks still remains a challenging problem. Aiming at\nthe reproducibility of interaction tasks and the lack of a generalized force\ncontrol framework for multi-task scenarios, this paper proposes a novel hybrid\ncontrol framework based on active admittance control with iterative learning\nparameters-tunning mechanism. The method adopts admittance control as the\nunderlying algorithm to ensure flexibility, and iterative learning as the\nhigh-level algorithm to regulate the parameters of the admittance model. The\nwhole algorithm has flexibility and learning ability, which is capable of\nachieving the goal of excellent versatility. Four representative interactive\nrobot manipulation tasks are chosen to investigate the consistency and\ngeneralisability of the proposed method. Experiments are designed to verify the\neffectiveness of the whole framework, and an average of 98.21% and 91.52%\nimprovement of RMSE is obtained relative to the traditional admittance control\nas well as the model-free adaptive control, respectively.\n","authors":["Bo Zhou","Yuyao Sun","Wenbo Liu","Ruixuan Jiao","Fang Fang","Shihua Li"],"pdf_url":"https://arxiv.org/pdf/2403.16560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16535v1","updated":"2024-03-25T08:26:20Z","published":"2024-03-25T08:26:20Z","title":"Arm-Constrained Curriculum Learning for Loco-Manipulation of the\n  Wheel-Legged Robot","summary":"  Incorporating a robotic manipulator into a wheel-legged robot enhances its\nagility and expands its potential for practical applications. However, the\npresence of potential instability and uncertainties presents additional\nchallenges for control objectives. In this paper, we introduce an\narm-constrained curriculum learning architecture to tackle the issues\nintroduced by adding the manipulator. Firstly, we develop an arm-constrained\nreinforcement learning algorithm to ensure safety and stability in control\nperformance. Additionally, to address discrepancies in reward settings between\nthe arm and the base, we propose a reward-aware curriculum learning method. The\npolicy is first trained in Isaac gym and transferred to the physical robot to\ndo dynamic grasping tasks, including the door-opening task, fan-twitching task\nand the relay-baton-picking and following task. The results demonstrate that\nour proposed approach effectively controls the arm-equipped wheel-legged robot\nto master dynamic grasping skills, allowing it to chase and catch a moving\nobject while in motion. The code can be found at\nhttps://github.com/aCodeDog/legged-robots-manipulation. To view the\nsupplemental video, please visit https://youtu.be/sNXT-rwPNMM.\n","authors":["Zifan Wang","Yufei Jia","Lu Shi","Haoyu Wang","Haizhou Zhao","Xueyang Li","Jinni Zhou","Jun Ma","Guyue Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.16535v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16527v1","updated":"2024-03-25T08:11:02Z","published":"2024-03-25T08:11:02Z","title":"Hallucination Detection in Foundation Models for Decision-Making: A\n  Flexible Definition and Review of the State of the Art","summary":"  Autonomous systems are soon to be ubiquitous, from manufacturing autonomy to\nagricultural field robots, and from health care assistants to the entertainment\nindustry. The majority of these systems are developed with modular\nsub-components for decision-making, planning, and control that may be\nhand-engineered or learning-based. While these existing approaches have been\nshown to perform well under the situations they were specifically designed for,\nthey can perform especially poorly in rare, out-of-distribution scenarios that\nwill undoubtedly arise at test-time. The rise of foundation models trained on\nmultiple tasks with impressively large datasets from a variety of fields has\nled researchers to believe that these models may provide common sense reasoning\nthat existing planners are missing. Researchers posit that this common sense\nreasoning will bridge the gap between algorithm development and deployment to\nout-of-distribution tasks, like how humans adapt to unexpected scenarios. Large\nlanguage models have already penetrated the robotics and autonomous systems\ndomains as researchers are scrambling to showcase their potential use cases in\ndeployment. While this application direction is very promising empirically,\nfoundation models are known to hallucinate and generate decisions that may\nsound reasonable, but are in fact poor. We argue there is a need to step back\nand simultaneously design systems that can quantify the certainty of a model's\ndecision, and detect when it may be hallucinating. In this work, we discuss the\ncurrent use cases of foundation models for decision-making tasks, provide a\ngeneral definition for hallucinations with examples, discuss existing\napproaches to hallucination detection and mitigation with a focus on decision\nproblems, and explore areas for further research in this exciting field.\n","authors":["Neeloy Chakraborty","Melkior Ornik","Katherine Driggs-Campbell"],"pdf_url":"https://arxiv.org/pdf/2403.16527v1.pdf","comment":"31 pages, 2 tables"},{"id":"http://arxiv.org/abs/2403.16489v1","updated":"2024-03-25T07:17:44Z","published":"2024-03-25T07:17:44Z","title":"Spatially temporally distributed informative path planning for\n  multi-robot systems","summary":"  This paper investigates the problem of informative path planning for a mobile\nrobotic sensor network in spatially temporally distributed mapping. The robots\nare able to gather noisy measurements from an area of interest during their\nmovements to build a Gaussian Process (GP) model of a spatio-temporal field.\nThe model is then utilized to predict the spatio-temporal phenomenon at\ndifferent points of interest. To spatially and temporally navigate the group of\nrobots so that they can optimally acquire maximal information gains while their\nconnectivity is preserved, we propose a novel multistep prediction informative\npath planning optimization strategy employing our newly defined local cost\nfunctions. By using the dual decomposition method, it is feasible and practical\nto effectively solve the optimization problem in a distributed manner. The\nproposed method was validated through synthetic experiments utilizing\nreal-world data sets.\n","authors":["Binh Nguyen","Linh Nguyen","Truong X. Nghiem","Hung La","Jose Baca","Pablo Rangel","Miguel Cid Montoya","Thang Nguyen"],"pdf_url":"https://arxiv.org/pdf/2403.16489v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16485v1","updated":"2024-03-25T07:12:51Z","published":"2024-03-25T07:12:51Z","title":"Real-time Model Predictive Control with Zonotope-Based Neural Networks\n  for Bipedal Social Navigation","summary":"  This study addresses the challenge of bipedal navigation in a dynamic\nhuman-crowded environment, a research area that remains largely underexplored\nin the field of legged navigation. We propose two cascaded zonotope-based\nneural networks: a Pedestrian Prediction Network (PPN) for pedestrians' future\ntrajectory prediction and an Ego-agent Social Network (ESN) for ego-agent\nsocial path planning. Representing future paths as zonotopes allows for\nefficient reachability-based planning and collision checking. The ESN is then\nintegrated with a Model Predictive Controller (ESN-MPC) for footstep planning\nfor our bipedal robot Digit designed by Agility Robotics. ESN-MPC solves for a\ncollision-free optimal trajectory by optimizing through the gradients of ESN.\nESN-MPC optimal trajectory is sent to the low-level controller for full-order\nsimulation of Digit. The overall proposed framework is validated with extensive\nsimulations on randomly generated initial settings with varying human crowd\ndensities.\n","authors":["Abdulaziz Shamsah","Krishanu Agarwal","Shreyas Kousik","Ye Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.16485v1.pdf","comment":"8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.16478v1","updated":"2024-03-25T07:04:24Z","published":"2024-03-25T07:04:24Z","title":"Towards Cooperative Maneuver Planning in Mixed Traffic at Urban\n  Intersections","summary":"  Connected automated driving promises a significant improvement of traffic\nefficiency and safety on highways and in urban areas. Apart from sharing of\nawareness and perception information over wireless communication links,\ncooperative maneuver planning may facilitate active guidance of connected\nautomated vehicles at urban intersections. Research in automatic intersection\nmanagement put forth a large body of works that mostly employ rule-based or\noptimization-based approaches primarily in fully automated simulated\nenvironments. In this work, we present two cooperative planning approaches that\nare capable of handling mixed traffic, i.e., the road being shared by automated\nvehicles and regular vehicles driven by humans. Firstly, we propose an\noptimization-based planner trained on real driving data that cyclically selects\nthe most efficient out of multiple predicted coordinated maneuvers.\nAdditionally, we present a cooperative planning approach based on graph-based\nreinforcement learning, which conquers the lack of ground truth data for\ncooperative maneuvers. We present evaluation results of both cooperative\nplanners in high-fidelity simulation and real-world traffic. Simulative\nexperiments in fully automated traffic and mixed traffic show that cooperative\nmaneuver planning leads to less delay due to interaction and a reduced number\nof stops. In real-world experiments with three prototype connected automated\nvehicles in public traffic, both planners demonstrate their ability to perform\nefficient cooperative maneuvers.\n","authors":["Marvin Klimke","Max Bastian Mertens","Benjamin Völz","Michael Buchholz"],"pdf_url":"https://arxiv.org/pdf/2403.16478v1.pdf","comment":"M. Klimke and M. Mertens are both first authors with equal\n  contribution. 11 pages, 10 figures, 2 tables, submitted to IEEE Transactions\n  on Intelligent Vehicles"},{"id":"http://arxiv.org/abs/2403.16439v1","updated":"2024-03-25T05:58:33Z","published":"2024-03-25T05:58:33Z","title":"Producing and Leveraging Online Map Uncertainty in Trajectory Prediction","summary":"  High-definition (HD) maps have played an integral role in the development of\nmodern autonomous vehicle (AV) stacks, albeit with high associated labeling and\nmaintenance costs. As a result, many recent works have proposed methods for\nestimating HD maps online from sensor data, enabling AVs to operate outside of\npreviously-mapped regions. However, current online map estimation approaches\nare developed in isolation of their downstream tasks, complicating their\nintegration in AV stacks. In particular, they do not produce uncertainty or\nconfidence estimates. In this work, we extend multiple state-of-the-art online\nmap estimation methods to additionally estimate uncertainty and show how this\nenables more tightly integrating online mapping with trajectory forecasting. In\ndoing so, we find that incorporating uncertainty yields up to 50% faster\ntraining convergence and up to 15% better prediction performance on the\nreal-world nuScenes driving dataset.\n","authors":["Xunjiang Gu","Guanyu Song","Igor Gilitschenski","Marco Pavone","Boris Ivanovic"],"pdf_url":"https://arxiv.org/pdf/2403.16439v1.pdf","comment":"14 pages, 14 figures, 6 tables. CVPR 2024"},{"id":"http://arxiv.org/abs/2403.16430v1","updated":"2024-03-25T05:21:19Z","published":"2024-03-25T05:21:19Z","title":"AeroBridge: Autonomous Drone Handoff System for Emergency Battery\n  Service","summary":"  This paper proposes an Emergency Battery Service (EBS) for drones in which an\nEBS drone flies to a drone in the field with a depleted battery and transfers a\nfresh battery to the exhausted drone. The authors present a unique battery\ntransfer mechanism and drone localization that uses the Cross Marker Position\n(CMP) method. The main challenges include a stable and balanced transfer that\nprecisely localizes the receiver drone. The proposed EBS drone mitigates the\neffects of downwash due to the vertical proximity between the drones by\nimplementing diagonal alignment with the receiver, reducing the distance to 0.5\nm between the two drones. CFD analysis shows that diagonal instead of\nperpendicular alignment minimizes turbulence, and the authors verify the actual\nsystem for change in output airflow and thrust measurements. The CMP\nmarker-based localization method enables position lock for the EBS drone with\nup to 0.9 cm accuracy. The performance of the transfer mechanism is validated\nexperimentally by successful mid-air transfer in 5 seconds, where the EBS drone\nis within 0.5 m vertical distance from the receiver drone, wherein 4m/s\nturbulence does not affect the transfer process.\n","authors":["Avishkar Seth","Alice James","Endrowednes Kuantama","Richard Han","Subhas Mukhopadhyay"],"pdf_url":"https://arxiv.org/pdf/2403.16430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.17463v2","updated":"2024-03-25T05:16:38Z","published":"2024-01-30T21:51:57Z","title":"A Group Theoretic Metric for Robot State Estimation Leveraging Chebyshev\n  Interpolation","summary":"  We propose a new metric for robot state estimation based on the recently\nintroduced $\\text{SE}_2(3)$ Lie group definition. Our metric is related to\nprior metrics for SLAM but explicitly takes into account the linear velocity of\nthe state estimate, improving over current pose-based trajectory analysis. This\nhas the benefit of providing a single, quantitative metric to evaluate state\nestimation algorithms against, while being compatible with existing tools and\nlibraries. Since ground truth data generally consists of pose data from motion\ncapture systems, we also propose an approach to compute the ground truth linear\nvelocity based on polynomial interpolation. Using Chebyshev interpolation and a\npseudospectral parameterization, we can accurately estimate the ground truth\nlinear velocity of the trajectory in an optimal fashion with best approximation\nerror. We demonstrate how this approach performs on multiple robotic platforms\nwhere accurate state estimation is vital, and compare it to alternative\napproaches such as finite differences. The pseudospectral parameterization also\nprovides a means of trajectory data compression as an additional benefit.\nExperimental results show our method provides a valid and accurate means of\ncomparing state estimation systems, which is also easy to interpret and report.\n","authors":["Varun Agrawal","Frank Dellaert"],"pdf_url":"https://arxiv.org/pdf/2401.17463v2.pdf","comment":"Accepted to ICRA 2024"},{"id":"http://arxiv.org/abs/2403.16425v1","updated":"2024-03-25T05:10:34Z","published":"2024-03-25T05:10:34Z","title":"Enhancing Visual Place Recognition via Fast and Slow Adaptive Biasing in\n  Event Cameras","summary":"  Event cameras are increasingly popular in robotics due to their beneficial\nfeatures, such as low latency, energy efficiency, and high dynamic range.\nNevertheless, their downstream task performance is greatly influenced by the\noptimization of bias parameters. These parameters, for instance, regulate the\nnecessary change in light intensity to trigger an event, which in turn depends\non factors such as the environment lighting and camera motion. This paper\nintroduces feedback control algorithms that automatically tune the bias\nparameters through two interacting methods: 1) An immediate, on-the-fly fast\nadaptation of the refractory period, which sets the minimum interval between\nconsecutive events, and 2) if the event rate exceeds the specified bounds even\nafter changing the refractory period repeatedly, the controller adapts the\npixel bandwidth and event thresholds, which stabilizes after a short period of\nnoise events across all pixels (slow adaptation). Our evaluation focuses on the\nvisual place recognition task, where incoming query images are compared to a\ngiven reference database. We conducted comprehensive evaluations of our\nalgorithms' adaptive feedback control in real-time. To do so, we collected the\nQCR-Fast-and-Slow dataset that contains DAVIS346 event camera streams from 366\nrepeated traversals of a Scout Mini robot navigating through a 100 meter long\nindoor lab setting (totaling over 35km distance traveled) in varying brightness\nconditions with ground truth location information. Our proposed feedback\ncontrollers result in superior performance when compared to the standard bias\nsettings and prior feedback control methods. Our findings also detail the\nimpact of bias adjustments on task performance and feature ablation studies on\nthe fast and slow adaptation mechanisms.\n","authors":["Gokul B. Nair","Michael Milford","Tobias Fischer"],"pdf_url":"https://arxiv.org/pdf/2403.16425v1.pdf","comment":"8 pages, 9 figures, paper under review"},{"id":"http://arxiv.org/abs/2312.03009v2","updated":"2024-03-25T05:04:04Z","published":"2023-12-04T19:01:19Z","title":"I-PHYRE: Interactive Physical Reasoning","summary":"  Current evaluation protocols predominantly assess physical reasoning in\nstationary scenes, creating a gap in evaluating agents' abilities to interact\nwith dynamic events. While contemporary methods allow agents to modify initial\nscene configurations and observe consequences, they lack the capability to\ninteract with events in real time. To address this, we introduce I-PHYRE, a\nframework that challenges agents to simultaneously exhibit intuitive physical\nreasoning, multi-step planning, and in-situ intervention. Here, intuitive\nphysical reasoning refers to a quick, approximate understanding of physics to\naddress complex problems; multi-step denotes the need for extensive sequence\nplanning in I-PHYRE, considering each intervention can significantly alter\nsubsequent choices; and in-situ implies the necessity for timely object\nmanipulation within a scene, where minor timing deviations can result in task\nfailure. We formulate four game splits to scrutinize agents' learning and\ngeneralization of essential principles of interactive physical reasoning,\nfostering learning through interaction with representative scenarios. Our\nexploration involves three planning strategies and examines several supervised\nand reinforcement agents' zero-shot generalization proficiency on I-PHYRE. The\noutcomes highlight a notable gap between existing learning algorithms and human\nperformance, emphasizing the imperative for more research in enhancing agents\nwith interactive physical reasoning capabilities. The environment and baselines\nwill be made publicly available.\n","authors":["Shiqian Li","Kewen Wu","Chi Zhang","Yixin Zhu"],"pdf_url":"https://arxiv.org/pdf/2312.03009v2.pdf","comment":"21 pages, ICLR 2024"},{"id":"http://arxiv.org/abs/2403.16419v1","updated":"2024-03-25T04:45:16Z","published":"2024-03-25T04:45:16Z","title":"Terrain-Attentive Learning for Efficient 6-DoF Kinodynamic Modeling on\n  Vertically Challenging Terrain","summary":"  Wheeled robots have recently demonstrated superior mechanical capability to\ntraverse vertically challenging terrain (e.g., extremely rugged boulders\ncomparable in size to the vehicles themselves). Negotiating such terrain\nintroduces significant variations of vehicle pose in all six Degrees-of-Freedom\n(DoFs), leading to imbalanced contact forces, varying momentum, and chassis\ndeformation due to non-rigid tires and suspensions. To autonomously navigate on\nvertically challenging terrain, all these factors need to be efficiently\nreasoned within limited onboard computation and strict real-time constraints.\nIn this paper, we propose a 6-DoF kinodynamics learning approach that is\nattentive only to the specific underlying terrain critical to the current\nvehicle-terrain interaction, so that it can be efficiently queried in real-time\nmotion planners onboard small robots. Physical experiment results show our\nTerrain-Attentive Learning demonstrates on average 51.1% reduction in model\nprediction error among all 6 DoFs compared to a state-of-the-art model for\nvertically challenging terrain.\n","authors":["Aniket Datar","Chenhui Pan","Mohammad Nazeri","Anuj Pokhrel","Xuesu Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.16419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16400v1","updated":"2024-03-25T03:30:37Z","published":"2024-03-25T03:30:37Z","title":"ASDF: Assembly State Detection Utilizing Late Fusion by Integrating 6D\n  Pose Estimation","summary":"  In medical and industrial domains, providing guidance for assembly processes\nis critical to ensure efficiency and safety. Errors in assembly can lead to\nsignificant consequences such as extended surgery times, and prolonged\nmanufacturing or maintenance times in industry. Assembly scenarios can benefit\nfrom in-situ AR visualization to provide guidance, reduce assembly times and\nminimize errors. To enable in-situ visualization 6D pose estimation can be\nleveraged. Existing 6D pose estimation techniques primarily focus on individual\nobjects and static captures. However, assembly scenarios have various dynamics\nincluding occlusion during assembly and dynamics in the assembly objects\nappearance. Existing work, combining object detection/6D pose estimation and\nassembly state detection focuses either on pure deep learning-based approaches,\nor limit the assembly state detection to building blocks. To address the\nchallenges of 6D pose estimation in combination with assembly state detection,\nour approach ASDF builds upon the strengths of YOLOv8, a real-time capable\nobject detection framework. We extend this framework, refine the object pose\nand fuse pose knowledge with network-detected pose information. Utilizing our\nlate fusion in our Pose2State module results in refined 6D pose estimation and\nassembly state detection. By combining both pose and state information, our\nPose2State module predicts the final assembly state with precision. Our\nevaluation on our ASDF dataset shows that our Pose2State module leads to an\nimproved assembly state detection and that the improvement of the assembly\nstate further leads to a more robust 6D pose estimation. Moreover, on the GBOT\ndataset, we outperform the pure deep learning-based network, and even\noutperform the hybrid and pure tracking-based approaches.\n","authors":["Hannah Schieber","Shiyu Li","Niklas Corell","Philipp Beckerle","Julian Kreimeier","Daniel Roth"],"pdf_url":"https://arxiv.org/pdf/2403.16400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10670v2","updated":"2024-03-25T02:52:43Z","published":"2024-02-16T13:21:33Z","title":"OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via\n  Vision-Language Foundation Models","summary":"  Object navigation (ObjectNav) requires an agent to navigate through unseen\nenvironments to find queried objects. Many previous methods attempted to solve\nthis task by relying on supervised or reinforcement learning, where they are\ntrained on limited household datasets with close-set objects. However, two key\nchallenges are unsolved: understanding free-form natural language instructions\nthat demand open-set objects, and generalizing to new environments in a\nzero-shot manner. Aiming to solve the two challenges, in this paper, we propose\nOpenFMNav, an Open-set Foundation Model based framework for zero-shot object\nNavigation. We first unleash the reasoning abilities of large language models\n(LLMs) to extract proposed objects from natural language instructions that meet\nthe user's demand. We then leverage the generalizability of large vision\nlanguage models (VLMs) to actively discover and detect candidate objects from\nthe scene, building a Versatile Semantic Score Map (VSSM). Then, by conducting\ncommon sense reasoning on VSSM, our method can perform effective\nlanguage-guided exploration and exploitation of the scene and finally reach the\ngoal. By leveraging the reasoning and generalizing abilities of foundation\nmodels, our method can understand free-form human instructions and perform\neffective open-set zero-shot navigation in diverse environments. Extensive\nexperiments on the HM3D ObjectNav benchmark show that our method surpasses all\nthe strong baselines on all metrics, proving our method's effectiveness.\nFurthermore, we perform real robot demonstrations to validate our method's\nopen-set-ness and generalizability to real-world environments.\n","authors":["Yuxuan Kuang","Hai Lin","Meng Jiang"],"pdf_url":"https://arxiv.org/pdf/2402.10670v2.pdf","comment":"NAACL 2024 Findings"},{"id":"http://arxiv.org/abs/2403.16366v1","updated":"2024-03-25T02:04:06Z","published":"2024-03-25T02:04:06Z","title":"SE(3) Linear Parameter Varying Dynamical Systems for Globally\n  Asymptotically Stable End-Effector Control","summary":"  Linear Parameter Varying Dynamical Systems (LPV-DS) encode trajectories into\nan autonomous first-order DS that enables reactive responses to perturbations,\nwhile ensuring globally asymptotic stability at the target. However, the\ncurrent LPV-DS framework is established on Euclidean data only and has not been\napplicable to broader robotic applications requiring pose control. In this\npaper we present an extension to the current LPV-DS framework, named\nQuaternion-DS, which efficiently learns a DS-based motion policy for\norientation. Leveraging techniques from differential geometry and Riemannian\nstatistics, our approach properly handles the non-Euclidean orientation data in\nquaternion space, enabling the integration with positional control, namely\nSE(3) LPV-DS, so that the synergistic behaviour within the full SE(3) pose is\npreserved. Through simulation and real robot experiments, we validate our\nmethod, demonstrating its ability to efficiently and accurately reproduce the\noriginal SE(3) trajectory while exhibiting strong robustness to perturbations\nin task space.\n","authors":["Sunan Sun","Nadia Figueroa"],"pdf_url":"https://arxiv.org/pdf/2403.16366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.02609v3","updated":"2024-03-25T01:50:40Z","published":"2023-09-05T22:53:37Z","title":"Directionality-Aware Mixture Model Parallel Sampling for Efficient\n  Linear Parameter Varying Dynamical System Learning","summary":"  The Linear Parameter Varying Dynamical System (LPV-DS) is an effective\napproach that learns stable, time-invariant motion policies using statistical\nmodeling and semi-definite optimization to encode complex motions for reactive\nrobot control. Despite its strengths, the LPV-DS learning approach faces\nchallenges in achieving a high model accuracy without compromising the\ncomputational efficiency. To address this, we introduce the\nDirectionality-Aware Mixture Model (DAMM), a novel statistical model that\napplies the Riemannian metric on the n-sphere $\\mathbb{S}^n$ to efficiently\nblend non-Euclidean directional data with $\\mathbb{R}^m$ Euclidean states.\nAdditionally, we develop a hybrid Markov chain Monte Carlo technique that\ncombines Gibbs Sampling with Split/Merge Proposal, allowing for parallel\ncomputation to drastically speed up inference. Our extensive empirical tests\ndemonstrate that LPV-DS integrated with DAMM achieves higher reproduction\naccuracy, better model efficiency, and near real-time/online learning compared\nto standard estimation methods on various datasets. Lastly, we demonstrate its\nsuitability for incrementally learning multi-behavior policies in real-world\nrobot experiments.\n","authors":["Sunan Sun","Haihui Gao","Tianyu Li","Nadia Figueroa"],"pdf_url":"https://arxiv.org/pdf/2309.02609v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16356v1","updated":"2024-03-25T01:33:03Z","published":"2024-03-25T01:33:03Z","title":"Bipedal Safe Navigation over Uncertain Rough Terrain: Unifying Terrain\n  Mapping and Locomotion Stability","summary":"  We study the problem of bipedal robot navigation in complex environments with\nuncertain and rough terrain. In particular, we consider a scenario in which the\nrobot is expected to reach a desired goal location by traversing an environment\nwith uncertain terrain elevation. Such terrain uncertainties induce not only\nuntraversable regions but also robot motion perturbations. Thus, the problems\nof terrain mapping and locomotion stability are intertwined. We evaluate three\ndifferent kernels for Gaussian process (GP) regression to learn the terrain\nelevation. We also learn the motion deviation resulting from both the terrain\nas well as the discrepancy between the reduced-order Prismatic Inverted\nPendulum Model used for planning and the full-order locomotion dynamics. We\npropose a hierarchical locomotion-dynamics-aware sampling-based navigation\nplanner. The global navigation planner plans a series of local waypoints to\nreach the desired goal locations while respecting locomotion stability\nconstraints. Then, a local navigation planner is used to generate a sequence of\ndynamically feasible footsteps to reach local waypoints. We develop a novel\ntrajectory evaluation metric to minimize motion deviation and maximize\ninformation gain of the terrain elevation map. We evaluate the efficacy of our\nplanning framework on Digit bipedal robot simulation in MuJoCo.\n","authors":["Kasidit Muenprasitivej","Jesse Jiang","Abdulaziz Shamsah","Samuel Coogan","Ye Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.16356v1.pdf","comment":"9 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.09900v2","updated":"2024-03-25T01:29:27Z","published":"2024-03-14T22:22:22Z","title":"DTG : Diffusion-based Trajectory Generation for Mapless Global\n  Navigation","summary":"  We present a novel end-to-end diffusion-based trajectory generation method,\nDTG, for mapless global navigation in challenging outdoor scenarios with\nocclusions and unstructured off-road features like grass, buildings, bushes,\netc. Given a distant goal, our approach computes a trajectory that satisfies\nthe following goals: (1) minimize the travel distance to the goal; (2) maximize\nthe traversability by choosing paths that do not lie in undesirable areas.\nSpecifically, we present a novel Conditional RNN(CRNN) for diffusion models to\nefficiently generate trajectories. Furthermore, we propose an adaptive training\nmethod that ensures that the diffusion model generates more traversable\ntrajectories. We evaluate our methods in various outdoor scenes and compare the\nperformance with other global navigation algorithms on a Husky robot. In\npractice, we observe at least a 15% improvement in traveling distance and\naround a 7% improvement in traversability.\n","authors":["Jing Liang","Amirreza Payandeh","Daeun Song","Xuesu Xiao","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2403.09900v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2403.14887v2","updated":"2024-03-25T19:25:21Z","published":"2024-03-21T23:44:42Z","title":"GelLink: A Compact Multi-phalanx Finger with Vision-based Tactile\n  Sensing and Proprioception","summary":"  Compared to fully-actuated robotic end-effectors, underactuated ones are\ngenerally more adaptive, robust, and cost-effective. However, state estimation\nfor underactuated hands is usually more challenging. Vision-based tactile\nsensors, like Gelsight, can mitigate this issue by providing high-resolution\ntactile sensing and accurate proprioceptive sensing. As such, we present\nGelLink, a compact, underactuated, linkage-driven robotic finger with low-cost,\nhigh-resolution vision-based tactile sensing and proprioceptive sensing\ncapabilities. In order to reduce the amount of embedded hardware, i.e. the\ncameras and motors, we optimize the linkage transmission with a planar linkage\nmechanism simulator and develop a planar reflection simulator to simplify the\ntactile sensing hardware. As a result, GelLink only requires one motor to\nactuate the three phalanges, and one camera to capture tactile signals along\nthe entire finger. Overall, GelLink is a compact robotic finger that shows\nadaptability and robustness when performing grasping tasks. The integration of\nvision-based tactile sensors can significantly enhance the capabilities of\nunderactuated fingers and potentially broaden their future usage.\n","authors":["Yuxiang Ma","Jialiang Zhao","Edward Adelson"],"pdf_url":"https://arxiv.org/pdf/2403.14887v2.pdf","comment":"Supplement video: https://www.youtube.com/watch?v=hZwUpAig5C0 . 7\n  pages, 9 figures. ICRA 2024 (IEEE International Conference on Robotics and\n  Automation)"},{"id":"http://arxiv.org/abs/2403.17270v1","updated":"2024-03-25T23:28:57Z","published":"2024-03-25T23:28:57Z","title":"Human Stress Response and Perceived Safety during Encounters with\n  Quadruped Robots","summary":"  Despite the rise of mobile robot deployments in home and work settings,\nperceived safety of users and bystanders is understudied in the human-robot\ninteraction (HRI) literature. To address this, we present a study designed to\nidentify elements of a human-robot encounter that correlate with observed\nstress response. Stress is a key component of perceived safety and is strongly\nassociated with human physiological response. In this study a Boston Dynamics\nSpot and a Unitree Go1 navigate autonomously through a shared environment\noccupied by human participants wearing multimodal physiological sensors to\ntrack their electrocardiography (ECG) and electrodermal activity (EDA). The\nencounters are varied through several trials and participants self-rate their\nstress levels after each encounter. The study resulted in a multidimensional\ndataset archiving various objective and subjective aspects of a human-robot\nencounter, containing insights for understanding perceived safety in such\nencounters. To this end, acute stress responses were decoded from the human\nparticipants' ECG and EDA and compared across different human-robot encounter\nconditions. Statistical analysis of data indicate that on average (1)\nparticipants feel more stress during encounters compared to baselines, (2)\nparticipants feel more stress encountering multiple robots compared to a single\nrobot and (3) participants stress increases during navigation behavior compared\nwith search behavior.\n","authors":["Ryan Gupta","Hyonyoung Shin","Emily Norman","Keri K. Stephens","Nanshu Lu","Luis Sentis"],"pdf_url":"https://arxiv.org/pdf/2403.17270v1.pdf","comment":"7 pages, 7 figs, 5 tables"},{"id":"http://arxiv.org/abs/2309.14150v8","updated":"2024-03-25T23:25:17Z","published":"2023-09-25T14:04:31Z","title":"Fast LiDAR Informed Visual Search in Unseen Indoor Environments","summary":"  This paper explores the problem of planning for visual search without prior\nmap information. We leverage the pixel-wise environment perception problem\nwhere one is given wide Field of View 2D scan data and must perform LiDAR\nsegmentation to contextually label points in the surroundings. These pixel\nclassifications provide an informed prior on which to plan next best viewpoints\nduring visual search tasks. We present LIVES: LiDAR Informed Visual Search, a\nmethod aimed at finding objects of interest in unknown indoor environments. A\nrobust map-free classifier is trained from expert data collected using a simple\ncart platform equipped with a map-based classifier. An autonomous exploration\nplanner takes the contextual data from scans and uses that prior to plan\nviewpoints more likely to yield detection of the search target. We propose a\nutility function that accounts for traditional metrics like information gain\nand path cost and for the contextual information. LIVES is baselined against\nseveral existing exploration methods in simulation to verify its performance.\nIt is validated in real-world experiments with single and multiple search\nobjects with a Spot robot in two unseen environments. Videos of experiments,\nimplementation details and open source code can be found at\nhttps://sites.google.com/view/lives-2024/home.\n","authors":["Ryan Gupta","Kyle Morgenstein","Steven Ortega","Luis Sentis"],"pdf_url":"https://arxiv.org/pdf/2309.14150v8.pdf","comment":"6 pages + references. 6 figures. 1 algorithm. 1 table"},{"id":"http://arxiv.org/abs/2403.17266v1","updated":"2024-03-25T23:19:19Z","published":"2024-03-25T23:19:19Z","title":"Exploring CausalWorld: Enhancing robotic manipulation via knowledge\n  transfer and curriculum learning","summary":"  This study explores a learning-based tri-finger robotic arm manipulating\ntask, which requires complex movements and coordination among the fingers. By\nemploying reinforcement learning, we train an agent to acquire the necessary\nskills for proficient manipulation. To enhance the efficiency and effectiveness\nof the learning process, two knowledge transfer strategies, fine-tuning and\ncurriculum learning, were utilized within the soft actor-critic architecture.\nFine-tuning allows the agent to leverage pre-trained knowledge and adapt it to\nnew tasks. Several variations like model transfer, policy transfer, and\nacross-task transfer were implemented and evaluated. To eliminate the need for\npretraining, curriculum learning decomposes the advanced task into simpler,\nprogressive stages, mirroring how humans learn. The number of learning stages,\nthe context of the sub-tasks, and the transition timing were found to be the\ncritical design parameters. The key factors of two learning strategies and\ncorresponding effects were explored in context-aware and context-unaware\nscenarios, enabling us to identify the scenarios where the methods demonstrate\noptimal performance, derive conclusive insights, and contribute to a broader\nrange of learning-based engineering applications.\n","authors":["Xinrui Wang","Yan Jin"],"pdf_url":"https://arxiv.org/pdf/2403.17266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12465v2","updated":"2024-03-25T22:57:28Z","published":"2024-03-19T05:46:20Z","title":"Diagrammatic Instructions to Specify Spatial Objectives and Constraints\n  with Applications to Mobile Base Placement","summary":"  This paper introduces Spatial Diagrammatic Instructions (SDIs), an approach\nfor human operators to specify objectives and constraints that are related to\nspatial regions in the working environment. Human operators are enabled to\nsketch out regions directly on camera images that correspond to the objectives\nand constraints. These sketches are projected to 3D spatial coordinates, and\ncontinuous Spatial Instruction Maps (SIMs) are learned upon them. These maps\ncan then be integrated into optimization problems for tasks of robots. In\nparticular, we demonstrate how Spatial Diagrammatic Instructions can be applied\nto solve the Base Placement Problem of mobile manipulators, which concerns the\nbest place to put the manipulator to facilitate a certain task. Human operators\ncan specify, via sketch, spatial regions of interest for a manipulation task\nand permissible regions for the mobile manipulator to be at. Then, an\noptimization problem that maximizes the manipulator's reachability, or\ncoverage, over the designated regions of interest while remaining in the\npermissible regions is solved. We provide extensive empirical evaluations, and\nshow that our formulation of Spatial Instruction Maps provides accurate\nrepresentations of user-specified diagrammatic instructions. Furthermore, we\ndemonstrate that our diagrammatic approach to the Mobile Base Placement Problem\nenables higher quality solutions and faster run-time.\n","authors":["Qilin Sun","Weiming Zhi","Tianyi Zhang","Matthew Johnson-Roberson"],"pdf_url":"https://arxiv.org/pdf/2403.12465v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17249v1","updated":"2024-03-25T22:51:27Z","published":"2024-03-25T22:51:27Z","title":"Impact-Aware Bimanual Catching of Large-Momentum Objects","summary":"  This paper investigates one of the most challenging tasks in dynamic\nmanipulation -- catching large-momentum moving objects. Beyond the realm of\nquasi-static manipulation, dealing with highly dynamic objects can\nsignificantly improve the robot's capability of interacting with its\nsurrounding environment. Yet, the inevitable motion mismatch between the fast\nmoving object and the approaching robot will result in large impulsive forces,\nwhich lead to the unstable contacts and irreversible damage to both the object\nand the robot. To address the above problems, we propose an online optimization\nframework to: 1) estimate and predict the linear and angular motion of the\nobject; 2) search and select the optimal contact locations across every surface\nof the object to mitigate impact through sequential quadratic programming\n(SQP); 3) simultaneously optimize the end-effector motion, stiffness, and\ncontact force for both robots using multi-mode trajectory optimization (MMTO);\nand 4) realise the impact-aware catching motion on the compliant robotic system\nbased on indirect force controller. We validate the impulse distribution,\ncontact selection, and impact-aware MMTO algorithms in simulation and\ndemonstrate the benefits of the proposed framework in real-world experiments\nincluding catching large-momentum moving objects with well-defined motion,\nconstrained motion and free-flying motion.\n","authors":["Lei Yan","Theodoros Stouraitis","João Moura","Wenfu Xu","Michael Gienger","Sethu Vijayakumar"],"pdf_url":"https://arxiv.org/pdf/2403.17249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17247v1","updated":"2024-03-25T22:49:56Z","published":"2024-03-25T22:49:56Z","title":"DASA: Delay-Adaptive Multi-Agent Stochastic Approximation","summary":"  We consider a setting in which $N$ agents aim to speedup a common Stochastic\nApproximation (SA) problem by acting in parallel and communicating with a\ncentral server. We assume that the up-link transmissions to the server are\nsubject to asynchronous and potentially unbounded time-varying delays. To\nmitigate the effect of delays and stragglers while reaping the benefits of\ndistributed computation, we propose \\texttt{DASA}, a Delay-Adaptive algorithm\nfor multi-agent Stochastic Approximation. We provide a finite-time analysis of\n\\texttt{DASA} assuming that the agents' stochastic observation processes are\nindependent Markov chains. Significantly advancing existing results,\n\\texttt{DASA} is the first algorithm whose convergence rate depends only on the\nmixing time $\\tmix$ and on the average delay $\\tau_{avg}$ while jointly\nachieving an $N$-fold convergence speedup under Markovian sampling. Our work is\nrelevant for various SA applications, including multi-agent and distributed\ntemporal difference (TD) learning, Q-learning and stochastic optimization with\ncorrelated data.\n","authors":["Nicolo Dal Fabbro","Arman Adibi","H. Vincent Poor","Sanjeev R. Kulkarni","Aritra Mitra","George J. Pappas"],"pdf_url":"https://arxiv.org/pdf/2403.17247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17246v1","updated":"2024-03-25T22:47:13Z","published":"2024-03-25T22:47:13Z","title":"TwoStep: Multi-agent Task Planning using Classical Planners and Large\n  Language Models","summary":"  Classical planning formulations like the Planning Domain Definition Language\n(PDDL) admit action sequences guaranteed to achieve a goal state given an\ninitial state if any are possible. However, reasoning problems defined in PDDL\ndo not capture temporal aspects of action taking, for example that two agents\nin the domain can execute an action simultaneously if postconditions of each do\nnot interfere with preconditions of the other. A human expert can decompose a\ngoal into largely independent constituent parts and assign each agent to one of\nthese subgoals to take advantage of simultaneous actions for faster execution\nof plan steps, each using only single agent planning. By contrast, large\nlanguage models (LLMs) used for directly inferring plan steps do not guarantee\nexecution success, but do leverage commonsense reasoning to assemble action\nsequences. We combine the strengths of classical planning and LLMs by\napproximating human intuitions for two-agent planning goal decomposition. We\ndemonstrate that LLM-based goal decomposition leads to faster planning times\nthan solving multi-agent PDDL problems directly while simultaneously achieving\nfewer plan execution steps than a single agent plan alone and preserving\nexecution success. Additionally, we find that LLM-based approximations of\nsubgoals can achieve similar multi-agent execution steps than those specified\nby human experts. Website and resources at https://glamor-usc.github.io/twostep\n","authors":["Ishika Singh","David Traum","Jesse Thomason"],"pdf_url":"https://arxiv.org/pdf/2403.17246v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2403.17238v1","updated":"2024-03-25T22:39:20Z","published":"2024-03-25T22:39:20Z","title":"Temporal and Semantic Evaluation Metrics for Foundation Models in\n  Post-Hoc Analysis of Robotic Sub-tasks","summary":"  Recent works in Task and Motion Planning (TAMP) show that training control\npolicies on language-supervised robot trajectories with quality labeled data\nmarkedly improves agent task success rates. However, the scarcity of such data\npresents a significant hurdle to extending these methods to general use cases.\nTo address this concern, we present an automated framework to decompose\ntrajectory data into temporally bounded and natural language-based descriptive\nsub-tasks by leveraging recent prompting strategies for Foundation Models (FMs)\nincluding both Large Language Models (LLMs) and Vision Language Models (VLMs).\nOur framework provides both time-based and language-based descriptions for\nlower-level sub-tasks that comprise full trajectories. To rigorously evaluate\nthe quality of our automatic labeling framework, we contribute an algorithm\nSIMILARITY to produce two novel metrics, temporal similarity and semantic\nsimilarity. The metrics measure the temporal alignment and semantic fidelity of\nlanguage descriptions between two sub-task decompositions, namely an FM\nsub-task decomposition prediction and a ground-truth sub-task decomposition. We\npresent scores for temporal similarity and semantic similarity above 90%,\ncompared to 30% of a randomized baseline, for multiple robotic environments,\ndemonstrating the effectiveness of our proposed framework. Our results enable\nbuilding diverse, large-scale, language-supervised datasets for improved\nrobotic TAMP.\n","authors":["Jonathan Salfity","Selma Wanna","Minkyu Choi","Mitch Pryor"],"pdf_url":"https://arxiv.org/pdf/2403.17238v1.pdf","comment":"8 pages, 3 figures. IROS 2024 Submission"},{"id":"http://arxiv.org/abs/2403.17234v1","updated":"2024-03-25T22:21:23Z","published":"2024-03-25T22:21:23Z","title":"Speeding Up Path Planning via Reinforcement Learning in MCTS for\n  Automated Parking","summary":"  In this paper, we address a method that integrates reinforcement learning\ninto the Monte Carlo tree search to boost online path planning under fully\nobservable environments for automated parking tasks. Sampling-based planning\nmethods under high-dimensional space can be computationally expensive and\ntime-consuming. State evaluation methods are useful by leveraging the prior\nknowledge into the search steps, making the process faster in a real-time\nsystem. Given the fact that automated parking tasks are often executed under\ncomplex environments, a solid but lightweight heuristic guidance is challenging\nto compose in a traditional analytical way. To overcome this limitation, we\npropose a reinforcement learning pipeline with a Monte Carlo tree search under\nthe path planning framework. By iteratively learning the value of a state and\nthe best action among samples from its previous cycle's outcomes, we are able\nto model a value estimator and a policy generator for given states. By doing\nthat, we build up a balancing mechanism between exploration and exploitation,\nspeeding up the path planning process while maintaining its quality without\nusing human expert driver data.\n","authors":["Xinlong Zheng","Xiaozhou Zhang","Donghao Xu"],"pdf_url":"https://arxiv.org/pdf/2403.17234v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17232v1","updated":"2024-03-25T22:19:58Z","published":"2024-03-25T22:19:58Z","title":"PROSPECT: Precision Robot Spectroscopy Exploration and Characterization\n  Tool","summary":"  Near Infrared (NIR) spectroscopy is widely used in industrial quality control\nand automation to test the purity and material quality of items. In this\nresearch, we propose a novel sensorized end effector and acquisition strategy\nto capture spectral signatures from objects and register them with a 3D point\ncloud. Our methodology first takes a 3D scan of an object generated by a\ntime-of-flight depth camera and decomposes the object into a series of planned\nviewpoints covering the surface. We generate motion plans for a robot\nmanipulator and end-effector to visit these viewpoints while maintaining a\nfixed distance and surface normal to ensure maximal spectral signal quality\nenabled by the spherical motion of the end-effector. By continuously acquiring\nsurface reflectance values as the end-effector scans the target object, the\nautonomous system develops a four-dimensional model of the target object:\nposition in an R^3 coordinate frame, and a wavelength vector denoting the\nassociated spectral signature. We demonstrate this system in building\nspectral-spatial object profiles of increasingly complex geometries. As a point\nof comparison, we show our proposed system and spectral acquisition planning\nyields more consistent signal signals than naive point scanning strategies for\ncapturing spectral information over complex surface geometries. Our work\nrepresents a significant step towards high-resolution spectral-spatial sensor\nfusion for automated quality assessment.\n","authors":["Nathaniel Hanson","Gary Lvov","Vedant Rautela","Samuel Hibbard","Ethan Holand","Charles DiMarzio","Taşkın Padır"],"pdf_url":"https://arxiv.org/pdf/2403.17232v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17231v1","updated":"2024-03-25T22:17:51Z","published":"2024-03-25T22:17:51Z","title":"Dyna-LfLH: Learning Agile Navigation in Dynamic Environments from\n  Learned Hallucination","summary":"  This paper presents a self-supervised learning method to safely learn a\nmotion planner for ground robots to navigate environments with dense and\ndynamic obstacles. When facing highly-cluttered, fast-moving, hard-to-predict\nobstacles, classical motion planners may not be able to keep up with limited\nonboard computation. For learning-based planners, high-quality demonstrations\nare difficult to acquire for imitation learning while reinforcement learning\nbecomes inefficient due to the high probability of collision during\nexploration. To safely and efficiently provide training data, the Learning from\nHallucination (LfH) approaches synthesize difficult navigation environments\nbased on past successful navigation experiences in relatively easy or\ncompletely open ones, but unfortunately cannot address dynamic obstacles. In\nour new Dynamic Learning from Learned Hallucination (Dyna-LfLH), we design and\nlearn a novel latent distribution and sample dynamic obstacles from it, so the\ngenerated training data can be used to learn a motion planner to navigate in\ndynamic environments. Dyna-LfLH is evaluated on a ground robot in both\nsimulated and physical environments and achieves up to 25% better success rate\ncompared to baselines.\n","authors":["Saad Abdul Ghani","Zizhao Wang","Peter Stone","Xuesu Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.17231v1.pdf","comment":"Submitted to International Conference on Intelligent Robots and\n  Systems (IROS) 2024"},{"id":"http://arxiv.org/abs/2310.10863v2","updated":"2024-03-25T20:46:28Z","published":"2023-10-16T22:23:18Z","title":"Greedy Perspectives: Multi-Drone View Planning for Collaborative\n  Perception in Cluttered Environments","summary":"  Deployment of teams of aerial robots could enable large-scale filming of\ndynamic groups of people (actors) in complex environments for applications in\nareas such as team sports and cinematography. Toward this end, methods for\nsubmodular maximization via sequential greedy planning can be used for scalable\noptimization of camera views across teams of robots but face challenges with\nefficient coordination in cluttered environments. Obstacles can produce\nocclusions and increase chances of inter-robot collision which can violate\nrequirements for near-optimality guarantees. To coordinate teams of aerial\nrobots in filming groups of people in dense environments, a more general\nview-planning approach is required. We explore how collision and occlusion\nimpact performance in filming applications through the development of a\nmulti-robot multi-actor view planner with an occlusion-aware objective for\nfilming groups of people and compare with a formation planner and a greedy\nplanner that ignores inter-robot collisions. We evaluate our approach based on\nfive test environments and complex multi-actor behaviors. Compared with a\nformation planner, our sequential planner generates 14% greater view reward\nover the actors for three scenarios and comparable performance to formation\nplanning on two others. We also observe near identical view rewards for\nsequential planning both with and without inter-robot collision constraints\nwhich indicates that robots are able to avoid collisions without impairing\nperformance in the perception task. Overall, we demonstrate effective\ncoordination of teams of aerial robots for filming groups that may split,\nmerge, or spread apart and in environments cluttered with obstacles that may\ncause collisions or occlusions.\n","authors":["Krishna Suresh","Aditya Rauniyar","Micah Corah","Sebastian Scherer"],"pdf_url":"https://arxiv.org/pdf/2310.10863v2.pdf","comment":"Submitted to IROS'24; 8 pages, 8 figures, 2 tables"},{"id":"http://arxiv.org/abs/2309.10275v2","updated":"2024-03-25T20:28:22Z","published":"2023-09-19T03:02:43Z","title":"Optimizing Crowd-Aware Multi-Agent Path Finding through Local\n  Broadcasting with Graph Neural Networks","summary":"  Multi-Agent Path Finding (MAPF) in crowded environments presents a\nchallenging problem in motion planning, aiming to find collision-free paths for\nall agents in the system. MAPF finds a wide range of applications in various\ndomains, including aerial swarms, autonomous warehouse robotics, and\nself-driving vehicles. Current approaches to MAPF generally fall into two main\ncategories: centralized and decentralized planning. Centralized planning\nsuffers from the curse of dimensionality when the number of agents or states\nincreases and thus does not scale well in large and complex environments. On\nthe other hand, decentralized planning enables agents to engage in real-time\npath planning within a partially observable environment, demonstrating implicit\ncoordination. However, they suffer from slow convergence and performance\ndegradation in dense environments. In this paper, we introduce CRAMP, a novel\ncrowd-aware decentralized reinforcement learning approach to address this\nproblem by enabling efficient local communication among agents via Graph Neural\nNetworks (GNNs), facilitating situational awareness and decision-making\ncapabilities in congested environments. We test CRAMP on simulated environments\nand demonstrate that our method outperforms the state-of-the-art decentralized\nmethods for MAPF on various metrics. CRAMP improves the solution quality up to\n59% measured in makespan and collision count, and up to 35% improvement in\nsuccess rate in comparison to previous methods.\n","authors":["Phu Pham","Aniket Bera"],"pdf_url":"https://arxiv.org/pdf/2309.10275v2.pdf","comment":"8 pages, 5 figures, 2 tables"},{"id":"http://arxiv.org/abs/2305.18983v3","updated":"2024-03-25T20:21:25Z","published":"2023-05-30T12:27:47Z","title":"SO(2)-Equivariant Downwash Models for Close Proximity Flight","summary":"  Multirotors flying in close proximity induce aerodynamic wake effects on each\nother through propeller downwash. Conventional methods have fallen short of\nproviding adequate 3D force-based models that can be incorporated into robust\ncontrol paradigms for deploying dense formations. Thus, learning a model for\nthese downwash patterns presents an attractive solution. In this paper, we\npresent a novel learning-based approach for modelling the downwash forces that\nexploits the latent geometries (i.e. symmetries) present in the problem. We\ndemonstrate that when trained with only 5 minutes of real-world flight data,\nour geometry-aware model outperforms state-of-the-art baseline models trained\nwith more than 15 minutes of data. In dense real-world flights with two\nvehicles, deploying our model online improves 3D trajectory tracking by nearly\n36% on average (and vertical tracking by 56%).\n","authors":["H. Smith","A. Shankar","J. Gielis","J. Blumenkamp","A. Prorok"],"pdf_url":"https://arxiv.org/pdf/2305.18983v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17161v1","updated":"2024-03-25T20:18:12Z","published":"2024-03-25T20:18:12Z","title":"Multi-Contact Inertial Estimation and Localization in Legged Robots","summary":"  Optimal estimation is a promising tool for multi-contact inertial estimation\nand localization. To harness its advantages in robotics, it is crucial to solve\nthese large and challenging optimization problems efficiently. To tackle this,\nwe (i) develop a multiple-shooting solver that exploits both temporal and\nparametric structures through a parametrized Riccati recursion. Additionally,\nwe (ii) propose an inertial local manifold that ensures its full physical\nconsistency. It also enhances convergence compared to the singularity-free\nlog-Cholesky approach. To handle its singularities, we (iii) introduce a\nnullspace approach in our optimal estimation solver. We (iv) finally develop\nthe analytical derivatives of contact dynamics for both inertial\nparametrizations. Our framework can successfully solve estimation problems for\ncomplex maneuvers such as brachiation in humanoids. We demonstrate its\nnumerical capabilities across various robotics tasks and its benefits in\nexperimental trials with the Go1 robot.\n","authors":["Sergi Martinez","Robert Griffin","Carlos Mastalli"],"pdf_url":"https://arxiv.org/pdf/2403.17161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01519v2","updated":"2024-03-25T20:14:46Z","published":"2023-10-02T18:11:01Z","title":"Decision-Oriented Learning Using Differentiable Submodular Maximization\n  for Multi-Robot Coordination","summary":"  We present a differentiable, decision-oriented learning framework for cost\nprediction in a class of multi-robot decision-making problems, in which the\nrobots need to trade off the task performance with the costs of taking actions\nwhen they select actions to take. Specifically, we consider the cases where the\ntask performance is measured by a known monotone submodular function (e.g.,\ncoverage, mutual information), and the cost of actions depends on the context\n(e.g., wind and terrain conditions). We need to learn a function that maps the\ncontext to the costs. Classically, we treat such a learning problem and the\ndownstream decision-making problem as two decoupled problems, i.e., we first\nlearn to predict the cost function without considering the downstream\ndecision-making problem, and then use the learned function for predicting the\ncost and using it in the decision-making problem. However, the loss function\nused in learning a prediction function may not be aligned with the downstream\ndecision-making. We propose a decision-oriented learning framework that\nincorporates the downstream task performance in the prediction phase via a\ndifferentiable optimization layer. The main computational challenge in such a\nframework is to make the combinatorial optimization, i.e., non-monotone\nsubmodular maximization, differentiable. This function is not naturally\ndifferentiable. We propose the Differentiable Cost Scaled Greedy algorithm\n(D-CSG), which is a continuous and differentiable relaxation of CSG. We\ndemonstrate the efficacy of the proposed framework through numerical\nsimulations. The results show that the proposed framework can result in better\nperformance than the traditional two-stage approach.\n","authors":["Guangyao Shi","Chak Lam Shek","Nare Karapetyan","Pratap Tokekar"],"pdf_url":"https://arxiv.org/pdf/2310.01519v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2303.01543"},{"id":"http://arxiv.org/abs/2403.17147v1","updated":"2024-03-25T19:50:07Z","published":"2024-03-25T19:50:07Z","title":"Hearing the shape of an arena with spectral swarm robotics","summary":"  Swarm robotics promises adaptability to unknown situations and robustness\nagainst failures. However, it still struggles with global tasks that require\nunderstanding the broader context in which the robots operate, such as\nidentifying the shape of the arena in which the robots are embedded. Biological\nswarms, such as shoals of fish, flocks of birds, and colonies of insects,\nroutinely solve global geometrical problems through the diffusion of local\ncues. This paradigm can be explicitly described by mathematical models that\ncould be directly computed and exploited by a robotic swarm. Diffusion over a\ndomain is mathematically encapsulated by the Laplacian, a linear operator that\nmeasures the local curvature of a function. Crucially the geometry of a domain\ncan generally be reconstructed from the eigenspectrum of its Laplacian. Here we\nintroduce spectral swarm robotics where robots diffuse information to their\nneighbors to emulate the Laplacian operator - enabling them to \"hear\" the\nspectrum of their arena. We reveal a universal scaling that links the optimal\nnumber of robots (a global parameter) with their optimal radius of interaction\n(a local parameter). We validate experimentally spectral swarm robotics under\nchallenging conditions with the one-shot classification of arena shapes using a\nsparse swarm of Kilobots. Spectral methods can assist with challenging tasks\nwhere robots need to build an emergent consensus on their environment, such as\nadaptation to unknown terrains, division of labor, or quorum sensing. Spectral\nmethods may extend beyond robotics to analyze and coordinate swarms of agents\nof various natures, such as traffic or crowds, and to better understand the\nlong-range dynamics of natural systems emerging from short-range interactions.\n","authors":["Leo Cazenille","Nicolas Lobato-Dauzier","Alessia Loi","Mika Ito","Olivier Marchal","Nathanael Aubert-Kato","Nicolas Bredeche","Anthony J. Genot"],"pdf_url":"https://arxiv.org/pdf/2403.17147v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07021v2","updated":"2024-03-25T19:46:25Z","published":"2023-10-10T21:16:29Z","title":"Pre-Trained Masked Image Model for Mobile Robot Navigation","summary":"  2D top-down maps are commonly used for the navigation and exploration of\nmobile robots through unknown areas. Typically, the robot builds the navigation\nmaps incrementally from local observations using onboard sensors. Recent works\nhave shown that predicting the structural patterns in the environment through\nlearning-based approaches can greatly enhance task efficiency. While many such\nworks build task-specific networks using limited datasets, we show that the\nexisting foundational vision networks can accomplish the same without any\nfine-tuning. Specifically, we use Masked Autoencoders, pre-trained on street\nimages, to present novel applications for field-of-view expansion, single-agent\ntopological exploration, and multi-agent exploration for indoor mapping, across\ndifferent input modalities. Our work motivates the use of foundational vision\nmodels for generalized structure prediction-driven applications, especially in\nthe dearth of training data. For more qualitative results see\nhttps://raaslab.org/projects/MIM4Robots.\n","authors":["Vishnu Dutt Sharma","Anukriti Singh","Pratap Tokekar"],"pdf_url":"https://arxiv.org/pdf/2310.07021v2.pdf","comment":"Accepted at ICRA 2024"},{"id":"http://arxiv.org/abs/2403.17136v1","updated":"2024-03-25T19:18:25Z","published":"2024-03-25T19:18:25Z","title":"Adaptive Step Duration for Precise Foot Placement: Achieving Robust\n  Bipedal Locomotion on Terrains with Restricted Footholds","summary":"  This paper introduces a novel multi-step preview foot placement planning\nalgorithm designed to enhance the robustness of bipedal robotic walking across\nchallenging terrains with restricted footholds. Traditional one-step preview\nplanning struggles to maintain stability when stepping areas are severely\nlimited, such as with random stepping stones. In this work, we developed a\ndiscrete-time Model Predictive Control (MPC) based on the step-to-step discrete\nevolution of the Divergent Component of Motion (DCM) of bipedal locomotion.\nThis approach adaptively changes the step duration for optimal foot placement\nunder constraints, thereby ensuring the robot's operational viability over\nmultiple future steps and significantly improving its ability to navigate\nthrough environments with tight constraints on possible footholds. The\neffectiveness of this planning algorithm is demonstrated through simulations\nthat include a variety of complex stepping-stone configurations and external\nperturbations. These tests underscore the algorithm's improved performance for\nnavigating foothold-restricted environments, even with the presence of external\ndisturbances.\n","authors":["Zhaoyang Xiang","Victor Paredes","Ayonga Hereid"],"pdf_url":"https://arxiv.org/pdf/2403.17136v1.pdf","comment":"8 pages, 8 figures, submitted to CDC 2024, for associated simulation\n  video, see https://youtu.be/2jhikPlZmbE"},{"id":"http://arxiv.org/abs/2403.05972v3","updated":"2024-03-25T19:15:44Z","published":"2024-03-09T17:37:05Z","title":"C3D: Cascade Control with Change Point Detection and Deep Koopman\n  Learning for Autonomous Surface Vehicles","summary":"  In this paper, we discuss the development and deployment of a robust\nautonomous system capable of performing various tasks in the maritime domain\nunder unknown dynamic conditions. We investigate a data-driven approach based\non modular design for ease of transfer of autonomy across different maritime\nsurface vessel platforms. The data-driven approach alleviates issues related to\na priori identification of system models that may become deficient under\nevolving system behaviors or shifting, unanticipated, environmental influences.\nOur proposed learning-based platform comprises a deep Koopman system model and\na change point detector that provides guidance on domain shifts prompting\nrelearning under severe exogenous and endogenous perturbations. Motion control\nof the autonomous system is achieved via an optimal controller design. The\nKoopman linearized model naturally lends itself to a linear-quadratic regulator\n(LQR) control design. We propose the C3D control architecture Cascade Control\nwith Change Point Detection and Deep Koopman Learning. The framework is\nverified in station keeping task on an ASV in both simulation and real\nexperiments. The approach achieved at least 13.9 percent improvement in mean\ndistance error in all test cases compared to the methods that do not consider\nsystem changes.\n","authors":["Jianwen Li","Hyunsang Park","Wenjian Hao","Lei Xin","Jalil Chavez-Galaviz","Ajinkya Chaudhary","Meredith Bloss","Kyle Pattison","Christopher Vo","Devesh Upadhyay","Shreyas Sundaram","Shaoshuai Mou","Nina Mahmoudian"],"pdf_url":"https://arxiv.org/pdf/2403.05972v3.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2403.17124v1","updated":"2024-03-25T19:04:59Z","published":"2024-03-25T19:04:59Z","title":"Grounding Language Plans in Demonstrations Through Counterfactual\n  Perturbations","summary":"  Grounding the common-sense reasoning of Large Language Models in physical\ndomains remains a pivotal yet unsolved problem for embodied AI. Whereas prior\nworks have focused on leveraging LLMs directly for planning in symbolic spaces,\nthis work uses LLMs to guide the search of task structures and constraints\nimplicit in multi-step demonstrations. Specifically, we borrow from\nmanipulation planning literature the concept of mode families, which group\nrobot configurations by specific motion constraints, to serve as an abstraction\nlayer between the high-level language representations of an LLM and the\nlow-level physical trajectories of a robot. By replaying a few human\ndemonstrations with synthetic perturbations, we generate coverage over the\ndemonstrations' state space with additional successful executions as well as\ncounterfactuals that fail the task. Our explanation-based learning framework\ntrains an end-to-end differentiable neural network to predict successful\ntrajectories from failures and as a by-product learns classifiers that ground\nlow-level states and images in mode families without dense labeling. The\nlearned grounding classifiers can further be used to translate language plans\ninto reactive policies in the physical domain in an interpretable manner. We\nshow our approach improves the interpretability and reactivity of imitation\nlearning through 2D navigation and simulated and real robot manipulation tasks.\nWebsite: https://sites.google.com/view/grounding-plans\n","authors":["Yanwei Wang","Tsun-Hsuan Wang","Jiayuan Mao","Michael Hagenow","Julie Shah"],"pdf_url":"https://arxiv.org/pdf/2403.17124v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17111v1","updated":"2024-03-25T18:49:12Z","published":"2024-03-25T18:49:12Z","title":"Vision-Based Dexterous Motion Planning by Dynamic Movement Primitives\n  with Human Hand Demonstration","summary":"  This paper proposes a vision-based framework for a 7-degree-of-freedom\nrobotic manipulator, with the primary objective of facilitating its capacity to\nacquire information from human hand demonstrations for the execution of\ndexterous pick-and-place tasks. Most existing works only focus on the position\ndemonstration without considering the orientations. In this paper, by employing\na single depth camera, MediaPipe is applied to generate the three-dimensional\ncoordinates of a human hand, thereby comprehensively recording the hand's\nmotion, encompassing the trajectory of the wrist, orientation of the hand, and\nthe grasp motion. A mean filter is applied during data pre-processing to smooth\nthe raw data. The demonstration is designed to pick up an object at a specific\nangle, navigate around obstacles in its path and subsequently, deposit it\nwithin a sloped container. The robotic system demonstrates its learning\ncapabilities, facilitated by the implementation of Dynamic Movement Primitives,\nenabling the assimilation of user actions into its trajectories with different\nstart and end poi\n","authors":["Nuo Chen","Ya-Jun Pan"],"pdf_url":"https://arxiv.org/pdf/2403.17111v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17099v1","updated":"2024-03-25T18:37:01Z","published":"2024-03-25T18:37:01Z","title":"Berry Twist: a Twisting-Tube Soft Robotic Gripper for Blackberry\n  Harvesting","summary":"  As global demand for fruits and vegetables continues to rise, the\nagricultural industry faces challenges in securing adequate labor. Robotic\nharvesting devices offer a promising solution to solve this issue. However,\nharvesting delicate fruits, notably blackberries, poses unique challenges due\nto their fragility. This study introduces and evaluates a prototype robotic\ngripper specifically designed for blackberry harvesting. The gripper features\nan innovative fabric tube mechanism employing motorized twisting action to\ngently envelop the fruit, ensuring uniform pressure application and minimizing\ndamage. Three types of tubes were developed, varying in elasticity and\ncompressibility using foam padding, spandex, and food-safe cotton cheesecloth.\nPerformance testing focused on assessing each gripper's ability to detach and\nrelease blackberries, with emphasis on quantifying damage rates. Results\nindicate the proposed gripper achieved an 82% success rate in detaching\nblackberries and a 95% success rate in releasing them, showcasing the promised\npotential for robotic harvesting applications.\n","authors":["Johannes F. Elfferich","Ebrahim Shahabi","Cosimo Della Santina","Dimitra Dodou"],"pdf_url":"https://arxiv.org/pdf/2403.17099v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2004.01041v8","updated":"2024-03-25T18:21:39Z","published":"2020-04-01T06:37:54Z","title":"On the Feedback Law in Stochastic Optimal Nonlinear Control","summary":"  We consider the problem of nonlinear stochastic optimal control. This problem\nis thought to be fundamentally intractable owing to Bellman's ``curse of\ndimensionality\". We present a result that shows that repeatedly solving an\nopen-loop deterministic problem from the current state with progressively\nshorter horizons, similar to Model Predictive Control (MPC), results in a\nfeedback policy that is $O(\\epsilon^4)$ near to the true global stochastic\noptimal policy, \\nxx{where $\\epsilon$ is a perturbation parameter modulating\nthe noise.} We show that the optimal deterministic feedback problem has a\nperturbation structure in that higher-order terms of the feedback law do not\naffect lower-order terms, and that this structure is lost in the optimal\nstochastic feedback problem. Consequently, solving the Stochastic Dynamic\nProgramming problem is highly susceptible to noise, even when tractable, and in\npractice, the MPC-type feedback law offers superior performance even for\nstochastic systems.\n","authors":["Mohamed Naveed Gul Mohamed","Suman Chakravorty","Raman Goyal","Ran Wang"],"pdf_url":"https://arxiv.org/pdf/2004.01041v8.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2002.10505,\n  arXiv:2002.09478"},{"id":"http://arxiv.org/abs/2403.17084v1","updated":"2024-03-25T18:18:12Z","published":"2024-03-25T18:18:12Z","title":"A Comparative Analysis of Visual Odometry in Virtual and Real-World\n  Railways Environments","summary":"  Perception tasks play a crucial role in the development of automated\noperations and systems across multiple application fields. In the railway\ntransportation domain, these tasks can improve the safety, reliability, and\nefficiency of various perations, including train localization, signal\nrecognition, and track discrimination. However, collecting considerable and\nprecisely labeled datasets for testing such novel algorithms poses extreme\nchallenges in the railway environment due to the severe restrictions in\naccessing the infrastructures and the practical difficulties associated with\nproperly equipping trains with the required sensors, such as cameras and\nLiDARs. The remarkable innovations of graphic engine tools offer new solutions\nto craft realistic synthetic datasets. To illustrate the advantages of\nemploying graphic simulation for early-stage testing of perception tasks in the\nrailway domain, this paper presents a comparative analysis of the performance\nof a SLAM algorithm applied both in a virtual synthetic environment and a\nreal-world scenario. The analysis leverages virtual railway environments\ncreated with the latest version of Unreal Engine, facilitating data collection\nand allowing the examination of challenging scenarios, including\nlow-visibility, dangerous operational modes, and complex environments. The\nresults highlight the feasibility and potentiality of graphic simulation to\nadvance perception tasks in the railway domain.\n","authors":["Gianluca D'Amico","Mauro Marinoni","Giorgio Buttazzo"],"pdf_url":"https://arxiv.org/pdf/2403.17084v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17067v1","updated":"2024-03-25T18:03:50Z","published":"2024-03-25T18:03:50Z","title":"Trajectory Optimization with Global Yaw Parameterization for\n  Field-of-View Constrained Autonomous Flight","summary":"  Trajectory generation for quadrotors with limited field-of-view sensors has\nnumerous applications such as aerial exploration, coverage, inspection,\nvideography, and target tracking. Most previous works simplify the task of\noptimizing yaw trajectories by either aligning the heading of the robot with\nits velocity, or potentially restricting the feasible space of candidate\ntrajectories by using a limited yaw domain to circumvent angular singularities.\nIn this paper, we propose a novel \\textit{global} yaw parameterization method\nfor trajectory optimization that allows a 360-degree yaw variation as demanded\nby the underlying algorithm. This approach effectively bypasses inherent\nsingularities by including supplementary quadratic constraints and transforming\nthe final decision variables into the desired state representation. This method\nsignificantly reduces the needed control effort, and improves optimization\nfeasibility. Furthermore, we apply the method to several examples of different\napplications that require jointly optimizing over both the yaw and position\ntrajectories. Ultimately, we present a comprehensive numerical analysis and\nevaluation of our proposed method in both simulation and real-world\nexperiments.\n","authors":["Yuwei Wu","Yuezhan Tao","Igor Spasojevic","Vijay Kumar"],"pdf_url":"https://arxiv.org/pdf/2403.17067v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17010v1","updated":"2024-03-25T17:59:59Z","published":"2024-03-25T17:59:59Z","title":"Calib3D: Calibrating Model Preferences for Reliable 3D Scene\n  Understanding","summary":"  Safety-critical 3D scene understanding tasks necessitate not only accurate\nbut also confident predictions from 3D perception models. This study introduces\nCalib3D, a pioneering effort to benchmark and scrutinize the reliability of 3D\nscene understanding models from an uncertainty estimation viewpoint. We\ncomprehensively evaluate 28 state-of-the-art models across 10 diverse 3D\ndatasets, uncovering insightful phenomena that cope with both the aleatoric and\nepistemic uncertainties in 3D scene understanding. We discover that despite\nachieving impressive levels of accuracy, existing models frequently fail to\nprovide reliable uncertainty estimates -- a pitfall that critically undermines\ntheir applicability in safety-sensitive contexts. Through extensive analysis of\nkey factors such as network capacity, LiDAR representations, rasterization\nresolutions, and 3D data augmentation techniques, we correlate these aspects\ndirectly with the model calibration efficacy. Furthermore, we introduce DeptS,\na novel depth-aware scaling approach aimed at enhancing 3D model calibration.\nExtensive experiments across a wide range of configurations validate the\nsuperiority of our method. We hope this work could serve as a cornerstone for\nfostering reliable 3D scene understanding. Code and benchmark toolkits are\npublicly available.\n","authors":["Lingdong Kong","Xiang Xu","Jun Cen","Wenwei Zhang","Liang Pan","Kai Chen","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2403.17010v1.pdf","comment":"Preprint; 37 pages, 8 figures, 11 tables; Code at\n  https://github.com/ldkong1205/Calib3D"},{"id":"http://arxiv.org/abs/2403.17009v1","updated":"2024-03-25T17:59:58Z","published":"2024-03-25T17:59:58Z","title":"Optimizing LiDAR Placements for Robust Driving Perception in Adverse\n  Conditions","summary":"  The robustness of driving perception systems under unprecedented conditions\nis crucial for safety-critical usages. Latest advancements have prompted\nincreasing interests towards multi-LiDAR perception. However, prevailing\ndriving datasets predominantly utilize single-LiDAR systems and collect data\ndevoid of adverse conditions, failing to capture the complexities of real-world\nenvironments accurately. Addressing these gaps, we proposed Place3D, a\nfull-cycle pipeline that encompasses LiDAR placement optimization, data\ngeneration, and downstream evaluations. Our framework makes three appealing\ncontributions. 1) To identify the most effective configurations for multi-LiDAR\nsystems, we introduce a Surrogate Metric of the Semantic Occupancy Grids\n(M-SOG) to evaluate LiDAR placement quality. 2) Leveraging the M-SOG metric, we\npropose a novel optimization strategy to refine multi-LiDAR placements. 3)\nCentered around the theme of multi-condition multi-LiDAR perception, we collect\na 364,000-frame dataset from both clean and adverse conditions. Extensive\nexperiments demonstrate that LiDAR placements optimized using our approach\noutperform various baselines. We showcase exceptional robustness in both 3D\nobject detection and LiDAR semantic segmentation tasks, under diverse adverse\nweather and sensor failure conditions. Code and benchmark toolkit are publicly\navailable.\n","authors":["Ye Li","Lingdong Kong","Hanjiang Hu","Xiaohao Xu","Xiaonan Huang"],"pdf_url":"https://arxiv.org/pdf/2403.17009v1.pdf","comment":"Preprint; 40 pages, 11 figures, 15 tables; Code at\n  https://github.com/ywyeli/Place3D"},{"id":"http://arxiv.org/abs/2403.16996v1","updated":"2024-03-25T17:59:01Z","published":"2024-03-25T17:59:01Z","title":"DriveCoT: Integrating Chain-of-Thought Reasoning with End-to-End Driving","summary":"  End-to-end driving has made significant progress in recent years,\ndemonstrating benefits such as system simplicity and competitive driving\nperformance under both open-loop and closed-loop settings. Nevertheless, the\nlack of interpretability and controllability in its driving decisions hinders\nreal-world deployment for end-to-end driving systems. In this paper, we collect\na comprehensive end-to-end driving dataset named DriveCoT, leveraging the CARLA\nsimulator. It contains sensor data, control decisions, and chain-of-thought\nlabels to indicate the reasoning process. We utilize the challenging driving\nscenarios from the CARLA leaderboard 2.0, which involve high-speed driving and\nlane-changing, and propose a rule-based expert policy to control the vehicle\nand generate ground truth labels for its reasoning process across different\ndriving aspects and the final decisions. This dataset can serve as an open-loop\nend-to-end driving benchmark, enabling the evaluation of accuracy in various\nchain-of-thought aspects and the final decision. In addition, we propose a\nbaseline model called DriveCoT-Agent, trained on our dataset, to generate\nchain-of-thought predictions and final decisions. The trained model exhibits\nstrong performance in both open-loop and closed-loop evaluations, demonstrating\nthe effectiveness of our proposed dataset.\n","authors":["Tianqi Wang","Enze Xie","Ruihang Chu","Zhenguo Li","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2403.16996v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16956v1","updated":"2024-03-25T17:17:35Z","published":"2024-03-25T17:17:35Z","title":"Bayesian Methods for Trust in Collaborative Multi-Agent Autonomy","summary":"  Multi-agent, collaborative sensor fusion is a vital component of a\nmulti-national intelligence toolkit. In safety-critical and/or contested\nenvironments, adversaries may infiltrate and compromise a number of agents. We\nanalyze state of the art multi-target tracking algorithms under this\ncompromised agent threat model. We prove that the track existence probability\ntest (\"track score\") is significantly vulnerable to even small numbers of\nadversaries. To add security awareness, we design a trust estimation framework\nusing hierarchical Bayesian updating. Our framework builds beliefs of trust on\ntracks and agents by mapping sensor measurements to trust pseudomeasurements\n(PSMs) and incorporating prior trust beliefs in a Bayesian context. In case\nstudies, our trust estimation algorithm accurately estimates the\ntrustworthiness of tracks/agents, subject to observability limitations.\n","authors":["R. Spencer Hallyburton","Miroslav Pajic"],"pdf_url":"https://arxiv.org/pdf/2403.16956v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16953v1","updated":"2024-03-25T17:15:13Z","published":"2024-03-25T17:15:13Z","title":"Learning Symbolic and Subsymbolic Temporal Task Constraints from\n  Bimanual Human Demonstrations","summary":"  Learning task models of bimanual manipulation from human demonstration and\ntheir execution on a robot should take temporal constraints between actions\ninto account. This includes constraints on (i) the symbolic level such as\nprecedence relations or temporal overlap in the execution, and (ii) the\nsubsymbolic level such as the duration of different actions, or their starting\nand end points in time. Such temporal constraints are crucial for temporal\nplanning, reasoning, and the exact timing for the execution of bimanual actions\non a bimanual robot. In our previous work, we addressed the learning of\ntemporal task constraints on the symbolic level and demonstrated how a robot\ncan leverage this knowledge to respond to failures during execution. In this\nwork, we propose a novel model-driven approach for the combined learning of\nsymbolic and subsymbolic temporal task constraints from multiple bimanual human\ndemonstrations. Our main contributions are a subsymbolic foundation of a\ntemporal task model that describes temporal nexuses of actions in the task\nbased on distributions of temporal differences between semantic action\nkeypoints, as well as a method based on fuzzy logic to derive symbolic temporal\ntask constraints from this representation. This complements our previous work\non learning comprehensive temporal task models by integrating symbolic and\nsubsymbolic information based on a subsymbolic foundation, while still\nmaintaining the symbolic expressiveness of our previous approach. We compare\nour proposed approach with our previous pure-symbolic approach and show that we\ncan reproduce and even outperform it. Additionally, we show how the subsymbolic\ntemporal task constraints can synchronize otherwise unimanual movement\nprimitives for bimanual behavior on a humanoid robot.\n","authors":["Christian Dreher","Tamim Asfour"],"pdf_url":"https://arxiv.org/pdf/2403.16953v1.pdf","comment":"8 pages, submitted to IROS 2024"},{"id":"http://arxiv.org/abs/2310.07822v2","updated":"2024-03-25T17:06:57Z","published":"2023-10-11T19:03:55Z","title":"Body-mounted MR-conditional Robot for Minimally Invasive Liver\n  Intervention","summary":"  MR-guided microwave ablation (MWA) has proven effective in treating\nhepatocellular carcinoma (HCC) with small-sized tumors, but the\nstate-of-the-art technique suffers from sub-optimal workflow due to speed and\naccuracy of needle placement. This paper presents a compact body-mounted\nMR-conditional robot that can operate in closed-bore MR scanners for accurate\nneedle guidance. The robotic platform consists of two stacked Cartesian XY\nstages, each with two degrees of freedom, that facilitate needle guidance. The\nrobot is actuated using 3D-printed pneumatic turbines with MR-conditional bevel\ngear transmission systems. Pneumatic valves and control mechatronics are\nlocated inside the MRI control room and are connected to the robot with\npneumatic transmission lines and optical fibers. Free space experiments\nindicated robot-assisted needle insertion error of 2.6$\\pm$1.3 mm at an\ninsertion depth of 80 mm. The MR-guided phantom studies were conducted to\nverify the MR-conditionality and targeting performance of the robot. Future\nwork will focus on the system optimization and validations in animal trials.\n","authors":["Zhefeng Huang","Anthony L. Gunderman","Samuel E. Wilcox","Saikat Sengupta","Jay Shah","Aiming Lu","David Woodrum","Yue Chen"],"pdf_url":"https://arxiv.org/pdf/2310.07822v2.pdf","comment":"10 figures"},{"id":"http://arxiv.org/abs/2403.06186v3","updated":"2024-03-25T16:18:38Z","published":"2024-03-10T12:06:45Z","title":"Mind Meets Robots: A Review of EEG-Based Brain-Robot Interaction Systems","summary":"  Brain-robot interaction (BRI) empowers individuals to control\n(semi-)automated machines through their brain activity, either passively or\nactively. In the past decade, BRI systems have achieved remarkable success,\npredominantly harnessing electroencephalogram (EEG) signals as the central\ncomponent. This paper offers an up-to-date and exhaustive examination of 87\ncurated studies published during the last five years (2018-2023), focusing on\nidentifying the research landscape of EEG-based BRI systems. This review aims\nto consolidate and underscore methodologies, interaction modes, application\ncontexts, system evaluation, existing challenges, and potential avenues for\nfuture investigations in this domain. Based on our analysis, we present a BRI\nsystem model with three entities: Brain, Robot, and Interaction, depicting the\ninternal relationships of a BRI system. We especially investigate the essence\nand principles on interaction modes between human brains and robots, a domain\nthat has not yet been identified anywhere. We then discuss these entities with\ndifferent dimensions encompassed. Within this model, we scrutinize and classify\ncurrent research, reveal insights, specify challenges, and provide\nrecommendations for future research trajectories in this field. Meanwhile, we\nenvision our findings offer a design space for future human-robot interaction\n(HRI) research, informing the creation of efficient BRI frameworks.\n","authors":["Yuchong Zhang","Nona Rajabi","Farzaneh Taleb","Andrii Matviienko","Yong Ma","Mårten Björkman","Danica Kragic"],"pdf_url":"https://arxiv.org/pdf/2403.06186v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10855v2","updated":"2024-03-25T16:07:24Z","published":"2024-03-16T08:30:55Z","title":"Reinforcement Learning with Options and State Representation","summary":"  The current thesis aims to explore the reinforcement learning field and build\non existing methods to produce improved ones to tackle the problem of learning\nin high-dimensional and complex environments. It addresses such goals by\ndecomposing learning tasks in a hierarchical fashion known as Hierarchical\nReinforcement Learning.\n  We start in the first chapter by getting familiar with the Markov Decision\nProcess framework and presenting some of its recent techniques that the\nfollowing chapters use. We then proceed to build our Hierarchical Policy\nlearning as an answer to the limitations of a single primitive policy. The\nhierarchy is composed of a manager agent at the top and employee agents at the\nlower level.\n  In the last chapter, which is the core of this thesis, we attempt to learn\nlower-level elements of the hierarchy independently of the manager level in\nwhat is known as the \"Eigenoption\". Based on the graph structure of the\nenvironment, Eigenoptions allow us to build agents that are aware of the\ngeometric and dynamic properties of the environment. Their decision-making has\na special property: it is invariant to symmetric transformations of the\nenvironment, allowing as a consequence to greatly reduce the complexity of the\nlearning task.\n","authors":["Ayoub Ghriss","Masashi Sugiyama","Alessandro Lazaric"],"pdf_url":"https://arxiv.org/pdf/2403.10855v2.pdf","comment":"Master Thesis 2018, MVA ENS Paris-Saclay, Tokyo RIKEN AIP"},{"id":"http://arxiv.org/abs/2403.16880v1","updated":"2024-03-25T15:47:06Z","published":"2024-03-25T15:47:06Z","title":"DHP-Mapping: A Dense Panoptic Mapping System with Hierarchical World\n  Representation and Label Optimization Techniques","summary":"  Maps provide robots with crucial environmental knowledge, thereby enabling\nthem to perform interactive tasks effectively. Easily accessing accurate\nabstract-to-detailed geometric and semantic concepts from maps is crucial for\nrobots to make informed and efficient decisions. To comprehensively model the\nenvironment and effectively manage the map data structure, we propose\nDHP-Mapping, a dense mapping system that utilizes multiple Truncated Signed\nDistance Field (TSDF) submaps and panoptic labels to hierarchically model the\nenvironment. The output map is able to maintain both voxel- and submap-level\nmetric and semantic information. Two modules are presented to enhance the\nmapping efficiency and label consistency: (1) an inter-submaps label fusion\nstrategy to eliminate duplicate points across submaps and (2) a conditional\nrandom field (CRF) based approach to enhance panoptic labels through object\nlabel comprehension and contextual information. We conducted experiments with\ntwo public datasets including indoor and outdoor scenarios. Our system performs\ncomparably to state-of-the-art (SOTA) methods across geometry and label\naccuracy evaluation metrics. The experiment results highlight the effectiveness\nand scalability of our system, as it is capable of constructing precise\ngeometry and maintaining consistent panoptic labels. Our code is publicly\navailable at https://github.com/hutslib/DHP-Mapping.\n","authors":["Tianshuai Hu","Jianhao Jiao","Yucheng Xu","Hongji Liu","Sheng Wang","Ming Liu"],"pdf_url":"https://arxiv.org/pdf/2403.16880v1.pdf","comment":"Submit to IROS 2024. Project website\n  https://github.com/hutslib/DHP-Mapping"},{"id":"http://arxiv.org/abs/2403.16877v1","updated":"2024-03-25T15:42:09Z","published":"2024-03-25T15:42:09Z","title":"Proprioception Is All You Need: Terrain Classification for Boreal\n  Forests","summary":"  Recent works in field robotics highlighted the importance of resiliency\nagainst different types of terrains. Boreal forests, in particular, are home to\nmany mobility-impeding terrains that should be considered for off-road\nautonomous navigation. Also, being one of the largest land biomes on Earth,\nboreal forests are an area where autonomous vehicles are expected to become\nincreasingly common. In this paper, we address this issue by introducing\nBorealTC, a publicly available dataset for proprioceptive-based terrain\nclassification (TC). Recorded with a Husky A200, our dataset contains 116 min\nof Inertial Measurement Unit (IMU), motor current, and wheel odometry data,\nfocusing on typical boreal forest terrains, notably snow, ice, and silty loam.\nCombining our dataset with another dataset from the state-of-the-art, we\nevaluate both a Convolutional Neural Network (CNN) and the novel state space\nmodel (SSM)-based Mamba architecture on a TC task. Interestingly, we show that\nwhile CNN outperforms Mamba on each separate dataset, Mamba achieves greater\naccuracy when trained on a combination of both. In addition, we demonstrate\nthat Mamba's learning capacity is greater than a CNN for increasing amounts of\ndata. We show that the combination of two TC datasets yields a latent space\nthat can be interpreted with the properties of the terrains. We also discuss\nthe implications of merging datasets on classification. Our source code and\ndataset are publicly available online:\nhttps://github.com/norlab-ulaval/BorealTC.\n","authors":["Damien LaRocque","William Guimont-Martin","David-Alexandre Duclos","Philippe Giguère","François Pomerleau"],"pdf_url":"https://arxiv.org/pdf/2403.16877v1.pdf","comment":"Submitted to the 2024 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS 2024)"},{"id":"http://arxiv.org/abs/2403.16875v1","updated":"2024-03-25T15:39:46Z","published":"2024-03-25T15:39:46Z","title":"TAIL: A Terrain-Aware Multi-Modal SLAM Dataset for Robot Locomotion in\n  Deformable Granular Environments","summary":"  Terrain-aware perception holds the potential to improve the robustness and\naccuracy of autonomous robot navigation in the wilds, thereby facilitating\neffective off-road traversals. However, the lack of multi-modal perception\nacross various motion patterns hinders the solutions of Simultaneous\nLocalization And Mapping (SLAM), especially when confronting non-geometric\nhazards in demanding landscapes. In this paper, we first propose a\nTerrain-Aware multI-modaL (TAIL) dataset tailored to deformable and sandy\nterrains. It incorporates various types of robotic proprioception and distinct\nground interactions for the unique challenges and benchmark of multi-sensor\nfusion SLAM. The versatile sensor suite comprises stereo frame cameras,\nmultiple ground-pointing RGB-D cameras, a rotating 3D LiDAR, an IMU, and an RTK\ndevice. This ensemble is hardware-synchronized, well-calibrated, and\nself-contained. Utilizing both wheeled and quadrupedal locomotion, we\nefficiently collect comprehensive sequences to capture rich unstructured\nscenarios. It spans the spectrum of scope, terrain interactions, scene changes,\nground-level properties, and dynamic robot characteristics. We benchmark\nseveral state-of-the-art SLAM methods against ground truth and provide\nperformance validations. Corresponding challenges and limitations are also\nreported. All associated resources are accessible upon request at\n\\url{https://tailrobot.github.io/}.\n","authors":["Chen Yao","Yangtao Ge","Guowei Shi","Zirui Wang","Ningbo Yang","Zheng Zhu","Hexiang Wei","Yuntian Zhao","Jing Wu","Zhenzhong Jia"],"pdf_url":"https://arxiv.org/pdf/2403.16875v1.pdf","comment":"Submitted to IEEE Robotics and Automation Letters"},{"id":"http://arxiv.org/abs/2403.16859v1","updated":"2024-03-25T15:23:14Z","published":"2024-03-25T15:23:14Z","title":"A Semi-Lagrangian Approach for Time and Energy Path Planning\n  Optimization in Static Flow Fields","summary":"  Efficient path planning for autonomous mobile robots is a critical problem\nacross numerous domains, where optimizing both time and energy consumption is\nparamount. This paper introduces a novel methodology that considers the dynamic\ninfluence of an environmental flow field and considers geometric constraints,\nincluding obstacles and forbidden zones, enriching the complexity of the\nplanning problem. We formulate it as a multi-objective optimal control problem,\npropose a novel transformation called Harmonic Transformation, and apply a\nsemi-Lagrangian scheme to solve it. The set of Pareto efficient solutions is\nobtained considering two distinct approaches: a deterministic method and an\nevolutionary-based one, both of which are designed to make use of the proposed\nHarmonic Transformation. Through an extensive analysis of these approaches, we\ndemonstrate their efficacy in finding optimized paths.\n","authors":["Víctor C. da S. Campos","Armando A. Neto","Douglas G. Macharet"],"pdf_url":"https://arxiv.org/pdf/2403.16859v1.pdf","comment":"12 pages, initial paper submission; Preprint submitted to the IEEE\n  Transactions on Intelligent Transportation Systems"},{"id":"http://arxiv.org/abs/2402.13848v2","updated":"2024-03-25T14:45:53Z","published":"2024-02-21T14:50:24Z","title":"Zero-BEV: Zero-shot Projection of Any First-Person Modality to BEV Maps","summary":"  Bird's-eye view (BEV) maps are an important geometrically structured\nrepresentation widely used in robotics, in particular self-driving vehicles and\nterrestrial robots. Existing algorithms either require depth information for\nthe geometric projection, which is not always reliably available, or are\ntrained end-to-end in a fully supervised way to map visual first-person\nobservations to BEV representation, and are therefore restricted to the output\nmodality they have been trained for. In contrast, we propose a new model\ncapable of performing zero-shot projections of any modality available in a\nfirst person view to the corresponding BEV map. This is achieved by\ndisentangling the geometric inverse perspective projection from the modality\ntransformation, eg. RGB to occupancy. The method is general and we showcase\nexperiments projecting to BEV three different modalities: semantic\nsegmentation, motion vectors and object bounding boxes detected in first\nperson. We experimentally show that the model outperforms competing methods, in\nparticular the widely used baseline resorting to monocular depth estimation.\n","authors":["Gianluca Monaci","Leonid Antsfeld","Boris Chidlovskii","Christian Wolf"],"pdf_url":"https://arxiv.org/pdf/2402.13848v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.09020v3","updated":"2024-03-25T09:38:32Z","published":"2022-09-09T09:48:35Z","title":"Vehicle Trajectory Tracking Through Magnetic Sensors: A Case Study of\n  Two-lane Road","summary":"  Intelligent Transportation Systems (ITS) have a pressing need for efficient\nand reliable traffic surveillance solutions. This paper for the first time\nproposes a surveillance system that utilizes low-cost magnetic sensors for\ndetecting and tracking vehicles continuously along the road. The system uses\nmultiple sensors mounted along the roadside and lane boundaries to capture the\nmovement of vehicles. Real-time measurement data is collected by base stations\nand processed to produce vehicle trajectories that include position, timestamp,\nand speed. To address the challenge of tracking vehicles continuously on a road\nnetwork using a large amount of unlabeled magnetic sensor measurements, we\nfirst define a vehicle trajectory tracking problem. We then propose a\ngraph-based data association algorithm to track each detected vehicle, and\ndesign a related online algorithm framework respectively. We finally validate\nthe performance via both experimental simulation and real-world road\ndeployment. The experimental results demonstrate that the proposed solution\nprovides a cost-effective solution to capture the driving status of vehicles\nand on that basis form various traffic safety and efficiency applications.\n","authors":["Xiaojiang Ren","Yan Wang","Yingfan Geng"],"pdf_url":"https://arxiv.org/pdf/2209.09020v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16374v1","updated":"2024-03-25T02:38:34Z","published":"2024-03-25T02:38:34Z","title":"ProIn: Learning to Predict Trajectory Based on Progressive Interactions\n  for Autonomous Driving","summary":"  Accurate motion prediction of pedestrians, cyclists, and other surrounding\nvehicles (all called agents) is very important for autonomous driving. Most\nexisting works capture map information through an one-stage interaction with\nmap by vector-based attention, to provide map constraints for social\ninteraction and multi-modal differentiation. However, these methods have to\nencode all required map rules into the focal agent's feature, so as to retain\nall possible intentions' paths while at the meantime to adapt to potential\nsocial interaction. In this work, a progressive interaction network is proposed\nto enable the agent's feature to progressively focus on relevant maps, in order\nto better learn agents' feature representation capturing the relevant map\nconstraints. The network progressively encode the complex influence of map\nconstraints into the agent's feature through graph convolutions at the\nfollowing three stages: after historical trajectory encoder, after social\ninteraction, and after multi-modal differentiation. In addition, a weight\nallocation mechanism is proposed for multi-modal training, so that each mode\ncan obtain learning opportunities from a single-mode ground truth. Experiments\nhave validated the superiority of progressive interactions to the existing\none-stage interaction, and demonstrate the effectiveness of each component.\nEncouraging results were obtained in the challenging benchmarks.\n","authors":["Yinke Dong","Haifeng Yuan","Hongkun Liu","Wei Jing","Fangzhen Li","Hongmin Liu","Bin Fan"],"pdf_url":"https://arxiv.org/pdf/2403.16374v1.pdf","comment":null}]},"2024-03-26T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2403.11384v3","updated":"2024-03-26T08:40:12Z","published":"2024-03-18T00:22:30Z","title":"Towards Massive Interaction with Generalist Robotics: A Systematic\n  Review of XR-enabled Remote Human-Robot Interaction Systems","summary":"  The rising interest of generalist robots seek to create robots with\nversatility to handle multiple tasks in a variety of environments, and human\nwill interact with such robots through immersive interfaces. In the context of\nhuman-robot interaction (HRI), this survey provides an exhaustive review of the\napplications of extended reality (XR) technologies in the field of remote HRI.\nWe developed a systematic search strategy based on the PRISMA methodology. From\nthe initial 2,561 articles selected, 100 research papers that met our inclusion\ncriteria were included. We categorized and summarized the domain in detail,\ndelving into XR technologies, including augmented reality (AR), virtual reality\n(VR), and mixed reality (MR), and their applications in facilitating intuitive\nand effective remote control and interaction with robotic systems. The survey\nhighlights existing articles on the application of XR technologies, user\nexperience enhancement, and various interaction designs for XR in remote HRI,\nproviding insights into current trends and future directions. We also\nidentified potential gaps and opportunities for future research to improve\nremote HRI systems through XR technology to guide and inform future XR and\nrobotics research.\n","authors":["Xian Wang","Luyao Shen","Lik-Hang Lee"],"pdf_url":"https://arxiv.org/pdf/2403.11384v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16291v2","updated":"2024-03-26T09:23:07Z","published":"2024-03-24T20:43:29Z","title":"Guessing human intentions to avoid dangerous situations in caregiving\n  robots","summary":"  For robots to interact socially, they must interpret human intentions and\nanticipate their potential outcomes accurately. This is particularly important\nfor social robots designed for human care, which may face potentially dangerous\nsituations for people, such as unseen obstacles in their way, that should be\navoided. This paper explores the Artificial Theory of Mind (ATM) approach to\ninferring and interpreting human intentions. We propose an algorithm that\ndetects risky situations for humans, selecting a robot action that removes the\ndanger in real time. We use the simulation-based approach to ATM and adopt the\n'like-me' policy to assign intentions and actions to people. Using this\nstrategy, the robot can detect and act with a high rate of success under\ntime-constrained situations. The algorithm has been implemented as part of an\nexisting robotics cognitive architecture and tested in simulation scenarios.\nThree experiments have been conducted to test the implementation's robustness,\nprecision and real-time response, including a simulated scenario, a\nhuman-in-the-loop hybrid configuration and a real-world scenario.\n","authors":["Noé Zapata","Gerardo Pérez","Lucas Bonilla","Pedro Núñez","Pilar Bachiller","Pablo Bustos"],"pdf_url":"https://arxiv.org/pdf/2403.16291v2.pdf","comment":"8 pages, 6 figures. Submitted to IROS2024. For associated mpeg file\n  see https://youtu.be/87UEB8P97KY"},{"id":"http://arxiv.org/abs/2403.17933v1","updated":"2024-03-26T17:58:29Z","published":"2024-03-26T17:58:29Z","title":"SLEDGE: Synthesizing Simulation Environments for Driving Agents with\n  Generative Models","summary":"  SLEDGE is the first generative simulator for vehicle motion planning trained\non real-world driving logs. Its core component is a learned model that is able\nto generate agent bounding boxes and lane graphs. The model's outputs serve as\nan initial state for traffic simulation. The unique properties of the entities\nto be generated for SLEDGE, such as their connectivity and variable count per\nscene, render the naive application of most modern generative models to this\ntask non-trivial. Therefore, together with a systematic study of existing lane\ngraph representations, we introduce a novel raster-to-vector autoencoder\n(RVAE). It encodes agents and the lane graph into distinct channels in a\nrasterized latent map. This facilitates both lane-conditioned agent generation\nand combined generation of lanes and agents with a Diffusion Transformer. Using\ngenerated entities in SLEDGE enables greater control over the simulation, e.g.\nupsampling turns or increasing traffic density. Further, SLEDGE can support\n500m long routes, a capability not found in existing data-driven simulators\nlike nuPlan. It presents new challenges for planning algorithms, evidenced by\nfailure rates of over 40% for PDM, the winner of the 2023 nuPlan challenge,\nwhen tested on hard routes and dense traffic generated by our model. Compared\nto nuPlan, SLEDGE requires 500$\\times$ less storage to set up (<4GB), making it\na more accessible option and helping with democratizing future research in this\nfield.\n","authors":["Kashyap Chitta","Daniel Dauner","Andreas Geiger"],"pdf_url":"https://arxiv.org/pdf/2403.17933v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17917v1","updated":"2024-03-26T17:54:05Z","published":"2024-03-26T17:54:05Z","title":"Multi-Agent Clarity-Aware Dynamic Coverage with Gaussian Processes","summary":"  This paper presents two algorithms for multi-agent dynamic coverage in\nspatiotemporal environments, where the coverage algorithms are informed by the\nmethod of data assimilation. In particular, we show that by considering the\ninformation assimilation algorithm, here a Numerical Gaussian Process Kalman\nFilter, the influence of measurements taken at one position on the uncertainty\nof the estimate at another location can be computed. We use this relationship\nto propose new coverage algorithms. Furthermore, we show that the controllers\nnaturally extend to the multi-agent context, allowing for a distributed-control\ncentral-information paradigm for multi-agent coverage. Finally, we demonstrate\nthe algorithms through a realistic simulation of a team of UAVs collecting wind\ndata over a region in Austria.\n","authors":["Devansh R. Agrawal","Dimitra Panagou"],"pdf_url":"https://arxiv.org/pdf/2403.17917v1.pdf","comment":"8 pages, 2 figures, submitted to CDC 2024"},{"id":"http://arxiv.org/abs/2403.17916v1","updated":"2024-03-26T17:53:27Z","published":"2024-03-26T17:53:27Z","title":"CMP: Cooperative Motion Prediction with Multi-Agent Communication","summary":"  The confluence of the advancement of Autonomous Vehicles (AVs) and the\nmaturity of Vehicle-to-Everything (V2X) communication has enabled the\ncapability of cooperative connected and automated vehicles (CAVs). Building on\ntop of cooperative perception, this paper explores the feasibility and\neffectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR\nsignals as input to enhance tracking and prediction capabilities. Unlike\nprevious work that focuses separately on either cooperative perception or\nmotion prediction, our framework, to the best of our knowledge, is the first to\naddress the unified problem where CAVs share information in both perception and\nprediction modules. Incorporated into our design is the unique capability to\ntolerate realistic V2X bandwidth limitations and transmission delays, while\ndealing with bulky perception representations. We also propose a prediction\naggregation module, which unifies the predictions obtained by different CAVs\nand generates the final prediction. Through extensive experiments and ablation\nstudies, we demonstrate the effectiveness of our method in cooperative\nperception, tracking, and motion prediction tasks. In particular, CMP reduces\nthe average prediction error by 17.2\\% with fewer missing detections compared\nwith the no cooperation setting. Our work marks a significant step forward in\nthe cooperative capabilities of CAVs, showcasing enhanced performance in\ncomplex scenarios.\n","authors":["Zhuoyuan Wu","Yuping Wang","Hengbo Ma","Zhaowei Li","Hang Qiu","Jiachen Li"],"pdf_url":"https://arxiv.org/pdf/2403.17916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.03773v3","updated":"2024-03-26T17:17:39Z","published":"2023-04-04T21:49:02Z","title":"Safe Explicable Planning","summary":"  Human expectations arise from their understanding of others and the world. In\nthe context of human-AI interaction, this understanding may not align with\nreality, leading to the AI agent failing to meet expectations and compromising\nteam performance. Explicable planning, introduced as a method to bridge this\ngap, aims to reconcile human expectations with the agent's optimal behavior,\nfacilitating interpretable decision-making. However, an unresolved critical\nissue is ensuring safety in explicable planning, as it could result in\nexplicable behaviors that are unsafe. To address this, we propose Safe\nExplicable Planning (SEP), which extends the prior work to support the\nspecification of a safety bound. The goal of SEP is to find behaviors that\nalign with human expectations while adhering to the specified safety criterion.\nOur approach generalizes the consideration of multiple objectives stemming from\nmultiple models rather than a single model, yielding a Pareto set of safe\nexplicable policies. We present both an exact method, guaranteeing finding the\nPareto set, and a more efficient greedy method that finds one of the policies\nin the Pareto set. Additionally, we offer approximate solutions based on state\naggregation to improve scalability. We provide formal proofs that validate the\ndesired theoretical properties of these methods. Evaluation through simulations\nand physical robot experiments confirms the effectiveness of our approach for\nsafe explicable planning.\n","authors":["Akkamahadevi Hanni","Andrew Boateng","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.03773v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17849v1","updated":"2024-03-26T16:38:12Z","published":"2024-03-26T16:38:12Z","title":"Multi Agent Pathfinding for Noise Restricted Hybrid Fuel Unmanned Aerial\n  Vehicles","summary":"  Multi Agent Path Finding (MAPF) seeks the optimal set of paths for multiple\nagents from respective start to goal locations such that no paths conflict. We\naddress the MAPF problem for a fleet of hybrid-fuel unmanned aerial vehicles\nwhich are subject to location-dependent noise restrictions. We solve this\nproblem by searching a constraint tree for which the subproblem at each node is\na set of shortest path problems subject to the noise and fuel constraints and\nconflict zone avoidance. A labeling algorithm is presented to solve this\nsubproblem, including the conflict zones which are treated as dynamic\nobstacles. We present the experimental results of the algorithms for various\ngraph sizes and number of agents.\n","authors":["Drew Scott","Satyanarayana G. Manyam","David W. Casbeer","Manish Kumar","Isaac E. Weintraub"],"pdf_url":"https://arxiv.org/pdf/2403.17849v1.pdf","comment":"6 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.17846v1","updated":"2024-03-26T16:36:43Z","published":"2024-03-26T16:36:43Z","title":"Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot\n  Navigation","summary":"  Recent open-vocabulary robot mapping methods enrich dense geometric maps with\npre-trained visual-language features. While these maps allow for the prediction\nof point-wise saliency maps when queried for a certain language concept,\nlarge-scale environments and abstract queries beyond the object level still\npose a considerable hurdle, ultimately limiting language-grounded robotic\nnavigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D\nscene graph mapping approach for language-grounded robot navigation. Leveraging\nopen-vocabulary vision foundation models, we first obtain state-of-the-art\nopen-vocabulary segment-level maps in 3D and subsequently construct a 3D scene\ngraph hierarchy consisting of floor, room, and object concepts, each enriched\nwith open-vocabulary features. Our approach is able to represent multi-story\nbuildings and allows robotic traversal of those using a cross-floor Voronoi\ngraph. HOV-SG is evaluated on three distinct datasets and surpasses previous\nbaselines in open-vocabulary semantic accuracy on the object, room, and floor\nlevel while producing a 75% reduction in representation size compared to dense\nopen-vocabulary maps. In order to prove the efficacy and generalization\ncapabilities of HOV-SG, we showcase successful long-horizon\nlanguage-conditioned robot navigation within real-world multi-storage\nenvironments. We provide code and trial video data at http://hovsg.github.io/.\n","authors":["Abdelrhman Werby","Chenguang Huang","Martin Büchner","Abhinav Valada","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2403.17846v1.pdf","comment":"Code and video are available at http://hovsg.github.io/"},{"id":"http://arxiv.org/abs/2309.02937v2","updated":"2024-03-26T15:51:49Z","published":"2023-09-06T12:04:24Z","title":"Resilient source seeking with robot swarms","summary":"  We present a solution for locating the source, or maximum, of an unknown\nscalar field using a swarm of mobile robots. Unlike relying on the traditional\ngradient information, the swarm determines an ascending direction to approach\nthe source with arbitrary precision. The ascending direction is calculated from\nmeasurements of the field strength at the robot locations and their relative\npositions concerning the centroid. Rather than focusing on individual robots,\nwe focus the analysis on the density of robots per unit area to guarantee a\nmore resilient swarm, i.e., the functionality remains even if individuals go\nmissing or are misplaced during the mission. We reinforce the robustness of the\nalgorithm by providing sufficient conditions for the swarm shape so that the\nascending direction is almost parallel to the gradient. The swarm can respond\nto an unexpected environment by morphing its shape and exploiting the existence\nof multiple ascending directions. Finally, we validate our approach numerically\nwith hundreds of robots. The fact that a large number of robots always\ncalculate an ascending direction compensates for the loss of individuals and\nmitigates issues arising from the actuator and sensor noises.\n","authors":["Antonio Acuaviva","Jesus Bautista","Weijia Yao","Juan Jimenez","Hector Garcia de Marina"],"pdf_url":"https://arxiv.org/pdf/2309.02937v2.pdf","comment":"7 pages, submitted to CDC 2024"},{"id":"http://arxiv.org/abs/2403.17805v1","updated":"2024-03-26T15:42:04Z","published":"2024-03-26T15:42:04Z","title":"Scenario-Based Curriculum Generation for Multi-Agent Autonomous Driving","summary":"  The automated generation of diverse and complex training scenarios has been\nan important ingredient in many complex learning tasks. Especially in\nreal-world application domains, such as autonomous driving, auto-curriculum\ngeneration is considered vital for obtaining robust and general policies.\nHowever, crafting traffic scenarios with multiple, heterogeneous agents is\ntypically considered as a tedious and time-consuming task, especially in more\ncomplex simulation environments. In our work, we introduce MATS-Gym, a\nMulti-Agent Traffic Scenario framework to train agents in CARLA, a\nhigh-fidelity driving simulator. MATS-Gym is a multi-agent training framework\nfor autonomous driving that uses partial scenario specifications to generate\ntraffic scenarios with variable numbers of agents. This paper unifies various\nexisting approaches to traffic scenario description into a single training\nframework and demonstrates how it can be integrated with techniques from\nunsupervised environment design to automate the generation of adaptive\nauto-curricula. The code is available at\nhttps://github.com/AutonomousDrivingExaminer/mats-gym.\n","authors":["Axel Brunnbauer","Luigi Berducci","Peter Priller","Dejan Nickovic","Radu Grosu"],"pdf_url":"https://arxiv.org/pdf/2403.17805v1.pdf","comment":"7 Pages, Under Review"},{"id":"http://arxiv.org/abs/2403.17788v1","updated":"2024-03-26T15:20:56Z","published":"2024-03-26T15:20:56Z","title":"System Calibration of a Field Phenotyping Robot with Multiple\n  High-Precision Profile Laser Scanners","summary":"  The creation of precise and high-resolution crop point clouds in agricultural\nfields has become a key challenge for high-throughput phenotyping applications.\nThis work implements a novel calibration method to calibrate the laser scanning\nsystem of an agricultural field robot consisting of two industrial-grade laser\nscanners used for high-precise 3D crop point cloud creation. The calibration\nmethod optimizes the transformation between the scanner origins and the robot\npose by minimizing 3D point omnivariances within the point cloud. Moreover, we\npresent a novel factor graph-based pose estimation method that fuses total\nstation prism measurements with IMU and GNSS heading information for\nhigh-precise pose determination during calibration. The root-mean-square error\nof the distances to a georeferenced ground truth point cloud results in 0.8 cm\nafter parameter optimization. Furthermore, our results show the importance of a\nreference point cloud in the calibration method needed to estimate the vertical\ntranslation of the calibration. Challenges arise due to non-static parameters\nwhile the robot moves, indicated by systematic deviations to a ground truth\nterrestrial laser scan.\n","authors":["Felix Esser","Gereon Tombrink","Andre Cornelißen","Lasse Klingbeil","Heiner Kuhlmann"],"pdf_url":"https://arxiv.org/pdf/2403.17788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17779v1","updated":"2024-03-26T15:12:46Z","published":"2024-03-26T15:12:46Z","title":"Optical Flow Based Detection and Tracking of Moving Objects for\n  Autonomous Vehicles","summary":"  Accurate velocity estimation of surrounding moving objects and their\ntrajectories are critical elements of perception systems in\nAutomated/Autonomous Vehicles (AVs) with a direct impact on their safety. These\nare non-trivial problems due to the diverse types and sizes of such objects and\ntheir dynamic and random behaviour. Recent point cloud based solutions often\nuse Iterative Closest Point (ICP) techniques, which are known to have certain\nlimitations. For example, their computational costs are high due to their\niterative nature, and their estimation error often deteriorates as the relative\nvelocities of the target objects increase (>2 m/sec). Motivated by such\nshortcomings, this paper first proposes a novel Detection and Tracking of\nMoving Objects (DATMO) for AVs based on an optical flow technique, which is\nproven to be computationally efficient and highly accurate for such problems.\n\\textcolor{black}{This is achieved by representing the driving scenario as a\nvector field and applying vector calculus theories to ensure spatiotemporal\ncontinuity.} We also report the results of a comprehensive performance\nevaluation of the proposed DATMO technique, carried out in this study using\nsynthetic and real-world data. The results of this study demonstrate the\nsuperiority of the proposed technique, compared to the DATMO techniques in the\nliterature, in terms of estimation accuracy and processing time in a wide range\nof relative velocities of moving objects. Finally, we evaluate and discuss the\nsensitivity of the estimation error of the proposed DATMO technique to various\nsystem and environmental parameters, as well as the relative velocities of the\nmoving objects.\n","authors":["MReza Alipour Sormoli","Mehrdad Dianati","Sajjad Mozaffari","Roger woodman"],"pdf_url":"https://arxiv.org/pdf/2403.17779v1.pdf","comment":"This manuscript has been accepted as a regular paper in Transactions\n  on Intelligent Transportation Systems (DOI: 10.1109/TITS.2024.3382495)"},{"id":"http://arxiv.org/abs/2403.17774v1","updated":"2024-03-26T15:07:27Z","published":"2024-03-26T15:07:27Z","title":"LiDAR-Based Crop Row Detection Algorithm for Over-Canopy Autonomous\n  Navigation in Agriculture Fields","summary":"  Autonomous navigation is crucial for various robotics applications in\nagriculture. However, many existing methods depend on RTK-GPS systems, which\nare expensive and susceptible to poor signal coverage. This paper introduces a\nstate-of-the-art LiDAR-based navigation system that can achieve over-canopy\nautonomous navigation in row-crop fields, even when the canopy fully blocks the\ninterrow spacing. Our crop row detection algorithm can detect crop rows across\ndiverse scenarios, encompassing various crop types, growth stages, weed\npresence, and discontinuities within the crop rows. Without utilizing the\nglobal localization of the robot, our navigation system can perform autonomous\nnavigation in these challenging scenarios, detect the end of the crop rows, and\nnavigate to the next crop row autonomously, providing a crop-agnostic approach\nto navigate the whole row-crop field. This navigation system has undergone\ntests in various simulated agricultural fields, achieving an average of\n$2.98cm$ autonomous driving accuracy without human intervention on the custom\nAmiga robot. In addition, the qualitative results of our crop row detection\nalgorithm from the actual soybean fields validate our LiDAR-based crop row\ndetection algorithm's potential for practical agricultural applications.\n","authors":["Ruiji Liu","Francisco Yandun","George Kantor"],"pdf_url":"https://arxiv.org/pdf/2403.17774v1.pdf","comment":"7 pages, 9 figures, submitted to IROS 2024"},{"id":"http://arxiv.org/abs/2311.15030v2","updated":"2024-03-26T14:33:51Z","published":"2023-11-25T13:51:14Z","title":"Tuning-free Quasi-stiffness Control Framework of a Powered Transfemoral\n  Prosthesis for Task-adaptive Walking","summary":"  Impedance-based control represents a prevalent strategy in the development of\npowered transfemoral prostheses. However, creating a task-adaptive, tuning-free\ncontroller that effectively generalizes across diverse locomotion modes and\nterrain conditions continues to be a significant challenge. This letter\nproposes a tuning-free and task-adaptive quasi-stiffness control framework for\npowered prostheses that generalizes across various walking tasks, including the\ntorque-angle relationship reconstruction part and the quasi-stiffness\ncontroller design part. A Gaussian Process Regression (GPR) model is introduced\nto predict the target features of the human joint angle and torque in a new\ntask. Subsequently, a Kernelized Movement Primitives (KMP) is employed to\nreconstruct the torque-angle relationship of the new task from multiple human\nreference trajectories and estimated target features. Based on the torque-angle\nrelationship of the new task, a quasi-stiffness control approach is designed\nfor a powered prosthesis. Finally, the proposed framework is validated through\npractical examples, including varying speeds and inclines walking tasks.\nNotably, the proposed framework not only aligns with but frequently surpasses\nthe performance of a benchmark finite state machine impedance controller\n(FSMIC) without necessitating manual impedance tuning and has the potential to\nexpand to variable walking tasks in daily life for the transfemoral amputees.\n","authors":["Teng Ma","Shucong Yin","Zhimin Hou","Binxin Huang","Haoyong Yu","Chenglong Fu"],"pdf_url":"https://arxiv.org/pdf/2311.15030v2.pdf","comment":"8 pages, 10 figures. This work has been submitted to the IEEE-RAL for\n  possible publication"},{"id":"http://arxiv.org/abs/2311.01885v2","updated":"2024-03-26T12:59:44Z","published":"2023-11-03T12:54:05Z","title":"Domain Randomization via Entropy Maximization","summary":"  Varying dynamics parameters in simulation is a popular Domain Randomization\n(DR) approach for overcoming the reality gap in Reinforcement Learning (RL).\nNevertheless, DR heavily hinges on the choice of the sampling distribution of\nthe dynamics parameters, since high variability is crucial to regularize the\nagent's behavior but notoriously leads to overly conservative policies when\nrandomizing excessively. In this paper, we propose a novel approach to address\nsim-to-real transfer, which automatically shapes dynamics distributions during\ntraining in simulation without requiring real-world data. We introduce DOmain\nRAndomization via Entropy MaximizatiON (DORAEMON), a constrained optimization\nproblem that directly maximizes the entropy of the training distribution while\nretaining generalization capabilities. In achieving this, DORAEMON gradually\nincreases the diversity of sampled dynamics parameters as long as the\nprobability of success of the current policy is sufficiently high. We\nempirically validate the consistent benefits of DORAEMON in obtaining highly\nadaptive and generalizable policies, i.e. solving the task at hand across the\nwidest range of dynamics parameters, as opposed to representative baselines\nfrom the DR literature. Notably, we also demonstrate the Sim2Real applicability\nof DORAEMON through its successful zero-shot transfer in a robotic manipulation\nsetup under unknown real-world parameters.\n","authors":["Gabriele Tiboni","Pascal Klink","Jan Peters","Tatiana Tommasi","Carlo D'Eramo","Georgia Chalvatzaki"],"pdf_url":"https://arxiv.org/pdf/2311.01885v2.pdf","comment":"Published as a conference paper at ICLR 2024. Project website at\n  https://gabrieletiboni.github.io/doraemon/"},{"id":"http://arxiv.org/abs/2403.17667v1","updated":"2024-03-26T12:57:05Z","published":"2024-03-26T12:57:05Z","title":"Learning Goal-Directed Object Pushing in Cluttered Scenes with\n  Location-Based Attention","summary":"  Non-prehensile planar pushing is a challenging task due to its underactuated\nnature with hybrid-dynamics, where a robot needs to reason about an object's\nlong-term behaviour and contact-switching, while being robust to contact\nuncertainty. The presence of clutter in the environment further complicates\nthis task, introducing the need to include more sophisticated spatial analysis\nto avoid collisions. Building upon prior work on reinforcement learning (RL)\nwith multimodal categorical exploration for planar pushing, in this paper we\nincorporate location-based attention to enable robust navigation through\nclutter. Unlike previous RL literature addressing this obstacle avoidance\npushing task, our framework requires no predefined global paths and considers\nthe target orientation of the manipulated object. Our results demonstrate that\nthe learned policies successfully navigate through a wide range of complex\nobstacle configurations, including dynamic obstacles, with smooth motions,\nachieving the desired target object pose. We also validate the transferability\nof the learned policies to robotic hardware using the KUKA iiwa robot arm.\n","authors":["Nils Dengler","Juan Del Aguila Ferrandis","João Moura","Sethu Vijayakumar","Maren Bennewitz"],"pdf_url":"https://arxiv.org/pdf/2403.17667v1.pdf","comment":"Submitted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS)"},{"id":"http://arxiv.org/abs/2402.03246v5","updated":"2024-03-26T12:35:03Z","published":"2024-02-05T18:03:53Z","title":"SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM","summary":"  We present SGS-SLAM, the first semantic visual SLAM system based on Gaussian\nSplatting. It incorporates appearance, geometry, and semantic features through\nmulti-channel optimization, addressing the oversmoothing limitations of neural\nimplicit SLAM systems in high-quality rendering, scene understanding, and\nobject-level geometry. We introduce a unique semantic feature loss that\neffectively compensates for the shortcomings of traditional depth and color\nlosses in object optimization. Through a semantic-guided keyframe selection\nstrategy, we prevent erroneous reconstructions caused by cumulative errors.\nExtensive experiments demonstrate that SGS-SLAM delivers state-of-the-art\nperformance in camera pose estimation, map reconstruction, precise semantic\nsegmentation, and object-level geometric accuracy, while ensuring real-time\nrendering capabilities.\n","authors":["Mingrui Li","Shuhong Liu","Heng Zhou","Guohao Zhu","Na Cheng","Tianchen Deng","Hongyu Wang"],"pdf_url":"https://arxiv.org/pdf/2402.03246v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17633v1","updated":"2024-03-26T12:08:14Z","published":"2024-03-26T12:08:14Z","title":"UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object\n  Detection with Sparse LiDAR and Large Domain Gaps","summary":"  In this study, we address a gap in existing unsupervised domain adaptation\napproaches on LiDAR-based 3D object detection, which have predominantly\nconcentrated on adapting between established, high-density autonomous driving\ndatasets. We focus on sparser point clouds, capturing scenarios from different\nperspectives: not just from vehicles on the road but also from mobile robots on\nsidewalks, which encounter significantly different environmental conditions and\nsensor configurations. We introduce Unsupervised Adversarial Domain Adaptation\nfor 3D Object Detection (UADA3D). UADA3D does not depend on pre-trained source\nmodels or teacher-student architectures. Instead, it uses an adversarial\napproach to directly learn domain-invariant features. We demonstrate its\nefficacy in various adaptation scenarios, showing significant improvements in\nboth self-driving car and mobile robot domains. Our code is open-source and\nwill be available soon.\n","authors":["Maciej K Wozniak","Mattias Hansson","Marko Thiel","Patric Jensfelt"],"pdf_url":"https://arxiv.org/pdf/2403.17633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.01598v3","updated":"2024-03-26T11:54:40Z","published":"2023-06-02T15:09:19Z","title":"Towards Source-free Domain Adaptive Semantic Segmentation via\n  Importance-aware and Prototype-contrast Learning","summary":"  Domain adaptive semantic segmentation enables robust pixel-wise understanding\nin real-world driving scenes. Source-free domain adaptation, as a more\npractical technique, addresses the concerns of data privacy and storage\nlimitations in typical unsupervised domain adaptation methods, making it\nespecially relevant in the context of intelligent vehicles. It utilizes a\nwell-trained source model and unlabeled target data to achieve adaptation in\nthe target domain. However, in the absence of source data and target labels,\ncurrent solutions cannot sufficiently reduce the impact of domain shift and\nfully leverage the information from the target data. In this paper, we propose\nan end-to-end source-free domain adaptation semantic segmentation method via\nImportance-Aware and Prototype-Contrast (IAPC) learning. The proposed IAPC\nframework effectively extracts domain-invariant knowledge from the well-trained\nsource model and learns domain-specific knowledge from the unlabeled target\ndomain. Specifically, considering the problem of domain shift in the prediction\nof the target domain by the source model, we put forward an importance-aware\nmechanism for the biased target prediction probability distribution to extract\ndomain-invariant knowledge from the source model. We further introduce a\nprototype-contrast strategy, which includes a prototype-symmetric cross-entropy\nloss and a prototype-enhanced cross-entropy loss, to learn target intra-domain\nknowledge without relying on labels. A comprehensive variety of experiments on\ntwo domain adaptive semantic segmentation benchmarks demonstrates that the\nproposed end-to-end IAPC solution outperforms existing state-of-the-art\nmethods. The source code is publicly available at\nhttps://github.com/yihong-97/Source-free-IAPC.\n","authors":["Yihong Cao","Hui Zhang","Xiao Lu","Zheng Xiao","Kailun Yang","Yaonan Wang"],"pdf_url":"https://arxiv.org/pdf/2306.01598v3.pdf","comment":"Accepted to IEEE Transactions on Intelligent Vehicles (T-IV). The\n  source code is publicly available at\n  https://github.com/yihong-97/Source-free-IAPC"},{"id":"http://arxiv.org/abs/2403.17622v1","updated":"2024-03-26T11:51:58Z","published":"2024-03-26T11:51:58Z","title":"Online Tree Reconstruction and Forest Inventory on a Mobile Robotic\n  System","summary":"  Terrestrial laser scanning (TLS) is the standard technique used to create\naccurate point clouds for digital forest inventories. However, the measurement\nprocess is demanding, requiring up to two days per hectare for data collection,\nsignificant data storage, as well as resource-heavy post-processing of 3D data.\nIn this work, we present a real-time mapping and analysis system that enables\nonline generation of forest inventories using mobile laser scanners that can be\nmounted e.g. on mobile robots. Given incrementally created and locally accurate\nsubmaps-data payloads-our approach extracts tree candidates using a custom,\nVoronoi-inspired clustering algorithm. Tree candidates are reconstructed using\nan adapted Hough algorithm, which enables robust modeling of the tree stem.\nFurther, we explicitly incorporate the incremental nature of the data\ncollection by consistently updating the database using a pose graph LiDAR SLAM\nsystem. This enables us to refine our estimates of the tree traits if an area\nis revisited later during a mission. We demonstrate competitive accuracy to TLS\nor manual measurements using laser scanners that we mounted on backpacks or\nmobile robots operating in conifer, broad-leaf and mixed forests. Our results\nachieve RMSE of 1.93 cm, a bias of 0.65 cm and a standard deviation of 1.81 cm\n(averaged across these sequences)-with no post-processing required after the\nmission is complete.\n","authors":["Leonard Freißmuth","Matias Mattamala","Nived Chebrolu","Simon Schaefer","Stefan Leutenegger","Maurice Fallon"],"pdf_url":"https://arxiv.org/pdf/2403.17622v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.02021v5","updated":"2024-03-26T11:41:36Z","published":"2022-09-05T15:41:13Z","title":"When Robotics Meets Wireless Communications: An Introductory Tutorial","summary":"  The importance of ground Mobile Robots (MRs) and Unmanned Aerial Vehicles\n(UAVs) within the research community, industry, and society is growing fast.\nMany of these agents are nowadays equipped with communication systems that are,\nin some cases, essential to successfully achieve certain tasks. In this\ncontext, we have begun to witness the development of a new interdisciplinary\nresearch field at the intersection of robotics and communications. This\nresearch field has been boosted by the intention of integrating UAVs within the\n5G and 6G communication networks. This research will undoubtedly lead to many\nimportant applications in the near future. Nevertheless, one of the main\nobstacles to the development of this research area is that most researchers\naddress these problems by oversimplifying either the robotics or the\ncommunications aspect. This impedes the ability of reaching the full potential\nof this new interdisciplinary research area. In this tutorial, we present some\nof the modelling tools necessary to address problems involving both robotics\nand communication from an interdisciplinary perspective. As an illustrative\nexample of such problems, we focus in this tutorial on the issue of\ncommunication-aware trajectory planning.\n","authors":["Daniel Bonilla Licea","Mounir Ghogho","Martin Saska"],"pdf_url":"https://arxiv.org/pdf/2209.02021v5.pdf","comment":"35 pages, 192 references"},{"id":"http://arxiv.org/abs/2403.17606v1","updated":"2024-03-26T11:38:19Z","published":"2024-03-26T11:38:19Z","title":"Interactive Identification of Granular Materials using Force\n  Measurements","summary":"  The ability to identify granular materials facilitates the emergence of\nvarious new applications in robotics, ranging from cooking at home to truck\nloading at mining sites. However, granular material identification remains a\nchallenging and underexplored area. In this work, we present a novel\ninteractive material identification framework that enables robots to identify a\nwide range of granular materials using only a force-torque sensor for\nperception. Our framework, comprising interactive exploration, feature\nextraction, and classification stages, prioritizes simplicity and transparency\nfor seamless integration into various manipulation pipelines. We evaluate the\nproposed approach through extensive experiments with a real-world dataset\ncomprising 11 granular materials, which we also make publicly available.\nAdditionally, we conducted a comprehensive qualitative analysis of the dataset\nto offer deeper insights into its nature, aiding future development. Our\nresults show that the proposed method is capable of accurately identifying a\nwide range of granular materials solely relying on force measurements obtained\nfrom direct interaction with the materials. Code and dataset are available at:\nhttps://irobotics.aalto.fi/indentify_granular/.\n","authors":["Samuli Hynninen","Tran Nguyen Le","Ville Kyrki"],"pdf_url":"https://arxiv.org/pdf/2403.17606v1.pdf","comment":"Submitted to 2024 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2024)"},{"id":"http://arxiv.org/abs/2403.13518v2","updated":"2024-03-26T11:16:47Z","published":"2024-03-20T11:38:30Z","title":"Motion Generation from Fine-grained Textual Descriptions","summary":"  The task of text2motion is to generate human motion sequences from given\ntextual descriptions, where the model explores diverse mappings from natural\nlanguage instructions to human body movements. While most existing works are\nconfined to coarse-grained motion descriptions, e.g., \"A man squats.\",\nfine-grained descriptions specifying movements of relevant body parts are\nbarely explored. Models trained with coarse-grained texts may not be able to\nlearn mappings from fine-grained motion-related words to motion primitives,\nresulting in the failure to generate motions from unseen descriptions. In this\npaper, we build a large-scale language-motion dataset specializing in\nfine-grained textual descriptions, FineHumanML3D, by feeding GPT-3.5-turbo with\nstep-by-step instructions with pseudo-code compulsory checks. Accordingly, we\ndesign a new text2motion model, FineMotionDiffuse, making full use of\nfine-grained textual information. Our quantitative evaluation shows that\nFineMotionDiffuse trained on FineHumanML3D improves FID by a large margin of\n0.38, compared with competitive baselines. According to the qualitative\nevaluation and case study, our model outperforms MotionDiffuse in generating\nspatially or chronologically composite motions, by learning the implicit\nmappings from fine-grained descriptions to the corresponding basic motions. We\nrelease our data at https://github.com/KunhangL/finemotiondiffuse.\n","authors":["Kunhang Li","Yansong Feng"],"pdf_url":"https://arxiv.org/pdf/2403.13518v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17565v1","updated":"2024-03-26T10:19:04Z","published":"2024-03-26T10:19:04Z","title":"Aerial Robots Carrying Flexible Cables: Dynamic Shape Optimal Control\n  via Spectral Method Model","summary":"  In this work, we present a model-based optimal boundary control design for an\naerial robotic system composed of a quadrotor carrying a flexible cable. The\nwhole system is modeled by partial differential equations (PDEs) combined with\nboundary conditions described by ordinary differential equations (ODEs). The\nproper orthogonal decomposition (POD) method is adopted to project the original\ninfinite-dimensional system on a subspace spanned by orthogonal basis\nfunctions. Based on the reduced order model, nonlinear model predictive control\n(NMPC) is implemented online to realize shape trajectory tracking of the\nflexible cable in an optimal predictive fashion. The proposed reduced modeling\nand optimal control paradigms are numerically verified against an accurate\nhigh-dimensional FDM-based model in different scenarios and the controller's\nsuperior performance is shown compared to an optimally tuned PID controller.\n","authors":["Yaolei Shen","Chiara Gabellieri","Antonio Franchi"],"pdf_url":"https://arxiv.org/pdf/2403.17565v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17551v1","updated":"2024-03-26T09:58:27Z","published":"2024-03-26T09:58:27Z","title":"Time-Optimal Flight with Safety Constraints and Data-driven Dynamics","summary":"  Time-optimal quadrotor flight is an extremely challenging problem due to the\nlimited control authority encountered at the limit of handling. Model\nPredictive Contouring Control (MPCC) has emerged as a leading model-based\napproach for time optimization problems such as drone racing. However, the\nstandard MPCC formulation used in quadrotor racing introduces the notion of the\ngates directly in the cost function, creating a multi-objective optimization\nthat continuously trades off between maximizing progress and tracking the path\naccurately. This paper introduces three key components that enhance the MPCC\napproach for drone racing. First and foremost, we provide safety guarantees in\nthe form of a constraint and terminal set. The safety set is designed as a\nspatial constraint which prevents gate collisions while allowing for\ntime-optimization only in the cost function. Second, we augment the existing\nfirst principles dynamics with a residual term that captures complex\naerodynamic effects and thrust forces learned directly from real world data.\nThird, we use Trust Region Bayesian Optimization (TuRBO), a state of the art\nglobal Bayesian Optimization algorithm, to tune the hyperparameters of the MPC\ncontroller given a sparse reward based on lap time minimization. The proposed\napproach achieves similar lap times to the best state-of-the-art RL and\noutperforms the best time-optimal controller while satisfying constraints. In\nboth simulation and real-world, our approach consistently prevents gate crashes\nwith 100\\% success rate, while pushing the quadrotor to its physical limit\nreaching speeds of more than 80km/h.\n","authors":["Maria Krinner","Angel Romero","Leonard Bauersfeld","Melanie Zeilinger","Andrea Carron","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2403.17551v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.17550v1","updated":"2024-03-26T09:58:06Z","published":"2024-03-26T09:58:06Z","title":"DeepMIF: Deep Monotonic Implicit Fields for Large-Scale LiDAR 3D Mapping","summary":"  Recently, significant progress has been achieved in sensing real large-scale\noutdoor 3D environments, particularly by using modern acquisition equipment\nsuch as LiDAR sensors. Unfortunately, they are fundamentally limited in their\nability to produce dense, complete 3D scenes. To address this issue, recent\nlearning-based methods integrate neural implicit representations and\noptimizable feature grids to approximate surfaces of 3D scenes. However,\nnaively fitting samples along raw LiDAR rays leads to noisy 3D mapping results\ndue to the nature of sparse, conflicting LiDAR measurements. Instead, in this\nwork we depart from fitting LiDAR data exactly, instead letting the network\noptimize a non-metric monotonic implicit field defined in 3D space. To fit our\nfield, we design a learning system integrating a monotonicity loss that enables\noptimizing neural monotonic fields and leverages recent progress in large-scale\n3D mapping. Our algorithm achieves high-quality dense 3D mapping performance as\ncaptured by multiple quantitative and perceptual measures and visual results\nobtained for Mai City, Newer College, and KITTI benchmarks. The code of our\napproach will be made publicly available.\n","authors":["Kutay Yılmaz","Matthias Nießner","Anastasiia Kornilova","Alexey Artemov"],"pdf_url":"https://arxiv.org/pdf/2403.17550v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2110.07953v2","updated":"2024-03-26T09:52:36Z","published":"2021-10-15T09:12:44Z","title":"Attention-based Estimation and Prediction of Human Intent to augment\n  Haptic Glove aided Control of Robotic Hand","summary":"  The letter focuses on Haptic Glove (HG) based control of a Robotic Hand (RH)\nexecuting in-hand manipulation of certain objects of interest. The high\ndimensional motion signals in HG and RH possess intrinsic variability of\nkinematics resulting in difficulty to establish a direct mapping of the motion\nsignals from HG onto the RH. An estimation mechanism is proposed to quantify\nthe motion signal acquired from the human controller in relation to the\nintended goal pose of the object being held by the robotic hand. A control\nalgorithm is presented to transform the synthesized intent at the RH and allow\nrelocation of the object to the expected goal pose. The lag in synthesis of the\nintent in the presence of communication delay leads to a requirement of\npredicting the estimated intent. We leverage an attention-based convolutional\nneural network encoder to predict the trajectory of intent for a certain\nlookahead to compensate for the delays. The proposed methodology is evaluated\nacross objects of different shapes, mass, and materials. We present a\ncomparative performance of the estimation and prediction mechanisms on\n5G-driven real-world robotic setup against benchmark methodologies. The\ntest-MSE in prediction of human intent is reported to yield ~ 97.3 -98.7%\nimprovement of accuracy in comparison to LSTM-based benchmark\n","authors":["Muneeb Ahmed","Rajesh Kumar","Qaim Abbas","Brejesh Lall","Arzad A. Kherani","Sudipto Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2110.07953v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17531v1","updated":"2024-03-26T09:36:26Z","published":"2024-03-26T09:36:26Z","title":"Design and Preliminary Evaluation of a Torso Stabiliser for Individuals\n  with Spinal Cord Injury","summary":"  Spinal cord injuries (SCIs) generally result in sensory and mobility\nimpairments, with torso instability being particularly debilitating. Existing\ntorso stabilisers are often rigid and restrictive. This paper presents an early\ninvestigation into a non-restrictive 1 degree-of-freedom (DoF) mechanical torso\nstabiliser inspired by devices such as centrifugal clutches and seat-belt\nmechanisms. Firstly, the paper presents a motion-capture (MoCap) and\nOpenSim-based kinematic analysis of the cable-based system to understand\nrequisite device characteristics. The simulated evaluation resulted in the\ncable-based device to require 55-60cm of unrestricted travel, and to lock at a\nthreshold cable velocity of 80-100cm/sec. Next, the developed 1-DoF device is\nintroduced. The proposed mechanical device is transparent during activities of\ndaily living, and transitions to compliant blocking when incipient fall is\ndetected. Prototype behaviour was then validated using a MoCap-based kinematic\nanalysis to verify non-restrictive movement, reliable transition to blocking,\nand compliance of the blocking.\n","authors":["Rejin John Varghese","Man-Yan Tong","Isabella Szczech","Peter Bryan","Dario Farina","Etienne Burdet"],"pdf_url":"https://arxiv.org/pdf/2403.17531v1.pdf","comment":"4 pages, 4 figures, 10 references. Submitted to IEEE EMBC 2024\n  conference"},{"id":"http://arxiv.org/abs/2011.07529v3","updated":"2024-03-26T08:54:31Z","published":"2020-11-15T13:40:21Z","title":"Full Attitude Intelligent Controller Design of a Heliquad under Complete\n  Failure of an Actuator","summary":"  In this paper, we design a reliable Heliquad and develop an intelligent\ncontroller to handle one actuators complete failure. Heliquad is a multi-copter\nsimilar to Quadcopter, with four actuators diagonally symmetric from the\ncenter. Each actuator has two control inputs; the first input changes the\npropeller blades collective pitch (also called variable pitch), and the other\ninput changes the rotation speed. For reliable operation and high torque\ncharacteristic requirement for yaw control, a cambered airfoil is used to\ndesign propeller blades. A neural network-based control allocation is designed\nto provide complete control authority even under a complete loss of one\nactuator. Nonlinear quaternion based outer loop position control, with\nproportional-derivative inner loop for attitude control and neural\nnetwork-based control allocation is used in controller design. The proposed\ncontroller and Heliquad designs performance is evaluated using a\nsoftware-in-loop simulation to track the position reference command under\nfailure. The results clearly indicate that the Heliquad with an intelligent\ncontroller provides necessary tracking performance even under a complete loss\nof one actuator.\n","authors":["Eeshan Kulkarni","Suresh Sundaram"],"pdf_url":"https://arxiv.org/pdf/2011.07529v3.pdf","comment":"7 pages, For video go to\n  https://indianinstituteofscience-my.sharepoint.com/:v:/g/personal/eeshank_iisc_ac_in/EcMg2uTtE91AsHDejNkb6YMBNckaXGjeh_YMzDV6sAHZAQ?e=DrRqmN"},{"id":"http://arxiv.org/abs/2304.02444v2","updated":"2024-03-26T08:13:01Z","published":"2023-04-05T14:02:53Z","title":"Autonomous Hook-Based Grasping and Transportation with Quadcopters","summary":"  Payload grasping and transportation with quadcopters is an active research\narea that has rapidly developed over the last decade. To grasp a payload\nwithout human interaction, most state-of-the-art approaches apply robotic arms\nthat are attached to the quadcopter body. However, due to the large weight and\npower consumption of these aerial manipulators, their agility and flight time\nare limited. This paper proposes a motion control and planning method for\ntransportation with a lightweight, passive manipulator structure that consists\nof a hook attached to a quadrotor using a 1 DoF revolute joint. To perform\npayload grasping, transportation, and release, first, time-optimal reference\ntrajectories are designed through specific waypoints to ensure the fast and\nreliable execution of the tasks. Then, a two-stage motion control approach is\ndeveloped based on a robust geometric controller for precise and reliable\nreference tracking and a linear--quadratic payload regulator for rapid setpoint\nstabilization of the payload swing. Furthermore, stability of the closed-loop\nsystem is mathematically proven to give safety guarantee for its operation. The\nproposed control architecture and design are evaluated in a high-fidelity\nphysical simulator, and also in real flight experiments, using a custom-made\nquadrotor--hook manipulator platform.\n","authors":["Péter Antal","Tamás Péni","Roland Tóth"],"pdf_url":"https://arxiv.org/pdf/2304.02444v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.07658v3","updated":"2024-03-26T08:01:29Z","published":"2024-01-15T13:00:35Z","title":"Robustness Evaluation of Localization Techniques for Autonomous Racing","summary":"  This work introduces SynPF, an MCL-based algorithm tailored for high-speed\nracing environments. Benchmarked against Cartographer, a state-of-the-art\npose-graph SLAM algorithm, SynPF leverages synergies from previous\nparticle-filtering methods and synthesizes them for the high-performance racing\ndomain. Our extensive in-field evaluations reveal that while Cartographer\nexcels under nominal conditions, it struggles when subjected to wheel-slip, a\ncommon phenomenon in a racing scenario due to varying grip levels and\naggressive driving behaviour. Conversely, SynPF demonstrates robustness in\nthese challenging conditions and a low-latency computation time of 1.25 ms on\non-board computers without a GPU. Using the F1TENTH platform, a 1:10 scaled\nautonomous racing vehicle, this work not only highlights the vulnerabilities of\nexisting algorithms in high-speed scenarios, tested up until 7.6 m/s, but also\nemphasizes the potential of SynPF as a viable alternative, especially in\ndeteriorating odometry conditions.\n","authors":["Tian Yi Lim","Edoardo Ghignone","Nicolas Baumann","Michele Magno"],"pdf_url":"https://arxiv.org/pdf/2401.07658v3.pdf","comment":"Accepted at the Design, Automation and Test in Europe Conference 2024\n  as an extended abstract"},{"id":"http://arxiv.org/abs/2403.17459v1","updated":"2024-03-26T07:47:52Z","published":"2024-03-26T07:47:52Z","title":"High-Power, Flexible, Robust Hand: Development of Musculoskeletal Hand\n  Using Machined Springs and Realization of Self-Weight Supporting Motion with\n  Humanoid","summary":"  Human can not only support their body during standing or walking, but also\nsupport them by hand, so that they can dangle a bar and others. But most\nhumanoid robots support their body only in the foot and they use their hand\njust to manipulate objects because their hands are too weak to support their\nbody. Strong hands are supposed to enable humanoid robots to act in much\nbroader scene. Therefore, we developed new life-size five-fingered hand that\ncan support the body of life-size humanoid robot. It is tendon-driven and\nunderactuated hand and actuators in forearms produce large gripping force. This\nhand has flexible joints using machined springs, which can be designed\nintegrally with the attachment. Thus, it has both structural strength and\nimpact resistance in spite of small size. As other characteristics, this hand\nhas force sensors to measure external force and the fingers can be flexed along\nobjects though the number of actuators to flex fingers is less than that of\nfingers. We installed the developed hand on musculoskeletal humanoid \"Kengoro\"\nand achieved two self-weight supporting motions: push-up motion and dangling\nmotion.\n","authors":["Shogo Makino","Kento Kawaharazuka","Masaya Kawamura","Yuki Asano","Kei Okada","Masayuki Inaba"],"pdf_url":"https://arxiv.org/pdf/2403.17459v1.pdf","comment":"accepted at IROS2017"},{"id":"http://arxiv.org/abs/2403.17452v1","updated":"2024-03-26T07:33:53Z","published":"2024-03-26T07:33:53Z","title":"Five-fingered Hand with Wide Range of Thumb Using Combination of\n  Machined Springs and Variable Stiffness Joints","summary":"  Human hands can not only grasp objects of various shape and size and\nmanipulate them in hands but also exert such a large gripping force that they\ncan support the body in the situations such as dangling a bar and climbing a\nladder. On the other hand, it is difficult for most robot hands to manage both.\nTherefore in this paper we developed the hand which can grasp various objects\nand exert large gripping force. To develop such hand, we focused on the thumb\nCM joint with wide range of motion and the MP joints of four fingers with the\nDOF of abduction and adduction. Based on the hand with large gripping force and\nflexibility using machined spring, we applied above mentioned joint mechanism\nto the hand. The thumb CM joint has wide range of motion because of the\ncombination of three machined springs and MP joints of four fingers have\nvariable rigidity mechanism instead of driving each joint independently in\norder to move joint in limited space and by limited actuators. Using the\ndeveloped hand, we achieved the grasping of various objects, supporting a large\nload and several motions with an arm.\n","authors":["Shogo Makino","Kento Kawaharazuka","Ayaka Fujii","Masaya Kawamura","Tasuku Makabe","Moritaka Onitsuka","Yuki Asano","Kei Okada","Koji Kawasaki","Masayuki Inaba"],"pdf_url":"https://arxiv.org/pdf/2403.17452v1.pdf","comment":"accepted at IROS2018"},{"id":"http://arxiv.org/abs/2403.17448v1","updated":"2024-03-26T07:26:27Z","published":"2024-03-26T07:26:27Z","title":"Adaptive Line-Of-Sight guidance law based on vector fields path\n  following for underactuated unmanned surface vehicle","summary":"  The focus of this paper is to develop a methodology that enables an unmanned\nsurface vehicle (USV) to efficiently track a planned path. The introduction of\na vector field-based adaptive line-of-sight guidance law (VFALOS) for accurate\ntrajectory tracking and minimizing the overshoot response time during USV\ntracking of curved paths improves the overall line-of-sight (LOS) guidance\nmethod. These improvements contribute to faster convergence to the desired\npath, reduce oscillations, and can mitigate the effects of persistent external\ndisturbances. It is shown that the proposed guidance law exhibits k-exponential\nstability when converging to the desired path consisting of straight and curved\nlines. The results in the paper show that the proposed method effectively\nimproves the accuracy of the USV tracking the desired path while ensuring the\nsafety of the USV work.\n","authors":["Jie Qi","Ronghua Wang","Nailong Wu","Yuxin Fan","Jigang Wang"],"pdf_url":"https://arxiv.org/pdf/2403.17448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17441v1","updated":"2024-03-26T07:19:06Z","published":"2024-03-26T07:19:06Z","title":"Adaptive LiDAR-Radar Fusion for Outdoor Odometry Across Dense Smoke\n  Conditions","summary":"  Robust odometry estimation in perceptually degraded environments represents a\nkey challenge in the field of robotics. In this paper, we propose a LiDAR-radar\nfusion method for robust odometry for adverse environment with LiDAR\ndegeneracy. By comparing the LiDAR point cloud with the radar static point\ncloud obtained through preprocessing module, it is possible to identify\ninstances of LiDAR degeneracy to overcome perceptual limits. We demonstrate the\neffectiveness of our method in challenging conditions such as dense smoke,\nshowcasing its ability to reliably estimate odometry and identify/remove\ndynamic points prone to LiDAR degeneracy.\n","authors":["Chiyun Noh","Ayoung Kim"],"pdf_url":"https://arxiv.org/pdf/2403.17441v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11011v2","updated":"2024-03-26T07:03:18Z","published":"2023-09-20T02:26:41Z","title":"OCC-VO: Dense Mapping via 3D Occupancy-Based Visual Odometry for\n  Autonomous Driving","summary":"  Visual Odometry (VO) plays a pivotal role in autonomous systems, with a\nprincipal challenge being the lack of depth information in camera images. This\npaper introduces OCC-VO, a novel framework that capitalizes on recent advances\nin deep learning to transform 2D camera images into 3D semantic occupancy,\nthereby circumventing the traditional need for concurrent estimation of ego\nposes and landmark locations. Within this framework, we utilize the TPV-Former\nto convert surround view cameras' images into 3D semantic occupancy. Addressing\nthe challenges presented by this transformation, we have specifically tailored\na pose estimation and mapping algorithm that incorporates Semantic Label\nFilter, Dynamic Object Filter, and finally, utilizes Voxel PFilter for\nmaintaining a consistent global semantic map. Evaluations on the Occ3D-nuScenes\nnot only showcase a 20.6% improvement in Success Ratio and a 29.6% enhancement\nin trajectory accuracy against ORB-SLAM3, but also emphasize our ability to\nconstruct a comprehensive map. Our implementation is open-sourced and available\nat: https://github.com/USTCLH/OCC-VO.\n","authors":["Heng Li","Yifan Duan","Xinran Zhang","Haiyi Liu","Jianmin Ji","Yanyong Zhang"],"pdf_url":"https://arxiv.org/pdf/2309.11011v2.pdf","comment":"7pages, 3 figures"},{"id":"http://arxiv.org/abs/2308.01557v2","updated":"2024-03-26T06:50:43Z","published":"2023-08-03T06:36:21Z","title":"Motion Planning Diffusion: Learning and Planning of Robot Motions with\n  Diffusion Models","summary":"  Learning priors on trajectory distributions can help accelerate robot motion\nplanning optimization. Given previously successful plans, learning trajectory\ngenerative models as priors for a new planning problem is highly desirable.\nPrior works propose several ways on utilizing this prior to bootstrapping the\nmotion planning problem. Either sampling the prior for initializations or using\nthe prior distribution in a maximum-a-posterior formulation for trajectory\noptimization. In this work, we propose learning diffusion models as priors. We\nthen can sample directly from the posterior trajectory distribution conditioned\non task goals, by leveraging the inverse denoising process of diffusion models.\nFurthermore, diffusion has been recently shown to effectively encode data\nmultimodality in high-dimensional settings, which is particularly well-suited\nfor large trajectory dataset. To demonstrate our method efficacy, we compare\nour proposed method - Motion Planning Diffusion - against several baselines in\nsimulated planar robot and 7-dof robot arm manipulator environments. To assess\nthe generalization capabilities of our method, we test it in environments with\npreviously unseen obstacles. Our experiments show that diffusion models are\nstrong priors to encode high-dimensional trajectory distributions of robot\nmotions.\n","authors":["Joao Carvalho","An T. Le","Mark Baierl","Dorothea Koert","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2308.01557v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17417v1","updated":"2024-03-26T06:14:58Z","published":"2024-03-26T06:14:58Z","title":"Cyclic pursuit formation control for arbitrary desired shapes","summary":"  A multi-agent system comprises numerous agents that autonomously make\ndecisions to collectively accomplish tasks, drawing significant attention for\ntheir wide-ranging applications. Within this context, formation control emerges\nas a prominent task, wherein agents collaboratively shape and maneuver while\npreserving formation integrity. Our focus centers on cyclic pursuit, a method\nfacilitating the formation of circles, ellipses, and figure-eights under the\nassumption that agents can only perceive the relative positions of those\npreceding them. However, this method's scope has been restricted to these\nspecific shapes, leaving the feasibility of forming other shapes uncertain. In\nresponse, our study proposes a novel method based on cyclic pursuit capable of\nforming a broader array of shapes, enabling agents to individually shape while\npursuing preceding agents, thereby extending the repertoire of achievable\nformations. We present two scenarios concerning the information available to\nagents and devise formation control methods tailored to each scenario. Through\nextensive simulations, we demonstrate the efficacy of our proposed method in\nforming multiple shapes, including those represented as Fourier series, thereby\nunderscoring the versatility and effectiveness of our approach.\n","authors":["Anna Fujioka","Masaki Ogura","Naoki Wakamiya"],"pdf_url":"https://arxiv.org/pdf/2403.17417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17392v1","updated":"2024-03-26T05:23:12Z","published":"2024-03-26T05:23:12Z","title":"Natural-artificial hybrid swarm: Cyborg-insect group navigation in\n  unknown obstructed soft terrain","summary":"  Navigating multi-robot systems in complex terrains has always been a\nchallenging task. This is due to the inherent limitations of traditional robots\nin collision avoidance, adaptation to unknown environments, and sustained\nenergy efficiency. In order to overcome these limitations, this research\nproposes a solution by integrating living insects with miniature electronic\ncontrollers to enable robotic-like programmable control, and proposing a novel\ncontrol algorithm for swarming. Although these creatures, called cyborg\ninsects, have the ability to instinctively avoid collisions with neighbors and\nobstacles while adapting to complex terrains, there is a lack of literature on\nthe control of multi-cyborg systems. This research gap is due to the difficulty\nin coordinating the movements of a cyborg system under the presence of insects'\ninherent individual variability in their reactions to control input. In\nresponse to this issue, we propose a novel swarm navigation algorithm\naddressing these challenges. The effectiveness of the algorithm is demonstrated\nthrough an experimental validation in which a cyborg swarm was successfully\nnavigated through an unknown sandy field with obstacles and hills. This\nresearch contributes to the domain of swarm robotics and showcases the\npotential of integrating biological organisms with robotics and control theory\nto create more intelligent autonomous systems with real-world applications.\n","authors":["Yang Bai","Phuoc Thanh Tran Ngoc","Huu Duoc Nguyen","Duc Long Le","Quang Huy Ha","Kazuki Kai","Yu Xiang See To","Yaosheng Deng","Jie Song","Naoki Wakamiya","Hirotaka Sato","Masaki Ogura"],"pdf_url":"https://arxiv.org/pdf/2403.17392v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17353v1","updated":"2024-03-26T03:32:45Z","published":"2024-03-26T03:32:45Z","title":"Multi-Objective Trajectory Planning with Dual-Encoder","summary":"  Time-jerk optimal trajectory planning is crucial in advancing robotic arms'\nperformance in dynamic tasks. Traditional methods rely on solving complex\nnonlinear programming problems, bringing significant delays in generating\noptimized trajectories. In this paper, we propose a two-stage approach to\naccelerate time-jerk optimal trajectory planning. Firstly, we introduce a\ndual-encoder based transformer model to establish a good preliminary\ntrajectory. This trajectory is subsequently refined through sequential\nquadratic programming to improve its optimality and robustness. Our approach\noutperforms the state-of-the-art by up to 79.72\\% in reducing trajectory\nplanning time. Compared with existing methods, our method shrinks the\noptimality gap with the objective function value decreasing by up to 29.9\\%.\n","authors":["Beibei Zhang","Tian Xiang","Chentao Mao","Yuhua Zheng","Shuai Li","Haoyi Niu","Xiangming Xi","Wenyuan Bai","Feng Gao"],"pdf_url":"https://arxiv.org/pdf/2403.17353v1.pdf","comment":"6 pages, 7 figures, conference"},{"id":"http://arxiv.org/abs/2403.17347v1","updated":"2024-03-26T03:15:14Z","published":"2024-03-26T03:15:14Z","title":"Unified Path and Gait Planning for Safe Bipedal Robot Navigation","summary":"  Safe path and gait planning are essential for bipedal robots to navigate\ncomplex real-world environments. The prevailing approaches often plan the path\nand gait separately in a hierarchical fashion, potentially resulting in unsafe\nmovements due to neglecting the physical constraints of walking robots. A\nsafety-critical path must not only avoid obstacles but also ensure that the\nrobot's gaits are subject to its dynamic and kinematic constraints. This work\npresents a novel approach that unifies path planning and gait planning via a\nModel Predictive Control (MPC) using the Linear Inverted Pendulum (LIP) model\nrepresenting bipedal locomotion. This approach considers environmental\nconstraints, such as obstacles, and the robot's kinematics and dynamics\nconstraints. By using discrete-time Control Barrier Functions for obstacle\navoidance, our approach generates the next foot landing position, ensuring\nrobust walking gaits and a safe navigation path within clustered environments.\nWe validated our proposed approach in simulation using a Digit robot in 20\nrandomly created environments. The results demonstrate improved performance in\nterms of safety and robustness when compared to hierarchical path and gait\nplanning frameworks.\n","authors":["Chengyang Peng","Victor Paredes","Ayonga Hereid"],"pdf_url":"https://arxiv.org/pdf/2403.17347v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2303.04225v2","updated":"2024-03-26T00:37:21Z","published":"2023-03-07T20:34:33Z","title":"Feeling Optimistic? Ambiguity Attitudes for Online Decision Making","summary":"  Due to the complexity of many decision making problems, tree search\nalgorithms often have inadequate information to produce accurate transition\nmodels. Robust methods, designed to make safe decisions when faced with these\nuncertainties, often overlook the impact expressions of uncertainty have on how\nthe decision is made. This work introduces the Ambiguity Attitude Graph Search\n(AAGS), advocating for more precise representation of ambiguities (uncertainty\nfrom a set of plausible models) in decision making. Additionally, AAGS allows\nusers to adjust their ambiguity attitude (or preference), promoting exploration\nand improving users' ability to control how an agent should respond when faced\nwith a set of valid alternatives. Simulation in a dynamic sailing environment\nshows how highly stochastic environments can lead robust methods to fail.\nResults further demonstrate how adjusting ambiguity attitudes better fulfills\nobjectives while mitigating this failure mode of robust approaches. Because\nthis approach is a generalization of the robust framework, these results\nfurther demonstrate how algorithms focused on ambiguity have applicability\nbeyond safety-critical systems.\n","authors":["Jared J. Beard","R. Michael Butts","Yu Gu"],"pdf_url":"https://arxiv.org/pdf/2303.04225v2.pdf","comment":"6 pages, 5 figures, 2 algorithms. Submitted to the 2024 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems in Abu Dhabi, UAE\n  (Oct 14-18, 2024)"},{"id":"http://arxiv.org/abs/2403.17288v1","updated":"2024-03-26T00:35:06Z","published":"2024-03-26T00:35:06Z","title":"Sparse-Graph-Enabled Formation Planning for Large-Scale Aerial Swarms","summary":"  The formation trajectory planning using complete graphs to model\ncollaborative constraints becomes computationally intractable as the number of\ndrones increases due to the curse of dimensionality. To tackle this issue, this\npaper presents a sparse graph construction method for formation planning to\nrealize better efficiency-performance trade-off. Firstly, a sparsification\nmechanism for complete graphs is designed to ensure the global rigidity of\nsparsified graphs, which is a necessary condition for uniquely corresponding to\na geometric shape. Secondly, a good sparse graph is constructed to preserve the\nmain structural feature of complete graphs sufficiently. Since the graph-based\nformation constraint is described by Laplacian matrix, the sparse graph\nconstruction problem is equivalent to submatrix selection, which has\ncombinatorial time complexity and needs a scoring metric. Via comparative\nsimulations, the Max-Trace matrix-revealing metric shows the promising\nperformance. The sparse graph is integrated into the formation planning.\nSimulation results with 72 drones in complex environments demonstrate that when\npreserving 30\\% connection edges, our method has comparative formation error\nand recovery performance w.r.t. complete graphs. Meanwhile, the planning\nefficiency is improved by approximate an order of magnitude. Benchmark\ncomparisons and ablation studies are conducted to fully validate the merits of\nour method.\n","authors":["Yuan Zhou","Lun Quan","Chao Xu","Guangtong Xu","Fei Gao"],"pdf_url":"https://arxiv.org/pdf/2403.17288v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16967v2","updated":"2024-03-26T22:00:27Z","published":"2024-03-25T17:26:08Z","title":"Visual Whole-Body Control for Legged Loco-Manipulation","summary":"  We study the problem of mobile manipulation using legged robots equipped with\nan arm, namely legged loco-manipulation. The robot legs, while usually utilized\nfor mobility, offer an opportunity to amplify the manipulation capabilities by\nconducting whole-body control. That is, the robot can control the legs and the\narm at the same time to extend its workspace. We propose a framework that can\nconduct the whole-body control autonomously with visual observations. Our\napproach, namely Visual Whole-Body Control(VBC), is composed of a low-level\npolicy using all degrees of freedom to track the end-effector manipulator\nposition and a high-level policy proposing the end-effector position based on\nvisual inputs. We train both levels of policies in simulation and perform\nSim2Real transfer for real robot deployment. We perform extensive experiments\nand show significant improvements over baselines in picking up diverse objects\nin different configurations (heights, locations, orientations) and\nenvironments. Project page: https://wholebody-b1.github.io\n","authors":["Minghuan Liu","Zixuan Chen","Xuxin Cheng","Yandong Ji","Ruihan Yang","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16967v2.pdf","comment":"The first two authors contribute equally. Project page:\n  https://wholebody-b1.github.io"}]},"2024-03-24T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2311.02787v2","updated":"2024-03-24T23:36:06Z","published":"2023-11-05T22:43:29Z","title":"Make a Donut: Hierarchical EMD-Space Planning for Zero-Shot Deformable\n  Manipulation with Tools","summary":"  Deformable object manipulation stands as one of the most captivating yet\nformidable challenges in robotics. While previous techniques have predominantly\nrelied on learning latent dynamics through demonstrations, typically\nrepresented as either particles or images, there exists a pertinent limitation:\nacquiring suitable demonstrations, especially for long-horizon tasks, can be\nelusive. Moreover, basing learning entirely on demonstrations can hamper the\nmodel's ability to generalize beyond the demonstrated tasks. In this work, we\nintroduce a demonstration-free hierarchical planning approach capable of\ntackling intricate long-horizon tasks without necessitating any training. We\nemploy large language models (LLMs) to articulate a high-level, stage-by-stage\nplan corresponding to a specified task. For every individual stage, the LLM\nprovides both the tool's name and the Python code to craft intermediate subgoal\npoint clouds. With the tool and subgoal for a particular stage at our disposal,\nwe present a granular closed-loop model predictive control strategy. This\nleverages Differentiable Physics with Point-to-Point correspondence\n(DiffPhysics-P2P) loss in the earth mover distance (EMD) space, applied\niteratively. Experimental findings affirm that our technique surpasses multiple\nbenchmarks in dough manipulation, spanning both short and long horizons.\nRemarkably, our model demonstrates robust generalization capabilities to novel\nand previously unencountered complex tasks without any preliminary\ndemonstrations. We further substantiate our approach with experimental trials\non real-world robotic platforms. Our project page:\nhttps://qq456cvb.github.io/projects/donut.\n","authors":["Yang You","Bokui Shen","Congyue Deng","Haoran Geng","Songlin Wei","He Wang","Leonidas Guibas"],"pdf_url":"https://arxiv.org/pdf/2311.02787v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2212.13332v3","updated":"2024-03-24T23:18:18Z","published":"2022-12-27T01:06:26Z","title":"Development and Evaluation of a Learning-based Model for Real-time\n  Haptic Texture Rendering","summary":"  Current Virtual Reality (VR) environments lack the rich haptic signals that\nhumans experience during real-life interactions, such as the sensation of\ntexture during lateral movement on a surface. Adding realistic haptic textures\nto VR environments requires a model that generalizes to variations of a user's\ninteraction and to the wide variety of existing textures in the world. Current\nmethodologies for haptic texture rendering exist, but they usually develop one\nmodel per texture, resulting in low scalability. We present a deep\nlearning-based action-conditional model for haptic texture rendering and\nevaluate its perceptual performance in rendering realistic texture vibrations\nthrough a multi part human user study. This model is unified over all materials\nand uses data from a vision-based tactile sensor (GelSight) to render the\nappropriate surface conditioned on the user's action in real time. For\nrendering texture, we use a high-bandwidth vibrotactile transducer attached to\na 3D Systems Touch device. The result of our user study shows that our\nlearning-based method creates high-frequency texture renderings with comparable\nor better quality than state-of-the-art methods without the need for learning a\nseparate model per texture. Furthermore, we show that the method is capable of\nrendering previously unseen textures using a single GelSight image of their\nsurface.\n","authors":["Negin Heravi","Heather Culbertson","Allison M. Okamura","Jeannette Bohg"],"pdf_url":"https://arxiv.org/pdf/2212.13332v3.pdf","comment":"Accepted for publication in IEEE Transactions on Haptics 2024. 12\n  pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.16320v1","updated":"2024-03-24T22:56:08Z","published":"2024-03-24T22:56:08Z","title":"Single-Motor Robotic Gripper with Multi-Surface Fingers for Variable\n  Grasping Configurations","summary":"  This study proposes a novel robotic gripper with variable grasping\nconfigurations for grasping various objects. The fingers of the developed\ngripper incorporate multiple different surfaces. The gripper possesses the\nfunction of altering the finger surfaces facing a target object by rotating the\nfingers in its longitudinal direction. In the proposed design equipped with two\nfingers, the two fingers incorporate three and four surfaces, respectively,\nresulting in the nine available grasping configurations by the combination of\nthese finger surfaces. The developed gripper is equipped with the functions of\nopening/closing its fingers for grasping and rotating its fingers to alter the\ngrasping configuration -all achieved with a single motor. To enable the two\nmotions using a single motor, this study introduces a self-motion switching\nmechanism utilizing magnets. This mechanism automatically transitions between\ngripper motions based on the direction of the motor rotation when the gripper\nis fully opened. In this state, rotating the motor towards closing initiates\nthe finger closing action, while further opening the fingers from the fully\nopened state activates the finger rotation. This letter presents the gripper\ndesign, the mechanics of the self-motion switching mechanism, the control\nmethod, and the grasping configuration selection strategy. The performance of\nthe gripper is experimentally demonstrated.\n","authors":["Toshihiro Nishimura","Yosuke Suzuki","Tokuo Tsuj","Tetsuyou Watanabe"],"pdf_url":"https://arxiv.org/pdf/2403.16320v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16277v1","updated":"2024-03-24T19:52:53Z","published":"2024-03-24T19:52:53Z","title":"Combined Task and Motion Planning Via Sketch Decompositions (Extended\n  Version with Supplementary Material)","summary":"  The challenge in combined task and motion planning (TAMP) is the effective\nintegration of a search over a combinatorial space, usually carried out by a\ntask planner, and a search over a continuous configuration space, carried out\nby a motion planner. Using motion planners for testing the feasibility of task\nplans and filling out the details is not effective because it makes the\ngeometrical constraints play a passive role. This work introduces a new\ninterleaved approach for integrating the two dimensions of TAMP that makes use\nof sketches, a recent simple but powerful language for expressing the\ndecomposition of problems into subproblems. A sketch has width 1 if it\ndecomposes the problem into subproblems that can be solved greedily in linear\ntime. In the paper, a general sketch is introduced for several classes of TAMP\nproblems which has width 1 under suitable assumptions. While sketch\ndecompositions have been developed for classical planning, they offer two\nimportant benefits in the context of TAMP. First, when a task plan is found to\nbe unfeasible due to the geometric constraints, the combinatorial search\nresumes in a specific sub-problem. Second, the sampling of object\nconfigurations is not done once, globally, at the start of the search, but\nlocally, at the start of each subproblem. Optimizations of this basic setting\nare also considered and experimental results over existing and new\npick-and-place benchmarks are reported.\n","authors":["Magí Dalmau-Moreno","Néstor García","Vicenç Gómez","Héctor Geffner"],"pdf_url":"https://arxiv.org/pdf/2403.16277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16275v1","updated":"2024-03-24T19:47:37Z","published":"2024-03-24T19:47:37Z","title":"M^3RS: Multi-robot, Multi-objective, and Multi-mode Routing and\n  Scheduling","summary":"  In this paper, we present a novel problem coined multi-robot,\nmulti-objective, and multi-mode routing and scheduling (M^3RS). The formulation\nfor M^3RS is introduced for time-bound multi-robot, multi-objective routing and\nscheduling missions where each task has multiple execution modes. Different\nexecution modes have distinct resource consumption, associated execution time,\nand quality. M^3RS assigns the optimal sequence of tasks and the execution\nmodes to each agent. The routes and associated modes depend on user preferences\nfor different objective criteria. The need for M^3RS comes from multi-robot\napplications in which a trade-off between multiple criteria arises from\ndifferent task execution modes. We use M^3RS for the application of multi-robot\ndisinfection in public locations. The objectives considered for disinfection\napplication are disinfection quality and number of tasks completed. A\nmixed-integer linear programming model is proposed for M^3RS. Then, a\ntime-efficient column generation scheme is presented to tackle the issue of\ncomputation times for larger problem instances. The advantage of using multiple\nmodes over fixed execution mode is demonstrated using experiments on synthetic\ndata. The results suggest that M^3RS provides flexibility to the user in terms\nof available solutions and performs well in joint performance metrics. The\napplication of the proposed problem is shown for a team of disinfection\nrobots.} The videos for the experiments are available on the project website:\nhttps://sites.google.com/view/g-robot/m3rs/ .\n","authors":["Ishaan Mehta","Junseo Kim","Sharareh Taghipour","Sajad Saeedi"],"pdf_url":"https://arxiv.org/pdf/2403.16275v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.08812v4","updated":"2024-03-24T19:25:20Z","published":"2022-09-19T07:52:02Z","title":"Generative Graphical Inverse Kinematics","summary":"  Quickly and reliably finding accurate inverse kinematics (IK) solutions\nremains a challenging problem for many robot manipulators. Existing numerical\nsolvers are broadly applicable but typically only produce a single solution and\nrely on local search techniques to minimize nonconvex objective functions. More\nrecent learning-based approaches that approximate the entire feasible set of\nsolutions have shown promise as a means to generate multiple fast and accurate\nIK results in parallel. However, existing learning-based techniques have a\nsignificant drawback: each robot of interest requires a specialized model that\nmust be trained from scratch. To address this key shortcoming, we propose a\nnovel distance-geometric robot representation coupled with a graph structure\nthat allows us to leverage the sample efficiency of Euclidean equivariant\nfunctions and the generalizability of graph neural networks (GNNs). Our\napproach is generative graphical inverse kinematics (GGIK), the first learned\nIK solver able to accurately and efficiently produce a large number of diverse\nsolutions in parallel while also displaying the ability to generalize -- a\nsingle learned model can be used to produce IK solutions for a variety of\ndifferent robots. When compared to several other learned IK methods, GGIK\nprovides more accurate solutions with the same amount of data. GGIK can\ngeneralize reasonably well to robot manipulators unseen during training.\nAdditionally, GGIK can learn a constrained distribution that encodes joint\nlimits and scales efficiently to larger robots and a high number of sampled\nsolutions. Finally, GGIK can be used to complement local IK solvers by\nproviding reliable initializations for a local optimization process.\n","authors":["Oliver Limoyo","Filip Marić","Matthew Giamou","Petra Alexson","Ivan Petrović","Jonathan Kelly"],"pdf_url":"https://arxiv.org/pdf/2209.08812v4.pdf","comment":"Submitted to IEEE Transactions on Robotics, June 2023"},{"id":"http://arxiv.org/abs/2403.16262v1","updated":"2024-03-24T18:49:16Z","published":"2024-03-24T18:49:16Z","title":"HT-LIP Model based Robust Control of Quadrupedal Robot Locomotion under\n  Unknown Vertical Ground Motion","summary":"  This paper presents a hierarchical control framework that enables robust\nquadrupedal locomotion on a dynamic rigid surface (DRS) with general and\nunknown vertical motions. The key novelty of the framework lies in its higher\nlayer, which is a discrete-time, provably stabilizing footstep controller. The\nbasis of the footstep controller is a new hybrid, time-varying, linear inverted\npendulum (HT-LIP) model that is low-dimensional and accurately captures the\nessential robot dynamics during DRS locomotion. A new set of sufficient\nstability conditions are then derived to directly guide the controller design\nfor ensuring the asymptotic stability of the HT-LIP model under general,\nunknown, vertical DRS motions. Further, the footstep controller is cast as a\ncomputationally efficient quadratic program that incorporates the proposed\nHT-LIP model and stability conditions. The middle layer takes the desired\nfootstep locations generated by the higher layer as input to produce\nkinematically feasible full-body reference trajectories, which are then\naccurately tracked by a lower-layer torque controller. Hardware experiments on\na Unitree Go1 quadrupedal robot confirm the robustness of the proposed\nframework under various unknown, aperiodic, vertical DRS motions and\nuncertainties (e.g., slippery and uneven surfaces, solid and liquid loads, and\nsudden pushes).\n","authors":["Amir Iqbal","Sushant Veer","Christopher Niezrecki","Yan Gu"],"pdf_url":"https://arxiv.org/pdf/2403.16262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16252v1","updated":"2024-03-24T18:10:30Z","published":"2024-03-24T18:10:30Z","title":"Legged Robot State Estimation within Non-inertial Environments","summary":"  This paper investigates the robot state estimation problem within a\nnon-inertial environment. The proposed state estimation approach relaxes the\ncommon assumption of static ground in the system modeling. The process and\nmeasurement models explicitly treat the movement of the non-inertial\nenvironments without requiring knowledge of its motion in the inertial frame or\nrelying on GPS or sensing environmental landmarks. Further, the proposed state\nestimator is formulated as an invariant extended Kalman filter (InEKF) with the\ndeterministic part of its process model obeying the group-affine property,\nleading to log-linear error dynamics. The observability analysis of the filter\nconfirms that the robot's pose (i.e., position and orientation) and velocity\nrelative to the non-inertial environment are observable. Hardware experiments\non a humanoid robot moving on a rotating and translating treadmill demonstrate\nthe high convergence rate and accuracy of the proposed InEKF even under\nsignificant treadmill pitch sway, as well as large estimation errors.\n","authors":["Zijian He","Sangli Teng","Tzu-Yuan Lin","Maani Ghaffari","Yan Gu"],"pdf_url":"https://arxiv.org/pdf/2403.16252v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.12188v2","updated":"2024-03-24T17:19:14Z","published":"2023-09-21T15:54:33Z","title":"SG-Bot: Object Rearrangement via Coarse-to-Fine Robotic Imagination on\n  Scene Graphs","summary":"  Object rearrangement is pivotal in robotic-environment interactions,\nrepresenting a significant capability in embodied AI. In this paper, we present\nSG-Bot, a novel rearrangement framework that utilizes a coarse-to-fine scheme\nwith a scene graph as the scene representation. Unlike previous methods that\nrely on either known goal priors or zero-shot large models, SG-Bot exemplifies\nlightweight, real-time, and user-controllable characteristics, seamlessly\nblending the consideration of commonsense knowledge with automatic generation\ncapabilities. SG-Bot employs a three-fold procedure--observation, imagination,\nand execution--to adeptly address the task. Initially, objects are discerned\nand extracted from a cluttered scene during the observation. These objects are\nfirst coarsely organized and depicted within a scene graph, guided by either\ncommonsense or user-defined criteria. Then, this scene graph subsequently\ninforms a generative model, which forms a fine-grained goal scene considering\nthe shape information from the initial scene and object semantics. Finally, for\nexecution, the initial and envisioned goal scenes are matched to formulate\nrobotic action policies. Experimental results demonstrate that SG-Bot\noutperforms competitors by a large margin.\n","authors":["Guangyao Zhai","Xiaoni Cai","Dianye Huang","Yan Di","Fabian Manhardt","Federico Tombari","Nassir Navab","Benjamin Busam"],"pdf_url":"https://arxiv.org/pdf/2309.12188v2.pdf","comment":"ICRA 2024 accepted. Project website:\n  https://sites.google.com/view/sg-bot"},{"id":"http://arxiv.org/abs/2403.16238v1","updated":"2024-03-24T17:00:01Z","published":"2024-03-24T17:00:01Z","title":"KITchen: A Real-World Benchmark and Dataset for 6D Object Pose\n  Estimation in Kitchen Environments","summary":"  Despite the recent progress on 6D object pose estimation methods for robotic\ngrasping, a substantial performance gap persists between the capabilities of\nthese methods on existing datasets and their efficacy in real-world mobile\nmanipulation tasks, particularly when robots rely solely on their monocular\negocentric field of view (FOV). Existing real-world datasets primarily focus on\ntable-top grasping scenarios, where a robotic arm is placed in a fixed position\nand the objects are centralized within the FOV of fixed external camera(s).\nAssessing performance on such datasets may not accurately reflect the\nchallenges encountered in everyday mobile manipulation tasks within kitchen\nenvironments such as retrieving objects from higher shelves, sinks,\ndishwashers, ovens, refrigerators, or microwaves. To address this gap, we\npresent Kitchen, a novel benchmark designed specifically for estimating the 6D\nposes of objects located in diverse positions within kitchen settings. For this\npurpose, we recorded a comprehensive dataset comprising around 205k real-world\nRGBD images for 111 kitchen objects captured in two distinct kitchens,\nutilizing one humanoid robot with its egocentric perspectives. Subsequently, we\ndeveloped a semi-automated annotation pipeline, to streamline the labeling\nprocess of such datasets, resulting in the generation of 2D object labels, 2D\nobject segmentation masks, and 6D object poses with minimized human effort. The\nbenchmark, the dataset, and the annotation pipeline are available at\nhttps://kitchen-dataset.github.io/KITchen.\n","authors":["Abdelrahman Younes","Tamim Asfour"],"pdf_url":"https://arxiv.org/pdf/2403.16238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16178v1","updated":"2024-03-24T14:38:18Z","published":"2024-03-24T14:38:18Z","title":"Mixed-Initiative Human-Robot Teaming under Suboptimality with Online\n  Bayesian Adaptation","summary":"  For effective human-agent teaming, robots and other artificial intelligence\n(AI) agents must infer their human partner's abilities and behavioral response\npatterns and adapt accordingly. Most prior works make the unrealistic\nassumption that one or more teammates can act near-optimally. In real-world\ncollaboration, humans and autonomous agents can be suboptimal, especially when\neach only has partial domain knowledge. In this work, we develop computational\nmodeling and optimization techniques for enhancing the performance of\nsuboptimal human-agent teams, where the human and the agent have asymmetric\ncapabilities and act suboptimally due to incomplete environmental knowledge. We\nadopt an online Bayesian approach that enables a robot to infer people's\nwillingness to comply with its assistance in a sequential decision-making game.\nOur user studies show that user preferences and team performance indeed vary\nwith robot intervention styles, and our approach for mixed-initiative\ncollaborations enhances objective team performance ($p<.001$) and subjective\nmeasures, such as user's trust ($p<.001$) and perceived likeability of the\nrobot ($p<.001$).\n","authors":["Manisha Natarajan","Chunyue Xue","Sanne van Waveren","Karen Feigh","Matthew Gombolay"],"pdf_url":"https://arxiv.org/pdf/2403.16178v1.pdf","comment":"8 pages, 4 pages for supplementary"},{"id":"http://arxiv.org/abs/2311.12261v2","updated":"2024-03-24T14:18:36Z","published":"2023-11-21T00:45:13Z","title":"EnduRL: Enhancing Safety, Stability, and Efficiency of Mixed Traffic\n  Under Real-World Perturbations Via Reinforcement Learning","summary":"  Human-driven vehicles (HVs) amplify naturally occurring perturbations in\ntraffic, leading to congestion--a major contributor to increased fuel\nconsumption, higher collision risks, and reduced road capacity utilization.\nWhile previous research demonstrates that Robot Vehicles (RVs) can be leveraged\nto mitigate these issues, most such studies rely on simulations with simplistic\nmodels of human car-following behaviors. In this work, we analyze real-world\ndriving trajectories and extract a wide range of acceleration profiles. We then\nincorporates these profiles into simulations for training RVs to mitigate\ncongestion. We evaluate the safety, efficiency, and stability of mixed traffic\nvia comprehensive experiments conducted in two mixed traffic environments (Ring\nand Bottleneck) at various traffic densities, configurations, and RV\npenetration rates. The results show that under real-world perturbations, prior\nRV controllers experience performance degradation on all three objectives\n(sometimes even lower than 100% HVs). To address this, we introduce a\nreinforcement learning based RV that employs a congestion stage classifier to\noptimize the safety, efficiency, and stability of mixed traffic. Our RVs\ndemonstrate significant improvements: safety by up to 66%, efficiency by up to\n54%, and stability by up to 97%.\n","authors":["Bibek Poudel","Weizi Li","Kevin Heaslip"],"pdf_url":"https://arxiv.org/pdf/2311.12261v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04846v2","updated":"2024-03-24T14:18:28Z","published":"2023-10-07T15:11:31Z","title":"Soft finger rotational stability for precision grasps","summary":"  Soft robotic fingers can safely grasp fragile or variable form objects, but\ntheir force capacity is limited, especially with less contact area: precision\ngrasps and when objects are smaller or not spherical. Current research is\nimproving force capacity through mechanical design by increasing contact area\nor stiffness, typically without models which explain soft finger force\nlimitations. To address this, this paper considers two types of soft grip\nfailure, slip and dynamic rotational stability. For slip, the validity of a\nCoulomb model investigated, identifying the effect of contact area, pressure,\nand relative pose. For rotational stability, bulk linear stiffness of the\nfingers is used to develop conditions for dynamic stability and identify when\nrotation leads to slip. Together, these models suggest contact area improves\nforce capacity by increasing transverse stiffness and normal force. The models\nare validated on pneumatic fingers, both custom PneuNets-based and commercially\navailable. The models are used to find grip parameters which increase force\ncapacity without failure.\n","authors":["Hun Jang","Valentyn Petrichenko","Joonbum Bae","Kevin Haninger"],"pdf_url":"https://arxiv.org/pdf/2310.04846v2.pdf","comment":"Submitted IROS24"},{"id":"http://arxiv.org/abs/2310.04822v2","updated":"2024-03-24T13:57:35Z","published":"2023-10-07T14:21:43Z","title":"Combining Sampling- and Gradient-based Planning for Contact-rich\n  Manipulation","summary":"  Planning over discontinuous dynamics is needed for robotics tasks like\ncontact-rich manipulation, which presents challenges in the numerical stability\nand speed of planning methods when either neural network or analytical models\nare used. On the one hand, sampling-based planners require higher sample\ncomplexity in high-dimensional problems and cannot describe safety constraints\nsuch as force limits. On the other hand, gradient-based solvers can suffer from\nlocal optima and convergence issues when the Hessian is poorly conditioned. We\npropose a planning method with both sampling- and gradient-based elements,\nusing the Cross-entropy Method to initialize a gradient-based solver, providing\nbetter search over local minima and the ability to handle explicit constraints.\nWe show the approach allows smooth, stable contact-rich planning for an\nimpedance-controlled robot making contact with a stiff environment,\nbenchmarking against gradient-only MPC and CEM.\n","authors":["Filippo Rozzi","Loris Roveda","Kevin Haninger"],"pdf_url":"https://arxiv.org/pdf/2310.04822v2.pdf","comment":"Submitted ICRA24. Video available at https://youtu.be/COqR90392Kw\n  Code available at https://gitlab.cc-asp.fraunhofer.de/hanikevi/contact_mpc"},{"id":"http://arxiv.org/abs/2403.16146v1","updated":"2024-03-24T13:36:23Z","published":"2024-03-24T13:36:23Z","title":"Realtime Robust Shape Estimation of Deformable Linear Object","summary":"  Realtime shape estimation of continuum objects and manipulators is essential\nfor developing accurate planning and control paradigms. The existing methods\nthat create dense point clouds from camera images, and/or use distinguishable\nmarkers on a deformable body have limitations in realtime tracking of large\ncontinuum objects/manipulators. The physical occlusion of markers can often\ncompromise accurate shape estimation. We propose a robust method to estimate\nthe shape of linear deformable objects in realtime using scattered and\nunordered key points. By utilizing a robust probability-based labeling\nalgorithm, our approach identifies the true order of the detected key points\nand then reconstructs the shape using piecewise spline interpolation. The\napproach only relies on knowing the number of the key points and the interval\nbetween two neighboring points. We demonstrate the robustness of the method\nwhen key points are partially occluded. The proposed method is also integrated\ninto a simulation in Unity for tracking the shape of a cable with a length of\n1m and a radius of 5mm. The simulation results show that our proposed approach\nachieves an average length error of 1.07% over the continuum's centerline and\nan average cross-section error of 2.11mm. The real-world experiments of\ntracking and estimating a heavy-load cable prove that the proposed approach is\nrobust under occlusion and complex entanglement scenarios.\n","authors":["Jiaming Zhang","Zhaomeng Zhang","Yihao Liu","Yaqian Chen","Amir Kheradmand","Mehran Armand"],"pdf_url":"https://arxiv.org/pdf/2403.16146v1.pdf","comment":"This paper has been accepted to IEEE ICRA 2024 as a contributed paper"},{"id":"http://arxiv.org/abs/2403.16095v1","updated":"2024-03-24T11:19:59Z","published":"2024-03-24T11:19:59Z","title":"CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D\n  Gaussian Field","summary":"  Recently neural radiance fields (NeRF) have been widely exploited as 3D\nrepresentations for dense simultaneous localization and mapping (SLAM). Despite\ntheir notable successes in surface modeling and novel view synthesis, existing\nNeRF-based methods are hindered by their computationally intensive and\ntime-consuming volume rendering pipeline. This paper presents an efficient\ndense RGB-D SLAM system, i.e., CG-SLAM, based on a novel uncertainty-aware 3D\nGaussian field with high consistency and geometric stability. Through an\nin-depth analysis of Gaussian Splatting, we propose several techniques to\nconstruct a consistent and stable 3D Gaussian field suitable for tracking and\nmapping. Additionally, a novel depth uncertainty model is proposed to ensure\nthe selection of valuable Gaussian primitives during optimization, thereby\nimproving tracking efficiency and accuracy. Experiments on various datasets\ndemonstrate that CG-SLAM achieves superior tracking and mapping performance\nwith a notable tracking speed of up to 15 Hz. We will make our source code\npublicly available. Project page: https://zju3dv.github.io/cg-slam.\n","authors":["Jiarui Hu","Xianhao Chen","Boyin Feng","Guanglin Li","Liangjing Yang","Hujun Bao","Guofeng Zhang","Zhaopeng Cui"],"pdf_url":"https://arxiv.org/pdf/2403.16095v1.pdf","comment":"Project Page: https://zju3dv.github.io/cg-slam"},{"id":"http://arxiv.org/abs/2403.16092v1","updated":"2024-03-24T11:09:41Z","published":"2024-03-24T11:09:41Z","title":"Are NeRFs ready for autonomous driving? Towards closing the\n  real-to-simulation gap","summary":"  Neural Radiance Fields (NeRFs) have emerged as promising tools for advancing\nautonomous driving (AD) research, offering scalable closed-loop simulation and\ndata augmentation capabilities. However, to trust the results achieved in\nsimulation, one needs to ensure that AD systems perceive real and rendered data\nin the same way. Although the performance of rendering methods is increasing,\nmany scenarios will remain inherently challenging to reconstruct faithfully. To\nthis end, we propose a novel perspective for addressing the real-to-simulated\ndata gap. Rather than solely focusing on improving rendering fidelity, we\nexplore simple yet effective methods to enhance perception model robustness to\nNeRF artifacts without compromising performance on real data. Moreover, we\nconduct the first large-scale investigation into the real-to-simulated data gap\nin an AD setting using a state-of-the-art neural rendering technique.\nSpecifically, we evaluate object detectors and an online mapping model on real\nand simulated data, and study the effects of different pre-training strategies.\nOur results show notable improvements in model robustness to simulated data,\neven improving real-world performance in some cases. Last, we delve into the\ncorrelation between the real-to-simulated gap and image reconstruction metrics,\nidentifying FID and LPIPS as strong indicators.\n","authors":["Carl Lindström","Georg Hess","Adam Lilja","Maryam Fatemi","Lars Hammarstrand","Christoffer Petersson","Lennart Svensson"],"pdf_url":"https://arxiv.org/pdf/2403.16092v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16699v2","updated":"2024-03-24T09:46:04Z","published":"2024-02-26T16:13:09Z","title":"SwarmPRM: Probabilistic Roadmap Motion Planning for Large-Scale Swarm\n  Robotic Systems","summary":"  Large-scale swarm robotic systems consisting of numerous cooperative agents\nshow considerable promise for performing autonomous tasks across various\nsectors. Nonetheless, traditional motion planning approaches often face a\ntrade-off between scalability and solution quality due to the exponential\ngrowth of the joint state space of robots. In response, this work proposes\nSwarmPRM, a hierarchical, scalable, computationally efficient, and risk-aware\nsampling-based motion planning approach for large-scale swarm robots. SwarmPRM\nutilizes a Gaussian Mixture Model (GMM) to represent the swarm's macroscopic\nstate and constructs a Probabilistic Roadmap in Gaussian space, referred to as\nthe Gaussian roadmap, to generate a transport trajectory of GMM. This\ntrajectory is then followed by each robot at the microscopic stage. To enhance\ntrajectory safety, SwarmPRM incorporates the conditional value-at-risk (CVaR)\nin the collision checking process to impart the property of risk awareness to\nthe constructed Gaussian roadmap. SwarmPRM then crafts a linear programming\nformulation to compute the optimal GMM transport trajectory within this\nroadmap. Extensive simulations demonstrate that SwarmPRM outperforms\nstate-of-the-art methods in computational efficiency, scalability, and\ntrajectory quality while offering the capability to adjust the risk tolerance\nof generated trajectories.\n","authors":["Yunze Hu","Xuru Yang","Kangjie Zhou","Qinghang Liu","Kang Ding","Han Gao","Pingping Zhu","Chang Liu"],"pdf_url":"https://arxiv.org/pdf/2402.16699v2.pdf","comment":"Submitted to IROS 2024"},{"id":"http://arxiv.org/abs/2403.16023v1","updated":"2024-03-24T05:55:39Z","published":"2024-03-24T05:55:39Z","title":"RPMArt: Towards Robust Perception and Manipulation for Articulated\n  Objects","summary":"  Articulated objects are commonly found in daily life. It is essential that\nrobots can exhibit robust perception and manipulation skills for articulated\nobjects in real-world robotic applications. However, existing methods for\narticulated objects insufficiently address noise in point clouds and struggle\nto bridge the gap between simulation and reality, thus limiting the practical\ndeployment in real-world scenarios. To tackle these challenges, we propose a\nframework towards Robust Perception and Manipulation for Articulated Objects\n(RPMArt), which learns to estimate the articulation parameters and manipulate\nthe articulation part from the noisy point cloud. Our primary contribution is a\nRobust Articulation Network (RoArtNet) that is able to predict both joint\nparameters and affordable points robustly by local feature learning and point\ntuple voting. Moreover, we introduce an articulation-aware classification\nscheme to enhance its ability for sim-to-real transfer. Finally, with the\nestimated affordable point and articulation joint constraint, the robot can\ngenerate robust actions to manipulate articulated objects. After learning only\nfrom synthetic data, RPMArt is able to transfer zero-shot to real-world\narticulated objects. Experimental results confirm our approach's effectiveness,\nwith our framework achieving state-of-the-art performance in both noise-added\nsimulation and real-world environments. The code and data will be open-sourced\nfor reproduction. More results are published on the project website at\nhttps://r-pmart.github.io .\n","authors":["Junbo Wang","Wenhai Liu","Qiaojun Yu","Yang You","Liu Liu","Weiming Wang","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2403.16023v1.pdf","comment":"8 pages, 7 figures, submitted to 2024 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS 2024), project website at\n  https://r-pmart.github.io"},{"id":"http://arxiv.org/abs/2403.16015v1","updated":"2024-03-24T05:13:37Z","published":"2024-03-24T05:13:37Z","title":"MQE: Unleashing the Power of Interaction with Multi-agent Quadruped\n  Environment","summary":"  The advent of deep reinforcement learning (DRL) has significantly advanced\nthe field of robotics, particularly in the control and coordination of\nquadruped robots. However, the complexity of real-world tasks often\nnecessitates the deployment of multi-robot systems capable of sophisticated\ninteraction and collaboration. To address this need, we introduce the\nMulti-agent Quadruped Environment (MQE), a novel platform designed to\nfacilitate the development and evaluation of multi-agent reinforcement learning\n(MARL) algorithms in realistic and dynamic scenarios. MQE emphasizes complex\ninteractions between robots and objects, hierarchical policy structures, and\nchallenging evaluation scenarios that reflect real-world applications. We\npresent a series of collaborative and competitive tasks within MQE, ranging\nfrom simple coordination to complex adversarial interactions, and benchmark\nstate-of-the-art MARL algorithms. Our findings indicate that hierarchical\nreinforcement learning can simplify task learning, but also highlight the need\nfor advanced algorithms capable of handling the intricate dynamics of\nmulti-agent interactions. MQE serves as a stepping stone towards bridging the\ngap between simulation and practical deployment, offering a rich environment\nfor future research in multi-agent systems and robot learning. For open-sourced\ncode and more details of MQE, please refer to\nhttps://ziyanx02.github.io/multiagent-quadruped-environment/ .\n","authors":["Ziyan Xiong","Bo Chen","Shiyu Huang","Wei-Wei Tu","Zhaofeng He","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2403.16015v1.pdf","comment":"Open-source code is available at\n  https://github.com/ziyanx02/multiagent-quadruped-environment"},{"id":"http://arxiv.org/abs/2308.15991v3","updated":"2024-03-24T04:45:03Z","published":"2023-08-30T12:24:30Z","title":"DRL-Based Trajectory Tracking for Motion-Related Modules in Autonomous\n  Driving","summary":"  Autonomous driving systems are always built on motion-related modules such as\nthe planner and the controller. An accurate and robust trajectory tracking\nmethod is indispensable for these motion-related modules as a primitive\nroutine. Current methods often make strong assumptions about the model such as\nthe context and the dynamics, which are not robust enough to deal with the\nchanging scenarios in a real-world system. In this paper, we propose a Deep\nReinforcement Learning (DRL)-based trajectory tracking method for the\nmotion-related modules in autonomous driving systems. The representation\nlearning ability of DL and the exploration nature of RL bring strong robustness\nand improve accuracy. Meanwhile, it enhances versatility by running the\ntrajectory tracking in a model-free and data-driven manner. Through extensive\nexperiments, we demonstrate both the efficiency and effectiveness of our method\ncompared to current methods. Code and documentation are released to facilitate\nboth further research and industrial deployment.\n","authors":["Yinda Xu","Lidong Yu"],"pdf_url":"https://arxiv.org/pdf/2308.15991v3.pdf","comment":"Technical report. Code:\n  https://github.com/MARMOTatZJU/drl-based-trajectory-tracking Documentation:\n  https://drl-based-trajectory-tracking.readthedocs.io"},{"id":"http://arxiv.org/abs/2403.15993v1","updated":"2024-03-24T03:10:18Z","published":"2024-03-24T03:10:18Z","title":"Robust-Locomotion-by-Logic: Perturbation-Resilient Bipedal Locomotion\n  via Signal Temporal Logic Guided Model Predictive Control","summary":"  This study introduces a robust planning framework that utilizes a model\npredictive control (MPC) approach, enhanced by incorporating signal temporal\nlogic (STL) specifications. This marks the first-ever study to apply STL-guided\ntrajectory optimization for bipedal locomotion, specifically designed to handle\nboth translational and orientational perturbations. Existing recovery\nstrategies often struggle with reasoning complex task logic and evaluating\nlocomotion robustness systematically, making them susceptible to failures\ncaused by inappropriate recovery strategies or lack of robustness. To address\nthese issues, we design an analytical robustness metric for bipedal locomotion\nand quantify this metric using STL specifications, which guide the generation\nof recovery trajectories to achieve maximum locomotion robustness. To enable\nsafe and computational-efficient crossed-leg maneuver, we design data-driven\nself-leg-collision constraints that are $1000$ times faster than the\ntraditional inverse-kinematics-based approach. Our framework outperforms a\nstate-of-the-art locomotion controller, a standard MPC without STL, and a\nlinear-temporal-logic-based planner in a high-fidelity dynamic simulation,\nespecially in scenarios involving crossed-leg maneuvers. Additionally, the\nCassie bipedal robot achieves robust performance under horizontal and\norientational perturbations such as those observed in ship motions. These\nenvironments are validated in simulations and deployed on hardware.\nFurthermore, our proposed method demonstrates versatility on stepping stones\nand terrain-agnostic features on inclined terrains.\n","authors":["Zhaoyuan Gu","Yuntian Zhao","Yipu Chen","Rongming Guo","Jennifer K. Leestma","Gregory S. Sawicki","Ye Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.15993v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2012.04132v4","updated":"2024-03-24T01:23:11Z","published":"2020-12-08T00:37:35Z","title":"A Number Sense as an Emergent Property of the Manipulating Brain","summary":"  The ability to understand and manipulate numbers and quantities emerges\nduring childhood, but the mechanism through which humans acquire and develop\nthis ability is still poorly understood. We explore this question through a\nmodel, assuming that the learner is able to pick up and place small objects\nfrom, and to, locations of its choosing, and will spontaneously engage in such\nundirected manipulation. We further assume that the learner's visual system\nwill monitor the changing arrangements of objects in the scene and will learn\nto predict the effects of each action by comparing perception with a\nsupervisory signal from the motor system. We model perception using standard\ndeep networks for feature extraction and classification, and gradient descent\nlearning. Our main finding is that, from learning the task of action\nprediction, an unexpected image representation emerges exhibiting regularities\nthat foreshadow the perception and representation of numbers and quantity.\nThese include distinct categories for zero and the first few natural numbers, a\nstrict ordering of the numbers, and a one-dimensional signal that correlates\nwith numerical quantity. As a result, our model acquires the ability to\nestimate numerosity, i.e. the number of objects in the scene, as well as\nsubitization, i.e. the ability to recognize at a glance the exact number of\nobjects in small scenes. Remarkably, subitization and numerosity estimation\nextrapolate to scenes containing many objects, far beyond the three objects\nused during training. We conclude that important aspects of a facility with\nnumbers and quantities may be learned with supervision from a simple\npre-training task. Our observations suggest that cross-modal learning is a\npowerful learning mechanism that may be harnessed in artificial intelligence.\n","authors":["Neehar Kondapaneni","Pietro Perona"],"pdf_url":"https://arxiv.org/pdf/2012.04132v4.pdf","comment":"16 pages, 5 figures, 15 supplemental figures"},{"id":"http://arxiv.org/abs/2205.11624v5","updated":"2024-03-24T20:03:38Z","published":"2022-05-23T20:49:40Z","title":"Effective Integration of Weighted Cost-to-go and Conflict Heuristic\n  within Suboptimal CBS","summary":"  Conflict-Based Search (CBS) is a popular multi-agent path finding (MAPF)\nsolver that employs a low-level single agent planner and a high-level\nconstraint tree to resolve conflicts. The vast majority of modern MAPF solvers\nfocus on improving CBS by reducing the size of this tree through various\nstrategies with few methods modifying the low level planner. Typically low\nlevel planners in existing CBS methods use an unweighted cost-to-go heuristic,\nwith suboptimal CBS methods also using a conflict heuristic to help the high\nlevel search. In this paper, we show that, contrary to prevailing CBS beliefs,\na weighted cost-to-go heuristic can be used effectively alongside the conflict\nheuristic in two possible variants. In particular, one of these variants can\nobtain large speedups, 2-100x, across several scenarios and suboptimal CBS\nmethods. Importantly, we discover that performance is related not to the\nweighted cost-to-go heuristic but rather to the relative conflict heuristic\nweight's ability to effectively balance low-level and high-level work.\nAdditionally, to the best of our knowledge, we show the first theoretical\nrelation of prioritized planning and bounded suboptimal CBS and demonstrate\nthat our methods are their natural generalization. Update March 2024: We found\nthat the relative speedup decreases to around 1.2-10x depending on how the\nconflict heuristic is computed (see appendix for more details).\n","authors":["Rishi Veerapaneni","Tushar Kusnur","Maxim Likhachev"],"pdf_url":"https://arxiv.org/pdf/2205.11624v5.pdf","comment":"Published in AAAI 2023"}]},"2024-03-27T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2403.17367v2","updated":"2024-03-27T06:16:06Z","published":"2024-03-26T04:05:01Z","title":"RoboDuet: A Framework Affording Mobile-Manipulation and Cross-Embodiment","summary":"  Combining the mobility of legged robots with the manipulation skills of arms\nhas the potential to significantly expand the operational range and enhance the\ncapabilities of robotic systems in performing various mobile manipulation\ntasks. Existing approaches are confined to imprecise six degrees of freedom\n(DoF) manipulation and possess a limited arm workspace. In this paper, we\npropose a novel framework, RoboDuet, which employs two collaborative policies\nto realize locomotion and manipulation simultaneously, achieving whole-body\ncontrol through interactions between each other. Surprisingly, going beyond the\nlarge-range pose tracking, we find that the two-policy framework may enable\ncross-embodiment deployment such as using different quadrupedal robots or other\narms. Our experiments demonstrate that the policies trained through RoboDuet\ncan accomplish stable gaits, agile 6D end-effector pose tracking, and zero-shot\nexchange of legged robots, and can be deployed in the real world to perform\nvarious mobile manipulation tasks. Our project page with demo videos is at\nhttps://locomanip-duet.github.io .\n","authors":["Guoping Pan","Qingwei Ben","Zhecheng Yuan","Guangqi Jiang","Yandong Ji","Jiangmiao Pang","Houde Liu","Huazhe Xu"],"pdf_url":"https://arxiv.org/pdf/2403.17367v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17320v2","updated":"2024-03-27T02:39:30Z","published":"2024-03-26T02:02:35Z","title":"Leveraging Symmetry in RL-based Legged Locomotion Control","summary":"  Model-free reinforcement learning is a promising approach for autonomously\nsolving challenging robotics control problems, but faces exploration difficulty\nwithout information of the robot's kinematics and dynamics morphology. The\nunder-exploration of multiple modalities with symmetric states leads to\nbehaviors that are often unnatural and sub-optimal. This issue becomes\nparticularly pronounced in the context of robotic systems with morphological\nsymmetries, such as legged robots for which the resulting asymmetric and\naperiodic behaviors compromise performance, robustness, and transferability to\nreal hardware. To mitigate this challenge, we can leverage symmetry to guide\nand improve the exploration in policy learning via equivariance/invariance\nconstraints. In this paper, we investigate the efficacy of two approaches to\nincorporate symmetry: modifying the network architectures to be strictly\nequivariant/invariant, and leveraging data augmentation to approximate\nequivariant/invariant actor-critics. We implement the methods on challenging\nloco-manipulation and bipedal locomotion tasks and compare with an\nunconstrained baseline. We find that the strictly equivariant policy\nconsistently outperforms other methods in sample efficiency and task\nperformance in simulation. In addition, symmetry-incorporated approaches\nexhibit better gait quality, higher robustness and can be deployed zero-shot in\nreal-world experiments.\n","authors":["Zhi Su","Xiaoyu Huang","Daniel Ordoñez-Apraez","Yunfei Li","Zhongyu Li","Qiayuan Liao","Giulio Turrisi","Massimiliano Pontil","Claudio Semini","Yi Wu","Koushil Sreenath"],"pdf_url":"https://arxiv.org/pdf/2403.17320v2.pdf","comment":null}]}}